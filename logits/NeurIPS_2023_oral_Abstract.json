{"notes": [{"id": "NnMEadcdyD", "number": 15266, "cdate": 1683833001484, "tcdate": 1683833001484, "mdate": 1698949792790, "tmdate": 1698949792790, "signatures": ["NeurIPS.cc/2023/Conference/Submission15266/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15266/Authors"], "forum": "NnMEadcdyD", "content": {"title": {"value": "Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation"}, "authors": {"value": ["Diederik P Kingma", "Ruiqi Gao"]}, "authorids": {"value": ["~Diederik_P_Kingma1", "~Ruiqi_Gao1"]}, "keywords": {"value": ["Diffusion Model", "Evidence Lower Bound", "Maximum Likelihood"]}, "TLDR": {"value": "We develop a theoretical understanding of the training objective of diffusion models as a the ELBOs subject to data augmentation."}, "abstract": {"value": "To achieve the highest perceptual quality, state-of-the-art diffusion models are optimized with objectives that typically look very different from the maximum likelihood and the Evidence Lower Bound (ELBO) objectives. In this work, we reveal that diffusion model objectives are actually closely related to the ELBO.\n\nSpecifically, we show that all commonly used diffusion model objectives equate to a weighted integral of ELBOs over different noise levels, where the weighting depends on the specific objective used. Under the condition of monotonic weighting, the connection is even closer: the diffusion objective then equals the ELBO, combined with simple data augmentation, namely Gaussian noise perturbation. We show that this condition holds for a number of state-of-the-art diffusion models. \n\nIn experiments, we explore new monotonic weightings and demonstrate their effectiveness, achieving state-of-the-art FID scores on the high-resolution ImageNet benchmark."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d16cfcd14613fc1198cfb086ecf7b31148c34bc7.pdf"}, "_bibtex": {"value": "@inproceedings{\nkingma2023understanding,\ntitle={Understanding Diffusion Objectives as the {ELBO} with Simple Data Augmentation},\nauthor={Diederik P Kingma and Ruiqi Gao},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=NnMEadcdyD}\n}"}, "paperhash": {"value": "kingma|understanding_diffusion_objectives_as_the_elbo_with_simple_data_augmentation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15266/-/Revision", "NeurIPS.cc/2023/Conference/Submission15266/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15266/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326173784, "odate": 1698949792778, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "TXoZiUZywf", "number": 14987, "cdate": 1683830706617, "tcdate": 1683830706617, "mdate": 1698949791548, "tmdate": 1698949791548, "signatures": ["NeurIPS.cc/2023/Conference/Submission14987/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14987/Authors"], "forum": "TXoZiUZywf", "content": {"title": {"value": "Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures"}, "authors": {"value": ["Hamish Flynn", "David Reeb", "Melih Kandemir", "Jan Peters"]}, "authorids": {"value": ["~Hamish_Flynn1", "~David_Reeb2", "~Melih_Kandemir1", "~Jan_Peters3"]}, "keywords": {"value": ["Linear bandits", "confidence sequences", "martingales", "convex optimization", "cumulative regret", "regret analysis"]}, "abstract": {"value": "We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used \"optimism in the face of uncertainty\" principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confidence sequences give improved performance in several hyperparameter tuning tasks."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Based on novel mixture martingales, we obtain tighter confidence bounds for linear bandits resulting in better algorithms with performance guarantees."}, "pdf": {"value": "/pdf/3a71f29ab9ae45f25289a53b54d94f20e1b477dd.pdf"}, "_bibtex": {"value": "@inproceedings{\nflynn2023improved,\ntitle={Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures},\nauthor={Hamish Flynn and David Reeb and Melih Kandemir and Jan Peters},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=TXoZiUZywf}\n}"}, "paperhash": {"value": "flynn|improved_algorithms_for_stochastic_linear_bandits_using_tail_bounds_for_martingale_mixtures"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14987/-/Revision", "NeurIPS.cc/2023/Conference/Submission14987/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14987/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326167710, "odate": 1698949791535, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "KvPwXVcslY", "number": 14438, "cdate": 1683826633800, "tcdate": 1683826633800, "mdate": 1699201227531, "tmdate": 1699201227531, "signatures": ["NeurIPS.cc/2023/Conference/Submission14438/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14438/Authors"], "forum": "KvPwXVcslY", "content": {"title": {"value": "Spatial-frequency channels, shape bias, and adversarial robustness"}, "authors": {"value": ["Ajay Subramanian", "Elena Sizikova", "Najib J. Majaj", "Denis G. Pelli"]}, "authorids": {"value": ["~Ajay_Subramanian1", "~Elena_Sizikova1", "~Najib_J._Majaj1", "denis.pelli@nyu.edu"]}, "keywords": {"value": ["object recognition", "critical band masking", "spatial-frequency channels", "shape bias", "adversarial robustness"]}, "TLDR": {"value": "Critical band masking, a tool from neuroscience, reveals a large difference in the spatial frequency information used by humans and neural networks to recognize objects in natural images."}, "abstract": {"value": "What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or \"channel\") that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. Unlike humans, the neural network channel is very broad, 2-4 times wider than the human channel. This means that the network channel extends to frequencies higher and lower than those that humans are sensitive to. Thus, noise at those frequencies will impair network performance and spare human performance. Adversarial and augmented-image training are commonly used to increase network robustness and shape bias. Does this training align network and human object recognition channels? Three network channel properties (bandwidth, center frequency, peak noise sensitivity) correlate strongly with shape bias (51% variance explained) and robustness of adversarially-trained networks (66% variance explained). Adversarial training increases robustness but expands the channel bandwidth even further beyond the human bandwidth. Thus, critical band masking reveals that the network channel is more than twice as wide as the human channel, and that adversarial training only makes it worse. Networks with narrower channels might be more robust."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/16400c67bce0d09070a4c9624a8deaaec58dc145.pdf"}, "supplementary_material": {"value": "/attachment/ede11ef43c7ed0ec1a0ea216abbc9b2e579911da.pdf"}, "_bibtex": {"value": "@inproceedings{\nsubramanian2023spatialfrequency,\ntitle={Spatial-frequency channels, shape bias, and adversarial robustness},\nauthor={Ajay Subramanian and Elena Sizikova and Najib J. Majaj and Denis G. Pelli},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=KvPwXVcslY}\n}"}, "paperhash": {"value": "subramanian|spatialfrequency_channels_shape_bias_and_adversarial_robustness"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14438/-/Revision", "NeurIPS.cc/2023/Conference/Submission14438/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14438/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326155192, "odate": 1698949788947, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "AOKU4nRw1W", "number": 14396, "cdate": 1683826364565, "tcdate": 1683826364565, "mdate": 1698949788736, "tmdate": 1698949788736, "signatures": ["NeurIPS.cc/2023/Conference/Submission14396/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14396/Authors"], "forum": "AOKU4nRw1W", "content": {"title": {"value": "Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment"}, "authors": {"value": ["Royi Rassin", "Eran Hirsch", "Daniel Glickman", "Shauli Ravfogel", "Yoav Goldberg", "Gal Chechik"]}, "authorids": {"value": ["~Royi_Rassin1", "~Eran_Hirsch1", "~Daniel_Glickman1", "~Shauli_Ravfogel1", "~Yoav_Goldberg1", "~Gal_Chechik1"]}, "keywords": {"value": ["syntax", "diffusion", "stable diffusion", "attribute", "attention"]}, "abstract": {"value": "Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes. This reflects an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one example, a query like ``a pink sunflower and a yellow flamingo'' may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an approach which first syntactically analyses the prompt to identify entities and their modifiers, and then uses a novel loss function that encourages the cross-attention maps to agree with the linguistic binding reflected by the syntax. Specifically, we encourage large overlap between attention maps of entities and their modifiers, and small overlap with other entities and modifier words. The loss is optimized during inference, without retraining or fine-tuning the model. Human evaluation on three datasets, including one new and challenging set, demonstrate significant improvements of SynGen compared with current state of the art methods. This work highlights how making use of sentence structure during inference can efficiently and substantially improve the faithfulness of text-to-image generation."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/05a4aa72bf2534848727734db94aa4c366fcd86a.pdf"}, "supplementary_material": {"value": "/attachment/e62ae60400a4bfd6c56f40dad1c6a6453a39dbcc.zip"}, "_bibtex": {"value": "@inproceedings{\nrassin2023linguistic,\ntitle={Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment},\nauthor={Royi Rassin and Eran Hirsch and Daniel Glickman and Shauli Ravfogel and Yoav Goldberg and Gal Chechik},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=AOKU4nRw1W}\n}"}, "paperhash": {"value": "rassin|linguistic_binding_in_diffusion_models_enhancing_attribute_correspondence_through_attention_map_alignment"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14396/-/Revision", "NeurIPS.cc/2023/Conference/Submission14396/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14396/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326154179, "odate": 1698949788719, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "jDIlzSU8wJ", "number": 14395, "cdate": 1683826363874, "tcdate": 1683826363874, "mdate": 1698949788719, "tmdate": 1698949788719, "signatures": ["NeurIPS.cc/2023/Conference/Submission14395/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14395/Authors"], "forum": "jDIlzSU8wJ", "content": {"title": {"value": "The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation"}, "authors": {"value": ["Saurabh Saxena", "Charles Herrmann", "Junhwa Hur", "Abhishek Kar", "Mohammad Norouzi", "Deqing Sun", "David J. Fleet"]}, "authorids": {"value": ["~Saurabh_Saxena1", "~Charles_Herrmann1", "~Junhwa_Hur1", "~Abhishek_Kar1", "~Mohammad_Norouzi1", "~Deqing_Sun2", "~David_J._Fleet1"]}, "keywords": {"value": ["Monocular depth", "optical flow", "diffusion", "depth", "flow"]}, "TLDR": {"value": "Advances in denoising diffusion to handle limited, noisy, incomplete labels of dense vision tasks, specifically monocular depth estimation and optical flow, achieving sota results"}, "abstract": {"value": "Denoising diffusion probabilistic models have transformed image generation with their impressive fidelity and diversity.\nWe show that they also excel in estimating optical flow and monocular depth, surprisingly without task-specific architectures and loss functions that are predominant for these tasks. \nCompared to the point estimates of conventional regression-based methods, diffusion models also enable Monte Carlo inference, e.g., capturing uncertainty and ambiguity in flow and depth.\nWith self-supervised pre-training, the combined use of synthetic and real data for supervised training, and technical innovations (infilling and step-unrolled denoising diffusion training) to handle noisy-incomplete training data, one can train state-of-the-art diffusion models for depth and optical flow estimation, with additional zero-shot coarse-to-fine refinement for high resolution estimates. \nExtensive experiments focus on quantitative performance against benchmarks, ablations, and the model's ability to capture uncertainty and multimodality, and impute missing values. Our model obtains a state-of-the-art relative depth error of 0.074 on the indoor NYU benchmark and an Fl-all score of 3.26\\% on the KITTI  optical flow benchmark, about 25\\% better than the best published method."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a9803ecc1ba03bbc2373e4eda61e4a21987c6093.pdf"}, "_bibtex": {"value": "@inproceedings{\nsaxena2023the,\ntitle={The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation},\nauthor={Saurabh Saxena and Charles Herrmann and Junhwa Hur and Abhishek Kar and Mohammad Norouzi and Deqing Sun and David J. Fleet},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=jDIlzSU8wJ}\n}"}, "paperhash": {"value": "saxena|the_surprising_effectiveness_of_diffusion_models_for_optical_flow_and_monocular_depth_estimation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14395/-/Revision", "NeurIPS.cc/2023/Conference/Submission14395/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14395/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326154119, "odate": 1698949788701, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "HPuSIXJaa9", "number": 14322, "cdate": 1683825832961, "tcdate": 1683825832961, "mdate": 1698949788376, "tmdate": 1698949788376, "signatures": ["NeurIPS.cc/2023/Conference/Submission14322/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14322/Authors"], "forum": "HPuSIXJaa9", "content": {"title": {"value": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, "authors": {"value": ["Rafael Rafailov", "Archit Sharma", "Eric Mitchell", "Christopher D Manning", "Stefano Ermon", "Chelsea Finn"]}, "authorids": {"value": ["~Rafael_Rafailov1", "~Archit_Sharma1", "~Eric_Mitchell1", "~Christopher_D_Manning1", "~Stefano_Ermon1", "~Chelsea_Finn1"]}, "keywords": {"value": ["reinforcement learning from human feedback", "language models", "RLHF", "preferences"]}, "abstract": {"value": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Fine-tuning with RLHF is complicated; we show that it doesn't need to be."}, "pdf": {"value": "/pdf/4d127e3396b404865afc9d826a0170a85d17adda.pdf"}, "supplementary_material": {"value": "/attachment/140240afc2922cc7a2c9479ced99e819c7191f9f.pdf"}, "_bibtex": {"value": "@inproceedings{\nrafailov2023direct,\ntitle={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},\nauthor={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=HPuSIXJaa9}\n}"}, "paperhash": {"value": "rafailov|direct_preference_optimization_your_language_model_is_secretly_a_reward_model"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14322/-/Revision", "NeurIPS.cc/2023/Conference/Submission14322/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14322/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326152050, "odate": 1698949788362, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "sW8yGZ4uVJ", "number": 14254, "cdate": 1683825360316, "tcdate": 1683825360316, "mdate": 1698949788135, "tmdate": 1698949788135, "signatures": ["NeurIPS.cc/2023/Conference/Submission14254/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14254/Authors"], "forum": "sW8yGZ4uVJ", "content": {"title": {"value": "Ordering-based Conditions for Global Convergence of Policy Gradient Methods"}, "authors": {"value": ["Jincheng Mei", "Bo Dai", "Alekh Agarwal", "Mohammad Ghavamzadeh", "Csaba Szepesvari", "Dale Schuurmans"]}, "authorids": {"value": ["~Jincheng_Mei1", "~Bo_Dai1", "~Alekh_Agarwal2", "~Mohammad_Ghavamzadeh2", "~Csaba_Szepesvari1", "~Dale_Schuurmans1"]}, "keywords": {"value": ["reinforcement learning", "policy gradient", "policy optimization", "function approximation", "global convergence"]}, "abstract": {"value": "We prove that, for finite-arm bandits with linear function approximation, the global convergence of policy gradient (PG) methods depends on inter-related properties between the policy update and the representation. textcolor{blue}{First}, we establish a few key observations that frame the study: \\textbf{(i)} Global convergence can be achieved under linear function approximation without policy or reward realizability, both for the standard Softmax PG and natural policy gradient (NPG). \\textbf{(ii)} Approximation error is not a key quantity for characterizing global convergence in either algorithm. \\textbf{(iii)} The conditions on the representation that imply global convergence are different between these two algorithms. Overall, these observations call into question approximation error as an appropriate quantity for characterizing the global convergence of PG methods under linear function approximation. \\textcolor{blue}{Second}, motivated by these observations, we establish new general results: \\textbf{(i)} NPG with linear function approximation achieves global convergence \\emph{if and only if} the projection of the reward  onto the representable space preserves the optimal action's rank, a quantity that is not strongly related to approximation error. \\textbf{(ii)} The global convergence of Softmax PG occurs if the representation satisfies a non-domination condition and can preserve the ranking of rewards, which goes well beyond policy or reward realizability. We provide experimental results to support these theoretical findings."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "PG method converges whenever there exists an adequate linear function that ranks actions in the same order as the ground-truth reward function."}, "pdf": {"value": "/pdf/4e7ad02450eb8c50a7cd991f28cf2844de6500ab.pdf"}, "supplementary_material": {"value": "/attachment/29990e1600b0b1cd014d6efc42617301e1ffb7d4.pdf"}, "_bibtex": {"value": "@inproceedings{\nmei2023orderingbased,\ntitle={Ordering-based Conditions for Global Convergence of Policy Gradient Methods},\nauthor={Jincheng Mei and Bo Dai and Alekh Agarwal and Mohammad Ghavamzadeh and Csaba Szepesvari and Dale Schuurmans},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=sW8yGZ4uVJ}\n}"}, "paperhash": {"value": "mei|orderingbased_conditions_for_global_convergence_of_policy_gradient_methods"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14254/-/Revision", "NeurIPS.cc/2023/Conference/Submission14254/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14254/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326150193, "odate": 1698949788121, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Kv8GJkV19S", "number": 14165, "cdate": 1683824666127, "tcdate": 1683824666127, "mdate": 1698949787660, "tmdate": 1698949787660, "signatures": ["NeurIPS.cc/2023/Conference/Submission14165/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14165/Authors"], "forum": "Kv8GJkV19S", "content": {"title": {"value": "Tester-Learners for Halfspaces: Universal Algorithms"}, "authors": {"value": ["Aravind Gollakota", "Adam Klivans", "Konstantinos Stavropoulos", "Arsen Vasilyan"]}, "authorids": {"value": ["~Aravind_Gollakota1", "~Adam_Klivans1", "~Konstantinos_Stavropoulos1", "~Arsen_Vasilyan1"]}, "keywords": {"value": ["testable learning", "pac learning", "agnostic learning", "Massart label noise", "adversarial label noise", "distribution testing"]}, "abstract": {"value": "We give the first tester-learner for halfspaces that succeeds universally over a wide class of structured distributions. Our universal tester-learner runs in fully polynomial time and has the following guarantee: the learner achieves error $O(\\mathrm{opt}) + \\epsilon$ on any labeled distribution that the tester accepts, and moreover, the tester accepts whenever the marginal is any distribution that satisfies a Poincare inequality. In contrast to prior work on testable learning, our tester is not tailored to any single target distribution but rather succeeds for an entire target class of distributions. The class of Poincare distributions includes all strongly log-concave distributions, and, assuming the Kannan--Lovasz--Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner achieves error $\\mathrm{opt} + \\epsilon$ while accepting all log-concave distributions unconditionally (without assuming KLS).\nOur tests rely on checking hypercontractivity of the unknown distribution using a sum-of-squares (SOS) program, and crucially make use of the fact that Poincare distributions are certifiably hypercontractive in the SOS framework."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/60d1fa250f7381027b1c366c889919a927efca7f.pdf"}, "_bibtex": {"value": "@inproceedings{\ngollakota2023testerlearners,\ntitle={Tester-Learners for Halfspaces: Universal Algorithms},\nauthor={Aravind Gollakota and Adam Klivans and Konstantinos Stavropoulos and Arsen Vasilyan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Kv8GJkV19S}\n}"}, "paperhash": {"value": "gollakota|testerlearners_for_halfspaces_universal_algorithms"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14165/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14165/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326148106, "odate": 1698949787649, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "IwnINorSZ5", "number": 13745, "cdate": 1683821391344, "tcdate": 1683821391344, "mdate": 1698949785193, "tmdate": 1698949785193, "signatures": ["NeurIPS.cc/2023/Conference/Submission13745/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13745/Authors"], "forum": "IwnINorSZ5", "content": {"title": {"value": "Conformal Meta-learners for Predictive Inference of Individual Treatment Effects"}, "authors": {"value": ["Ahmed Alaa", "Zaid Ahmad", "Mark van der Laan"]}, "authorids": {"value": ["~Ahmed_Alaa1", "zaidahmad@berkeley.edu", "laan@berkeley.edu"]}, "keywords": {"value": ["Heterogeneous treatment effects", "conformal prediction"]}, "abstract": {"value": "We investigate the problem of machine learning-based (ML) predictive inference on individual treatment effects (ITEs). Previous work has focused primarily on developing ML-based \u201cmeta-learners\u201d that can provide point estimates of the conditional average treatment effect (CATE)\u2014these are model-agnostic approaches for combining intermediate nuisance estimates to produce estimates of CATE. In this paper, we develop conformal meta-learners, a general framework for issuing predictive intervals for ITEs by applying the standard conformal prediction (CP) procedure on top of CATE meta-learners. We focus on a broad class of meta-learners based on two-stage pseudo-outcome regression and develop a stochastic ordering framework to study their validity. We show that inference with conformal meta-learners is marginally valid if their (pseudo-outcome) conformity scores stochastically dominate \u201coracle\u201d conformity scores evaluated on the unobserved ITEs. Additionally, we prove that commonly used CATE meta-learners, such as the doubly-robust learner, satisfy a model- and distribution-free stochastic (or convex) dominance condition, making their conformal inferences valid for practically-relevant levels of target coverage. Whereas existing procedures conduct inference on nuisance parameters (i.e., potential outcomes) via weighted CP, conformal meta-learners enable direct inference on the target parameter (ITE). Numerical experiments show that conformal meta-learners provide valid intervals with competitive efficiency while retaining the favorable point estimation properties of CATE meta-learners."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/71a53f799d2d38af1f45a8cfe5e91dc17acccea5.pdf"}, "_bibtex": {"value": "@inproceedings{\nalaa2023conformal,\ntitle={Conformal Meta-learners for Predictive Inference of Individual Treatment Effects},\nauthor={Ahmed Alaa and Zaid Ahmad and Mark van der Laan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=IwnINorSZ5}\n}"}, "paperhash": {"value": "alaa|conformal_metalearners_for_predictive_inference_of_individual_treatment_effects"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13745/-/Revision", "NeurIPS.cc/2023/Conference/Submission13745/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13745/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326137119, "odate": 1698949785175, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "mmTy1iyU5G", "number": 12619, "cdate": 1683812858201, "tcdate": 1683812858201, "mdate": 1698949779212, "tmdate": 1698949779212, "signatures": ["NeurIPS.cc/2023/Conference/Submission12619/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12619/Authors"], "forum": "mmTy1iyU5G", "content": {"title": {"value": "Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Method"}, "authors": {"value": ["Constantine Caramanis", "Dimitris Fotakis", "Alkis Kalavasis", "Vasilis Kontonis", "Christos Tzamos"]}, "authorids": {"value": ["~Constantine_Caramanis1", "~Dimitris_Fotakis1", "~Alkis_Kalavasis1", "~Vasilis_Kontonis1", "~Christos_Tzamos1"]}, "keywords": {"value": ["Policy Gradient", "Combinatorial Optimization", "Gradient Descent"]}, "TLDR": {"value": "We theoretically study the problem of optimizing solution-samplers for combinatorial problems via gradient-based methods."}, "abstract": {"value": "Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions.\nIn this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. As a byproduct of our analysis we introduce a novel regularization process over vanilla gradient descent and provide theoretical and experimental evidence that it helps address vanishing-gradient issues and escape bad stationary points."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5faf55cfe724b64002bfc1813849da3fb0cf7fbe.pdf"}, "supplementary_material": {"value": "/attachment/dda6ab3226bdb4f4616896a90218a06d87097c28.zip"}, "_bibtex": {"value": "@inproceedings{\ncaramanis2023optimizing,\ntitle={Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Method},\nauthor={Constantine Caramanis and Dimitris Fotakis and Alkis Kalavasis and Vasilis Kontonis and Christos Tzamos},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=mmTy1iyU5G}\n}"}, "paperhash": {"value": "caramanis|optimizing_solutionsamplers_for_combinatorial_problems_the_landscape_of_policygradient_method"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12619/-/Revision", "NeurIPS.cc/2023/Conference/Submission12619/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12619/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326108078, "odate": 1698949779199, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Pe9WxkN8Ff", "number": 12482, "cdate": 1683811655827, "tcdate": 1683811655827, "mdate": 1698949778599, "tmdate": 1698949778599, "signatures": ["NeurIPS.cc/2023/Conference/Submission12482/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12482/Authors"], "forum": "Pe9WxkN8Ff", "content": {"title": {"value": "Learning Transformer Programs"}, "authors": {"value": ["Dan Friedman", "Alexander Wettig", "Danqi Chen"]}, "authorids": {"value": ["~Dan_Friedman2", "~Alexander_Wettig1", "~Danqi_Chen1"]}, "keywords": {"value": ["mechanistic interpretability", "transformers"]}, "abstract": {"value": "Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the \u201ccircuits\u201d used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ef31a762e143cce8427cedf3e1c02d37cbc8acf3.pdf"}, "supplementary_material": {"value": "/attachment/fce5c2f04d332b4ee67bbd01bdfcc9d75ac2b4cc.pdf"}, "_bibtex": {"value": "@inproceedings{\nfriedman2023learning,\ntitle={Learning Transformer Programs},\nauthor={Dan Friedman and Alexander Wettig and Danqi Chen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Pe9WxkN8Ff}\n}"}, "paperhash": {"value": "friedman|learning_transformer_programs"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12482/-/Revision", "NeurIPS.cc/2023/Conference/Submission12482/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12482/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326104562, "odate": 1698949778581, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "n84bzMrGUD", "number": 12201, "cdate": 1683808539376, "tcdate": 1683808539376, "mdate": 1698949776774, "tmdate": 1698949776774, "signatures": ["NeurIPS.cc/2023/Conference/Submission12201/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12201/Authors"], "forum": "n84bzMrGUD", "content": {"title": {"value": "Clifford Group Equivariant Neural Networks"}, "authors": {"value": ["David Ruhe", "Johannes Brandstetter", "Patrick Forr\u00e9"]}, "authorids": {"value": ["~David_Ruhe1", "~Johannes_Brandstetter1", "~Patrick_Forr\u00e91"]}, "keywords": {"value": ["Clifford algebras", "geometric deep dearning", "Clifford group equivariance", "E(n)-equivariant neural networks", "O(n)-equivariant neural networks"]}, "TLDR": {"value": "A method to construct E(n)- and O(n)-equivariant neural networks using Clifford algebras."}, "abstract": {"value": "We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\\mathrm{O}(n)$- and $\\mathrm{E}(n)$-equivariant models. We identify and study the *Clifford group*: a subgroup inside the Clifford algebra tailored to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, including their grade projections, constitutes an equivariant map with respect to the Clifford group, allowing us to parameterize equivariant neural network layers. An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a single core implementation, state-of-the-art performance on several distinct tasks, including a three-dimensional $n$-body experiment, a four-dimensional Lorentz-equivariant high-energy physics experiment, and a five-dimensional convex hull experiment."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1be85d922d5108bf72a28a8622c234fbde8e470f.pdf"}, "supplementary_material": {"value": "/attachment/cf361e5c1c85cdca4f889260cb513999b0a99d93.pdf"}, "_bibtex": {"value": "@inproceedings{\nruhe2023clifford,\ntitle={Clifford Group Equivariant Neural Networks},\nauthor={David Ruhe and Johannes Brandstetter and Patrick Forr{\\'e}},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=n84bzMrGUD}\n}"}, "paperhash": {"value": "ruhe|clifford_group_equivariant_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12201/-/Revision", "NeurIPS.cc/2023/Conference/Submission12201/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12201/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326096057, "odate": 1698949776750, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "q131tA7HCT", "number": 11309, "cdate": 1683795261935, "tcdate": 1683795261935, "mdate": 1698949772259, "tmdate": 1698949772259, "signatures": ["NeurIPS.cc/2023/Conference/Submission11309/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11309/Authors"], "forum": "q131tA7HCT", "content": {"title": {"value": "Learning Linear Causal Representations from Interventions under General Nonlinear Mixing"}, "authors": {"value": ["Simon Buchholz", "Goutham Rajendran", "Elan Rosenfeld", "Bryon Aragam", "Bernhard Sch\u00f6lkopf", "Pradeep Kumar Ravikumar"]}, "authorids": {"value": ["~Simon_Buchholz1", "~Goutham_Rajendran1", "~Elan_Rosenfeld1", "~Bryon_Aragam1", "~Bernhard_Sch\u00f6lkopf1", "~Pradeep_Kumar_Ravikumar1"]}, "keywords": {"value": ["Causal Representation Learning", "Interventional data", "Gaussian Structural Causal models"]}, "TLDR": {"value": "We prove identifiability of causal representation learning from interventions with general nonlinear mixing functions and unknown, latent interventions, and propose a contrastive learning algorithm to learn it."}, "abstract": {"value": "We study the problem of learning causal representations from unknown, latent interventions in a general setting, where the latent distribution is Gaussian but the mixing function is completely general. We prove strong identifiability results given unknown single-node interventions, i.e., without having access to the intervention targets. This generalizes prior works which have focused on weaker classes, such as linear maps or paired counterfactual data. This is also the first instance of identifiability from non-paired interventions for deep neural network embeddings and general causal structures. Our proof relies on carefully uncovering the high-dimensional geometric structure present in the data distribution after a non-linear density transformation, which we capture by analyzing quadratic forms of precision matrices of the latent distributions. Finally, we propose a contrastive algorithm to identify the latent variables in practice and evaluate its performance on various tasks."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/cccc0ab0af9c4137fe26e8bb07075faaff6a5c88.pdf"}, "supplementary_material": {"value": "/attachment/af656c00b44f83b1e9e8a9a0803dd7286475d847.pdf"}, "_bibtex": {"value": "@inproceedings{\nbuchholz2023learning,\ntitle={Learning Linear Causal Representations from Interventions under General Nonlinear Mixing},\nauthor={Simon Buchholz and Goutham Rajendran and Elan Rosenfeld and Bryon Aragam and Bernhard Sch{\\\"o}lkopf and Pradeep Kumar Ravikumar},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=q131tA7HCT}\n}"}, "paperhash": {"value": "buchholz|learning_linear_causal_representations_from_interventions_under_general_nonlinear_mixing"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11309/-/Revision", "NeurIPS.cc/2023/Conference/Submission11309/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11309/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326072331, "odate": 1698949772244, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "RSGNGiB1q4", "number": 11165, "cdate": 1683793469971, "tcdate": 1683793469971, "mdate": 1698949771558, "tmdate": 1698949771558, "signatures": ["NeurIPS.cc/2023/Conference/Submission11165/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11165/Authors"], "forum": "RSGNGiB1q4", "content": {"title": {"value": "How to Turn Your Knowledge Graph Embeddings into Generative Models"}, "authors": {"value": ["Lorenzo Loconte", "Nicola Di Mauro", "Robert Peharz", "Antonio Vergari"]}, "authorids": {"value": ["~Lorenzo_Loconte1", "~Nicola_Di_Mauro1", "~Robert_Peharz5", "~Antonio_Vergari3"]}, "keywords": {"value": ["knowledge graph", "knowledge graph embeddings", "probabilistic circuits", "probabilistic reasoning", "tractable inference"]}, "TLDR": {"value": "We cast existing state-of-the-art knowledge graph embedding models into generative models that enable us to perform exact and efficient marginalisation, sampling and to integrate hard constraints with theoretical guarantees."}, "abstract": {"value": "Some of the most successful knowledge graph embedding (KGE) models for link prediction \u2013 CP, RESCAL, TuckER, ComplEx \u2013 can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits \u2013 constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/4759664f12bccd54ad5d1558ed34c3dcf94fd0a5.pdf"}, "supplementary_material": {"value": "/attachment/d65d6612f77b89f4bd043fd788d6212bf54ca3c2.zip"}, "_bibtex": {"value": "@inproceedings{\nloconte2023how,\ntitle={How to Turn Your Knowledge Graph Embeddings into Generative Models},\nauthor={Lorenzo Loconte and Nicola Di Mauro and Robert Peharz and Antonio Vergari},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=RSGNGiB1q4}\n}"}, "paperhash": {"value": "loconte|how_to_turn_your_knowledge_graph_embeddings_into_generative_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11165/-/Revision", "NeurIPS.cc/2023/Conference/Submission11165/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11165/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326068542, "odate": 1698949771545, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "fg7iyNK81W", "number": 10855, "cdate": 1683789369643, "tcdate": 1683789369643, "mdate": 1698949769887, "tmdate": 1698949769887, "signatures": ["NeurIPS.cc/2023/Conference/Submission10855/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10855/Authors"], "forum": "fg7iyNK81W", "content": {"title": {"value": "Rotating Features for Object Discovery"}, "authors": {"value": ["Sindy L\u00f6we", "Phillip Lippe", "Francesco Locatello", "Max Welling"]}, "authorids": {"value": ["~Sindy_L\u00f6we1", "~Phillip_Lippe1", "~Francesco_Locatello1", "~Max_Welling1"]}, "keywords": {"value": ["Object Discovery", "Object-Centric Representations", "Structured Representation Learning"]}, "abstract": {"value": "The binding problem in human cognition, concerning how the brain represents and connects objects within a fixed network of neural connections, remains a subject of intense debate. Most machine learning efforts addressing this issue in an unsupervised setting have focused on slot-based methods, which may be limiting due to their discrete nature and difficulty to express uncertainty. Recently, the Complex AutoEncoder was proposed as an alternative that learns continuous and distributed object-centric representations. However, it is only applicable to simple toy data. In this paper, we present Rotating Features, a generalization of complex-valued features to higher dimensions, and a new evaluation procedure for extracting objects from distributed representations. Additionally, we show the applicability of our approach to pre-trained features. Together, these advancements enable us to scale distributed object-centric representations from simple toy to real-world data. We believe this work advances a new paradigm for addressing the binding problem in machine learning and has the potential to inspire further innovation in the field."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "This paper introduces several advancements for continuous and distributed object-centric representations, scaling them from simple toy to real-world data, and thereby paving the way for a new paradigm in objects discovery."}, "pdf": {"value": "/pdf/1b8d41a127649b96bbe45664739a0d2037646098.pdf"}, "supplementary_material": {"value": "/attachment/80427da35b9dfcbc0591d39b21ba7261881553cc.zip"}, "_bibtex": {"value": "@inproceedings{\nl{\\\"o}we2023rotating,\ntitle={Rotating Features for Object Discovery},\nauthor={Sindy L{\\\"o}we and Phillip Lippe and Francesco Locatello and Max Welling},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=fg7iyNK81W}\n}"}, "paperhash": {"value": "l\u00f6we|rotating_features_for_object_discovery"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10855/-/Revision", "NeurIPS.cc/2023/Conference/Submission10855/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10855/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326060066, "odate": 1698949769871, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "QzcZb3fWmW", "number": 10513, "cdate": 1683784752454, "tcdate": 1683784752454, "mdate": 1698949767897, "tmdate": 1698949767897, "signatures": ["NeurIPS.cc/2023/Conference/Submission10513/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10513/Authors"], "forum": "QzcZb3fWmW", "content": {"title": {"value": "Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity"}, "authors": {"value": ["Tianqin Li", "Ziqi Wen", "Yangfan Li", "Tai Sing Lee"]}, "authorids": {"value": ["~Tianqin_Li2", "~Ziqi_Wen2", "~Yangfan_Li2", "~Tai_Sing_Lee1"]}, "keywords": {"value": ["neuroscience", "computer vision", "shape & texture bias"]}, "TLDR": {"value": "This study demonstrates that shape bias can emerge through the principle of efficient coding, paving the way for more human-like visual perception in CNNs."}, "abstract": {"value": "Current deep-learning models for object recognition are known to be heavily biased toward texture. In contrast, human visual systems are known to be biased toward shape and structure. What could be the design principles in human visual systems that led to this difference? How could we introduce more shape bias into the deep learning models? In this paper, we report that sparse coding, a ubiquitous principle in the brain,  can in itself introduce shape bias into the network. We found that enforcing the sparse coding constraint using a non-differential Top-K operation  can lead to the emergence of structural encoding in neurons in convolutional neural networks,  resulting in a smooth decomposition of objects into parts and subparts and endowing the networks with shape bias.  We demonstrated this emergence of shape bias and its functional benefits for different network structures with various datasets. For object recognition convolutional neural networks, the shape bias leads to greater robustness against style and pattern change distraction. For the image synthesis generative adversary networks,  the emerged shape bias leads to more coherent and decomposable structures in the synthesized images. Ablation studies suggest that sparse codes tend to encode structures, whereas the more distributed codes tend to favor texture. Our code is host at the github repository: \n\\url{https://github.com/Crazy-Jack/nips2023_shape_vs_texture}"}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a2682fdb4f697a049b07fbbef51b00edf36839de.pdf"}, "supplementary_material": {"value": "/attachment/4b4991e7a1e1408e5020d1c22373a1bfbc5db1b8.pdf"}, "_bibtex": {"value": "@inproceedings{\nli2023emergence,\ntitle={Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity},\nauthor={Tianqin Li and Ziqi Wen and Yangfan Li and Tai Sing Lee},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=QzcZb3fWmW}\n}"}, "paperhash": {"value": "li|emergence_of_shape_bias_in_convolutional_neural_networks_through_activation_sparsity"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10513/-/Revision", "NeurIPS.cc/2023/Conference/Submission10513/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10513/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326050874, "odate": 1698949767884, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "1zo4iioUEs", "number": 10463, "cdate": 1683783915104, "tcdate": 1683783915104, "mdate": 1698949767523, "tmdate": 1698949767523, "signatures": ["NeurIPS.cc/2023/Conference/Submission10463/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10463/Authors"], "forum": "1zo4iioUEs", "content": {"title": {"value": "DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models"}, "authors": {"value": ["Tsun-Hsuan Wang", "Juntian Zheng", "Pingchuan Ma", "Yilun Du", "Byungchul Kim", "Andrew Everett Spielberg", "Joshua B. Tenenbaum", "Chuang Gan", "Daniela Rus"]}, "authorids": {"value": ["~Tsun-Hsuan_Wang2", "~Juntian_Zheng1", "~Pingchuan_Ma3", "~Yilun_Du1", "~Byungchul_Kim1", "~Andrew_Everett_Spielberg1", "~Joshua_B._Tenenbaum1", "~Chuang_Gan1", "~Daniela_Rus1"]}, "keywords": {"value": ["soft robot", "diffusion model", "co-design"]}, "abstract": {"value": "Nature evolves creatures with a high complexity of morphological and behavioral intelligence, meanwhile computational methods lag in approaching that diversity and efficacy.  Co-optimization of artificial creatures' morphology and control in silico shows promise for applications in physical soft robotics and virtual character creation; such approaches, however, require developing new learning algorithms that can reason about function atop pure structure. In this paper, we present DiffuseBot, a physics-augmented diffusion model that generates soft robot morphologies capable of excelling in a wide spectrum of tasks. \\name bridges the gap between virtually generated content and physical utility by (i) augmenting the diffusion process with a physical dynamical simulation which provides a certificate of performance, and (ii) introducing a co-design procedure that jointly optimizes physical design and control by leveraging information about physical sensitivities from differentiable simulation.  We showcase a range of simulated and fabricated robots along with their capabilities. Check our website: https://diffusebot.github.io/"}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Use diffusion models augmented by physics-based simulation to breed soft robots"}, "pdf": {"value": "/pdf/1452901f7e4db28ca5c2d0c1ca22eda555f1c073.pdf"}, "supplementary_material": {"value": "/attachment/ffc986d9683f1056512609ca36966932ba11a6bd.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023diffusebot,\ntitle={DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models},\nauthor={Tsun-Hsuan Wang and Juntian Zheng and Pingchuan Ma and Yilun Du and Byungchul Kim and Andrew Everett Spielberg and Joshua B. Tenenbaum and Chuang Gan and Daniela Rus},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=1zo4iioUEs}\n}"}, "paperhash": {"value": "wang|diffusebot_breeding_soft_robots_with_physicsaugmented_generative_diffusion_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10463/-/Revision", "NeurIPS.cc/2023/Conference/Submission10463/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10463/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326048953, "odate": 1698949767493, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cB0BImqSS9", "number": 10349, "cdate": 1683782222666, "tcdate": 1683782222666, "mdate": 1698949766736, "tmdate": 1698949766736, "signatures": ["NeurIPS.cc/2023/Conference/Submission10349/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10349/Authors"], "forum": "cB0BImqSS9", "content": {"title": {"value": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"}, "authors": {"value": ["Daniel Y Fu", "Simran Arora", "Jessica Grogan", "Isys Johnson", "Sabri Eyuboglu", "Armin W Thomas", "Benjamin Frederick Spector", "Michael Poli", "Atri Rudra", "Christopher Re"]}, "authorids": {"value": ["~Daniel_Y_Fu1", "~Simran_Arora1", "~Jessica_Grogan1", "~Isys_Johnson1", "~Sabri_Eyuboglu1", "~Armin_W_Thomas1", "~Benjamin_Frederick_Spector1", "~Michael_Poli1", "~Atri_Rudra1", "~Christopher_Re1"]}, "keywords": {"value": ["structured matrices", "transformers", "efficiency"]}, "TLDR": {"value": "We present Monarch Mixer, a new simple architecture that scales sub-quadratically in sequence length and model dimension."}, "abstract": {"value": "Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE\u2014showing for the first time that it may be possible to match Transformer quality without attention or MLPs."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7af991b6aae89f3ee5ee28e0351716d5e915ba13.pdf"}, "_bibtex": {"value": "@inproceedings{\nfu2023monarch,\ntitle={Monarch Mixer: A Simple Sub-Quadratic {GEMM}-Based Architecture},\nauthor={Daniel Y Fu and Simran Arora and Jessica Grogan and Isys Johnson and Sabri Eyuboglu and Armin W Thomas and Benjamin Frederick Spector and Michael Poli and Atri Rudra and Christopher Re},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cB0BImqSS9}\n}"}, "paperhash": {"value": "fu|monarch_mixer_a_simple_subquadratic_gemmbased_architecture"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10349/-/Revision", "NeurIPS.cc/2023/Conference/Submission10349/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10349/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326045327, "odate": 1698949766720, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "OUIFPHEgJU", "number": 10066, "cdate": 1683777719005, "tcdate": 1683777719005, "mdate": 1699317717786, "tmdate": 1699317717786, "signatures": ["NeurIPS.cc/2023/Conference/Submission10066/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10066/Authors"], "forum": "OUIFPHEgJU", "content": {"title": {"value": "QLoRA: Efficient Finetuning of Quantized LLMs"}, "authors": {"value": ["Tim Dettmers", "Artidoro Pagnoni", "Ari Holtzman", "Luke Zettlemoyer"]}, "authorids": {"value": ["~Tim_Dettmers2", "~Artidoro_Pagnoni1", "~Ari_Holtzman1", "~Luke_Zettlemoyer1"]}, "keywords": {"value": ["finetuning", "llama", "instructions", "quantization"]}, "TLDR": {"value": "We develop a method that enabling the finetuning of a 65B model on a single GPU without performance degradation achieving ChatGPT-quaity results on the Vicuna benchmark."}, "abstract": {"value": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information-theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small, high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations, showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/2ec2f739340ddea41364c1b8e9c386c36f96da8a.pdf"}, "supplementary_material": {"value": "/attachment/fd52cb849c593de766b9b189e7de1a4e7eb7df27.pdf"}, "_bibtex": {"value": "@inproceedings{\ndettmers2023qlora,\ntitle={{QL}o{RA}: Efficient Finetuning of Quantized {LLM}s},\nauthor={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=OUIFPHEgJU}\n}"}, "paperhash": {"value": "dettmers|qlora_efficient_finetuning_of_quantized_llms"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10066/-/Revision", "NeurIPS.cc/2023/Conference/Submission10066/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10066/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326036336, "odate": 1698949762170, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "PITeSdYQkv", "number": 10015, "cdate": 1683777063093, "tcdate": 1683777063093, "mdate": 1698949761338, "tmdate": 1698949761338, "signatures": ["NeurIPS.cc/2023/Conference/Submission10015/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10015/Authors"], "forum": "PITeSdYQkv", "content": {"title": {"value": "User-Level Differential Privacy With Few Examples Per User"}, "authors": {"value": ["Badih Ghazi", "Pritish Kamath", "Ravi Kumar", "Pasin Manurangsi", "Raghu Meka", "Chiyuan Zhang"]}, "authorids": {"value": ["~Badih_Ghazi1", "~Pritish_Kamath2", "~Ravi_Kumar1", "~Pasin_Manurangsi2", "~Raghu_Meka1", "~Chiyuan_Zhang1"]}, "keywords": {"value": ["differential privacy", "user-level privacy", "PAC learning"]}, "abstract": {"value": "Previous work on user-level differential privacy (DP) [Ghazi et al. NeurIPS 2021, Bun et al. STOC 2023] obtained generic algorithms that work for various learning tasks. However, their focus was on the *example-rich* regime, where the users have so many examples that each user could themselves solve the problem. In this work we consider the *example-scarce* regime, where each user has only a few examples, and obtain the following results:\n* For approximate-DP, we give a generic transformation of any item-level DP algorithm to a user-level DP algorithm. Roughly speaking, the latter gives a (multiplicative) savings of $O_{\\varepsilon,\\delta}(\\sqrt{m})$ in terms of the number of users required for achieving the same utility, where $m$ is the number of examples per user. This algorithm, while recovering most known bounds for specific problems, also gives new bounds, e.g., for PAC learning. \n* For pure-DP, we present a simple technique for adapting the exponential mechanism [McSherry & Talwar, FOCS 2007] to the user-level setting. This gives new bounds for a variety of tasks, such as private PAC learning, hypothesis selection, and distribution learning. For some of these problems, we show that our bounds are near-optimal."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3b94a69b2f79f80083b0fd496be78b7836bc2897.pdf"}, "_bibtex": {"value": "@inproceedings{\nghazi2023userlevel,\ntitle={User-Level Differential Privacy With Few Examples Per User},\nauthor={Badih Ghazi and Pritish Kamath and Ravi Kumar and Pasin Manurangsi and Raghu Meka and Chiyuan Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=PITeSdYQkv}\n}"}, "TLDR": {"value": "We give a generic transformation of any item-level DP algorithm to a user-level DP algorithm, that holds even when each user has only a few examples."}, "paperhash": {"value": "ghazi|userlevel_differential_privacy_with_few_examples_per_user"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10015/-/Revision", "NeurIPS.cc/2023/Conference/Submission10015/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10015/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326034825, "odate": 1698949760893, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "rcXXNFVlEn", "number": 9962, "cdate": 1683776526927, "tcdate": 1683776526927, "mdate": 1698949760096, "tmdate": 1698949760096, "signatures": ["NeurIPS.cc/2023/Conference/Submission9962/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9962/Authors"], "forum": "rcXXNFVlEn", "content": {"title": {"value": "Why think step by step? Reasoning emerges from the locality of experience"}, "authors": {"value": ["Ben Prystawski", "Michael Y. Li", "Noah Goodman"]}, "authorids": {"value": ["~Ben_Prystawski1", "~Michael_Y._Li1", "~Noah_Goodman1"]}, "keywords": {"value": ["chain-of-thought; language models; reasoning"]}, "TLDR": {"value": "Chain-of-thought reasoning is effective in autoregressive language models because of the local structure of the training data."}, "abstract": {"value": "Humans have a powerful and mysterious capacity to reason. Working through a set of mental steps enables us to make inferences we would not be capable of making directly even though we get no additional data from the world. Similarly, when large language models generate intermediate steps (a chain of thought) before answering a question, they often produce better answers than they would directly. We investigate why and how chain-of-thought reasoning is useful in language models, testing the hypothesis that reasoning is effective when training data consists of overlapping local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences to estimate relationships between variables that were not seen together in training. We prove that there will exist a \"reasoning gap\", where reasoning through intermediate variables reduces bias, for the simple case of an autoregressive density estimator trained on local samples from a chain-structured probabilistic model. We then test our hypothesis experimentally in more complex models, training an autoregressive language model on samples from Bayes nets but only including a subset of variables in each sample. We test language models\u2019 ability to match conditional probabilities with and without intermediate reasoning steps, finding that intermediate steps are only helpful when the training data is locally structured with respect to dependencies between variables. The combination of locally structured observations and reasoning is much more data-efficient than training on all variables. Our results illustrate how the effectiveness of reasoning step by step is rooted in the local statistical structure of the training data."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/2164530af75c0b5214c9411be9fa3db26bf3b516.pdf"}, "_bibtex": {"value": "@inproceedings{\nprystawski2023why,\ntitle={Why think step by step? Reasoning emerges from the locality of experience},\nauthor={Ben Prystawski and Michael Y. Li and Noah Goodman},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=rcXXNFVlEn}\n}"}, "paperhash": {"value": "prystawski|why_think_step_by_step_reasoning_emerges_from_the_locality_of_experience"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9962/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9962/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326032943, "odate": 1698949760083, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "9VqMaSjf7U", "number": 9681, "cdate": 1683772762095, "tcdate": 1683772762095, "mdate": 1698949758642, "tmdate": 1698949758642, "signatures": ["NeurIPS.cc/2023/Conference/Submission9681/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9681/Authors"], "forum": "9VqMaSjf7U", "content": {"title": {"value": "Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models"}, "authors": {"value": ["Andrew Luo", "Margaret Marie Henderson", "Leila Wehbe", "Michael J. Tarr"]}, "authorids": {"value": ["~Andrew_Luo2", "~Margaret_Marie_Henderson1", "~Leila_Wehbe1", "~Michael_J._Tarr1"]}, "keywords": {"value": ["neuroscience", "brain", "fmri", "generative models", "diffusion models", "image synthesis", "visual cortex"]}, "abstract": {"value": "A long standing goal in neuroscience has been to elucidate the functional organization of the brain. Within higher visual cortex, functional accounts have remained relatively coarse, focusing on regions of interest (ROIs) and taking the form of selectivity for broad categories such as faces, places, bodies, food, or words. Because the identification of such ROIs has typically relied on manually assembled stimulus sets consisting of isolated objects in non-ecological contexts, exploring functional organization without robust a priori hypotheses has been challenging. To overcome these limitations, we introduce a data-driven approach in which we synthesize images predicted to activate a given brain region using paired natural images and fMRI recordings, bypassing the need for category-specific stimuli. Our approach -- Brain Diffusion for Visual Exploration (\"BrainDiVE\") -- builds on recent generative methods by combining large-scale diffusion models with brain-guided image synthesis. Validating our method, we demonstrate the ability to synthesize preferred images with appropriate semantic specificity for well-characterized category-selective ROIs. We then show that BrainDiVE can characterize differences between ROIs selective for the same high-level category. Finally we identify novel functional subdivisions within these ROIs, validated with behavioral data. These  results advance our understanding of the fine-grained functional organization of human visual cortex, and provide well-specified constraints for further examination of cortical organization using hypothesis-driven methods."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/92e3925bf5b5af1eeee5bb0e2ccb57871bce879b.pdf"}, "_bibtex": {"value": "@inproceedings{\nluo2023brain,\ntitle={Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models},\nauthor={Andrew Luo and Margaret Marie Henderson and Leila Wehbe and Michael J. Tarr},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=9VqMaSjf7U}\n}"}, "paperhash": {"value": "luo|brain_diffusion_for_visual_exploration_cortical_discovery_using_large_scale_generative_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9681/-/Revision", "NeurIPS.cc/2023/Conference/Submission9681/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9681/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326025137, "odate": 1698949758627, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ITw9edRDlD", "number": 9624, "cdate": 1683771993512, "tcdate": 1683771993512, "mdate": 1698949758370, "tmdate": 1698949758370, "signatures": ["NeurIPS.cc/2023/Conference/Submission9624/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9624/Authors"], "forum": "ITw9edRDlD", "content": {"title": {"value": "Are Emergent Abilities of Large Language Models a Mirage?"}, "authors": {"value": ["Rylan Schaeffer", "Brando Miranda", "Sanmi Koyejo"]}, "authorids": {"value": ["~Rylan_Schaeffer2", "~Brando_Miranda1", "~Sanmi_Koyejo1"]}, "keywords": {"value": ["large language models", "foundation models", "natural language processing", "language modeling", "emergent abilities"]}, "TLDR": {"value": "Some have argued that some improvements in model capabilities are unpredictable; we argue that many claimed emergent capabilities are predictable, either using better statistics or alternative metrics"}, "abstract": {"value": "Recent work claims that large language models display \\textit{emergent abilities}, abilities not present in smaller-scale models that are present in larger-scale models.\nWhat makes emergent abilities intriguing is two-fold: their \\textit{sharpness}, transitioning seemingly instantaneously from not present to present, and their \\textit{unpredictability}, appearing at seemingly unforeseeable model scales.\nHere, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due the researcher\u2019s choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance.\nWe present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks.\nVia all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models."}, "pdf": {"value": "/pdf/a1d4bae597f26697edc2580f69f44a6ce45a6c98.pdf"}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "supplementary_material": {"value": "/attachment/3867bc492a264c38b861a89807bd05eb6d7c07da.pdf"}, "_bibtex": {"value": "@inproceedings{\nschaeffer2023are,\ntitle={Are Emergent Abilities of Large Language Models a Mirage?},\nauthor={Rylan Schaeffer and Brando Miranda and Sanmi Koyejo},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ITw9edRDlD}\n}"}, "paperhash": {"value": "schaeffer|are_emergent_abilities_of_large_language_models_a_mirage"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9624/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9624/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326023269, "odate": 1698949758356, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "R6KJN1AUAR", "number": 9502, "cdate": 1683770361479, "tcdate": 1683770361479, "mdate": 1698949757259, "tmdate": 1698949757259, "signatures": ["NeurIPS.cc/2023/Conference/Submission9502/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9502/Authors"], "forum": "R6KJN1AUAR", "content": {"title": {"value": "Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation"}, "authors": {"value": ["Sebastien Lachapelle", "Divyat Mahajan", "Ioannis Mitliagkas", "Simon Lacoste-Julien"]}, "authorids": {"value": ["~Sebastien_Lachapelle1", "~Divyat_Mahajan1", "~Ioannis_Mitliagkas1", "~Simon_Lacoste-Julien1"]}, "keywords": {"value": ["identifiability", "nonlinear ICA", "causal representation learning", "disentanglement", "object-centric representation learning", "extrapolation"]}, "TLDR": {"value": "We show that additive decoders have an identifiable representation and allow to generate novel images never seen during training, an ability we refer to as Cartesian-product extrapolation."}, "abstract": {"value": "We tackle the problems of latent variables identification and \"out-of-support'' image generation in representation learning. We show that both are possible for a class of decoders that we call additive, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can generate novel images by recombining observed factors of variations in novel ways, an ability we refer to as Cartesian-product extrapolation. We show empirically that additivity is crucial for both identifiability and extrapolation on simulated data."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/53ecc8f20d0162d461fe74ebd87fb99d412b32ae.pdf"}, "_bibtex": {"value": "@inproceedings{\nlachapelle2023additive,\ntitle={Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation},\nauthor={Sebastien Lachapelle and Divyat Mahajan and Ioannis Mitliagkas and Simon Lacoste-Julien},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=R6KJN1AUAR}\n}"}, "paperhash": {"value": "lachapelle|additive_decoders_for_latent_variables_identification_and_cartesianproduct_extrapolation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9502/-/Revision", "NeurIPS.cc/2023/Conference/Submission9502/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9502/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326019888, "odate": 1698949757245, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Vota6rFhBQ", "number": 9161, "cdate": 1683763014896, "tcdate": 1683763014896, "mdate": 1698949754059, "tmdate": 1698949754059, "signatures": ["NeurIPS.cc/2023/Conference/Submission9161/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9161/Authors"], "forum": "Vota6rFhBQ", "content": {"title": {"value": "Fine-Tuning Language Models with Just Forward Passes"}, "authors": {"value": ["Sadhika Malladi", "Tianyu Gao", "Eshaan Nichani", "Alex Damian", "Jason D. Lee", "Danqi Chen", "Sanjeev Arora"]}, "authorids": {"value": ["~Sadhika_Malladi2", "~Tianyu_Gao1", "~Eshaan_Nichani1", "~Alex_Damian1", "~Jason_D._Lee1", "~Danqi_Chen1", "~Sanjeev_Arora1"]}, "keywords": {"value": ["language models", "fine-tuning", "zeroth order optimization", "memory efficiency"]}, "abstract": {"value": "Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12\u00d7 memory reduction and up to 2\u00d7 GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0cbbbd759c11a9d8c6df40a6e3a298142da7725d.pdf"}, "supplementary_material": {"value": "/attachment/ee7393dc6772e185bedd07302ab82308bb3409b0.zip"}, "_bibtex": {"value": "@inproceedings{\nmalladi2023finetuning,\ntitle={Fine-Tuning Language Models with Just Forward Passes},\nauthor={Sadhika Malladi and Tianyu Gao and Eshaan Nichani and Alex Damian and Jason D. Lee and Danqi Chen and Sanjeev Arora},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Vota6rFhBQ}\n}"}, "paperhash": {"value": "malladi|finetuning_language_models_with_just_forward_passes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9161/-/Revision", "NeurIPS.cc/2023/Conference/Submission9161/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9161/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326008730, "odate": 1698949754046, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "27TdrEvqLD", "number": 9105, "cdate": 1683761786515, "tcdate": 1683761786515, "mdate": 1698949753528, "tmdate": 1698949753528, "signatures": ["NeurIPS.cc/2023/Conference/Submission9105/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9105/Authors"], "forum": "27TdrEvqLD", "content": {"title": {"value": "Going beyond persistent homology using persistent homology"}, "authors": {"value": ["Johanna Emilia Immonen", "Amauri H Souza", "Vikas Garg"]}, "authorids": {"value": ["~Johanna_Emilia_Immonen1", "~Amauri_H_Souza1", "~Vikas_Garg2"]}, "keywords": {"value": ["graph representation learning", "topological deep learning", "persistent homology", "graph neural networks"]}, "abstract": {"value": "Representational limits of message-passing graph neural networks (MP-GNNs), e.g., in terms of the Weisfeiler-Leman (WL) test for isomorphism, are well understood. Augmenting these graph models with topological features via persistent homology (PH) has gained prominence, but identifying the class of attributed graphs that PH can recognize remains open.  We introduce a novel concept of color-separating sets to provide a complete resolution to this important problem. Specifically, we establish the necessary and sufficient conditions for distinguishing graphs based on the persistence of their connected components, obtained from filter functions on vertex and edge colors. Our constructions expose the limits of vertex- and edge-level PH, proving that neither category subsumes the other. Leveraging these theoretical insights, we propose RePHINE for learning topological features on graphs. RePHINE efficiently combines vertex- and edge-level PH, achieving a scheme that is provably more powerful than both. Integrating RePHINE into MP-GNNs boosts their expressive power, resulting in gains over standard PH on several benchmarks for graph classification."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/01ed9223fd4aa8da5d156b8764ebccd97db9d644.pdf"}, "_bibtex": {"value": "@inproceedings{\nimmonen2023going,\ntitle={Going beyond persistent homology using persistent homology},\nauthor={Johanna Emilia Immonen and Amauri H Souza and Vikas Garg},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=27TdrEvqLD}\n}"}, "paperhash": {"value": "immonen|going_beyond_persistent_homology_using_persistent_homology"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9105/-/Revision", "NeurIPS.cc/2023/Conference/Submission9105/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9105/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326006847, "odate": 1698949753515, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "w116w62fxH", "number": 8990, "cdate": 1683759103634, "tcdate": 1683759103634, "mdate": 1698949752624, "tmdate": 1698949752624, "signatures": ["NeurIPS.cc/2023/Conference/Submission8990/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8990/Authors"], "forum": "w116w62fxH", "content": {"title": {"value": "Optimal Learners for Realizable Regression: PAC Learning and Online Learning"}, "authors": {"value": ["Idan Attias", "Steve Hanneke", "Alkis Kalavasis", "Amin Karbasi", "Grigoris Velegkas"]}, "authorids": {"value": ["~Idan_Attias1", "~Steve_Hanneke1", "~Alkis_Kalavasis1", "~Amin_Karbasi3", "~Grigoris_Velegkas1"]}, "keywords": {"value": ["Learning Theory", "Regression", "PAC Learning", "Online Learning"]}, "TLDR": {"value": "We provide (almost) optimal learners, in terms of their sample complexity, for realizable regression in the context of PAC learning and in the context of online learning."}, "abstract": {"value": "In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting. Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since  the work of Simon 1997 (SICOMP '97). To this end,  we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable.  We then identify a combinatorial dimension related to the graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in this context. Additionally, in the context of online learning we provide a dimension that characterizes the minimax instance optimal cumulative loss up to a constant factor and design an optimal online learner for realizable regression, thus resolving an open question raised by Daskalakis and Golowich in STOC '22."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0e3ef00368f375320e98c79ae6fa8e1dcb02aa2f.pdf"}, "supplementary_material": {"value": "/attachment/c516fec60973e253824b02ab98c6c00002ee3549.pdf"}, "_bibtex": {"value": "@inproceedings{\nattias2023optimal,\ntitle={Optimal Learners for Realizable Regression: {PAC} Learning and Online Learning},\nauthor={Idan Attias and Steve Hanneke and Alkis Kalavasis and Amin Karbasi and Grigoris Velegkas},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=w116w62fxH}\n}"}, "paperhash": {"value": "attias|optimal_learners_for_realizable_regression_pac_learning_and_online_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8990/-/Revision", "NeurIPS.cc/2023/Conference/Submission8990/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8990/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326003077, "odate": 1698949752609, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "oML3v2cFg2", "number": 8938, "cdate": 1683757732050, "tcdate": 1683757732050, "mdate": 1698949752170, "tmdate": 1698949752170, "signatures": ["NeurIPS.cc/2023/Conference/Submission8938/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8938/Authors"], "forum": "oML3v2cFg2", "content": {"title": {"value": "When Demonstrations meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning"}, "authors": {"value": ["Siliang Zeng", "Chenliang Li", "Alfredo Garcia", "Mingyi Hong"]}, "authorids": {"value": ["~Siliang_Zeng1", "~Chenliang_Li3", "~Alfredo_Garcia1", "~Mingyi_Hong1"]}, "keywords": {"value": ["Inverse Reinforcement Learning", "Model-based Offline Inverse Reinforcement Learning"]}, "abstract": {"value": "Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world''). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncertainty of the estimated model of the world. We propose a new algorithmic framework to solve the bi-level optimization problem formulation and provide statistical and computational guarantees of performance for the associated optimal reward estimator. Finally,  we demonstrate that the proposed algorithm outperforms the state-of-the-art offline IRL and imitation learning benchmarks by a large margin, over the continuous control tasks in MuJoCo and different datasets in the D4RL benchmark."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5effba89baa04d23c51b2a81c216979fb04614ae.pdf"}, "supplementary_material": {"value": "/attachment/3c367c7acabb836bd0d9b99d66f5b70d651fd421.pdf"}, "_bibtex": {"value": "@inproceedings{\nzeng2023when,\ntitle={When Demonstrations meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning},\nauthor={Siliang Zeng and Chenliang Li and Alfredo Garcia and Mingyi Hong},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=oML3v2cFg2}\n}"}, "paperhash": {"value": "zeng|when_demonstrations_meet_generative_world_models_a_maximum_likelihood_framework_for_offline_inverse_reinforcement_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8938/-/Revision", "NeurIPS.cc/2023/Conference/Submission8938/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8938/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326001388, "odate": 1698949752153, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Lr2swAfwff", "number": 8816, "cdate": 1683755331894, "tcdate": 1683755331894, "mdate": 1699033786996, "tmdate": 1699033786996, "signatures": ["NeurIPS.cc/2023/Conference/Submission8816/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8816/Authors"], "forum": "Lr2swAfwff", "content": {"title": {"value": "Bridging RL Theory and Practice with the Effective Horizon"}, "authors": {"value": ["Cassidy Laidlaw", "Stuart Russell", "Anca Dragan"]}, "authorids": {"value": ["~Cassidy_Laidlaw1", "~Stuart_Russell1", "~Anca_Dragan1"]}, "keywords": {"value": ["reinforcement learning", "RL theory", "theory of reinforcement learning", "instance-dependent bounds", "empirical validation of theory"]}, "TLDR": {"value": "We prove new sample complexity bounds for reinforcement learning (RL) and demonstrate they closely reflect the performance of deep RL algorithms."}, "abstract": {"value": "Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the *random* policy also have the highest Q-values under the *optimal* policy\u2014i.e., when it is optimal to act greedily with respect to the random's policy Q function\u2014deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the *effective horizon*, which roughly corresponds to how many steps of lookahead search would be needed in that MDP in order to identify the next optimal action, when leaf nodes are evaluated with random rollouts. Using BRIDGE, we show that the effective horizon-based bounds are more closely reflective of the empirical performance of PPO and DQN than prior sample complexity bounds across four metrics. We also show that, unlike existing bounds, the effective horizon can predict the effects of using reward shaping or a pre-trained exploration policy. Our code and data are available at https://github.com/cassidylaidlaw/effective-horizon."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5336668699c1c5c9450cfa228e0375fa3fc7eb41.pdf"}, "_bibtex": {"value": "@inproceedings{\nlaidlaw2023bridging,\ntitle={Bridging {RL} Theory and Practice with the Effective Horizon},\nauthor={Cassidy Laidlaw and Stuart Russell and Anca Dragan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Lr2swAfwff}\n}"}, "paperhash": {"value": "laidlaw|bridging_rl_theory_and_practice_with_the_effective_horizon"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8816/-/Revision", "NeurIPS.cc/2023/Conference/Submission8816/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8816/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325997604, "odate": 1698949751062, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "a2Yg9Za6Rb", "number": 8791, "cdate": 1683754785550, "tcdate": 1683754785550, "mdate": 1698949750961, "tmdate": 1698949750961, "signatures": ["NeurIPS.cc/2023/Conference/Submission8791/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8791/Authors"], "forum": "a2Yg9Za6Rb", "content": {"title": {"value": "Students Parrot Their Teachers: Membership Inference on Model Distillation"}, "authors": {"value": ["Matthew Jagielski", "Milad Nasr", "Katherine Lee", "Christopher A. Choquette-Choo", "Nicholas Carlini", "Florian Tram\u00e8r"]}, "authorids": {"value": ["~Matthew_Jagielski1", "~Milad_Nasr2", "~Katherine_Lee1", "~Christopher_A._Choquette-Choo1", "~Nicholas_Carlini1", "~Florian_Tram\u00e8r1"]}, "keywords": {"value": ["model distillation", "membership inference", "privacy", "dark knowledge"]}, "TLDR": {"value": "Model distillation is not resistant to strong membership inference attacks, and we investigate why."}, "abstract": {"value": "Model distillation is frequently proposed as a technique to reduce the privacy leakage of machine learning. These empirical privacy defenses rely on the intuition that distilled ``student'' models protect the privacy of training data, as they only interact with this data indirectly through a ``teacher'' model. In this work, we design membership inference attacks to systematically study the privacy provided by knowledge distillation to both the teacher and student training sets. Our new attacks show that distillation alone provides only limited privacy across a number of domains. We explain the success of our attacks on distillation by showing that membership inference attacks on a private dataset can succeed even if the target model is never queried on any actual training points, but only on inputs whose predictions are highly influenced by training data. Finally, we show that our attacks are strongest when student and teacher sets are similar, or when the attacker can poison the teacher set."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8c05fc594d00524a53aa379d756be0e28a18e4d1.pdf"}, "supplementary_material": {"value": "/attachment/3e1b6cc738c6578f4335c1939793e0a7c0633bb7.pdf"}, "_bibtex": {"value": "@inproceedings{\njagielski2023students,\ntitle={Students Parrot Their Teachers: Membership Inference on Model Distillation},\nauthor={Matthew Jagielski and Milad Nasr and Katherine Lee and Christopher A. Choquette-Choo and Nicholas Carlini and Florian Tram{\\`e}r},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=a2Yg9Za6Rb}\n}"}, "paperhash": {"value": "jagielski|students_parrot_their_teachers_membership_inference_on_model_distillation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8791/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8791/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325996910, "odate": 1698949750947, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "dVnhdm9MIg", "number": 8750, "cdate": 1683754113725, "tcdate": 1683754113725, "mdate": 1698949750631, "tmdate": 1698949750631, "signatures": ["NeurIPS.cc/2023/Conference/Submission8750/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8750/Authors"], "forum": "dVnhdm9MIg", "content": {"title": {"value": "Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language"}, "authors": {"value": ["Kevin Ellis"]}, "authorids": {"value": ["~Kevin_Ellis1"]}, "keywords": {"value": ["Cognitive science", "Bayesian", "Language model", "Induction", "Psychology", "Reasoning"]}, "TLDR": {"value": "We build a model of human concept learning by integrating language models with probabilistic reasoning"}, "abstract": {"value": "A core tension in models of concept learning is that the model must carefully balance the tractability of inference against the expressivity of the hypothesis class. Humans, however, can efficiently learn a broad range of concepts. \nWe introduce a model of inductive learning that seeks to be human-like in that sense.\nIt implements a Bayesian reasoning process where a language model first proposes candidate hypotheses expressed in natural language, which are then re-weighed by a prior and a likelihood.\nBy estimating the prior from human data, we can predict human judgments on learning problems involving numbers and sets, spanning concepts that are generative, discriminative, propositional, and higher-order."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/365617a4ee6f25e8e6dcafc99a67d86919974771.pdf"}, "supplementary_material": {"value": "/attachment/580e66d8c96f01560bcf82f7044ad62b0065aeda.pdf"}, "_bibtex": {"value": "@inproceedings{\nellis2023humanlike,\ntitle={Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language},\nauthor={Kevin Ellis},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=dVnhdm9MIg}\n}"}, "paperhash": {"value": "ellis|humanlike_fewshot_learning_via_bayesian_reasoning_over_natural_language"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8750/-/Revision", "NeurIPS.cc/2023/Conference/Submission8750/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8750/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325995528, "odate": 1698949750616, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "yC3q7vInux", "number": 8555, "cdate": 1683750109235, "tcdate": 1683750109235, "mdate": 1698949749222, "tmdate": 1698949749222, "signatures": ["NeurIPS.cc/2023/Conference/Submission8555/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8555/Authors"], "forum": "yC3q7vInux", "content": {"title": {"value": "Siamese Masked Autoencoders"}, "authors": {"value": ["Agrim Gupta", "Jiajun Wu", "Jia Deng", "Li Fei-Fei"]}, "authorids": {"value": ["~Agrim_Gupta1", "~Jiajun_Wu1", "~Jia_Deng1", "~Li_Fei-Fei1"]}, "keywords": {"value": ["Representation Learning", "Visual Correspondence", "Self-supervised learning", "Videos"]}, "TLDR": {"value": "We propose Siamese Mased Autoencoders to learn visual correspondence from videos."}, "abstract": {"value": "Establishing correspondence between images or scenes is a significant challenge in computer vision, especially given occlusions, viewpoint changes, and varying object appearances. In this paper, we present Siamese Masked Autoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) for learning visual correspondence from videos. SiamMAE operates on pairs of randomly sampled video frames and asymmetrically masks them. These frames are processed independently by an encoder network, and a decoder composed of a sequence of cross-attention layers is tasked with predicting the missing patches in the future frame. By masking a large fraction (95%) of patches in the future frame while leaving the past frame unchanged, SiamMAE encourages the network to focus on object motion and learn object-centric representations. Despite its conceptual simplicity, features learned via SiamMAE outperform state-of-the-art self-supervised methods on video object segmentation, pose keypoint propagation, and semantic part propagation tasks. SiamMAE achieves competitive results without relying on data augmentation, handcrafted tracking-based pretext tasks, or other techniques to prevent representational collapse."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/550170d21b196dff6d63fa351910f0f9a2ef94d1.pdf"}, "supplementary_material": {"value": "/attachment/63b716dbcb6a61f8942e59ae58bcb9168ddee94b.pdf"}, "_bibtex": {"value": "@inproceedings{\ngupta2023siamese,\ntitle={Siamese Masked Autoencoders},\nauthor={Agrim Gupta and Jiajun Wu and Jia Deng and Li Fei-Fei},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=yC3q7vInux}\n}"}, "paperhash": {"value": "gupta|siamese_masked_autoencoders"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8555/-/Revision", "NeurIPS.cc/2023/Conference/Submission8555/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8555/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325989773, "odate": 1698949749209, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Sf9goJtTCE", "number": 8394, "cdate": 1683746410330, "tcdate": 1683746410330, "mdate": 1698954283055, "tmdate": 1698954283055, "signatures": ["NeurIPS.cc/2023/Conference/Submission8394/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8394/Authors"], "forum": "Sf9goJtTCE", "content": {"title": {"value": "Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent"}, "authors": {"value": ["Jihao Andreas Lin", "Javier Antoran", "Shreyas Padhy", "David Janz", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Alexander Terenin"]}, "authorids": {"value": ["~Jihao_Andreas_Lin1", "~Javier_Antoran1", "~Shreyas_Padhy1", "~David_Janz1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Alexander_Terenin1"]}, "keywords": {"value": ["Gaussian processes", "scalable learning", "posterior sampling", "Bayesian optimization"]}, "abstract": {"value": "Gaussian processes are a powerful framework for quantifying uncertainty and for sequential decision-making but are limited by the requirement of solving linear systems. In general, this has a cubic cost in dataset size and is sensitive to conditioning. We explore stochastic gradient algorithms as a computationally efficient method of approximately solving these linear systems: we develop low-variance optimization objectives for sampling from the posterior and extend these to inducing points. Counterintuitively, stochastic gradient descent often produces accurate predictions, even in cases where it does not converge quickly to the optimum. We explain this through a spectral characterization of the implicit bias from non-convergence. We show that stochastic gradient descent produces predictive distributions close to the true posterior both in regions with sufficient data coverage, and in regions sufficiently far away from the data. Experimentally, stochastic gradient descent achieves state-of-the-art performance on sufficiently large-scale or ill-conditioned regression tasks. Its uncertainty estimates match the performance of significantly more expensive baselines on a large-scale Bayesian~optimization~task."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d6f2b8af6f2ce0abc24e26abbedb8b8049a7c3cb.pdf"}, "TLDR": {"value": "We sample from GP posteriors using SGD and develop a spectral characterization for why it works, even in cases of non-convergence."}, "_bibtex": {"value": "@inproceedings{\nlin2023sampling,\ntitle={Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent},\nauthor={Jihao Andreas Lin and Javier Antoran and Shreyas Padhy and David Janz and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Alexander Terenin},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Sf9goJtTCE}\n}"}, "paperhash": {"value": "lin|sampling_from_gaussian_process_posteriors_using_stochastic_gradient_descent"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8394/-/Revision", "NeurIPS.cc/2023/Conference/Submission8394/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8394/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325984607, "odate": 1698949747748, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "Dkmpa6wCIx", "number": 8237, "cdate": 1683743460945, "tcdate": 1683743460945, "mdate": 1698949746515, "tmdate": 1698949746515, "signatures": ["NeurIPS.cc/2023/Conference/Submission8237/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8237/Authors"], "forum": "Dkmpa6wCIx", "content": {"title": {"value": "Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization"}, "authors": {"value": ["Kaiyue Wen", "Zhiyuan Li", "Tengyu Ma"]}, "authorids": {"value": ["~Kaiyue_Wen1", "~Zhiyuan_Li2", "~Tengyu_Ma1"]}, "keywords": {"value": ["Sharpness", "Flatness", "Generalization", "Generalization Bound", "SAM"]}, "abstract": {"value": "Despite extensive studies, the underlying reason as to why overparameterized\nneural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus\na natural potential explanation is that flatness implies generalization. This work\ncritically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1)\nflatness provably implies generalization; (2) there exist non-generalizing flattest\nmodels and sharpness minimization algorithms fail to generalize poorly, and (3)\nperhaps most strikingly, there exist non-generalizing flattest models, but sharpness\nminimization algorithms still generalize. Our results suggest that the relationship\nbetween sharpness and generalization subtly depends on the data distributions\nand the model architectures and sharpness minimization algorithms do not only\nminimize sharpness to achieve better generalization. This calls for the search for\nother explanations for the generalization of over-parameterized neural networks"}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "For 2 layer ReLU networks, sharpness may not always imply generalization but sharpness minimization algorithms may still generalize even when non-generalizing flattest models exist."}, "pdf": {"value": "/pdf/792a3dd52dec1befb169589b56c61753983f0c0a.pdf"}, "supplementary_material": {"value": "/attachment/4b0c11e1ac24ed5af8744525d9ec3d9a75c6be16.zip"}, "_bibtex": {"value": "@inproceedings{\nwen2023sharpness,\ntitle={Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization},\nauthor={Kaiyue Wen and Zhiyuan Li and Tengyu Ma},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Dkmpa6wCIx}\n}"}, "paperhash": {"value": "wen|sharpness_minimization_algorithms_do_not_only_minimize_sharpness_to_achieve_better_generalization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8237/-/Revision", "NeurIPS.cc/2023/Conference/Submission8237/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8237/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325979859, "odate": 1698949746499, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "rybsHQ4DXy", "number": 8221, "cdate": 1683743169361, "tcdate": 1683743169361, "mdate": 1698949746377, "tmdate": 1698949746377, "signatures": ["NeurIPS.cc/2023/Conference/Submission8221/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8221/Authors"], "forum": "rybsHQ4DXy", "content": {"title": {"value": "EgoEnv: Human-centric environment representations from egocentric video"}, "authors": {"value": ["Tushar Nagarajan", "Santhosh Kumar Ramakrishnan", "Ruta Desai", "James Hillis", "Kristen Grauman"]}, "authorids": {"value": ["~Tushar_Nagarajan1", "~Santhosh_Kumar_Ramakrishnan1", "~Ruta_Desai1", "~James_Hillis1", "~Kristen_Grauman1"]}, "keywords": {"value": ["egocentric video", "3D environment", "sim2real", "sim-to-real", "episodic memory"]}, "TLDR": {"value": "We learn \"environment-aware\" ego-video representations that encode not just a short 1-2s clip, but also the local surroundings of the camera-wearer (e.g., what objects are nearby, how far are they)."}, "abstract": {"value": "First-person video highlights a camera-wearer's activities in the context of their persistent environment. However, current video understanding approaches reason over visual features from short video clips that are detached from the underlying physical space and  capture only what is immediately visible. To facilitate human-centric environment understanding, we present an approach that links egocentric video and the environment by learning representations that are predictive of the camera-wearer's (potentially unseen) local surroundings. We train such models using videos from agents in simulated 3D environments where the environment is fully observable, and test them on human-captured real-world videos from unseen environments. On two human-centric video tasks, we show that models equipped with our environment-aware features consistently outperform their counterparts with traditional clip features. Moreover, despite being trained exclusively on simulated videos, our approach successfully handles real-world videos from HouseTours and Ego4D, and achieves state-of-the-art results on the Ego4D NLQ challenge."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0d65bead5503521456342f7aa37f6075f4b5a8ad.pdf"}, "supplementary_material": {"value": "/attachment/081de6e53ea6e63d27a71aecaba0a91a17270059.pdf"}, "_bibtex": {"value": "@inproceedings{\nnagarajan2023egoenv,\ntitle={EgoEnv: Human-centric environment representations from egocentric video},\nauthor={Tushar Nagarajan and Santhosh Kumar Ramakrishnan and Ruta Desai and James Hillis and Kristen Grauman},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=rybsHQ4DXy}\n}"}, "paperhash": {"value": "nagarajan|egoenv_humancentric_environment_representations_from_egocentric_video"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8221/-/Revision", "NeurIPS.cc/2023/Conference/Submission8221/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8221/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325979335, "odate": 1698949746362, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "O0Lz8XZT2b", "number": 8217, "cdate": 1683743127622, "tcdate": 1683743127622, "mdate": 1698949746325, "tmdate": 1698949746325, "signatures": ["NeurIPS.cc/2023/Conference/Submission8217/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8217/Authors"], "forum": "O0Lz8XZT2b", "content": {"title": {"value": "A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning"}, "authors": {"value": ["Alicia Curth", "Alan Jeffares", "Mihaela van der Schaar"]}, "authorids": {"value": ["~Alicia_Curth1", "~Alan_Jeffares1", "~Mihaela_van_der_Schaar2"]}, "keywords": {"value": ["Double Descent", "Statistical Machine Learning", "Interpolation Regime", "Effective Parameters"]}, "abstract": {"value": "Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a _U-shaped curve_ reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count $p$ grows past sample size $n$  -- a  phenomenon dubbed  _double descent_. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include _linear regression, trees, and boosting_. In this work, we take a closer look at the evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of  double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to _what is being plotted_ on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple, distinct complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and _only_) when and where the transition between these underlying axes occurs, and that its location is thus _not_ inherently tied to the interpolation threshold $p=n$. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as _smoothers_ and propose a generalized measure for the _effective_ number of parameters they use _on unseen examples_, using which we find that their apparent double descent curves do indeed fold back into more traditional convex shapes -- providing a resolution to the ostensible tension between double descent and traditional statistical intuition."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/464a3c661a68bdde98264b88747ede03136011d7.pdf"}, "_bibtex": {"value": "@inproceedings{\ncurth2023a,\ntitle={A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning},\nauthor={Alicia Curth and Alan Jeffares and Mihaela van der Schaar},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=O0Lz8XZT2b}\n}"}, "TLDR": {"value": "By demonstrating that the double descent shape observed in recent studies of non-deep ML methods is a direct consequence of the x-axes used to present it, we provide a resolution to the tension between statistical intuition and double descent."}, "paperhash": {"value": "curth|a_uturn_on_double_descent_rethinking_parameter_counting_in_statistical_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8217/-/Revision", "NeurIPS.cc/2023/Conference/Submission8217/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8217/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325979228, "odate": 1698949746309, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "S5wmbQc1We", "number": 8064, "cdate": 1683740093949, "tcdate": 1683740093949, "mdate": 1698949744996, "tmdate": 1698949744996, "signatures": ["NeurIPS.cc/2023/Conference/Submission8064/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8064/Authors"], "forum": "S5wmbQc1We", "content": {"title": {"value": "The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks"}, "authors": {"value": ["Ziqian Zhong", "Ziming Liu", "Max Tegmark", "Jacob Andreas"]}, "authorids": {"value": ["~Ziqian_Zhong1", "~Ziming_Liu2", "~Max_Tegmark1", "~Jacob_Andreas1"]}, "keywords": {"value": ["mechanistic interpretability", "algorithmic phase transitions", "arithmetic learning", "neural network", "transformer", "ensemble"]}, "abstract": {"value": "Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms? Several recent studies, on tasks ranging from group operations to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex: small changes to model hyperparameters and initializations can induce discovery of qualitatively different algorithms from a fixed training set, and even learning of multiple different solutions in parallel. In modular addition, we specifically show that models learn a known *Clock* algorithm, a previously undescribed, less intuitive, but comprehensible procedure we term the *Pizza* algorithm, and a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for mechanistically characterizing the behavior of neural networks across the algorithmic phase space."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/be3e48fe084e938e3ef89c2e59fab5250ce135b5.pdf"}, "TLDR": {"value": "We find that neural networks do not always rediscover known algorithms (Clock), but also discover new ones (Pizza), with modular addition as a prototypical example."}, "supplementary_material": {"value": "/attachment/924775104b53fd9742ebd3872f156cc829bcbfdb.zip"}, "_bibtex": {"value": "@inproceedings{\nzhong2023the,\ntitle={The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks},\nauthor={Ziqian Zhong and Ziming Liu and Max Tegmark and Jacob Andreas},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=S5wmbQc1We}\n}"}, "paperhash": {"value": "zhong|the_clock_and_the_pizza_two_stories_in_mechanistic_explanation_of_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8064/-/Revision", "NeurIPS.cc/2023/Conference/Submission8064/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8064/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325974246, "odate": 1698949744983, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "HV85SiyrsV", "number": 7884, "cdate": 1683736681089, "tcdate": 1683736681089, "mdate": 1698949743962, "tmdate": 1698949743962, "signatures": ["NeurIPS.cc/2023/Conference/Submission7884/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7884/Authors"], "forum": "HV85SiyrsV", "content": {"title": {"value": "Online RL in Linearly $q^\\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore"}, "authors": {"value": ["Gell\u00e9rt Weisz", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesvari"]}, "authorids": {"value": ["~Gell\u00e9rt_Weisz2", "~Andr\u00e1s_Gy\u00f6rgy2", "~Csaba_Szepesvari1"]}, "keywords": {"value": ["Reinforcement learning", "linear function approximation", "online learning"]}, "abstract": {"value": "We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\\pi$-realizability assumption, where it is assumed that the action-values of all policies can be  expressed as linear functions of state-action features. This class is known to be more general than  linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\\pi$-realizable MDPs where for any policy, all the actions have  approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorithm on the linear MDP hidden in the problem. The method returns an $\\epsilon$-optimal policy after $\\text{polylog}(H, d)/\\epsilon^2$ interactions with the MDP, where $H$ is the time horizon and $d$ is the dimension of the feature vectors, giving the first polynomial-sample-complexity online RL algorithm for this setting. The results are proved for the misspecified case, where the sample complexity is shown to degrade gracefully with the misspecification error."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f9e9c1e8617f1c1001712f9028e28c8cdcb730d4.pdf"}, "_bibtex": {"value": "@inproceedings{\nweisz2023online,\ntitle={Online {RL} in Linearly \\$q{\\textasciicircum}{\\textbackslash}pi\\$-Realizable {MDP}s Is as Easy as in Linear {MDP}s If You Learn What to Ignore},\nauthor={Gell{\\'e}rt Weisz and Andr{\\'a}s Gy{\\\"o}rgy and Csaba Szepesvari},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=HV85SiyrsV}\n}"}, "paperhash": {"value": "weisz|online_rl_in_linearly_q^\\pirealizable_mdps_is_as_easy_as_in_linear_mdps_if_you_learn_what_to_ignore"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7884/-/Revision", "NeurIPS.cc/2023/Conference/Submission7884/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7884/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325969090, "odate": 1698949743942, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5Xc1ecxO1h", "number": 7806, "cdate": 1683735099552, "tcdate": 1683735099552, "mdate": 1698949743431, "tmdate": 1698949743431, "signatures": ["NeurIPS.cc/2023/Conference/Submission7806/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7806/Authors"], "forum": "5Xc1ecxO1h", "content": {"title": {"value": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"}, "authors": {"value": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "Thomas L. Griffiths", "Yuan Cao", "Karthik R Narasimhan"]}, "authorids": {"value": ["~Shunyu_Yao1", "~Dian_Yu2", "~Jeffrey_Zhao1", "~Izhak_Shafran1", "~Thomas_L._Griffiths1", "~Yuan_Cao2", "~Karthik_R_Narasimhan1"]}, "keywords": {"value": ["large language model", "general problem solving", "heuristic search", "reasoning", "planning", "decision making"]}, "abstract": {"value": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.\nOur experiments show that ToT significantly enhances language models\u2019 problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\\% of tasks, our method achieved a success rate of 74\\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6498bb3a1f9ede56976650e1008819e246dd979a.pdf"}, "TLDR": {"value": "We combine LLM's capabilities of generating and evaluating diverse \u201cthoughts\u201d with search algorithms for robust problem solving."}, "_bibtex": {"value": "@inproceedings{\nyao2023tree,\ntitle={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\nauthor={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik R Narasimhan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5Xc1ecxO1h}\n}"}, "paperhash": {"value": "yao|tree_of_thoughts_deliberate_problem_solving_with_large_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7806/-/Revision", "NeurIPS.cc/2023/Conference/Submission7806/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7806/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325966816, "odate": 1698949743417, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "A7feCufBhL", "number": 7713, "cdate": 1683733708886, "tcdate": 1683733708886, "mdate": 1698949742987, "tmdate": 1698949742987, "signatures": ["NeurIPS.cc/2023/Conference/Submission7713/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7713/Authors"], "forum": "A7feCufBhL", "content": {"title": {"value": "Image Captioners Are Scalable Vision Learners Too"}, "authors": {"value": ["Michael Tschannen", "Manoj Kumar", "Andreas Peter Steiner", "Xiaohua Zhai", "Neil Houlsby", "Lucas Beyer"]}, "authorids": {"value": ["~Michael_Tschannen1", "~Manoj_Kumar1", "~Andreas_Peter_Steiner1", "~Xiaohua_Zhai2", "~Neil_Houlsby1", "~Lucas_Beyer1"]}, "keywords": {"value": ["contrastive learning", "CLIP", "CapPa", "Cap", "vision-language", "image captioning", "visual representation learning", "weakly supervised learning", "VLM", "multimodal learning", "VQA", "image classification"]}, "abstract": {"value": "Contrastive pretraining on image-text pairs from the web is one of the most popular large-scale pretraining strategies for vision backbones, especially in the context of large multimodal models. At the same time, image captioning on this type of data is commonly considered an inferior pretraining strategy. In this paper, we perform a fair comparison of these two pretraining strategies, carefully matching training data, compute, and model capacity. Using a standard encoder-decoder transformer, we find that captioning alone is surprisingly effective: on classification tasks, captioning produces vision encoders competitive with contrastively pretrained encoders, while surpassing them on vision & language tasks. We further analyze the effect of the model architecture and scale, as well as the pretraining data on the representation quality, and find that captioning exhibits the same or better scaling behavior along these axes. Overall our results show that plain image captioning is a more powerful pretraining strategy than was previously believed."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We present an extensive comparison of contrastive representation learning and representation learning via image captioning from large image-text data sets."}, "pdf": {"value": "/pdf/68717c36cafbfbd3224040608f5eda6e85ccc20d.pdf"}, "supplementary_material": {"value": "/attachment/413b1837dcde6c679c515856e31fcc634487550c.pdf"}, "_bibtex": {"value": "@inproceedings{\ntschannen2023image,\ntitle={Image Captioners Are Scalable Vision Learners Too},\nauthor={Michael Tschannen and Manoj Kumar and Andreas Peter Steiner and Xiaohua Zhai and Neil Houlsby and Lucas Beyer},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=A7feCufBhL}\n}"}, "paperhash": {"value": "tschannen|image_captioners_are_scalable_vision_learners_too"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7713/-/Revision", "NeurIPS.cc/2023/Conference/Submission7713/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7713/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325963724, "odate": 1698949742973, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "j5BuTrEj35", "number": 7298, "cdate": 1683727728331, "tcdate": 1683727728331, "mdate": 1698949740308, "tmdate": 1698949740308, "signatures": ["NeurIPS.cc/2023/Conference/Submission7298/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7298/Authors"], "forum": "j5BuTrEj35", "content": {"title": {"value": "Scaling Data-Constrained Language Models"}, "authors": {"value": ["Niklas Muennighoff", "Alexander M Rush", "Boaz Barak", "Teven Le Scao", "Nouamane Tazi", "Aleksandra Piktus", "Sampo Pyysalo", "Thomas Wolf", "Colin Raffel"]}, "authorids": {"value": ["~Niklas_Muennighoff1", "~Alexander_M_Rush1", "~Boaz_Barak2", "~Teven_Le_Scao1", "~Nouamane_Tazi1", "~Aleksandra_Piktus1", "~Sampo_Pyysalo2", "~Thomas_Wolf1", "~Colin_Raffel1"]}, "keywords": {"value": ["large language models", "scaling laws", "data engineering"]}, "abstract": {"value": "The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Scaling laws for training LLMs on multiple epochs & code filling + filtering experiments"}, "pdf": {"value": "/pdf/03ec57a3ce6b03fb08789fe3ab535f98b1b0e51a.pdf"}, "supplementary_material": {"value": "/attachment/5520ed6bfca03ba7cee444d619bced7f0ab65067.pdf"}, "_bibtex": {"value": "@inproceedings{\nmuennighoff2023scaling,\ntitle={Scaling Data-Constrained Language Models},\nauthor={Niklas Muennighoff and Alexander M Rush and Boaz Barak and Teven Le Scao and Nouamane Tazi and Aleksandra Piktus and Sampo Pyysalo and Thomas Wolf and Colin Raffel},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=j5BuTrEj35}\n}"}, "paperhash": {"value": "muennighoff|scaling_dataconstrained_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission7298/-/Revision", "NeurIPS.cc/2023/Conference/Submission7298/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7298/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325951289, "odate": 1698949740284, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "0A9f2jZDGW", "number": 7139, "cdate": 1683724821363, "tcdate": 1683724821363, "mdate": 1698949739254, "tmdate": 1698949739254, "signatures": ["NeurIPS.cc/2023/Conference/Submission7139/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7139/Authors"], "forum": "0A9f2jZDGW", "content": {"title": {"value": "Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models"}, "authors": {"value": ["Guillermo Ortiz-Jimenez", "Alessandro Favero", "Pascal Frossard"]}, "authorids": {"value": ["~Guillermo_Ortiz-Jimenez1", "~Alessandro_Favero1", "~Pascal_Frossard1"]}, "keywords": {"value": ["model editing", "transfer learning", "neural tangent kernel", "vision-language pre-training", "deep learning science"]}, "TLDR": {"value": "We present a comprehensive study of task arithmetic and find that linearizing models before fine-tuning improves their performance after editing."}, "abstract": {"value": "Task arithmetic has recently emerged as a cost-effective and scalable approach to edit pre-trained models directly in weight space: By adding the fine-tuned weights of different tasks, the model's performance can be improved on these tasks, while negating them leads to task forgetting. Yet, our understanding of the effectiveness of task arithmetic and its underlying principles remains limited. We present a comprehensive study of task arithmetic in vision-language models and show that weight disentanglement is the crucial factor that makes it effective. This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models. Building on these findings, we provide theoretical and empirical analyses of the neural tangent kernel (NTK) of these models and establish a compelling link between task arithmetic and the spatial localization of the NTK eigenfunctions. Overall, our work uncovers novel insights into the fundamental mechanisms of task arithmetic and offers a more reliable and effective approach to edit pre-trained models through the NTK linearization."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1e605936d9544b6e5e448363572bb06fcdf70f57.pdf"}, "supplementary_material": {"value": "/attachment/ec080011525ad4f95e9330e9cb3f6a8247a1f132.pdf"}, "_bibtex": {"value": "@inproceedings{\nortiz-jimenez2023task,\ntitle={Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models},\nauthor={Guillermo Ortiz-Jimenez and Alessandro Favero and Pascal Frossard},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=0A9f2jZDGW}\n}"}, "paperhash": {"value": "ortizjimenez|task_arithmetic_in_the_tangent_space_improved_editing_of_pretrained_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7139/-/Revision", "NeurIPS.cc/2023/Conference/Submission7139/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7139/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325946125, "odate": 1698949739240, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "QIFoCI7ca1", "number": 7006, "cdate": 1683722490590, "tcdate": 1683722490590, "mdate": 1698949738326, "tmdate": 1698949738326, "signatures": ["NeurIPS.cc/2023/Conference/Submission7006/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7006/Authors"], "forum": "QIFoCI7ca1", "content": {"title": {"value": "Causal normalizing flows: from theory to practice"}, "authors": {"value": ["Adri\u00e1n Javaloy", "Pablo Sanchez Martin", "Isabel Valera"]}, "authorids": {"value": ["~Adri\u00e1n_Javaloy1", "~Pablo_Sanchez_Martin1", "~Isabel_Valera1"]}, "keywords": {"value": ["causality", "causal inference", "normalizing flows", "identifiability", "interventions", "counterfactuals"]}, "abstract": {"value": "In this work, we deepen on the use of normalizing flows for causal reasoning. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows (NFs). Second, we analyze different design and learning choices for *causal normalizing flows* to capture the underlying causal data-generating process. Third, we describe how to implement the *do-operator* in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems\u2014where the presence of mixed discrete-continuous data and partial knowledge on the causal graph is the norm. The code for this work can be found at https://github.com/psanch21/causal-flows."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Armed with identifiability results, we demonstraste how to use normalizing flows to capture a causal model and perform causal inference with it."}, "pdf": {"value": "/pdf/a2973b5a4a277e34fb1f444c440e839f73d0f992.pdf"}, "supplementary_material": {"value": "/attachment/9a621aeae59f146973dd162520840e5f8581d9ca.zip"}, "_bibtex": {"value": "@inproceedings{\njavaloy2023causal,\ntitle={Causal normalizing flows: from theory to practice},\nauthor={Adri{\\'a}n Javaloy and Pablo Sanchez Martin and Isabel Valera},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=QIFoCI7ca1}\n}"}, "paperhash": {"value": "javaloy|causal_normalizing_flows_from_theory_to_practice"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7006/-/Revision", "NeurIPS.cc/2023/Conference/Submission7006/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7006/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325941999, "odate": 1698949738313, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "QDByreuQyk", "number": 6725, "cdate": 1683716250606, "tcdate": 1683716250606, "mdate": 1698949736866, "tmdate": 1698949736866, "signatures": ["NeurIPS.cc/2023/Conference/Submission6725/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6725/Authors"], "forum": "QDByreuQyk", "content": {"title": {"value": "Nearly Tight Bounds For Differentially Private Multiway Cut"}, "authors": {"value": ["Mina Dalirrooyfard", "Slobodan Mitrovic", "Yuriy Nevmyvaka"]}, "authorids": {"value": ["~Mina_Dalirrooyfard1", "~Slobodan_Mitrovic1", "~Yuriy_Nevmyvaka1"]}, "keywords": {"value": ["Differential Privacy", "clustering", "multiway cut", "min cut", "graph partitioning"]}, "abstract": {"value": "Finding min $s$-$t$ cuts in graphs is a basic algorithmic tool, with applications in image segmentation, community detection, reinforcement learning, and data clustering. In this problem, we are given two nodes as terminals and the goal is to remove the smallest number of edges from the graph so that these two terminals are disconnected. We study the complexity of differential privacy for the min $s$-$t$ cut problem and show nearly tight lower and upper bounds where we achieve privacy at no cost for running time efficiency. We also develop a differentially private algorithm for the multiway $k$-cut problem, in which we are given $k$ nodes as terminals that we would like to disconnect.\n    As a function of $k$, we obtain privacy guarantees that are exponentially more efficient than applying the advanced composition theorem to known algorithms for multiway $k$-cut.\n    Finally, we empirically evaluate the approximation of our differentially private min $s$-$t$ cut algorithm and show that it almost matches the quality of the output of non-private ones."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We provide the first DP algorithm for min s-t cut and multiway k cut."}, "pdf": {"value": "/pdf/b9962f68f524ece242cbe1e7fd1224122c4feac0.pdf"}, "supplementary_material": {"value": "/attachment/38ad5946889dc2c42415737fb2f5de8490beb750.pdf"}, "_bibtex": {"value": "@inproceedings{\ndalirrooyfard2023nearly,\ntitle={Nearly Tight Bounds For Differentially Private Multiway Cut},\nauthor={Mina Dalirrooyfard and Slobodan Mitrovic and Yuriy Nevmyvaka},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=QDByreuQyk}\n}"}, "paperhash": {"value": "dalirrooyfard|nearly_tight_bounds_for_differentially_private_multiway_cut"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6725/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6725/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325933939, "odate": 1698949736851, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "73XPopmbXH", "number": 6222, "cdate": 1683703616445, "tcdate": 1683703616445, "mdate": 1698949734118, "tmdate": 1698949734118, "signatures": ["NeurIPS.cc/2023/Conference/Submission6222/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6222/Authors"], "forum": "73XPopmbXH", "content": {"title": {"value": "Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models"}, "authors": {"value": ["Alex Damian", "Eshaan Nichani", "Rong Ge", "Jason D. Lee"]}, "authorids": {"value": ["~Alex_Damian1", "~Eshaan_Nichani1", "~Rong_Ge1", "~Jason_D._Lee1"]}, "keywords": {"value": ["statistical learning", "learning theory", "single index model", "gradient descent", "stochastic gradient descent"]}, "TLDR": {"value": "We prove that online SGD on a smoothed loss achieves optimal sample complexity for learning single index models."}, "abstract": {"value": "We focus on the task of learning a single index model $\\sigma(w^\\star \\cdot x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions. Prior work has shown that the sample complexity of learning $w^\\star$ is governed by the information exponent $k^\\star$ of the link function $\\sigma$, which is defined as the index of the first nonzero Hermite coefficient of $\\sigma$. Ben Arous et al. (2021) showed that $n \\gtrsim d^{k^\\star-1}$ samples suffice for learning $w^\\star$ and that this is tight for online SGD. However, the CSQ lower bound for gradient based methods only shows that $n \\gtrsim d^{k^\\star/2}$ samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns $w^\\star$ with $n \\gtrsim d^{k^\\star/2}$ samples. We also draw connections to statistical analyses of tensor PCA and to the implicit regularization effects of minibatch SGD on empirical losses."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a99d3df10743aef943855a7d223d123069ed9acd.pdf"}, "supplementary_material": {"value": "/attachment/cf6198449546f66315a55be2cce0204934c8ff3b.zip"}, "_bibtex": {"value": "@inproceedings{\ndamian2023smoothing,\ntitle={Smoothing the Landscape Boosts the Signal for {SGD}: Optimal Sample Complexity for Learning Single Index Models},\nauthor={Alex Damian and Eshaan Nichani and Rong Ge and Jason D. Lee},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=73XPopmbXH}\n}"}, "paperhash": {"value": "damian|smoothing_the_landscape_boosts_the_signal_for_sgd_optimal_sample_complexity_for_learning_single_index_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6222/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6222/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325919403, "odate": 1698949734107, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Yacmpz84TH", "number": 6215, "cdate": 1683703376403, "tcdate": 1683703376403, "mdate": 1698949734096, "tmdate": 1698949734096, "signatures": ["NeurIPS.cc/2023/Conference/Submission6215/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6215/Authors"], "forum": "Yacmpz84TH", "content": {"title": {"value": "Toolformer: Language Models Can Teach Themselves to Use Tools"}, "authors": {"value": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto Dessi", "Roberta Raileanu", "Maria Lomeli", "Eric Hambro", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"]}, "authorids": {"value": ["~Timo_Schick1", "~Jane_Dwivedi-Yu1", "~Roberto_Dessi1", "~Roberta_Raileanu2", "~Maria_Lomeli2", "~Eric_Hambro1", "~Luke_Zettlemoyer1", "ncan@meta.com", "~Thomas_Scialom1"]}, "keywords": {"value": ["Language Models", "Zero-Shot Learning", "Tool Use", "APIs"]}, "abstract": {"value": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller specialized models excel. In this paper, we show that LMs can teach themselves to *use external tools* via simple APIs and achieve the best of both worlds. We introduce *Toolformer*, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."}, "pdf": {"value": "/pdf/b2e1ecf89cf4267621b45a071bc6b838f88e3884.pdf"}, "supplementary_material": {"value": "/attachment/21bae39b339152aac0f41c417befde081b3b7cdd.pdf"}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nschick2023toolformer,\ntitle={Toolformer: Language Models Can Teach Themselves to Use Tools},\nauthor={Timo Schick and Jane Dwivedi-Yu and Roberto Dessi and Roberta Raileanu and Maria Lomeli and Eric Hambro and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Yacmpz84TH}\n}"}, "TLDR": {"value": "We introduce Toolformer, a language model trained in a self-supervised way to know when and how to use external tools, achieving substantially improved zero-shot performance across a variety of downstream tasks."}, "paperhash": {"value": "schick|toolformer_language_models_can_teach_themselves_to_use_tools"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6215/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325919248, "odate": 1698949734084, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "financial_support"}, {"name": "reviewer_nomination"}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "i913TUOvTK", "number": 5979, "cdate": 1683696746340, "tcdate": 1683696746340, "mdate": 1698949733036, "tmdate": 1698949733036, "signatures": ["NeurIPS.cc/2023/Conference/Submission5979/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5979/Authors"], "forum": "i913TUOvTK", "content": {"title": {"value": "Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity"}, "authors": {"value": ["Zijiao Chen", "Jiaxin Qing", "Juan Helen Zhou"]}, "authorids": {"value": ["~Zijiao_Chen1", "~Jiaxin_Qing1", "~Juan_Helen_Zhou1"]}, "keywords": {"value": ["Video Reconstruction from Brain Activities", "Diffusion Model", "Contrastive Learning"]}, "TLDR": {"value": "We present Mind-Video for reconstructing continuous video from fMRI data via multimodal contrastive learning and inflated Stable Diffusion model."}, "abstract": {"value": "Reconstructing human vision from brain activities has been an appealing task that helps to understand our cognitive process. Even though recent research has seen great success in reconstructing static images from non-invasive brain recordings, work on recovering continuous visual experiences in the form of videos is limited. In this work, we propose Mind-Video that learns spatiotemporal information from continuous fMRI data of the cerebral cortex progressively through masked brain modeling, multimodal contrastive learning with spatiotemporal attention, and co-training with an augmented Stable Diffusion model that incorporates network temporal inflation. \nWe show that high-quality videos of arbitrary frame rates can be reconstructed with Mind-Video using adversarial guidance. The recovered videos were evaluated with various semantic and pixel-level metrics. We achieved an average accuracy of 85% in semantic classification tasks and 0.19 in structural similarity index (SSIM), outperforming the previous state-of-the-art by 45%. We also show that our model is biologically plausible and interpretable, reflecting established physiological processes."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a41d3804a52829c85b919fe13b6d8f28bf219366.pdf"}, "supplementary_material": {"value": "/attachment/7cc2efc6eac852345ff79d6437293330327aa112.zip"}, "_bibtex": {"value": "@inproceedings{\nchen2023cinematic,\ntitle={Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity},\nauthor={Zijiao Chen and Jiaxin Qing and Juan Helen Zhou},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=i913TUOvTK}\n}"}, "paperhash": {"value": "chen|cinematic_mindscapes_highquality_video_reconstruction_from_brain_activity"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5979/-/Revision", "NeurIPS.cc/2023/Conference/Submission5979/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5979/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325912520, "odate": 1698949733024, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "y8UAQQHVTX", "number": 5647, "cdate": 1683686166624, "tcdate": 1683686166624, "mdate": 1698949730313, "tmdate": 1698949730313, "signatures": ["NeurIPS.cc/2023/Conference/Submission5647/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5647/Authors"], "forum": "y8UAQQHVTX", "content": {"title": {"value": "Private Everlasting Prediction"}, "authors": {"value": ["Moni Naor", "Kobbi Nissim", "Uri Stemmer", "Chao Yan"]}, "authorids": {"value": ["moni.naor@gmail.com", "~Kobbi_Nissim2", "~Uri_Stemmer1", "~Chao_Yan2"]}, "keywords": {"value": ["Differential privacy", "private learning", "private prediction"]}, "abstract": {"value": "A private learner is trained on a sample of labeled points and generatesa hypothesis that can be used for predicting the labels of newly sampled points while protecting the privacy of the training set [Kasiviswannathan et al., FOCS 2008]. Research uncovered that private learners may need to exhibit significantly higher sample complexity than non-private learners as is the case with, e.g., learning of one-dimensional threshold functions [Bun et al., FOCS 2015, Alon et al., STOC 2019].\n\nWe explore prediction as an alternative to learning. Instead of putting forward a hypothesis, a predictor answers a stream of classification queries. Earlier work has considered a private prediction model with just a single classification query [Dwork and Feldman, COLT 2018]. We observe that when answering a stream of queries, a predictor must modify the hypothesis it uses over time, and, furthermore, that it must use the queries for this modification, hence introducing potential privacy risks with respect to the queries themselves.\n\nWe introduce private everlasting prediction taking into account the privacy of both the training set and the (adaptively chosen) queries made to the predictor. We then present a generic construction of private everlasting predictors in the PAC model. The sample complexity of the initial training sample in our construction is quadratic (up to polylog factors) in the VC dimension of the concept class. Our construction allows prediction for all concept classes with finite VC dimension, and in particular threshold functions with constant size initial training sample, even when considered over infinite domains, whereas it is known that the sample complexity of privately learning threshold functions must grow as a function of the domain size and hence is impossible for infinite domains."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3c128e6d91de6655f09612228dab9bf625268a74.pdf"}, "supplementary_material": {"value": "/attachment/104331818fd334848f016fd632748676a248d740.pdf"}, "_bibtex": {"value": "@inproceedings{\nnaor2023private,\ntitle={Private Everlasting Prediction},\nauthor={Moni Naor and Kobbi Nissim and Uri Stemmer and Chao Yan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=y8UAQQHVTX}\n}"}, "paperhash": {"value": "naor|private_everlasting_prediction"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5647/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5647/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325899544, "odate": 1698949730298, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "BHXsb69bSx", "number": 5556, "cdate": 1683683375175, "tcdate": 1683683375175, "mdate": 1698949729724, "tmdate": 1698949729724, "signatures": ["NeurIPS.cc/2023/Conference/Submission5556/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5556/Authors"], "forum": "BHXsb69bSx", "content": {"title": {"value": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings"}, "authors": {"value": ["Shibo Hao", "Tianyang Liu", "Zhen Wang", "Zhiting Hu"]}, "authorids": {"value": ["~Shibo_Hao1", "~Tianyang_Liu2", "~Zhen_Wang6", "~Zhiting_Hu3"]}, "keywords": {"value": ["large language model", "tool learning"]}, "TLDR": {"value": "We propose to use tool embeddings to augment large language models with tools"}, "abstract": {"value": "Integrating large language models (LLMs) with various tools has led to increased attention in the field. Existing approaches either involve fine-tuning the LLM, which is both computationally costly and limited to a fixed set of tools, or prompting LLMs by in-context tool demonstrations. Although the latter method offers adaptability to new tools, it struggles with the inherent context length constraint of LLMs when many new tools are presented, and mastering a new set of tools with few-shot examples remains challenging, resulting in suboptimal performance. To address these limitations, we propose a novel solution, named **ToolkenGPT**, wherein LLMs effectively learn to master tools as predicting tokens through **tool embeddings** for solving complex tasks. In this framework, each tool is transformed into vector embeddings and plugged into the language model head. Once the function is triggered during text generation, the LLM enters a special function mode to execute the tool calls. Our experiments show that function embeddings effectively help LLMs understand tool use and improve on several tasks, including numerical reasoning, knowledge-based question answering and embodied decision-making."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a3491ed269b1ada560b51040d1d6791bfbcff978.pdf"}, "supplementary_material": {"value": "/attachment/1460b6696f16562b4018db076399f3ed7d97a330.zip"}, "_bibtex": {"value": "@inproceedings{\nhao2023toolkengpt,\ntitle={Toolken{GPT}: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings},\nauthor={Shibo Hao and Tianyang Liu and Zhen Wang and Zhiting Hu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BHXsb69bSx}\n}"}, "paperhash": {"value": "hao|toolkengpt_augmenting_frozen_language_models_with_massive_tools_via_tool_embeddings"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5556/-/Revision", "NeurIPS.cc/2023/Conference/Submission5556/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5556/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325895149, "odate": 1698949729710, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "w0H2xGHlkw", "number": 5336, "cdate": 1683668328056, "tcdate": 1683668328056, "mdate": 1698949728086, "tmdate": 1698949728086, "signatures": ["NeurIPS.cc/2023/Conference/Submission5336/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5336/Authors"], "forum": "w0H2xGHlkw", "content": {"title": {"value": "Visual Instruction Tuning"}, "authors": {"value": ["Haotian Liu", "Chunyuan Li", "Qingyang Wu", "Yong Jae Lee"]}, "authorids": {"value": ["~Haotian_Liu1", "~Chunyuan_Li1", "~Qingyang_Wu1", "~Yong_Jae_Lee2"]}, "keywords": {"value": ["visual instruction tuning", "instruction tuning", "multimodal", "LLM", "GPT"]}, "abstract": {"value": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general-purpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8ec2de30800edb13f33bf46d3f735b91f7561ce0.pdf"}, "_bibtex": {"value": "@inproceedings{\nliu2023visual,\ntitle={Visual Instruction Tuning},\nauthor={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=w0H2xGHlkw}\n}"}, "paperhash": {"value": "liu|visual_instruction_tuning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5336/-/Revision", "NeurIPS.cc/2023/Conference/Submission5336/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5336/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325887621, "odate": 1698949728074, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "wIlmx4bHrO", "forum": "wIlmx4bHrO", "number": 5251, "cdate": 1683661737035, "tcdate": 1683661737035, "mdate": 1698949727287, "tmdate": 1698949727287, "signatures": ["NeurIPS.cc/2023/Conference/Submission5251/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5251/Authors"], "content": {"title": {"value": "A Single-Loop Accelerated Extra-Gradient Difference Algorithm with Improved Complexity Bounds for Constrained Minimax Optimization"}, "authors": {"value": ["Yuanyuan Liu", "Fanhua Shang", "Weixin An", "Junhao Liu", "Hongying Liu", "Zhouchen Lin"]}, "authorids": {"value": ["~Yuanyuan_Liu1", "~Fanhua_Shang2", "~Weixin_An1", "~Junhao_Liu3", "~Hongying_Liu2", "~Zhouchen_Lin1"]}, "keywords": {"value": ["Constrained Minimax Optimization; nonconvex- nonconcave"]}, "abstract": {"value": "In this paper, we propose a novel extra-gradient difference acceleration algorithm for solving constrained nonconvex-nonconcave (NC-NC) minimax problems. In particular, we design a new extra-gradient difference step to obtain an important quasi-cocoercivity property, which plays a key role to significantly improve the convergence rate in the constrained NC-NC setting without additional structural assumption. Then momentum acceleration is also introduced into our dual accelerating update step. Moreover, we prove that, to find an $\\epsilon$-stationary point of the function $f$, our algorithm attains the complexity $\\mathcal{O}(\\epsilon^{-2})$ in the constrained NC-NC setting, while the best-known complexity bound is $\\widetilde{\\mathcal{O}}(\\epsilon^{-4})$, where $\\widetilde{\\mathcal{O}}(\\cdot)$ hides logarithmic factors compared to $\\mathcal{O}(\\cdot)$. As the special cases of the constrained NC-NC setting, our algorithm can also obtain the same complexity $\\mathcal{O}(\\epsilon^{-2})$ for both the nonconvex-concave (NC-C) and convex-nonconcave (C-NC) cases, while the best-known complexity bounds are $\\widetilde{\\mathcal{O}}(\\epsilon^{-2.5})$ for the NC-C case and $\\widetilde{\\mathcal{O}}(\\epsilon^{-4})$ for the C-NC case. For fair comparison with existing algorithms, we also analyze the complexity bound to find $\\epsilon$-stationary point of the primal function $\\phi$ for the constrained NC-C problem, which shows that our algorithm can improve the complexity bound from $\\widetilde{\\mathcal{O}}(\\epsilon^{-3})$ to $\\mathcal{O}(\\epsilon^{-2})$. To the best of our knowledge, this is the first time that the proposed algorithm improves the best-known complexity bounds from $\\mathcal{O}(\\epsilon^{-4})$ and $\\widetilde{\\mathcal{O}}(\\epsilon^{-3})$ to $\\mathcal{O}(\\epsilon^{-2})$ in both the NC-NC and NC-C settings."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1e269f46b6083f1d64c56f6193b62244969bddf1.pdf"}, "supplementary_material": {"value": "/attachment/0d93bd18befd0637b82a35fc9be68f3b9be0e37c.pdf"}, "_bibtex": {"value": "@inproceedings{\nliu2023a,\ntitle={A Single-Loop Accelerated Extra-Gradient Difference Algorithm with Improved Complexity Bounds for Constrained Minimax Optimization},\nauthor={Yuanyuan Liu and Fanhua Shang and Weixin An and Junhao Liu and Hongying Liu and Zhouchen Lin},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=wIlmx4bHrO}\n}"}, "paperhash": {"value": "liu|a_singleloop_accelerated_extragradient_difference_algorithm_with_improved_complexity_bounds_for_constrained_minimax_optimization"}}, "pdate": 1695325884239, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5251/-/Revision", "NeurIPS.cc/2023/Conference/Submission5251/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5251/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "odate": 1698949727275, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "eD534mPhAg", "number": 4916, "cdate": 1683645549213, "tcdate": 1683645549213, "mdate": 1698949724747, "tmdate": 1698949724747, "signatures": ["NeurIPS.cc/2023/Conference/Submission4916/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4916/Authors"], "forum": "eD534mPhAg", "content": {"title": {"value": "Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis"}, "authors": {"value": ["Junfeng Fang", "Wei Liu", "Yuan Gao", "Zemin Liu", "An Zhang", "Xiang Wang", "Xiangnan He"]}, "authorids": {"value": ["~Junfeng_Fang1", "~Wei_Liu34", "~Yuan_Gao18", "~Zemin_Liu1", "~An_Zhang2", "~Xiang_Wang6", "~Xiangnan_He1"]}, "keywords": {"value": ["Post-hoc Explainability", "Explanation Evaluation", "Graph Neural Network", "Robustness Analysis"]}, "abstract": {"value": "This work studies the evaluation of explaining graph neural networks (GNNs), which is crucial to the credibility of post-hoc explainability in practical usage. Conventional evaluation metrics, and even explanation methods -- which mainly follow the paradigm of feeding the explanatory subgraph and measuring output difference -- always suffer from the notorious out-of-distribution (OOD) issue. In this work, we endeavor to confront the issue by introducing a novel evaluation metric, termed **O**OD-resistant **A**dversarial **R**obustness (OAR). Specifically, we draw inspiration from the notion of adversarial robustness and evaluate post-hoc explanation subgraphs by calculating their robustness under attack. On top of that, an elaborate OOD reweighting block is inserted into the pipeline to confine the evaluation process to the original data distribution. For applications involving large datasets, we further devise a **Sim**plified version of **OAR** (SimOAR), which achieves a significant improvement in computational efficiency at the cost of a small amount of performance. Extensive empirical studies validate the effectiveness of our OAR and SimOAR."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f9088a857e15726caa9414c9e0d90b5b2b9f54ed.pdf"}, "_bibtex": {"value": "@inproceedings{\nfang2023evaluating,\ntitle={Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis},\nauthor={Junfeng Fang and Wei Liu and Yuan Gao and Zemin Liu and An Zhang and Xiang Wang and Xiangnan He},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=eD534mPhAg}\n}"}, "TLDR": {"value": "This work draws inspiration from the notion of adversarial robustness and introduces a novel evaluation metric, termed OOD-resistant Adversarial Robustness (OAR)."}, "paperhash": {"value": "fang|evaluating_posthoc_explanations_for_graph_neural_networks_via_robustness_analysis"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4916/-/Revision", "NeurIPS.cc/2023/Conference/Submission4916/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4916/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325872600, "odate": 1698949724736, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "qHrADgAdYu", "number": 4396, "cdate": 1683622213180, "tcdate": 1683622213180, "mdate": 1698949721509, "tmdate": 1698949721509, "signatures": ["NeurIPS.cc/2023/Conference/Submission4396/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4396/Authors"], "forum": "qHrADgAdYu", "content": {"title": {"value": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective"}, "authors": {"value": ["Guhao Feng", "Bohang Zhang", "Yuntian Gu", "Haotian Ye", "Di He", "Liwei Wang"]}, "authorids": {"value": ["~Guhao_Feng1", "~Bohang_Zhang1", "~Yuntian_Gu1", "~Haotian_Ye1", "~Di_He1", "~Liwei_Wang1"]}, "keywords": {"value": ["Chain-of-Thought Prompting", "Large Language Models", "Theory", "Circuit Complexity", "Dynamic Programming"]}, "TLDR": {"value": "This paper theoretically and empirically show the utility of CoT in LLMs."}, "abstract": {"value": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the \\emph{expressivity} of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows \\emph{super-polynomially} with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of \\emph{constant size} suffice to solve both tasks by generating CoT derivations using a commonly used math language format. Moreover, we show LLMs with CoT can handle a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, an extensive set of experiments show that, while Transformers always fail to directly predict the answers, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/72a9ba61f298f8bf65982219e1d96a557cf3f4f1.pdf"}, "_bibtex": {"value": "@inproceedings{\nfeng2023towards,\ntitle={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective},\nauthor={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=qHrADgAdYu}\n}"}, "paperhash": {"value": "feng|towards_revealing_the_mystery_behind_chain_of_thought_a_theoretical_perspective"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4396/-/Revision", "NeurIPS.cc/2023/Conference/Submission4396/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4396/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325855977, "odate": 1698949721494, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "FtNruwFEs3", "number": 4256, "cdate": 1683616342845, "tcdate": 1683616342845, "mdate": 1698949720540, "tmdate": 1698949720540, "signatures": ["NeurIPS.cc/2023/Conference/Submission4256/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4256/Authors"], "forum": "FtNruwFEs3", "content": {"title": {"value": "Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach"}, "authors": {"value": ["Fabian Zaiser", "Andrzej S Murawski", "Luke Ong"]}, "authorids": {"value": ["~Fabian_Zaiser1", "~Andrzej_S_Murawski1", "~Luke_Ong1"]}, "keywords": {"value": ["Bayesian statistics", "probabliistic programming", "exact inference", "discrete models", "probability generating functions"]}, "abstract": {"value": "We present an exact Bayesian inference method for discrete statistical models, which can find exact solutions to a large class of discrete inference problems, even with infinite support and continuous priors.\nTo express such models, we introduce a probabilistic programming language that supports discrete and continuous sampling, discrete observations, affine functions, (stochastic) branching, and conditioning on discrete events.\nOur key tool is *probability generating functions*:\nthey provide a compact closed-form representation of distributions that are definable by programs, thus enabling the exact computation of posterior probabilities, expectation, variance, and higher moments.\nOur inference method is provably correct and fully automated in a tool called *Genfer*, which uses automatic differentiation (specifically, Taylor polynomials), but does not require computer algebra.\nOur experiments show that Genfer is often faster than the existing exact inference tools PSI, Dice, and Prodigy.\nOn a range of real-world inference problems that none of these exact tools can solve, Genfer's performance is competitive with approximate Monte Carlo methods, while avoiding approximation errors."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/244ae93dfe1df9844d74b9481f716ba9fdd9689b.pdf"}, "supplementary_material": {"value": "/attachment/00ae51f659ea1e5585cc2abbc08b316fccfa53d0.zip"}, "_bibtex": {"value": "@inproceedings{\nzaiser2023exact,\ntitle={Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach},\nauthor={Fabian Zaiser and Andrzej S Murawski and Luke Ong},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=FtNruwFEs3}\n}"}, "paperhash": {"value": "zaiser|exact_bayesian_inference_on_discrete_models_via_probability_generating_functions_a_probabilistic_programming_approach"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4256/-/Revision", "NeurIPS.cc/2023/Conference/Submission4256/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4256/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325851397, "odate": 1698949720527, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "mayAyPrhJI", "number": 3782, "cdate": 1683576766641, "tcdate": 1683576766641, "mdate": 1698949716863, "tmdate": 1698949716863, "signatures": ["NeurIPS.cc/2023/Conference/Submission3782/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3782/Authors"], "forum": "mayAyPrhJI", "content": {"title": {"value": "Bridging Discrete and Backpropagation: Straight-Through and Beyond"}, "authors": {"value": ["Liyuan Liu", "Chengyu Dong", "Xiaodong Liu", "Bin Yu", "Jianfeng Gao"]}, "authorids": {"value": ["~Liyuan_Liu3", "~Chengyu_Dong1", "~Xiaodong_Liu1", "~Bin_Yu5", "~Jianfeng_Gao1"]}, "keywords": {"value": ["discrete random variables", "back-propagation", "straight through"]}, "abstract": {"value": "Backpropagation, the cornerstone of deep learning, is limited to computing gradients for continuous variables. This limitation poses challenges for problems involving discrete latent variables. To address this issue, we propose a novel approach to approximate the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose ReinMax, which achieves second-order accuracy by integrating Heun\u2019s method, a second-order numerical method for solving ODEs. ReinMax does not require Hessian or other second-order derivatives, thus having negligible computation overheads. Extensive experimental results on various tasks demonstrate the superiority of ReinMax over the state of the art."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We show Straight-Through works as a first-order approximation of the gradient and propose ReinMax, which achieves second-order accuracy with negligible computation overheads."}, "pdf": {"value": "/pdf/4b7414632febe87b9e460ac7e8c125455e35b54a.pdf"}, "supplementary_material": {"value": "/attachment/9b106468ce664fbec1e37ba725e427c6f82b6989.zip"}, "_bibtex": {"value": "@inproceedings{\nliu2023bridging,\ntitle={Bridging Discrete and Backpropagation: Straight-Through and Beyond},\nauthor={Liyuan Liu and Chengyu Dong and Xiaodong Liu and Bin Yu and Jianfeng Gao},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=mayAyPrhJI}\n}"}, "paperhash": {"value": "liu|bridging_discrete_and_backpropagation_straightthrough_and_beyond"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3782/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3782/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325836393, "odate": 1698949716848, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "eTHawKFT4h", "number": 3633, "cdate": 1683562730795, "tcdate": 1683562730795, "mdate": 1698949715658, "tmdate": 1698949715658, "signatures": ["NeurIPS.cc/2023/Conference/Submission3633/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3633/Authors"], "forum": "eTHawKFT4h", "content": {"title": {"value": "A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods"}, "authors": {"value": ["Veit David Wild", "Sahra Ghalebikesabi", "Dino Sejdinovic", "Jeremias Knoblauch"]}, "authorids": {"value": ["~Veit_David_Wild1", "~Sahra_Ghalebikesabi1", "~Dino_Sejdinovic1", "~Jeremias_Knoblauch1"]}, "keywords": {"value": ["Wasserstein gradient flow", "generalised variational inference", "deep ensembles", "Bayesian deep learning", "variational Bayes"]}, "TLDR": {"value": "We establish the first unified theory connecting Bayesian, variational Bayesian, and ensemble methods for deep learning by leveraging Wasserstein Gradient Flows."}, "abstract": {"value": "We establish the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. A key step towards this it to reformulate the non-convex optimisation problem typically encountered in deep learning as a convex optimisation in the space of probability measures. On a technical level, our contribution amounts to studying generalised variational inference through the lense of Wasserstein gradient flows. The result is a unified theory of various seemingly disconnected approaches that are commonly used for uncertainty quantification in deep learning---including deep ensembles and (variational) Bayesian methods. This offers a fresh perspective on the reasons behind the success of deep ensembles over procedures based on parameterised variational inference, and allows the derivation of new ensembling schemes with convergence guarantees. We showcase this by proposing a family of interacting deep ensembles with direct parallels to the interactions of particle systems in thermodynamics, and use our theory to prove the convergence of these algorithms to a well-defined global minimiser on the space of probability measures."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/704a823d72ae267fa31197305e03593ac2212acb.pdf"}, "supplementary_material": {"value": "/attachment/4459723091322ec7c99ec810c28ce419c67babb9.pdf"}, "_bibtex": {"value": "@inproceedings{\nwild2023a,\ntitle={A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods},\nauthor={Veit David Wild and Sahra Ghalebikesabi and Dino Sejdinovic and Jeremias Knoblauch},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=eTHawKFT4h}\n}"}, "paperhash": {"value": "wild|a_rigorous_link_between_deep_ensembles_and_variational_bayesian_methods"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3633/-/Revision", "NeurIPS.cc/2023/Conference/Submission3633/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3633/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325831342, "odate": 1698949715644, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "MFWgLCWgUB", "number": 3589, "cdate": 1683560058834, "tcdate": 1683560058834, "mdate": 1698949715251, "tmdate": 1698949715251, "signatures": ["NeurIPS.cc/2023/Conference/Submission3589/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3589/Authors"], "forum": "MFWgLCWgUB", "content": {"title": {"value": "Random Cuts are Optimal for Explainable k-Medians"}, "authors": {"value": ["Konstantin Makarychev", "Liren Shan"]}, "authorids": {"value": ["~Konstantin_Makarychev1", "~Liren_Shan1"]}, "keywords": {"value": ["Clustering", "k-medians", "Decision Tree", "Explainability"]}, "TLDR": {"value": "We provide the optimal competitive ratio for explainable k-medians."}, "abstract": {"value": "We show that the RandomCoordinateCut algorithm gives the optimal competitive ratio for explainable $k$-medians in $\\ell_1$. The problem of explainable $k$-medians was introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian in 2020. Several groups of authors independently proposed a simple polynomial-time randomized algorithm for the problem and showed that this algorithm is $O(\\log k \\log\\log k)$ competitive.  We provide a tight analysis of the algorithm and prove that its competitive ratio is upper bounded by $2\\ln k+2$. This bound matches the $\\Omega(\\log k)$ lower bound by Dasgupta et al (2020)."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/bb8719116080561461e3e5886e13698302a67ef8.pdf"}, "supplementary_material": {"value": "/attachment/d8398adbdd7a2d983c92c37af1cd805c47bed1a1.pdf"}, "_bibtex": {"value": "@inproceedings{\nmakarychev2023random,\ntitle={Random Cuts are Optimal for Explainable k-Medians},\nauthor={Konstantin Makarychev and Liren Shan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=MFWgLCWgUB}\n}"}, "paperhash": {"value": "makarychev|random_cuts_are_optimal_for_explainable_kmedians"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3589/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3589/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325829814, "odate": 1698949715238, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "fHyLsfMDIs", "number": 3529, "cdate": 1683556617006, "tcdate": 1683556617006, "mdate": 1698949714857, "tmdate": 1698949714857, "signatures": ["NeurIPS.cc/2023/Conference/Submission3529/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3529/Authors"], "forum": "fHyLsfMDIs", "content": {"title": {"value": "Entropic Neural Optimal Transport via Diffusion Processes"}, "authors": {"value": ["Nikita Gushchin", "Alexander Kolesov", "Alexander Korotin", "Dmitry P. Vetrov", "Evgeny Burnaev"]}, "authorids": {"value": ["~Nikita_Gushchin1", "~Alexander_Kolesov1", "~Alexander_Korotin2", "~Dmitry_P._Vetrov1", "~Evgeny_Burnaev1"]}, "keywords": {"value": ["Optimal transport", "Schr\u00f6dinger Bridge", "Entropy regularized OT", "Neural Networks", "Unpaired Learning"]}, "abstract": {"value": "We propose a novel neural algorithm for the fundamental problem of computing the entropic optimal transport (EOT) plan between probability distributions which are accessible by samples. Our algorithm is based on the saddle point reformulation of the dynamic version of EOT which is known as the Schr\u00f6dinger Bridge problem. In contrast to the prior methods for large-scale EOT, our algorithm is end-to-end and consists of a single learning step, has fast inference procedure, and allows handling small values of the entropy regularization coefficient which is of particular importance in some applied problems. Empirically, we show the performance of the method on several large-scale EOT tasks. The code for the ENOT solver can be found at https://github.com/ngushchin/EntropicNeuralOptimalTransport"}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0d912e03c8deec99275ebeb36c142a9c268fb1a2.pdf"}, "supplementary_material": {"value": "/attachment/8ab8b0283f285f8bfaa4649ddd8841ee9ce9ac57.pdf"}, "_bibtex": {"value": "@inproceedings{\ngushchin2023entropic,\ntitle={Entropic Neural Optimal Transport via Diffusion Processes},\nauthor={Nikita Gushchin and Alexander Kolesov and Alexander Korotin and Dmitry P. Vetrov and Evgeny Burnaev},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=fHyLsfMDIs}\n}"}, "paperhash": {"value": "gushchin|entropic_neural_optimal_transport_via_diffusion_processes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3529/-/Revision", "NeurIPS.cc/2023/Conference/Submission3529/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3529/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325828209, "odate": 1698949714843, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "liMSqUuVg9", "number": 2921, "cdate": 1683488427698, "tcdate": 1683488427698, "mdate": 1698949710243, "tmdate": 1698949710243, "signatures": ["NeurIPS.cc/2023/Conference/Submission2921/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2921/Authors"], "forum": "liMSqUuVg9", "content": {"title": {"value": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"}, "authors": {"value": ["Yu Bai", "Fan Chen", "Huan Wang", "Caiming Xiong", "Song Mei"]}, "authorids": {"value": ["~Yu_Bai1", "~Fan_Chen4", "~Huan_Wang1", "~Caiming_Xiong1", "~Song_Mei1"]}, "keywords": {"value": ["in-context learning", "transformers", "deep learning theory", "learning theory"]}, "abstract": {"value": "Neural sequence models based on the transformer architecture have demonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences.\n    \nBuilding on these ``base'' ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving \\emph{in-context algorithm selection}, akin to what a statistician can do in real life---A \\emph{single} transformer can adaptively select different base ICL algorithms---or even perform qualitatively different tasks---on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging task---noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Transformers can learn in context like statisticians --- A single transformer can select different algorithms for different data at hand."}, "pdf": {"value": "/pdf/56b8f3ee4bb98edac7d196156e54e2b17c966c99.pdf"}, "_bibtex": {"value": "@inproceedings{\nbai2023transformers,\ntitle={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},\nauthor={Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=liMSqUuVg9}\n}"}, "paperhash": {"value": "bai|transformers_as_statisticians_provable_incontext_learning_with_incontext_algorithm_selection"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2921/-/Revision", "NeurIPS.cc/2023/Conference/Submission2921/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2921/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325809463, "odate": 1698949710230, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "sPLTQSf6GI", "number": 2548, "cdate": 1683394102927, "tcdate": 1683394102927, "mdate": 1698949707364, "tmdate": 1698949707364, "signatures": ["NeurIPS.cc/2023/Conference/Submission2548/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2548/Authors"], "forum": "sPLTQSf6GI", "content": {"title": {"value": "A Measure-Theoretic Axiomatisation of Causality"}, "authors": {"value": ["Junhyung Park", "Simon Buchholz", "Bernhard Sch\u00f6lkopf", "Krikamol Muandet"]}, "authorids": {"value": ["~Junhyung_Park1", "~Simon_Buchholz1", "~Bernhard_Sch\u00f6lkopf1", "~Krikamol_Muandet1"]}, "keywords": {"value": ["Causality", "probability theory", "causal models"]}, "TLDR": {"value": "We propose causal spaces, a measure-theoretic axiomatisation of causality."}, "abstract": {"value": "Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8d2c7b3e86fcac33cf5feb8b504a10e4231e49bb.pdf"}, "supplementary_material": {"value": "/attachment/487f39f59dd098b2244d532c3e84b0aaf9a63216.pdf"}, "_bibtex": {"value": "@inproceedings{\npark2023a,\ntitle={A Measure-Theoretic Axiomatisation of Causality},\nauthor={Junhyung Park and Simon Buchholz and Bernhard Sch{\\\"o}lkopf and Krikamol Muandet},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=sPLTQSf6GI}\n}"}, "paperhash": {"value": "park|a_measuretheoretic_axiomatisation_of_causality"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2548/-/Revision", "NeurIPS.cc/2023/Conference/Submission2548/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2548/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325797828, "odate": 1698949707348, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "kMueEV8Eyy", "number": 1677, "cdate": 1683147299323, "tcdate": 1683147299323, "mdate": 1698949701598, "tmdate": 1698949701598, "signatures": ["NeurIPS.cc/2023/Conference/Submission1677/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1677/Authors"], "forum": "kMueEV8Eyy", "content": {"title": {"value": "Abide by the law and follow the flow: conservation laws for gradient flows"}, "authors": {"value": ["Sibylle Marcotte", "R\u00e9mi Gribonval", "Gabriel Peyr\u00e9"]}, "authorids": {"value": ["~Sibylle_Marcotte1", "~R\u00e9mi_Gribonval1", "~Gabriel_Peyr\u00e92"]}, "keywords": {"value": ["Implicit bias", "conservation laws", "gradient flow", "linear neural network", "matrix factorization"]}, "abstract": {"value": "Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This \"implicit bias\" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of \"conservation laws\", that define quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the maximal number of independent conservation laws\nby performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms to: a) compute a family of polynomial laws; b) compute the maximal number of (not necessarily polynomial) independent conservation laws. We provide showcase examples that we fully work out theoretically. Besides, applying the two algorithms confirms for a number of ReLU network architectures that all known laws are recovered by the algorithm, and that there are no other independent laws. Such computational tools pave the way to understanding desirable properties of optimization initialization in large machine learning models."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7b766b2698788a69d39692e5b346421ae2b4c83e.pdf"}, "supplementary_material": {"value": "/attachment/bce115bec8e52f5168e05eb8ae1397c1d50bbcb9.pdf"}, "_bibtex": {"value": "@inproceedings{\nmarcotte2023abide,\ntitle={Abide by the law and follow the flow: conservation laws for gradient flows},\nauthor={Sibylle Marcotte and R{\\'e}mi Gribonval and Gabriel Peyr{\\'e}},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=kMueEV8Eyy}\n}"}, "paperhash": {"value": "marcotte|abide_by_the_law_and_follow_the_flow_conservation_laws_for_gradient_flows"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1677/-/Revision", "NeurIPS.cc/2023/Conference/Submission1677/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1677/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325772086, "odate": 1698949701584, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "jA235JGM09", "number": 1527, "cdate": 1683086417187, "tcdate": 1683086417187, "mdate": 1698954269061, "tmdate": 1698954269061, "signatures": ["NeurIPS.cc/2023/Conference/Submission1527/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1527/Authors"], "forum": "jA235JGM09", "content": {"title": {"value": "Jailbroken: How Does LLM Safety Training Fail?"}, "authors": {"value": ["Alexander Wei", "Nika Haghtalab", "Jacob Steinhardt"]}, "authorids": {"value": ["~Alexander_Wei2", "~Nika_Haghtalab2", "~Jacob_Steinhardt1"]}, "keywords": {"value": ["red teaming", "safety", "RLHF", "large language models"]}, "abstract": {"value": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \u201cjailbreak\u201d attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model\u2019s capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI\u2019s GPT-4 and Anthropic\u2019s Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models\u2019 red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity\u2014that safety mechanisms should be as sophisticated as the underlying model\u2014and argues against the idea that scaling alone can resolve these safety failure modes."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Competing objectives and mismatched generalization lead to jailbreak attacks on safety-trained LLMs."}, "pdf": {"value": "/pdf/6ef6c4c838351879286475fe5c123aad6815fb02.pdf"}, "_bibtex": {"value": "@inproceedings{\nwei2023jailbroken,\ntitle={Jailbroken: How Does {LLM} Safety Training Fail?},\nauthor={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=jA235JGM09}\n}"}, "paperhash": {"value": "wei|jailbroken_how_does_llm_safety_training_fail"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1527/-/Revision", "NeurIPS.cc/2023/Conference/Submission1527/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1527/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325763696, "odate": 1698949700413, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "5W7cXno10k", "number": 899, "cdate": 1682588427344, "tcdate": 1682588427344, "mdate": 1698949695922, "tmdate": 1698949695922, "signatures": ["NeurIPS.cc/2023/Conference/Submission899/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission899/Authors"], "forum": "5W7cXno10k", "content": {"title": {"value": "Characteristic Circuits"}, "authors": {"value": ["Zhongjie Yu", "Martin Trapp", "Kristian Kersting"]}, "authorids": {"value": ["~Zhongjie_Yu2", "~Martin_Trapp2", "~Kristian_Kersting1"]}, "keywords": {"value": ["Characteristic Circuit", "Characteristic Function", "Probabilistic Circuit", "Heterogeneous Data", "Density Estimation"]}, "TLDR": {"value": "Characteristic circuit is a deep probabilistic model over characteristic functions that enables a unified view for discrete and continuous random variables and allows to learn distributions that do not have closed form expressions for their density."}, "abstract": {"value": "In many real-world scenarios it is crucial to be able to reliably and efficiently reason under uncertainty while capturing complex relationships in data.\n  Probabilistic circuits (PCs), a prominent family of tractable probabilistic models, offer a remedy to this challenge by composing simple, tractable distributions into a high-dimensional probability distribution. \n  However, learning PCs on heterogeneous data is challenging and densities of some parametric distributions are not available in closed form, limiting their potential use. \n  We introduce characteristic circuits (CCs), a family of tractable probabilistic models providing a unified formalization of distributions over heterogeneous data in the spectral domain.\n  The one-to-one relationship between characteristic functions and probability measures enables us to learn high-dimensional distributions on heterogeneous data domains and facilitates efficient probabilistic inference even when no closed-form density function is available. \n  We show that the structure and parameters of CCs can be learned efficiently from the data and find that CCs outperform state-of-the-art density estimators for heterogeneous data domains on common benchmark data sets."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f1069d9f1c620036c9fe6bf90b8b0a73b3c10384.pdf"}, "supplementary_material": {"value": "/attachment/4da1c9b653ff8dc8b298841b097c506239789be5.zip"}, "_bibtex": {"value": "@inproceedings{\nyu2023characteristic,\ntitle={Characteristic Circuits},\nauthor={Zhongjie Yu and Martin Trapp and Kristian Kersting},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5W7cXno10k}\n}"}, "paperhash": {"value": "yu|characteristic_circuits"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission899/-/Revision", "NeurIPS.cc/2023/Conference/Submission899/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission899/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325744704, "odate": 1698949695906, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "gI1SOgW3kw", "number": 640, "cdate": 1682388077328, "tcdate": 1682388077328, "mdate": 1698949694173, "tmdate": 1698949694173, "signatures": ["NeurIPS.cc/2023/Conference/Submission640/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission640/Authors"], "forum": "gI1SOgW3kw", "content": {"title": {"value": "Generalizing Nonlinear ICA Beyond Structural Sparsity"}, "authors": {"value": ["Yujia Zheng", "Kun Zhang"]}, "authorids": {"value": ["~Yujia_Zheng1", "~Kun_Zhang1"]}, "keywords": {"value": ["Latent variable models", "nonlinear independent component analysis"]}, "TLDR": {"value": "We establish a set of new identifiability results of nonlinear ICA in the general settings of undercompleteness, partial sparsity and source dependence, and flexible grouping structures."}, "abstract": {"value": "Nonlinear independent component analysis (ICA) aims to uncover the true latent sources from their observable nonlinear mixtures. Despite its significance, the identifiability of nonlinear ICA is known to be impossible without additional assumptions. Recent advances have proposed conditions on the connective structure from sources to observed variables, known as Structural Sparsity, to achieve identifiability in an unsupervised manner. However, the sparsity constraint may not hold universally for all sources in practice. Furthermore, the assumptions of bijectivity of the mixing process and independence among all sources, which arise from the setting of ICA, may also be violated in many real-world scenarios. To address these limitations and generalize nonlinear ICA, we propose a set of new identifiability results in the general settings of undercompleteness, partial sparsity and source dependence, and flexible grouping structures. Specifically, we prove identifiability when there are more observed variables than sources (undercomplete), and when certain sparsity and/or source independence assumptions are not met for some changing sources. Moreover, we show that even in cases with flexible grouping structures (e.g., part of the sources can be divided into irreducible independent groups with various sizes), appropriate identifiability results can also be established. Theoretical claims are supported empirically on both synthetic and real-world datasets."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c8cacba57be0e9de38f156a8525e062ef65ed2e7.pdf"}, "_bibtex": {"value": "@inproceedings{\nzheng2023generalizing,\ntitle={Generalizing Nonlinear {ICA} Beyond Structural Sparsity},\nauthor={Yujia Zheng and Kun Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=gI1SOgW3kw}\n}"}, "paperhash": {"value": "zheng|generalizing_nonlinear_ica_beyond_structural_sparsity"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission640/-/Revision", "NeurIPS.cc/2023/Conference/Submission640/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission640/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325738512, "odate": 1698949694159, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "APGXBNkt6h", "number": 487, "cdate": 1682287098348, "tcdate": 1682287098348, "mdate": 1698949693344, "tmdate": 1698949693344, "signatures": ["NeurIPS.cc/2023/Conference/Submission487/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission487/Authors"], "forum": "APGXBNkt6h", "content": {"title": {"value": "When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment"}, "authors": {"value": ["Tianwei Ni", "Michel Ma", "Benjamin Eysenbach", "Pierre-Luc Bacon"]}, "authorids": {"value": ["~Tianwei_Ni1", "~Michel_Ma1", "~Benjamin_Eysenbach1", "~Pierre-Luc_Bacon1"]}, "keywords": {"value": ["Memory-based RL", "Transformers", "Credit Assignment", "Online RL", "Model-free RL"]}, "abstract": {"value": "Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The Transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capability of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our results provide an explanation for the success of Transformers in RL, while also highlighting an important area for future research and benchmark design. Our code is open-sourced at https://github.com/twni2016/Memory-RL."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Transformers can help learn long-term memory but not long-term credit assignment in online model-free RL."}, "pdf": {"value": "/pdf/d661af11afd49d4215dc6fb59c106dcf57e1423b.pdf"}, "supplementary_material": {"value": "/attachment/a74be6582e68f9a92f357daa2fb7de3a7b33418f.pdf"}, "_bibtex": {"value": "@inproceedings{\nni2023when,\ntitle={When Do Transformers Shine in {RL}? Decoupling Memory from Credit Assignment},\nauthor={Tianwei Ni and Michel Ma and Benjamin Eysenbach and Pierre-Luc Bacon},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=APGXBNkt6h}\n}"}, "paperhash": {"value": "ni|when_do_transformers_shine_in_rl_decoupling_memory_from_credit_assignment"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission487/-/Revision", "NeurIPS.cc/2023/Conference/Submission487/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission487/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325735160, "odate": 1698949693331, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "1vzF4zWQ1E", "number": 81, "cdate": 1681951766256, "tcdate": 1681951766256, "mdate": 1698954266064, "tmdate": 1698954266064, "signatures": ["NeurIPS.cc/2023/Conference/Submission81/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission81/Authors"], "forum": "1vzF4zWQ1E", "content": {"title": {"value": "Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition"}, "authors": {"value": ["Samuel Dooley", "Rhea Sanjay Sukthanker", "John P Dickerson", "Colin White", "Frank Hutter", "Micah Goldblum"]}, "authorids": {"value": ["~Samuel_Dooley1", "~Rhea_Sanjay_Sukthanker3", "~John_P_Dickerson1", "~Colin_White1", "~Frank_Hutter1", "~Micah_Goldblum1"]}, "keywords": {"value": ["Bias Mitigation", "Fairness", "Facial Recognition"]}, "abstract": {"value": "Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race.  Conventional wisdom dictates that model biases arise from biased training data.  As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition.  In our work, we discover that biases are actually inherent to neural network architectures themselves.  Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairness, often by large margins, on the two most widely used datasets for face identification, CelebA and VGGFace2. Furthermore, these models generalize to other datasets and sensitive attributes. We release our code, models and raw data files at https://github.com/dooleys/FR-NAS."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We find that bias is inherent to neural network architectures and hyperparameters, yet we can mitigate it by searching for fair ones"}, "pdf": {"value": "/pdf/0d8a5dfb11b22aec9bd240a8a3606d8c4cb0af59.pdf"}, "_bibtex": {"value": "@inproceedings{\ndooley2023rethinking,\ntitle={Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition},\nauthor={Samuel Dooley and Rhea Sanjay Sukthanker and John P Dickerson and Colin White and Frank Hutter and Micah Goldblum},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=1vzF4zWQ1E}\n}"}, "paperhash": {"value": "dooley|rethinking_bias_mitigation_fairer_architectures_make_for_fairer_face_recognition"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission81/-/Revision", "NeurIPS.cc/2023/Conference/Submission81/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission81/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325726025, "odate": 1698949690418, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "f38EY21lBw", "number": 22, "cdate": 1681868927766, "tcdate": 1681868927766, "mdate": 1698949690019, "tmdate": 1698949690019, "signatures": ["NeurIPS.cc/2023/Conference/Submission22/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission22/Authors"], "forum": "f38EY21lBw", "content": {"title": {"value": "Privacy Auditing with One (1) Training Run"}, "authors": {"value": ["Thomas Steinke", "Milad Nasr", "Matthew Jagielski"]}, "authorids": {"value": ["~Thomas_Steinke2", "~Milad_Nasr2", "~Matthew_Jagielski1"]}, "keywords": {"value": ["Differential privacy", "membership inference attacks", "privacy auditing"]}, "abstract": {"value": "We propose a scheme for auditing differentially private machine learning systems with a single training run. This exploits the parallelism of being able to add or remove multiple training examples independently. We analyze this using the connection between differential privacy and statistical generalization, which avoids the cost of group privacy. Our auditing scheme requires minimal assumptions about the algorithm and can be applied in the black-box or white-box setting. We demonstrate the effectiveness of our framework by applying it to DP-SGD, where we can achieve meaningful empirical privacy lower bounds by training only one model. In contrast, standard methods would require training hundreds of models."}, "venue": {"value": "NeurIPS 2023 oral"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We show how to compute lower bounds on the privacy parameters of an algorithm with only one run of that algorithm."}, "supplementary_material": {"value": "/attachment/6bf63f60c9f2255c824bb0672c728434fba2547f.pdf"}, "pdf": {"value": "/pdf/ea8cc13e0d434d255f2042b40f1791762c99ae92.pdf"}, "_bibtex": {"value": "@inproceedings{\nsteinke2023privacy,\ntitle={Privacy Auditing with One (1) Training Run},\nauthor={Thomas Steinke and Milad Nasr and Matthew Jagielski},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=f38EY21lBw}\n}"}, "paperhash": {"value": "steinke|privacy_auditing_with_one_1_training_run"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission22/-/Revision", "NeurIPS.cc/2023/Conference/Submission22/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325724751, "odate": 1698949690003, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}], "count": 67}