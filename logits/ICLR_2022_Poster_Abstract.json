{"notes": [{"id": "HndgQudNb91", "original": "REEuy0zWIoj", "number": 4722, "cdate": 1632948118733, "mdate": null, "ddate": null, "tcdate": 1632948118733, "tmdate": 1697934501628, "tddate": null, "forum": "HndgQudNb91", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Downsample for Segmentation of Ultra-High Resolution Images", "authorids": ["~Chen_Jin3", "~Ryutaro_Tanno1", "~Thomy_Mertzanidou1", "~Eleftheria_Panagiotaki1", "~Daniel_C._Alexander1"], "authors": ["Chen Jin", "Ryutaro Tanno", "Thomy Mertzanidou", "Eleftheria Panagiotaki", "Daniel C. Alexander"], "keywords": ["ultra-high resolution image segmentation", "non-uniform dowmsampling", "efficient segmentation", "large volume image segmentation", "medical image segmentation"], "abstract": "Many computer vision systems require low-cost segmentation algorithms based on deep learning, either because of the enormous size of input images or limited computational budget. Common solutions uniformly downsample the input images to meet memory constraints, assuming all pixels are equally informative. In this work, we demonstrate that this assumption can harm the segmentation performance\nbecause the segmentation difficulty varies spatially (see Figure 1 \u201cUniform\u201d). We combat this problem by introducing a learnable downsampling module, which can be optimised together with the given segmentation model in an end-to-end fashion. We formulate the problem of training such downsampling module as optimisation of sampling density distributions over the input images given their low-resolution views. To defend against degenerate solutions (e.g. over-sampling trivial regions like the backgrounds), we propose a regularisation term that encourages the sampling locations to concentrate around the object boundaries. We find the downsampling\nmodule learns to sample more densely at difficult locations, thereby improving the segmentation performance (see Figure 1 \"Ours\"). Our experiments on benchmarks of high-resolution street view, aerial and medical images demonstrate substantial improvements in terms of efficiency-and-accuracy trade-off compared to both uniform downsampling and two recent advanced downsampling techniques.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jin|learning_to_downsample_for_segmentation_of_ultrahigh_resolution_images", "pdf": "/pdf/d2ade7120315e0521c4b97b593c4a2ebd44b0652.pdf", "one-sentence_summary": "We propose a method for learning to downsample ultra high-resolution images that reflects the importance of each location.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.11071/code)", "_bibtex": "@inproceedings{\njin2022learning,\ntitle={Learning to Downsample for Segmentation of Ultra-High Resolution Images},\nauthor={Chen Jin and Ryutaro Tanno and Thomy Mertzanidou and Eleftheria Panagiotaki and Daniel C. Alexander},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HndgQudNb91}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "7fFO4cMBx_9", "original": "OU7lRMgefPc", "number": 4721, "cdate": 1632948118673, "mdate": null, "ddate": null, "tcdate": 1632948118673, "tmdate": 1697934501984, "tddate": null, "forum": "7fFO4cMBx_9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Variational Neural Cellular Automata", "authorids": ["~Rasmus_Berg_Palm1", "~Miguel_Gonz\u00e1lez_Duque1", "~Shyam_Sudhakaran1", "~Sebastian_Risi1"], "authors": ["Rasmus Berg Palm", "Miguel Gonz\u00e1lez Duque", "Shyam Sudhakaran", "Sebastian Risi"], "keywords": ["Neural Cellular Automata", "Cellular Automata", "Self-Organization", "Generative Models"], "abstract": "In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms --- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process.\nInspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, the self-organizing nature bestows the VNCA with some inherent robustness against perturbations in the early stages of growth.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "palm|variational_neural_cellular_automata", "pdf": "/pdf/abec641c2a0c18536da3345e5cd92d673d90b69d.pdf", "one-sentence_summary": "We propose and evaluate the Variational Neural Cellular Automata, a self-organising generative model based on neural cellular automata", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2201.12360/code)", "_bibtex": "@inproceedings{\npalm2022variational,\ntitle={Variational Neural Cellular Automata},\nauthor={Rasmus Berg Palm and Miguel Gonz{\\'a}lez Duque and Shyam Sudhakaran and Sebastian Risi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7fFO4cMBx_9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "FKp8-pIRo3y", "original": "whGXQ0YYNlq", "number": 4719, "cdate": 1632948118548, "mdate": null, "ddate": null, "tcdate": 1632948118548, "tmdate": 1676330442065, "tddate": null, "forum": "FKp8-pIRo3y", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation", "authorids": ["~Todor_Davchev1", "~Oleg_Olegovich_Sushkov1", "~Jean-Baptiste_Regli1", "~Stefan_Schaal1", "~Yusuf_Aytar1", "~Markus_Wulfmeier1", "~Jon_Scholz1"], "authors": ["Todor Davchev", "Oleg Olegovich Sushkov", "Jean-Baptiste Regli", "Stefan Schaal", "Yusuf Aytar", "Markus Wulfmeier", "Jon Scholz"], "keywords": ["goal-conditioned reinforcement learning", "learning from demonstrations", "long-horizon dexterous manipulation", "bi-manual manipulation"], "abstract": "Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of ``narrow passages'' in their state space. Solving such tasks with a sparse reward in a sample-efficient manner poses a challenge to modern reinforcement learning (RL) due to the associated long-horizon nature of the problem and the lack of sufficient positive signal during learning. \nVarious tools have been applied to address this challenge. When available, large sets of demonstrations can guide agent exploration. Hindsight relabelling on the other hand does not require additional sources of information. However, existing strategies explore based on task-agnostic goal distributions, which can render the solution of long-horizon tasks impractical. In this work, we extend hindsight relabelling mechanisms to guide exploration along task-specific distributions implied by a small set of successful demonstrations. We evaluate the approach on four complex, single and dual arm, robotics manipulation tasks against strong suitable baselines. The method requires far fewer demonstrations to solve all tasks and achieves a significantly higher overall performance as task complexity increases. Finally, we investigate the robustness of the proposed solution with respect to the quality of input representations and the number of demonstrations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "davchev|wish_you_were_here_hindsight_goal_selection_for_longhorizon_dexterous_manipulation", "pdf": "/pdf/524d4c3cacc5ff7803cd7061b33991511fee7db7.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\ndavchev2022wish,\ntitle={Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation},\nauthor={Todor Davchev and Oleg Olegovich Sushkov and Jean-Baptiste Regli and Stefan Schaal and Yusuf Aytar and Markus Wulfmeier and Jon Scholz},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FKp8-pIRo3y}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "KntaNRo6R48", "original": "y3mwowfMW6", "number": 4717, "cdate": 1632948118409, "mdate": null, "ddate": null, "tcdate": 1632948118409, "tmdate": 1676330442056, "tddate": null, "forum": "KntaNRo6R48", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "L0-Sparse Canonical Correlation Analysis", "authorids": ["~Ofir_Lindenbaum1", "~Moshe_Salhov1", "~Amir_Averbuch1", "~Yuval_Kluger1"], "authors": ["Ofir Lindenbaum", "Moshe Salhov", "Amir Averbuch", "Yuval Kluger"], "keywords": [], "abstract": "Canonical Correlation Analysis (CCA) models are powerful for studying the associations between two sets of variables. The canonically correlated representations, termed \\textit{canonical variates} are widely used in unsupervised learning to analyze unlabeled multi-modal registered datasets. Despite their success, CCA models may break (or overfit) if the number of variables in either of the modalities exceeds the number of samples. Moreover, often a significant fraction of the variables measures modality-specific information, and thus removing them is beneficial for identifying the \\textit{canonically correlated variates}. Here, we propose $\\ell_0$-CCA, a method for learning correlated representations based on sparse subsets of variables from two observed modalities.\nSparsity is obtained by multiplying the input variables by stochastic gates, whose parameters are learned together with the CCA weights via an $\\ell_0$-regularized correlation loss. \nWe further propose $\\ell_0$-Deep CCA for solving the problem of non-linear sparse CCA by modeling the correlated representations using deep nets. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, by gating nuisance input variables, our approach improves the extracted representations compared to other linear, non-linear and sparse CCA-based models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lindenbaum|l0sparse_canonical_correlation_analysis", "pdf": "/pdf/69ae8c04ac43812f7523f009313daec68f09ea3d.pdf", "one-sentence_summary": "We propose a new $\\ell_0$-CCA method for learning correlated representations based on sparse subsets of variables from two observed modalities.", "supplementary_material": "/attachment/8bfa11b6b541f2002ba5319d1a0792a920399814.zip", "_bibtex": "@inproceedings{\nlindenbaum2022lsparse,\ntitle={L0-Sparse Canonical Correlation Analysis},\nauthor={Ofir Lindenbaum and Moshe Salhov and Amir Averbuch and Yuval Kluger},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KntaNRo6R48}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "B7ZbqNLDn-_", "original": "e_CRxIsYxcO", "number": 4715, "cdate": 1632948118280, "mdate": null, "ddate": null, "tcdate": 1632948118280, "tmdate": 1676330442056, "tddate": null, "forum": "B7ZbqNLDn-_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?", "authorids": ["~Sheikh_Shams_Azam1", "~Seyyedali_Hosseinalipour1", "~Qiang_Qiu1", "~Christopher_Brinton1"], "authors": ["Sheikh Shams Azam", "Seyyedali Hosseinalipour", "Qiang Qiu", "Christopher Brinton"], "keywords": ["Distributed Machine Learning", "Federated Learning", "Gradient Subspace", "SGD"], "abstract": "In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning. We start by examining the rank characteristics of the subspace spanned by gradients (i.e., the gradient-space) in centralized model training, and observe that the gradient-space often consists of a few leading principal components accounting for an overwhelming majority (95-99%) of the explained variance. Motivated by this, we propose the \"Look-back Gradient Multiplier\" (LBGM) algorithm, which utilizes this low-rank property of the gradient-space in federated learning. Operationally, LBGM recycles the gradients between model update rounds to significantly reduce the number of parameters to be propagated through the system. We analytically characterize the convergence behavior of LBGM, revealing the nature of the trade-off between communication savings and model performance. Our subsequent experimental results demonstrate the improvement LBGM obtains on communication overhead compared to federated learning baselines. Additionally, we show that LBGM is a general plug-and-play algorithm that can be used standalone or stacked on top of existing sparsification techniques for distributed model training.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "azam|recycling_model_updates_in_federated_learning_are_gradient_subspaces_lowrank", "pdf": "/pdf/76e2433c08e957e7f19a49e6815d0f6b52da92cd.pdf", "one-sentence_summary": "We observe that \"gradient-space is low rank\" and propose the LBGM algorithm that utilitizes this low-rank property to recycle gradients between model update rounds in federated learning.", "_bibtex": "@inproceedings{\nazam2022recycling,\ntitle={Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?},\nauthor={Sheikh Shams Azam and Seyyedali Hosseinalipour and Qiang Qiu and Christopher Brinton},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=B7ZbqNLDn-_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "ucASPPD9GKN", "original": "WlGq4GsMbws", "number": 4711, "cdate": 1632875770874, "mdate": null, "ddate": null, "tcdate": 1632875770874, "tmdate": 1697934502870, "tddate": null, "forum": "ucASPPD9GKN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Is Homophily a Necessity for Graph Neural Networks?", "authorids": ["~Yao_Ma3", "~Xiaorui_Liu1", "~Neil_Shah2", "~Jiliang_Tang1"], "authors": ["Yao Ma", "Xiaorui Liu", "Neil Shah", "Jiliang Tang"], "keywords": [], "abstract": "Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification,  GNNs are widely believed to work well due to the homophily assumption (``like attracts like''), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance.  We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions and provides supporting theoretical understanding and empirical observations.  Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ma|is_homophily_a_necessity_for_graph_neural_networks", "pdf": "/pdf/dba6b2a528efebfb036a0b908ecfc59201204429.pdf", "supplementary_material": "/attachment/bbde85db7c61a5770e7eb7b6c2abeea42fa8e857.zip", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2106.06134/code)", "_bibtex": "@inproceedings{\nma2022is,\ntitle={Is Homophily a Necessity for Graph Neural Networks?},\nauthor={Yao Ma and Xiaorui Liu and Neil Shah and Jiliang Tang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ucASPPD9GKN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "Ve0Wth3ptT_", "original": "JdgIYu5GQua", "number": 4703, "cdate": 1632875770333, "mdate": null, "ddate": null, "tcdate": 1632875770333, "tmdate": 1676330442095, "tddate": null, "forum": "Ve0Wth3ptT_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DEGREE: Decomposition Based Explanation for Graph Neural Networks", "authorids": ["~Qizhang_Feng1", "~Ninghao_Liu2", "~Fan_Yang27", "~Ruixiang_Tang1", "~Mengnan_Du1", "~Xia_Hu4"], "authors": ["Qizhang Feng", "Ninghao Liu", "Fan Yang", "Ruixiang Tang", "Mengnan Du", "Xia Hu"], "keywords": ["XAI", "GNN"], "abstract": "Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with suffer from faithfulness problems and unnatural artifacts respectively. To tackle these problems, we propose DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of our algorithm can be further improved by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks.", "one-sentence_summary": "We propose a new decomposition based explanation for Graph Neural Networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "feng|degree_decomposition_based_explanation_for_graph_neural_networks", "pdf": "/pdf/fd7de8640028480fa9fe56dd9ed7bcad9182bf31.pdf", "data": "", "_bibtex": "@inproceedings{\nfeng2022degree,\ntitle={{DEGREE}: Decomposition Based Explanation for Graph Neural Networks},\nauthor={Qizhang Feng and Ninghao Liu and Fan Yang and Ruixiang Tang and Mengnan Du and Xia Hu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Ve0Wth3ptT_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "T0B9AoM_bFg", "original": "owIGL-Id-AN", "number": 4668, "cdate": 1632875768056, "mdate": null, "ddate": null, "tcdate": 1632875768056, "tmdate": 1676330443177, "tddate": null, "forum": "T0B9AoM_bFg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Improving Mutual Information Estimation with Annealed and Energy-Based Bounds", "authorids": ["~Rob_Brekelmans1", "~Sicong_Huang1", "~Marzyeh_Ghassemi2", "~Greg_Ver_Steeg1", "~Roger_Baker_Grosse1", "~Alireza_Makhzani1"], "authors": ["Rob Brekelmans", "Sicong Huang", "Marzyeh Ghassemi", "Greg Ver Steeg", "Roger Baker Grosse", "Alireza Makhzani"], "keywords": ["mutual information estimation", "annealed importance sampling", "energy-based models"], "abstract": "Mutual information (MI) is a fundamental quantity in information theory and machine learning. However, direct estimation of MI is intractable, even if the true joint probability density for the variables of interest is known, as it involves estimating a potentially high-dimensional log partition function. In this work, we present a unifying view of existing MI bounds from the perspective of importance sampling, and propose three novel bounds based on this approach. Since a tight MI bound without density information requires a sample size exponential in the true MI, we assume either a single marginal or the full joint density information is known. In settings where the full joint density is available, we propose Multi-Sample Annealed Importance Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large values of MI in our experiments. In settings where only a single marginal distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds. Our GIWAE bound unifies variational and contrastive bounds in a single framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our MINE-AIS method improves upon existing energy-based methods such as MINE-DV and MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC sampling to estimate gradients for training and Multi-Sample AIS for evaluating the bound. Our methods are particularly suitable for evaluating MI in deep generative models, since explicit forms of the marginal or joint densities are often available. We evaluate our bounds on estimating the MI of VAEs and GANs trained on the MNIST and CIFAR datasets, and showcase significant gains over existing bounds in these challenging settings with high ground truth MI.", "one-sentence_summary": "We derive new annealed importance sampling and energy-based bounds, resulting in vastly more accurate estimates of mutual information.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "brekelmans|improving_mutual_information_estimation_with_annealed_and_energybased_bounds", "pdf": "/pdf/a68f8e4bbad21f5599f372c94827c5f596c6555b.pdf", "data": "", "_bibtex": "@inproceedings{\nbrekelmans2022improving,\ntitle={Improving Mutual Information Estimation with Annealed and Energy-Based Bounds},\nauthor={Rob Brekelmans and Sicong Huang and Marzyeh Ghassemi and Greg Ver Steeg and Roger Baker Grosse and Alireza Makhzani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=T0B9AoM_bFg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "bp-LJ4y_XC", "original": "VqLa5ifS_oQ", "number": 4662, "cdate": 1632875767646, "mdate": null, "ddate": null, "tcdate": 1632875767646, "tmdate": 1676330443199, "tddate": null, "forum": "bp-LJ4y_XC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods", "authorids": ["~Xueyuan_She1", "~Saurabh_Dash1", "~Saibal_Mukhopadhyay1"], "authors": ["Xueyuan She", "Saurabh Dash", "Saibal Mukhopadhyay"], "keywords": ["spiking neural network", "spatiotemporal processing", "feedforward network"], "abstract": "A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns. In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN. Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain. Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN. Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data. A dual-search-space Bayesian optimization method is developed to optimize architecture and parameters of the proposed SNN with heterogeneous neuron dynamics and skip-layer connections. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "she|sequence_approximation_using_feedforward_spiking_neural_network_for_spatiotemporal_learning_theory_and_optimization_methods", "pdf": "/pdf/043f00a3e618d0c71bbd79dffbdfdaf6d9fd4d1b.pdf", "one-sentence_summary": "A theoretical approache to study the approximation capability of feedforward spiking neural network and optimization methods for such network.", "supplementary_material": "/attachment/07bfc11fcbf1f0d165886d9670fb0314518d9a36.zip", "_bibtex": "@inproceedings{\nshe2022sequence,\ntitle={Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods},\nauthor={Xueyuan She and Saurabh Dash and Saibal Mukhopadhyay},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bp-LJ4y_XC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "nwKXyFvaUm", "original": "jJprcUgbTo6", "number": 4660, "cdate": 1632875767512, "mdate": null, "ddate": null, "tcdate": 1632875767512, "tmdate": 1676330443254, "tddate": null, "forum": "nwKXyFvaUm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Diverse Client Selection for Federated Learning via Submodular Maximization", "authorids": ["~Ravikumar_Balakrishnan1", "~Tian_Li1", "~Tianyi_Zhou1", "nageen.himayat@intel.com", "~Virginia_Smith1", "~Jeff_Bilmes1"], "authors": ["Ravikumar Balakrishnan", "Tian Li", "Tianyi Zhou", "Nageen Himayat", "Virginia Smith", "Jeff Bilmes"], "keywords": ["federated learning", "submodularity", "diversity"], "abstract": "In every communication round of federated learning, a random subset of clients communicate  their  model  updates  back  to  the  server  which  then  aggregates them all.  The optimal size of this subset is not known and several studies have shown that typically random selection does not perform very well in terms of convergence, learning efficiency and fairness. We, in this paper, propose to select a small diverse subset of clients, namely those carrying representative gradient information, and we transmit only these updates to the server.  Our aim is for updating via only a subset to approximate updating via aggregating all client information. We achieve this by choosing a subset that maximizes a submodular facility location function defined over gradient space. We introduce \u201cfederated averaging with diverse client selection (DivFL)\u201d. We provide a thorough analysis of its convergence in the heterogeneous setting and apply it both to synthetic and to real datasets. Empirical results show several benefits to our approach including improved learning efficiency, faster convergence and also more uniform (i.e., fair) performance across clients. We further show a communication-efficient version of DivFL that can still outperform baselines on the above metrics.", "one-sentence_summary": "The paper addresses a key challenge of selecting the most representative clients iteratively for federated learning through formulating it as a submodular optimization problem and developing efficient algorithms.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "balakrishnan|diverse_client_selection_for_federated_learning_via_submodular_maximization", "pdf": "/pdf/4d539789e55d133a96781cda576be4ab34ec5982.pdf", "_bibtex": "@inproceedings{\nbalakrishnan2022diverse,\ntitle={Diverse Client Selection for Federated Learning via Submodular Maximization},\nauthor={Ravikumar Balakrishnan and Tian Li and Tianyi Zhou and Nageen Himayat and Virginia Smith and Jeff Bilmes},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nwKXyFvaUm}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "jT1EwXu-4hj", "original": "AJiP8QDRsVH", "number": 4651, "cdate": 1632875766895, "mdate": null, "ddate": null, "tcdate": 1632875766895, "tmdate": 1676330443316, "tddate": null, "forum": "jT1EwXu-4hj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation", "authorids": ["~Da_Xu2", "~Yuting_Ye3", "~Chuanwei_Ruan1", "~Evren_Korpeoglu1", "~Sushant_Kumar1", "~Kannan_Achan1"], "authors": ["Da Xu", "Yuting Ye", "Chuanwei Ruan", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "keywords": ["Information retrieval", "Learning theory", "Causal inference", "Missing data", "Overlapping", "Reweighting", "Optimal transport"], "abstract": "The interventional nature of recommendation has attracted increasing attention in recent years. It particularly motivates researchers to formulate learning and evaluating recommendation as causal inference and data missing-not-at-random problems. However, few take seriously the consequence of violating the critical assumption of overlapping, which we prove can significantly threaten the validity and interpretation of the outcome. We find a critical piece missing in the current understanding of information retrieval (IR) systems: as interventions, recommendation not only affects the already observed data, but it also interferes with the target domain (distribution) of interest. We then rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. Towards this end, we use domain transportation to characterize the learning-intervention mechanism of recommendation. We design a principled transportation-constraint risk minimization objective and convert it to a two-player minimax game.\nWe prove the consistency, generalization, and excessive risk bounds for the proposed objective, and elaborate how they compare to the current results. Finally, we carry out extensive real-data and semi-synthetic experiments to demonstrate the advantage of our approach, and launch online testing with a real-world IR system.", "one-sentence_summary": "We propose and study a novel domain-transportation view for optimizing recommendation for information retrieval systems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|from_intervention_to_domain_transportation_a_novel_perspective_to_optimize_recommendation", "pdf": "/pdf/22322b458fd437ff0b3cf13debd29cc381b25ccc.pdf", "supplementary_material": "/attachment/4b6d99ed3156754a98761a428ef733a1d68bf4ed.zip", "_bibtex": "@inproceedings{\nxu2022from,\ntitle={From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation},\nauthor={Da Xu and Yuting Ye and Chuanwei Ruan and Evren Korpeoglu and Sushant Kumar and Kannan Achan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=jT1EwXu-4hj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "JxFgJbZ-wft", "original": "pqcDjdr1zCf", "number": 4647, "cdate": 1632875766626, "mdate": null, "ddate": null, "tcdate": 1632875766626, "tmdate": 1697934505311, "tddate": null, "forum": "JxFgJbZ-wft", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Variational Predictive Routing with Nested Subjective Timescales", "authorids": ["~Alexey_Zakharov1", "~Qinghai_Guo1", "~Zafeirios_Fountas1"], "authors": ["Alexey Zakharov", "Qinghai Guo", "Zafeirios Fountas"], "keywords": ["Hierarchical temporal abstraction", "event discovery", "hierarchical generative models", "variational inference"], "abstract": "Discovery and learning of an underlying spatiotemporal hierarchy in sequential data is an important topic for machine learning. Despite this, little work has been done to explore hierarchical generative models that can flexibly adapt their layerwise representations in response to datasets with different temporal dynamics. Here, we present Variational Predictive Routing (VPR) \u2013 a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process. By employing an event detection mechanism that relies solely on the system\u2019s latent representations (without the need of a separate model), VPR is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model\u2019s latent hierarchy.  Using several video datasets, we show that VPR is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate time-agnostic rollouts of the future. Our approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zakharov|variational_predictive_routing_with_nested_subjective_timescales", "pdf": "/pdf/712c74938a55973dd0b3f46e154fc0696194b578.pdf", "one-sentence_summary": "Variational inference hierarchical model that relies on a change detection mechanism to impose a nested temporal hierarchy on its latent structure.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.11236/code)", "_bibtex": "@inproceedings{\nzakharov2022variational,\ntitle={Variational Predictive Routing with Nested Subjective Timescales},\nauthor={Alexey Zakharov and Qinghai Guo and Zafeirios Fountas},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JxFgJbZ-wft}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "RhB1AdoFfGE", "original": "IVBi_OS_zA", "number": 4630, "cdate": 1632875765523, "mdate": null, "ddate": null, "tcdate": 1632875765523, "tmdate": 1697934506263, "tddate": null, "forum": "RhB1AdoFfGE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sample and Computation Redistribution for Efficient Face Detection", "authorids": ["~Jia_Guo1", "~Jiankang_Deng1", "~Alexandros_Lattas1", "~Stefanos_Zafeiriou1"], "authors": ["Jia Guo", "Jiankang Deng", "Alexandros Lattas", "Stefanos Zafeiriou"], "keywords": ["efficient face detection", "computation redistribution", "sample redistribution"], "abstract": "Although tremendous strides have been made in uncontrolled face detection, accurate face detection with a low computation cost remains an open challenge. In this paper, we point out that computation distribution and scale augmentation are the keys to detecting small faces from low-resolution images. Motivated by these observations, we introduce two simple but effective methods: (1) Computation Redistribution (CR), which reallocates the computation between the backbone, neck and head of the model; and (2) Sample Redistribution (SR), which augments training samples for the most needed stages. The proposed Sample and Computation Redistribution for Face Detection (SCRFD) is implemented by a random search in a meticulously designed search space. Extensive experiments conducted on WIDER FACE demonstrate the state-of-the-art accuracy-efficiency trade-off for the proposed SCRFD family across a wide range of compute regimes. In particular, SCRFD-34GF outperforms the best competitor, TinaFace, by $4.78\\%$ (AP at hard set) while being more than 3$\\times$ faster on GPUs with VGA-resolution images. Code is available at: https://github.com/deepinsight/insightface/tree/master/detection/scrfd.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guo|sample_and_computation_redistribution_for_efficient_face_detection", "pdf": "/pdf/d7b9dd38011f418b1c66bb378aef38a25d8c9bf5.pdf", "one-sentence_summary": "We search for optimised computation distribution and training sample distribution for the task of face detection.", "supplementary_material": "/attachment/f9c1bf1166045e338636898678f68d9057ac2060.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2105.04714/code)", "_bibtex": "@inproceedings{\nguo2022sample,\ntitle={Sample and Computation Redistribution for Efficient Face Detection},\nauthor={Jia Guo and Jiankang Deng and Alexandros Lattas and Stefanos Zafeiriou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RhB1AdoFfGE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "NkZq4OEYN-", "original": "uo_XNKf3A6V", "number": 4629, "cdate": 1632875765455, "mdate": null, "ddate": null, "tcdate": 1632875765455, "tmdate": 1697934506494, "tddate": null, "forum": "NkZq4OEYN-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sound Adversarial Audio-Visual Navigation", "authorids": ["~Yinfeng_Yu1", "~Wenbing_Huang1", "~Fuchun_Sun2", "~Changan_Chen2", "~Yikai_Wang2", "~Xiaohong_Liu3"], "authors": ["Yinfeng Yu", "Wenbing Huang", "Fuchun Sun", "Changan Chen", "Yikai Wang", "Xiaohong Liu"], "keywords": [], "abstract": "Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: https://yyf17.github.io/SAAVN .", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|sound_adversarial_audiovisual_navigation", "pdf": "/pdf/892cdd541646cc28a0880494951fbd89079c2a3d.pdf", "one-sentence_summary": "This work aims to do an adversarial sound intervention for robust audio-visual navigation.", "supplementary_material": "/attachment/7645c11196a90a3cf589f48b53a0b82fb5a56a1c.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.10910/code)", "_bibtex": "@inproceedings{\nyu2022sound,\ntitle={Sound Adversarial Audio-Visual Navigation},\nauthor={Yinfeng Yu and Wenbing Huang and Fuchun Sun and Changan Chen and Yikai Wang and Xiaohong Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NkZq4OEYN-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 6}}, {"id": "12RoR2o32T", "original": "sXdqZAvtLR8v", "number": 4618, "cdate": 1632875764764, "mdate": null, "ddate": null, "tcdate": 1632875764764, "tmdate": 1676330444548, "tddate": null, "forum": "12RoR2o32T", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations", "authorids": ["~Aahlad_Manas_Puli1", "~Lily_H_Zhang1", "~Eric_Karl_Oermann1", "~Rajesh_Ranganath2"], "authors": ["Aahlad Manas Puli", "Lily H Zhang", "Eric Karl Oermann", "Rajesh Ranganath"], "keywords": ["spurious correlations", "out of distribution generalization", "ml for health", "representation learning"], "abstract": "In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "puli|outofdistribution_generalization_in_the_presence_of_nuisanceinduced_spurious_correlations", "pdf": "/pdf/7128d52f12e20439db2d07083f3de3995967bb53.pdf", "one-sentence_summary": "This paper build models robust to nuisance-induced spurious correlations by constructing a representation that distills out the influence of the nuisance variables, while also maximizing its information with the label.", "data": "", "_bibtex": "@inproceedings{\npuli2022outofdistribution,\ntitle={Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations},\nauthor={Aahlad Manas Puli and Lily H Zhang and Eric Karl Oermann and Rajesh Ranganath},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=12RoR2o32T}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 6}}, {"id": "OM_lYiHXiCL", "original": "BmArmBBCwr", "number": 4615, "cdate": 1632875764556, "mdate": null, "ddate": null, "tcdate": 1632875764556, "tmdate": 1676330444718, "tddate": null, "forum": "OM_lYiHXiCL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis", "authorids": ["~Junfeng_Guo2", "~Ang_Li1", "~Cong_Liu2"], "authors": ["Junfeng Guo", "Ang Li", "Cong Liu"], "keywords": [], "abstract": "Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor could be embedded in the target DNNs through injecting a backdoor trigger into the training examples,  which can cause the target DNNs misclassify an input attached with the backdoor trigger. Recent backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device de-ployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is a fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution;  a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) algorithm to detect backdoors in black-box neural networks. The AEVA algorithm is based on an extreme value analysis on the adversarial map, computed from the monte-carlo gradient estimation due to the black-box hard-label constraint. Evidenced by extensive experiments across three popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guo|aeva_blackbox_backdoor_detection_using_adversarial_extreme_value_analysis", "pdf": "/pdf/b8ad85b4ddd615a5abac4d7c1d5713fc92b9f0e9.pdf", "_bibtex": "@inproceedings{\nguo2022aeva,\ntitle={{AEVA}: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis},\nauthor={Junfeng Guo and Ang Li and Cong Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OM_lYiHXiCL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "5ECQL05ub0J", "original": "g9Kfym-I2Qa", "number": 4609, "cdate": 1632875764141, "mdate": null, "ddate": null, "tcdate": 1632875764141, "tmdate": 1676330444818, "tddate": null, "forum": "5ECQL05ub0J", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum", "authorids": ["~Kirby_Banman1", "~Garnet_Liam_Peet-Pare1", "~Nidhi_Hegde1", "~Alona_Fyshe1", "~Martha_White1"], "authors": ["Kirby Banman", "Garnet Liam Peet-Pare", "Nidhi Hegde", "Alona Fyshe", "Martha White"], "keywords": ["optimization", "momentum", "stochastic gradient descent", "non-iid sampling"], "abstract": "Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid  sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm.", "one-sentence_summary": "We show that SGDm under covariate shift with fixed step-size can be unstable and diverge due to a phenomenon known as parametric resonance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "banman|resonance_in_weight_space_covariate_shift_can_drive_divergence_of_sgd_with_momentum", "pdf": "/pdf/967691b8c1cb517500d87dfd7dbf7dd6293c0e89.pdf", "supplementary_material": "/attachment/0945c42d80cb1110d6da871961508649b30acbe7.zip", "_bibtex": "@inproceedings{\nbanman2022resonance,\ntitle={Resonance in Weight Space: Covariate Shift Can Drive Divergence of {SGD} with Momentum},\nauthor={Kirby Banman and Garnet Liam Peet-Pare and Nidhi Hegde and Alona Fyshe and Martha White},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5ECQL05ub0J}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "WqoBaaPHS-", "original": "R9VjyF_alUQ", "number": 4592, "cdate": 1632875762987, "mdate": null, "ddate": null, "tcdate": 1632875762987, "tmdate": 1676330445201, "tddate": null, "forum": "WqoBaaPHS-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Top-label calibration and multiclass-to-binary reductions", "authorids": ["~Chirag_Gupta1", "~Aaditya_Ramdas2"], "authors": ["Chirag Gupta", "Aaditya Ramdas"], "keywords": ["calibration", "multiclass", "uncertainty quantification", "distribution-free", "histogram binning"], "abstract": "We propose a new notion of multiclass calibration called top-label calibration. A classifier is said to be top-label calibrated if the reported probability for the predicted class label---the top-label---is calibrated, conditioned on the top-label. This conditioning is essential for practical utility of the calibration property, since the top-label is always reported and we must condition on what is reported. However, the popular notion of confidence calibration erroneously skips this conditioning. Furthermore, we outline a multiclass-to-binary (M2B) reduction framework that unifies confidence, top-label, and class-wise calibration, among others. As its name suggests, M2B works by reducing multiclass calibration to different binary calibration problems; various types of multiclass calibration can then be achieved using simple binary calibration routines. We instantiate the M2B framework with the well-studied histogram binning (HB) binary calibrator, and prove that the overall procedure is multiclass calibrated without making any assumptions on the underlying data distribution. In an empirical evaluation with four deep net architectures on CIFAR-10 and CIFAR-100, we find that the M2B + HB procedure achieves lower top-label and class-wise calibration error than other approaches such as temperature scaling. Code for this work is available at https://github.com/aigen/df-posthoc-calibration.", "one-sentence_summary": "We propose top-label calibration, a new and arguably natural notion for multiclass calibration, along with 'wrapper' calibration algorithms that reduce multiclass calibration to binary calibration.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gupta|toplabel_calibration_and_multiclasstobinary_reductions", "pdf": "/pdf/a580ad8d84d1a31adcccb9f9e2102c3b503121df.pdf", "supplementary_material": "/attachment/8c0bb4e26bc860bba21f67effcdb9613904271cd.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\ngupta2022toplabel,\ntitle={Top-label calibration and multiclass-to-binary reductions},\nauthor={Chirag Gupta and Aaditya Ramdas},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WqoBaaPHS-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "JfaWawZ8BmX", "original": "lF-Kjj8LkOP", "number": 4589, "cdate": 1632875762790, "mdate": null, "ddate": null, "tcdate": 1632875762790, "tmdate": 1676330445354, "tddate": null, "forum": "JfaWawZ8BmX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Anisotropic Random Feature Regression in High Dimensions", "authorids": ["~Gabriel_Mel1", "~Jeffrey_Pennington1"], "authors": ["Gabriel Mel", "Jeffrey Pennington"], "keywords": ["random feature models", "high dimensional asymptotics", "generalization", "learning curves", "double descent", "multiple descent", "alignment"], "abstract": "In contrast to standard statistical wisdom, modern learning algorithms typically find their best performance in the overparameterized regime in which the model has many more parameters than needed to fit the training data. A growing number of recent works have shown that random feature models can offer a detailed theoretical explanation for this unexpected behavior, but typically these analyses have utilized isotropic distributional assumptions on the underlying data generation process, thereby failing to provide a realistic characterization of real-world models that are designed to identify and harness the structure in natural data. In this work, we examine the high-dimensional asymptotics of random feature regression in the presence of structured data, allowing for arbitrary input correlations and arbitrary alignment between the data and the weights of the target function. We define a partial order on the space of weight-data alignments and prove that generalization performance improves in response to stronger alignment. We also clarify several previous observations in the literature by distinguishing the behavior of the sample-wise and parameter-wise learning curves, finding that sample-wise multiple descent can occur at scales dictated by the eigenstructure of the data covariance, but that parameter-wise multiple descent is limited to double descent, although strong anisotropy can induce additional signatures such as wide plateaus and steep cliffs. Finally, these signatures are related to phase transitions in the spectrum of the feature kernel matrix, and unlike the double descent peak, persist even under optimal regularization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mel|anisotropic_random_feature_regression_in_high_dimensions", "pdf": "/pdf/bc2ddad146bd93609c8510aac28ae824072d1832.pdf", "one-sentence_summary": "We derive exact asymptotic formulas for the total error, bias, and variance of random feature regression with anisotropic inputs and target weights, and identify a new type of singularity in sample-wise learning curves. ", "supplementary_material": "/attachment/3b2a48f01870eb81d540e9c5ce4f7b87bd016e94.zip", "_bibtex": "@inproceedings{\nmel2022anisotropic,\ntitle={Anisotropic Random Feature Regression in High Dimensions},\nauthor={Gabriel Mel and Jeffrey Pennington},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JfaWawZ8BmX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "L01Nn_VJ9i", "original": "RLGcEEsJEgG", "number": 4586, "cdate": 1632875762583, "mdate": null, "ddate": null, "tcdate": 1632875762583, "tmdate": 1676330445359, "tddate": null, "forum": "L01Nn_VJ9i", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future", "authorids": ["~Harshavardhan_Kamarthi1", "~Alexander_Rodr\u00edguez1", "~B._Aditya_Prakash2"], "authors": ["Harshavardhan Kamarthi", "Alexander Rodr\u00edguez", "B. Aditya Prakash"], "keywords": ["Epidemic Forecasting", "Data revisions", "Graph Representation learning", "Time Series Forecasting"], "abstract": "For real-time forecasting in domains like public health and macroeconomics, data collection is a non-trivial and demanding task. Often after being initially released, it undergoes several revisions later (maybe due to human or technical constraints) - as a result, it may take weeks until the data reaches a stable value. This so-called \u2018backfill\u2019 phenomenon and its effect on model performance have been barely addressed in the prior literature. In this paper, we introduce the multi-variate backfill problem using COVID-19 as the motivating example. \nWe construct a detailed dataset composed of relevant signals over the past year of the pandemic. \nWe then systematically characterize several patterns in backfill dynamics and leverage our observations for formulating a novel problem and neural framework, Back2Future, that aims to refines a given model's predictions in real-time. Our extensive experiments demonstrate that our method refines the performance of the diverse set of top models for COVID-19 forecasting and GDP growth forecasting. Specifically, we show that Back2Future refined top COVID-19 models by 6.65% to 11.24% and yield an 18% improvement over non-trivial baselines. In addition, we show that our model improves model evaluation too; hence policy-makers can better understand the true accuracy of forecasting models in real-time.", "one-sentence_summary": "We study the problem of multi-variate backfill for both features and targets and show how to leverage our insights for more general neural framework to improve both model predictions and evaluation", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kamarthi|back2future_leveraging_backfill_dynamics_for_improving_realtime_predictions_in_future", "pdf": "/pdf/5ff5a41a0773c6764d009a86a74cce3dd35e8ec3.pdf", "supplementary_material": "/attachment/2484a196fd983b98ffd8c549360fbe0f13543350.zip", "code": "", "_bibtex": "@inproceedings{\nkamarthi2022backfuture,\ntitle={Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future},\nauthor={Harshavardhan Kamarthi and Alexander Rodr{\\'\\i}guez and B. Aditya Prakash},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=L01Nn_VJ9i}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "lrocYB-0ST2", "original": "zFLFovIhB4E", "number": 4570, "cdate": 1632875761524, "mdate": null, "ddate": null, "tcdate": 1632875761524, "tmdate": 1676330446207, "tddate": null, "forum": "lrocYB-0ST2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Approximation and Learning with Deep Convolutional Models: a Kernel Perspective", "authorids": ["~Alberto_Bietti1"], "authors": ["Alberto Bietti"], "keywords": ["kernel methods", "deep learning theory", "convolution", "approximation", "generalization"], "abstract": "The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks. These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias. We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities.", "one-sentence_summary": "We study the inductive bias of multi-layer convolutional models through a kernel lens, showing generalization benefits of various architectural choices such as locality, depth, and pooling layers.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bietti|approximation_and_learning_with_deep_convolutional_models_a_kernel_perspective", "pdf": "/pdf/35eeb8c9531f39eb14e07db8fb296d38b7f1a369.pdf", "supplementary_material": "/attachment/f97f90f40e8281a4aaf207b0702056a29a294971.zip", "code": "", "_bibtex": "@inproceedings{\nbietti2022approximation,\ntitle={Approximation and Learning with Deep Convolutional Models: a Kernel Perspective},\nauthor={Alberto Bietti},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lrocYB-0ST2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "vgqS1vkkCbE", "original": "EGWmWbNRAzPC", "number": 4569, "cdate": 1632875761459, "mdate": null, "ddate": null, "tcdate": 1632875761459, "tmdate": 1676330446207, "tddate": null, "forum": "vgqS1vkkCbE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning", "authorids": ["~Dhruv_Shah1", "~Peng_Xu9", "~Yao_Lu13", "~Ted_Xiao1", "~Alexander_T_Toshev1", "~Sergey_Levine1", "~brian_ichter1"], "authors": ["Dhruv Shah", "Peng Xu", "Yao Lu", "Ted Xiao", "Alexander T Toshev", "Sergey Levine", "brian ichter"], "keywords": ["hierarchical reinforcement learning", "planning", "representation learning", "robotics"], "abstract": "Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies can further improve on this by abstracting the space states as well. We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies. We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a  representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods.", "pdf": "/pdf/c49d03d6fc757e37898cc5399159de2e30589146.pdf", "one-sentence_summary": "We introduce value function spaces, a learned representation of state through the values of low-level skills, which capture affordances and ignores distractors to enable long-horizon reasoning and zero-shot generalization.", "supplementary_material": "", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shah|value_function_spaces_skillcentric_state_abstractions_for_longhorizon_reasoning", "_bibtex": "@inproceedings{\nshah2022value,\ntitle={Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning},\nauthor={Dhruv Shah and Alexander T Toshev and Sergey Levine and brian ichter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vgqS1vkkCbE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "gNp54NxHUPJ", "original": "5e0Zyvu70R", "number": 4544, "cdate": 1632875759918, "mdate": null, "ddate": null, "tcdate": 1632875759918, "tmdate": 1676330446609, "tddate": null, "forum": "gNp54NxHUPJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fast Regression for Structured Inputs", "authorids": ["~Raphael_A_Meyer1", "~Cameron_N_Musco1", "~Christopher_P_Musco1", "~David_Woodruff1", "~Samson_Zhou1"], "authors": ["Raphael A Meyer", "Cameron N Musco", "Christopher P Musco", "David Woodruff", "Samson Zhou"], "keywords": ["regression", "sublinear time algorithm", "structured input"], "abstract": "We study the $\\ell_p$ regression problem, which requires finding $\\mathbf{x}\\in\\mathbb R^{d}$ that minimizes $\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\|_p$ for a matrix $\\mathbf{A}\\in\\mathbb R^{n \\times d}$ and response vector $\\mathbf{b}\\in\\mathbb R^{n}$. There has been recent interest in developing subsampling methods for this problem that can outperform standard techniques when $n$ is very large. However, all known subsampling approaches have run time that depends exponentially on $p$, typically, $d^{\\mathcal{O}(p)}$, which can be prohibitively expensive. \n\nWe improve on this work by showing that for a large class of common \\emph{structured matrices}, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, there are subsampling based methods for $\\ell_p$ regression that depend polynomially on $p$. For example, we give an algorithm for $\\ell_p$ regression on Vandermonde matrices that runs in time $\\mathcal{O}(n\\log^3 n+(dp^2)^{0.5+\\omega}\\cdot\\text{polylog}\\,n)$, where $\\omega$ is the exponent of matrix multiplication. The polynomial dependence on $p$ crucially allows our algorithms to extend naturally to efficient algorithms for $\\ell_\\infty$ regression, via approximation of $\\ell_\\infty$ by $\\ell_{\\mathcal{O}(\\log n)}$. Of practical interest, we also develop a new subsampling algorithm for $\\ell_p$ regression for arbitrary matrices, which is simpler than previous approaches for $p \\ge 4$.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "meyer|fast_regression_for_structured_inputs", "pdf": "/pdf/a76864e8c343a5dcb3414cc8caa6fc2fdd2afc19.pdf", "supplementary_material": "/attachment/33c6a51bd06a695c841c65487e83bf29502afc12.zip", "_bibtex": "@inproceedings{\nmeyer2022fast,\ntitle={Fast Regression for Structured Inputs},\nauthor={Raphael A Meyer and Cameron N Musco and Christopher P Musco and David Woodruff and Samson Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gNp54NxHUPJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 4}}, {"id": "qhC8mr2LEKq", "original": "6DY9XpRajsio", "number": 4543, "cdate": 1632875759849, "mdate": null, "ddate": null, "tcdate": 1632875759849, "tmdate": 1697934511774, "tddate": null, "forum": "qhC8mr2LEKq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CrossBeam: Learning to Search in Bottom-Up Program Synthesis", "authorids": ["~Kensen_Shi1", "~Hanjun_Dai1", "~Kevin_Ellis1", "~Charles_Sutton1"], "authors": ["Kensen Shi", "Hanjun Dai", "Kevin Ellis", "Charles Sutton"], "keywords": ["Program Synthesis", "Bottom-Up Search"], "abstract": "Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CrossBeam, uses the neural model to choose how to combine previously-explored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CrossBeam is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CrossBeam in two very different domains, string manipulation and logic programming. We observe that CrossBeam learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.\n", "one-sentence_summary": "We propose training a neural model to learn a hands-on search policy for bottom-up program synthesis, in an effort to tame the search space blowup.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shi|crossbeam_learning_to_search_in_bottomup_program_synthesis", "pdf": "/pdf/d098dde7689c9940303ddd8c11f5f44e8b866692.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.10452/code)", "_bibtex": "@inproceedings{\nshi2022crossbeam,\ntitle={CrossBeam: Learning to Search in Bottom-Up Program Synthesis},\nauthor={Kensen Shi and Hanjun Dai and Kevin Ellis and Charles Sutton},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qhC8mr2LEKq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "M6M8BEmd6dq", "original": "WwHWh6FrcuY", "number": 4542, "cdate": 1632875759780, "mdate": null, "ddate": null, "tcdate": 1632875759780, "tmdate": 1676330446723, "tddate": null, "forum": "M6M8BEmd6dq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning", "authorids": ["~Seng_Pei_Liew1", "~Tsubasa_Takahashi1", "~Michihiko_Ueno1"], "authors": ["Seng Pei Liew", "Tsubasa Takahashi", "Michihiko Ueno"], "keywords": ["Differential Privacy", "Generative Model"], "abstract": "We propose a new framework of synthesizing data using deep generative models in a differentially private manner.\nWithin our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data.\nHence, no extra privacy costs or model constraints are incurred, in contrast to popular gradient sanitization approaches, which, among other issues, cause degradation in privacy guarantees as the training iteration increases.\nWe demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well.\nOur proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liew|pearl_data_synthesis_via_private_embeddings_and_adversarial_reconstruction_learning", "pdf": "/pdf/3efedef6ce8396ae22861cd7154606c25bd31e95.pdf", "data": "", "code": "", "_bibtex": "@inproceedings{\nliew2022pearl,\ntitle={{PEARL}: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning},\nauthor={Seng Pei Liew and Tsubasa Takahashi and Michihiko Ueno},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=M6M8BEmd6dq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "aOX3a9q3RVV", "original": "FoplHL1Z3aGv", "number": 4523, "cdate": 1632875758497, "mdate": null, "ddate": null, "tcdate": 1632875758497, "tmdate": 1676330447840, "tddate": null, "forum": "aOX3a9q3RVV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Divisive Feature Normalization Improves Image Recognition Performance in AlexNet", "authorids": ["~Michelle_Miller3", "~SueYeon_Chung1", "~Kenneth_D._Miller2"], "authors": ["Michelle Miller", "SueYeon Chung", "Kenneth D. Miller"], "keywords": ["divisive normalization", "AlexNet", "ImageNet", "CIFAR-100", "manifold capacity", "sparsity", "receptive fields", "Batch Normalization", "Group Normalization", "Layer Normalization"], "abstract": "Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas. To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters. Developing features were arranged in a line topology, with the influence between features determined by an exponential function of the distance between them. We compared an AlexNet model with no normalization or with canonical normalizations (Batch, Group, Layer) to the same models with divisive normalization added. Divisive normalization always improved performance for models with batch or group or no normalization, generally by 1-2 percentage points, on both the CIFAR-100 and ImageNet databases. To gain insight into mechanisms underlying the improved performance, we examined several aspects of network representations. In the early layers both canonical and divisive normalizations reduced manifold capacities and increased average dimension of the individual categorical manifolds. In later layers the capacity was higher and manifold dimension lower for models roughly in order of their performance improvement. Examining the sparsity of activations across a given layer, divisive normalization layers increased sparsity, while the canonical normalization layers decreased it. Nonetheless, in the final layer, the sparseness of activity increased in the order of no normalization, divisive, com- bined, and canonical. We also investigated how the receptive fields (RFs) in the first convolutional layer (where RFs are most interpretable) change with normalization. Divisive normalization enhanced RF Fourier power at low wavelengths, while divisive+canonical enhanced power at mid (batch, group) or low (layer) wavelengths, compared to canonical alone or no normalization. In conclusion, divisive normalization enhances image recognition performance, most strongly when combined with canonical normalization, and in doing so it reduces manifold capacity and sparsity in early layers while increasing them in final layers, and increases low- or mid-wavelength power in the first-layer receptive fields.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "miller|divisive_feature_normalization_improves_image_recognition_performance_in_alexnet", "pdf": "/pdf/452011d69839dd4fa39ba4bec882b24cb5bb2649.pdf", "one-sentence_summary": "DIVISIVE FEATURE NORMALIZATION IMPROVES IMAGE RECOGNITION PERFORMANCE AND IN- CREASES MANIFOLD CAPACITY, SPARSITY, AND LOW-FREQUENCY REPRESENTATION IN DEEP NETS", "_bibtex": "@inproceedings{\nmiller2022divisive,\ntitle={Divisive Feature Normalization Improves Image Recognition Performance in AlexNet},\nauthor={Michelle Miller and SueYeon Chung and Kenneth D. Miller},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aOX3a9q3RVV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "bTteFbU99ye", "original": "74JjSWH2fdFx", "number": 4509, "cdate": 1632875757549, "mdate": null, "ddate": null, "tcdate": 1632875757549, "tmdate": 1697934513701, "tddate": null, "forum": "bTteFbU99ye", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Evaluating Distributional Distortion in Neural Language Modeling", "authorids": ["~Benjamin_LeBrun1", "~Alessandro_Sordoni2", "~Timothy_J._O'Donnell1"], "authors": ["Benjamin LeBrun", "Alessandro Sordoni", "Timothy J. O'Donnell"], "keywords": [], "abstract": "A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate.  As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for less-probable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lebrun|evaluating_distributional_distortion_in_neural_language_modeling", "pdf": "/pdf/c22ea9d1df97b96c390eb350b4c09eb8e2388128.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.12788/code)", "_bibtex": "@inproceedings{\nlebrun2022evaluating,\ntitle={Evaluating Distributional Distortion in Neural Language Modeling},\nauthor={Benjamin LeBrun and Alessandro Sordoni and Timothy J. O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bTteFbU99ye}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "r5qumLiYwf9", "original": "T70QgJawkK6", "number": 4501, "cdate": 1632875757005, "mdate": null, "ddate": null, "tcdate": 1632875757005, "tmdate": 1676330448204, "tddate": null, "forum": "r5qumLiYwf9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining", "authorids": ["~Ahmed_Imtiaz_Humayun1", "~Randall_Balestriero1", "~Richard_Baraniuk1"], "authors": ["Ahmed Imtiaz Humayun", "Randall Balestriero", "Richard Baraniuk"], "keywords": ["Deep Generative Networks", "Uniform Sampling", "Fairness", "Data Augmentation"], "abstract": "Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g. the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ). {\\em These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond.} In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision \\& recall by 4.12\\% \\& 3.01\\% and decreases gender bias by 41.2\\%, without requiring labels or retraining.", "one-sentence_summary": "We propose a differential-geometry-based technique to provably sample uniformly from the data manifold of a trained Deep Generative Network without the need for retraining.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "humayun|magnet_uniform_sampling_from_deep_generative_network_manifolds_without_retraining", "pdf": "/pdf/e9c0ccdf7ecc11a5666ac100d75f89816ce7c0f7.pdf", "_bibtex": "@inproceedings{\nhumayun2022magnet,\ntitle={Ma{GNET}: Uniform Sampling from Deep Generative Network Manifolds Without Retraining},\nauthor={Ahmed Imtiaz Humayun and Randall Balestriero and Richard Baraniuk},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=r5qumLiYwf9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "xnYACQquaGV", "original": "_FwD4VRCrKn", "number": 4495, "cdate": 1632875756593, "mdate": null, "ddate": null, "tcdate": 1632875756593, "tmdate": 1676330449034, "tddate": null, "forum": "xnYACQquaGV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Contextual Bandits with Deep Representation and Shallow Exploration", "authorids": ["~Pan_Xu1", "~Zheng_Wen1", "~Handong_Zhao3", "~Quanquan_Gu1"], "authors": ["Pan Xu", "Zheng Wen", "Handong Zhao", "Quanquan Gu"], "keywords": ["neural network", "deep representation learning"], "abstract": "We study neural contextual bandits, a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). We prove that under standard assumptions, our proposed algorithm achieves $\\tilde{O}(\\sqrt{T})$ finite-time regret, where $T$ is the learning time horizon. Compared with existing neural contextual bandit algorithms, our approach is computationally much more efficient since it only needs to explore in the last layer of the deep neural network.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|neural_contextual_bandits_with_deep_representation_and_shallow_exploration", "pdf": "/pdf/c6ee94e7fd22670895280aaf06535b6373d428eb.pdf", "one-sentence_summary": "A new neural network based algorithm for contextual bandit problems with theoretical guarantees and empirical advantages.", "supplementary_material": "/attachment/1468959f4251fd26e9ee75f2a76ef524622386c9.zip", "_bibtex": "@inproceedings{\nxu2022neural,\ntitle={Neural Contextual Bandits with Deep Representation and Shallow Exploration},\nauthor={Pan Xu and Zheng Wen and Handong Zhao and Quanquan Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xnYACQquaGV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "NoB8YgRuoFU", "original": "587jtafeI0", "number": 4494, "cdate": 1632875756528, "mdate": null, "ddate": null, "tcdate": 1632875756528, "tmdate": 1676330449032, "tddate": null, "forum": "NoB8YgRuoFU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PI3NN: Out-of-distribution-aware Prediction Intervals from Three Neural Networks", "authorids": ["~Siyan_Liu1", "~Pei_Zhang6", "~Dan_Lu1", "~Guannan_Zhang1"], "authors": ["Siyan Liu", "Pei Zhang", "Dan Lu", "Guannan Zhang"], "keywords": [], "abstract": "We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, they usually underestimate uncertainties of out-of-distribution (OOD) samples leading to over-confident PIs. Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Additionally, PI3NN does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples. Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|pi3nn_outofdistributionaware_prediction_intervals_from_three_neural_networks", "pdf": "/pdf/84a3741f26e65df3c7b232779bcfb5dac283d41e.pdf", "supplementary_material": "/attachment/9f1a45eff2b80564d6dd33651f9efd61ac745e40.zip", "_bibtex": "@inproceedings{\nliu2022pinn,\ntitle={{PI}3{NN}: Out-of-distribution-aware Prediction Intervals from Three Neural Networks},\nauthor={Siyan Liu and Pei Zhang and Dan Lu and Guannan Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NoB8YgRuoFU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "kj0_45Y4r9i", "original": "JfHu-xAajzT", "number": 4489, "cdate": 1632875756187, "mdate": null, "ddate": null, "tcdate": 1632875756187, "tmdate": 1676330449212, "tddate": null, "forum": "kj0_45Y4r9i", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Discriminative Similarity for Data Clustering", "authorids": ["~Yingzhen_Yang1", "~Ping_Li3"], "authors": ["Yingzhen Yang", "Ping Li"], "keywords": ["Discriminative Similarity", "Rademacher Complexity", "Generalization Bound", "Data Clustering"], "abstract": "Similarity-based clustering methods separate data into clusters according to the pairwise similarity between the data, and the pairwise similarity is crucial for their performance. In this paper, we propose {\\em Clustering by  Discriminative Similarity (CDS)}, a novel method which learns discriminative similarity for data clustering. CDS learns an unsupervised similarity-based classifier from each data partition, and searches for the optimal partition of the data by minimizing the generalization error of the learnt classifiers associated with the data partitions. By generalization analysis via Rademacher complexity, the generalization error bound for the unsupervised similarity-based classifier is expressed as the sum of discriminative similarity between the data from different classes. It is proved that the derived discriminative similarity can also be induced by the integrated squared error bound for kernel density classification. In order to evaluate the performance of the proposed discriminative similarity, we propose a new clustering method using a kernel as the similarity function, CDS via unsupervised kernel classification (CDSK), with its effectiveness demonstrated by experimental results.", "one-sentence_summary": "We present a novel discriminative similarity for data clustering, and the discriminative similarity is induced by generalization error bound for unsupervised classifier ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|discriminative_similarity_for_data_clustering", "pdf": "/pdf/b159fb24355dd1bf64f74a757973bbc8cc96d57e.pdf", "data": "", "_bibtex": "@inproceedings{\nyang2022discriminative,\ntitle={Discriminative Similarity for Data Clustering},\nauthor={Yingzhen Yang and Ping Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kj0_45Y4r9i}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "q4tZR1Y-UIs", "original": "TJl_HOwHWJq", "number": 4485, "cdate": 1632875755958, "mdate": null, "ddate": null, "tcdate": 1632875755958, "tmdate": 1676330449325, "tddate": null, "forum": "q4tZR1Y-UIs", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation", "authorids": ["~Yuqing_Du1", "~Pieter_Abbeel2", "~Aditya_Grover1"], "authors": ["Yuqing Du", "Pieter Abbeel", "Aditya Grover"], "keywords": ["curriculum generation", "unsupervised reinforcement learning", "goal conditioned reinforcement learning", "multi agent"], "abstract": "We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anti-catastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "du|it_takes_four_to_tango_multiagent_self_play_for_automatic_curriculum_generation", "pdf": "/pdf/68a6237e79699c723ce9c9c39537422391df3e2b.pdf", "supplementary_material": "/attachment/047ac30a146793649501c5ef3f9aac75db853146.zip", "_bibtex": "@inproceedings{\ndu2022it,\ntitle={It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation},\nauthor={Yuqing Du and Pieter Abbeel and Aditya Grover},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=q4tZR1Y-UIs}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "HOjLHrlZhmx", "original": "UYlV_r98EVh", "number": 4469, "cdate": 1632875754953, "mdate": null, "ddate": null, "tcdate": 1632875754953, "tmdate": 1676330449728, "tddate": null, "forum": "HOjLHrlZhmx", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing", "authorids": ["~Fan_Wu6", "~Linyi_Li1", "~Zijian_Huang2", "~Yevgeniy_Vorobeychik1", "~Ding_Zhao1", "~Bo_Li19"], "authors": ["Fan Wu", "Linyi Li", "Zijian Huang", "Yevgeniy Vorobeychik", "Ding Zhao", "Bo Li"], "keywords": [], "abstract": "As reinforcement learning (RL) has achieved great success and been even adopted in safety-critical domains such as autonomous vehicles, a range of empirical studies have been conducted to improve its robustness against adversarial attacks. However, how to certify its robustness with theoretical guarantees still remains challenging. In this paper, we present the \ufb01rst uni\ufb01ed framework CROP (Certifying Robust Policies for RL) to provide robustness certi\ufb01cation on both action and reward levels. In particular, we propose two robustness certi\ufb01cation criteria: robustness of per-state actions and lower bound of cumulative rewards. We then develop a local smoothing algorithm for policies derived from Q-functions to guarantee the robustness of actions taken along the trajectory; we also develop a global smoothing algorithm for certifying the lower bound of a \ufb01nite-horizon cumulative reward, as well as a novel local smoothing algorithm to perform adaptive search in order to obtain tighter reward certi\ufb01cation. Empirically, we apply CROP to evaluate several existing empirically robust RL algorithms, including adversarial training and different robust regularization, in four environments (two representative Atari games, Highway, and CartPole). Furthermore, by evaluating these algorithms against adversarial attacks, we demonstrate that our certi\ufb01cations are often tight. All experiment results are available at website https://crop-leaderboard.github.io.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|crop_certifying_robust_policies_for_reinforcement_learning_through_functional_smoothing", "pdf": "/pdf/b79f87ced196c2a5a13ca10bae3d39a8924b08b8.pdf", "supplementary_material": "/attachment/526a90f67b614ec1870f2265a9abb52a644f13e2.zip", "_bibtex": "@inproceedings{\nwu2022crop,\ntitle={{CROP}: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing},\nauthor={Fan Wu and Linyi Li and Zijian Huang and Yevgeniy Vorobeychik and Ding Zhao and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HOjLHrlZhmx}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "CCu6RcUMwK0", "original": "OVUzHQaNDS", "number": 4448, "cdate": 1632875753554, "mdate": null, "ddate": null, "tcdate": 1632875753554, "tmdate": 1697934518905, "tddate": null, "forum": "CCu6RcUMwK0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Link Prediction with Walk Pooling", "authorids": ["~Liming_Pan1", "~Cheng_Shi2", "~Ivan_Dokmani\u01071"], "authors": ["Liming Pan", "Cheng Shi", "Ivan Dokmani\u0107"], "keywords": ["Graph neural network", "Link prediction", "Random walk", "Graph topology."], "abstract": "Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes. Topology, however, is represented indirectly; state-of-the-art methods based on subgraph classification label nodes with distance to the target link, so that, although topological information is present, it is tempered by pooling. This makes it challenging to leverage features like loops and motifs associated with network formation mechanisms. We propose a link prediction algorithm based on a new pooling scheme called WalkPool. WalkPool combines the expressivity of topological heuristics with the feature-learning ability of neural networks. It summarizes a putative link by random walk probabilities of adjacent paths. Instead of extracting transition probabilities from the original graph, it computes the transition matrix of a ``predictive'' latent graph by applying attention to learned features; this may be interpreted as feature-sensitive topology fingerprinting. WalkPool can leverage unsupervised node features or be combined with GNNs and trained end-to-end. It outperforms state-of-the-art methods on all common link prediction benchmarks, both homophilic and heterophilic, with and without node attributes. Applying WalkPool to a set of unsupervised GNNs significantly improves prediction accuracy, suggesting that it may be used as a general-purpose graph pooling scheme.   ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pan|neural_link_prediction_with_walk_pooling", "pdf": "/pdf/ad031c5e836c55357e2f13cdb18fa502a7eecc80.pdf", "supplementary_material": "/attachment/5a654633d2e6d28cb2b30509b34926fd851a1c48.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.04375/code)", "_bibtex": "@inproceedings{\npan2022neural,\ntitle={Neural Link Prediction with Walk Pooling},\nauthor={Liming Pan and Cheng Shi and Ivan Dokmani{\\'c}},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CCu6RcUMwK0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "YeShU5mLfLt", "original": "a160ykT-syy", "number": 4436, "cdate": 1632875752760, "mdate": null, "ddate": null, "tcdate": 1632875752760, "tmdate": 1676330452004, "tddate": null, "forum": "YeShU5mLfLt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Convergence of Certified Robust Training with Interval Bound Propagation", "authorids": ["~Yihan_Wang2", "~Zhouxing_Shi1", "~Quanquan_Gu1", "~Cho-Jui_Hsieh1"], "authors": ["Yihan Wang", "Zhouxing Shi", "Quanquan Gu", "Cho-Jui Hsieh"], "keywords": ["Certified robustness", "Adversarial robustness", "Convergence"], "abstract": "Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature. In this paper, we present a theoretical analysis on the convergence of IBP training. With an overparameterized assumption, we analyze the convergence of IBP robust training. We show that when using  IBP training to train a randomly initialized two-layer ReLU neural network with logistic loss, gradient descent can linearly converge to zero robust training error with a high probability if  we have sufficiently small perturbation radius and large network width.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|on_the_convergence_of_certified_robust_training_with_interval_bound_propagation", "pdf": "/pdf/4e7f7f34a6f11b062e283b3a04324bb373e39067.pdf", "one-sentence_summary": "We present the first theoretical analysis on the convergence of certified robust training with interval bound propagation.", "_bibtex": "@inproceedings{\nwang2022on,\ntitle={On the Convergence of Certified Robust Training with Interval Bound Propagation},\nauthor={Yihan Wang and Zhouxing Shi and Quanquan Gu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YeShU5mLfLt}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "sX3XaHwotOg", "original": "3xkYmrpZMjX", "number": 4429, "cdate": 1632875752368, "mdate": null, "ddate": null, "tcdate": 1632875752368, "tmdate": 1697934520507, "tddate": null, "forum": "sX3XaHwotOg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators", "authorids": ["~Yu_Meng1", "~Chenyan_Xiong1", "~Payal_Bajaj2", "~saurabh_tiwary1", "~Paul_N._Bennett1", "~Jiawei_Han1", "~Xia_Song1"], "authors": ["Yu Meng", "Chenyan Xiong", "Payal Bajaj", "saurabh tiwary", "Paul N. Bennett", "Jiawei Han", "Xia Song"], "keywords": ["Language Model Pretraining"], "abstract": "We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs' outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "meng|pretraining_text_encoders_with_adversarial_mixture_of_training_signal_generators", "pdf": "/pdf/4127a755f1e5ee998e6423f7a8d734f9e88b8cab.pdf", "one-sentence_summary": "We present AMOS, a new method that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.03243/code)", "_bibtex": "@inproceedings{\nmeng2022pretraining,\ntitle={Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators},\nauthor={Yu Meng and Chenyan Xiong and Payal Bajaj and saurabh tiwary and Paul N. Bennett and Jiawei Han and Xia Song},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=sX3XaHwotOg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "0jP2n0YFmKG", "original": "f6c4B-7AYfd", "number": 4408, "cdate": 1632875750942, "mdate": null, "ddate": null, "tcdate": 1632875750942, "tmdate": 1676330452434, "tddate": null, "forum": "0jP2n0YFmKG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations", "authorids": ["~Anuroop_Sriram1", "~Abhishek_Das1", "~Brandon_M_Wood1", "~Siddharth_Goyal2", "~C._Lawrence_Zitnick2"], "authors": ["Anuroop Sriram", "Abhishek Das", "Brandon M Wood", "Siddharth Goyal", "C. Lawrence Zitnick"], "keywords": ["Graph Neural Networks", "Atomic Simulations", "Computational Chemistry"], "abstract": "Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the recently proposed DimeNet++ and GemNet models by over an order of magnitude in the number of parameters. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric on the S2EF task and 2) 21% on the AFbT metric on the IS2RS task, establishing new state-of-the-art results.", "pdf": "/pdf/d00345679f2290baeabb225428516fad14fea79e.pdf", "one-sentence_summary": "We scale GNNs used for modeling atomic simulations by an order of magnitude and obtain large performance improvements on the Open Catalyst 2020 dataset.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sriram|towards_training_billion_parameter_graph_neural_networks_for_atomic_simulations", "_bibtex": "@inproceedings{\nsriram2022towards,\ntitle={Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations},\nauthor={Anuroop Sriram and Abhishek Das and Brandon M Wood and C. Lawrence Zitnick},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0jP2n0YFmKG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "shbAgEsk3qM", "original": "gU5cgt5F8Sa", "number": 4376, "cdate": 1632875748869, "mdate": null, "ddate": null, "tcdate": 1632875748869, "tmdate": 1676330454106, "tddate": null, "forum": "shbAgEsk3qM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding and Leveraging Overparameterization in Recursive Value Estimation", "authorids": ["~Chenjun_Xiao1", "~Bo_Dai1", "~Jincheng_Mei1", "~Oscar_A_Ramirez1", "~Ramki_Gummadi1", "~Chris_Harris1", "~Dale_Schuurmans1"], "authors": ["Chenjun Xiao", "Bo Dai", "Jincheng Mei", "Oscar A Ramirez", "Ramki Gummadi", "Chris Harris", "Dale Schuurmans"], "keywords": ["Temporal Difference Learning", "Residual Minimization", "Value Estimation", "Overparameterization"], "abstract": "The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization.  Current deep architectures, however, operate in an overparameterized regime where approximation error is not necessarily a bottleneck.  To better understand the utility of deep models in RL we present an analysis of recursive value estimation using \\emph{overparameterized} linear representations that provides useful, transferable findings.  First, we show that classical updates such as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to \\emph{different} fixed points than residual minimization (RM) in the overparameterized linear case.  We then develop a unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to alternative constraints.  A practical consequence is that RM can be modified by a simple alteration of the backup targets to obtain the same fixed points as FVI and TD (when they converge), while universally ensuring stability.  Further, we provide an analysis of the generalization error of these methods, demonstrating per iterate bounds on the value prediction error of FVI, and fixed point bounds for TD and RM.  \nGiven this understanding, we then develop new algorithmic tools for improving recursive value estimation with deep models. \nIn particular, we extract two regularizers that penalize out-of-span top-layer weights and co-linearity in top-layer features respectively.  Empirically we find that these regularizers dramatically improve the stability of TD and FVI, while allowing RM to match and even sometimes surpass their generalization performance with assured stability. ", "one-sentence_summary": "We present an analysis of value estimation under overparameterized linear representations, and develop new algorithmic tools for improving recursive value estimation with deep models based on the new findings.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xiao|understanding_and_leveraging_overparameterization_in_recursive_value_estimation", "pdf": "/pdf/c5131ad5930c1a9f32ede673f284175158a75792.pdf", "_bibtex": "@inproceedings{\nxiao2022understanding,\ntitle={Understanding and Leveraging Overparameterization in Recursive Value Estimation},\nauthor={Chenjun Xiao and Bo Dai and Jincheng Mei and Oscar A Ramirez and Ramki Gummadi and Chris Harris and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=shbAgEsk3qM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 30}}, {"id": "dPyRNUlttBv", "original": "72eah3EepFC", "number": 4368, "cdate": 1632875748328, "mdate": null, "ddate": null, "tcdate": 1632875748328, "tmdate": 1676330454113, "tddate": null, "forum": "dPyRNUlttBv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Optimization and Adaptive Generalization of Three layer Neural Networks", "authorids": ["~Khashayar_Gatmiry1", "~Stefanie_Jegelka3", "~Jonathan_Kelner1"], "authors": ["Khashayar Gatmiry", "Stefanie Jegelka", "Jonathan Kelner"], "keywords": ["deep learning theory", "adaptive kernel", "robust deep learning", "neural tangent kernel", "adaptive generalization", "non-convex optimization"], "abstract": "While there has been substantial recent work studying  generalization of neural networks, \nthe ability of deep nets in automating the process of feature extraction still evades a thorough mathematical understanding.  \nAs a step toward this goal, we analyze learning and generalization of a three-layer neural network with ReLU activations in a regime that goes beyond the linear approximation of the network, and is hence not captured by the common Neural Tangent Kernel. We show that despite nonconvexity of the empirical loss, a variant of SGD converges in polynomially many iterations to a good solution that generalizes. In particular, our generalization bounds are adaptive: they automatically optimize over a family of kernels that includes the Neural Tangent Kernel, to provide the tightest bound.  ", "one-sentence_summary": "Algorithmically obtaining noise-robust and adaptive generalization bounds for a three layer network model by going beyond the linear approximation of the network", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gatmiry|optimization_and_adaptive_generalization_of_three_layer_neural_networks", "pdf": "/pdf/086ce10c9607a92d59635b0ac0f1f0bd8c86ae5b.pdf", "supplementary_material": "/attachment/ecbe39edaf5ae30cc427ed0c679a34b32f233847.zip", "_bibtex": "@inproceedings{\ngatmiry2022optimization,\ntitle={Optimization and Adaptive Generalization of Three layer Neural Networks},\nauthor={Khashayar Gatmiry and Stefanie Jegelka and Jonathan Kelner},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dPyRNUlttBv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "-TSe5o7STVR", "original": "uguqwDk2vKJA", "number": 4365, "cdate": 1632875748125, "mdate": null, "ddate": null, "tcdate": 1632875748125, "tmdate": 1697934524966, "tddate": null, "forum": "-TSe5o7STVR", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Non-Parallel Text Style Transfer with Self-Parallel Supervision", "authorids": ["~Ruibo_Liu1", "~Chongyang_Gao1", "~Chenyan_Jia1", "~Guangxuan_Xu1", "~Soroush_Vosoughi1"], "authors": ["Ruibo Liu", "Chongyang Gao", "Chenyan Jia", "Guangxuan Xu", "Soroush Vosoughi"], "keywords": ["style transfer", "non-parallel corpus", "imitation learning", "language models", "political stance transfer"], "abstract": "The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style.\n\nIn this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models.", "one-sentence_summary": "We propose a new text style transfer model for non-parallel corpus with supervision from intrinsic parallelism.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|nonparallel_text_style_transfer_with_selfparallel_supervision", "pdf": "/pdf/7858e341aa92c11991455a43e9a78c35ee4655a2.pdf", "supplementary_material": "/attachment/e2ae99e37d9af5e859e28d2d17d4dd76a85c092d.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2204.08123/code)", "_bibtex": "@inproceedings{\nliu2022nonparallel,\ntitle={Non-Parallel Text Style Transfer with Self-Parallel Supervision},\nauthor={Ruibo Liu and Chongyang Gao and Chenyan Jia and Guangxuan Xu and Soroush Vosoughi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-TSe5o7STVR}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "qhkFX-HLuHV", "original": "l85wba8eBw9", "number": 4364, "cdate": 1632875748055, "mdate": null, "ddate": null, "tcdate": 1632875748055, "tmdate": 1676330454650, "tddate": null, "forum": "qhkFX-HLuHV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Can an Image Classifier Suffice For Action Recognition?", "authorids": ["~Quanfu_Fan1", "~Chun-Fu_Chen1", "~Rameswar_Panda1"], "authors": ["Quanfu Fan", "Chun-Fu Chen", "Rameswar Panda"], "keywords": ["action recognition", "image classifier", "super image", "vision transformer"], "abstract": "We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task. Our approach rearranges input video frames into super images, which allow for training an image classifier directly to fulfill the task of action recognition, in exactly the same way as image classification. With such a simple idea, we show that transformer-based image classifiers alone can suffice for action recognition. In particular, our approach demonstrates strong and promising performance against SOTA methods on several public datasets including Kinetics400, Moments In Time, Something-Something V2 (SSV2), Jester and Diving48. We also experiment with the prevalent ResNet image classifiers in computer vision to further validate our idea. The results on both Kinetics400 and SSV2 are comparable to some of the best-performed CNN approaches based on spatio-temporal modeling. Our source codes and models are available at \\url{https://github.com/IBM/sifar-pytorch}.", "one-sentence_summary": "We propose the idea of super images to re-purpose an image classifer for action recognition.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fan|can_an_image_classifier_suffice_for_action_recognition", "pdf": "/pdf/30716aa30d9fbd5e0f9a95e4c0e1255607ab8bc4.pdf", "data": "", "_bibtex": "@inproceedings{\nfan2022can,\ntitle={Can an Image Classifier Suffice For Action Recognition?},\nauthor={Quanfu Fan and Chun-Fu Chen and Rameswar Panda},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qhkFX-HLuHV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "IK9ap6nxXr2", "original": "iy8-KjIy36P", "number": 4326, "cdate": 1632875745560, "mdate": null, "ddate": null, "tcdate": 1632875745560, "tmdate": 1697934528195, "tddate": null, "forum": "IK9ap6nxXr2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Interacting Contour Stochastic Gradient Langevin Dynamics", "authorids": ["~Wei_Deng1", "~Siqi_Liang1", "~Botao_Hao1", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Siqi Liang", "Botao Hao", "Guang Lin", "Faming Liang"], "keywords": ["stochastic gradient Langevin dynamics", "MCMC", "importance sampling", "Wang-Landau algorithm", "Parallel MCMC Methods", "stochastic approximation"], "abstract": "We propose an interacting contour stochastic gradient Langevin dynamics (ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show that ICSGLD can be theoretically more efficient than a single-chain CSGLD with an equivalent computational budget. We also present a novel random-field function, which facilitates the estimation of self-adapting parameters in big data and obtains free mode explorations. Empirically, we compare the proposed algorithm with popular benchmark methods for posterior sampling. The numerical results show a great potential of ICSGLD for large-scale uncertainty estimation tasks.", "one-sentence_summary": "We propose an interacting contour stochastic gradient Langevin dynamics sampler and prove it can be theoretically more efficient than a single-chain process with an equivalent computational budget.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "deng|interacting_contour_stochastic_gradient_langevin_dynamics", "pdf": "/pdf/bf454b672f7afe0c72e3a83029c7238309a1b4a0.pdf", "supplementary_material": "/attachment/322c413292673974c1abaa360c081e05e58a9c8a.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.09867/code)", "_bibtex": "@inproceedings{\ndeng2022interacting,\ntitle={Interacting Contour Stochastic Gradient Langevin Dynamics},\nauthor={Wei Deng and Siqi Liang and Botao Hao and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=IK9ap6nxXr2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "MIX3fJkl_1", "original": "IA5pAjTeV2Ac", "number": 4325, "cdate": 1632875745492, "mdate": null, "ddate": null, "tcdate": 1632875745492, "tmdate": 1697934528230, "tddate": null, "forum": "MIX3fJkl_1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "NeuPL: Neural Population Learning", "authorids": ["~Siqi_Liu1", "~Luke_Marris2", "~Daniel_Hennes1", "~Josh_Merel1", "~Nicolas_Heess1", "~Thore_Graepel1"], "authors": ["Siqi Liu", "Luke Marris", "Daniel Hennes", "Josh Merel", "Nicolas Heess", "Thore Graepel"], "keywords": ["Multi-Agent Learning", "Game Theory", "Population Learning"], "abstract": "Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|neupl_neural_population_learning", "pdf": "/pdf/eeeb391c4885267d9c80ba3a8ea3dfd9e9ea8832.pdf", "one-sentence_summary": "We propose NeuPL, a general and efficient population learning framework that learns and represents diverse policies in symmetric zero-sum games within a single conditional network via self-play.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.07415/code)", "_bibtex": "@inproceedings{\nliu2022neupl,\ntitle={Neu{PL}: Neural Population Learning},\nauthor={Siqi Liu and Luke Marris and Daniel Hennes and Josh Merel and Nicolas Heess and Thore Graepel},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MIX3fJkl_1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "hniLRD_XCA", "original": "Yhs6u0uPWets", "number": 4319, "cdate": 1632875745084, "mdate": null, "ddate": null, "tcdate": 1632875745084, "tmdate": 1676330456993, "tddate": null, "forum": "hniLRD_XCA", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DeSKO: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator", "authorids": ["~Minghao_Han2", "~Jacob_Euler-Rolle1", "~Robert_K._Katzschmann1"], "authors": ["Minghao Han", "Jacob Euler-Rolle", "Robert K. Katzschmann"], "keywords": ["Koopman Operator", "Robust Control", "Robotics", "Model Predictive Control", "Soft Robotics"], "abstract": "The Koopman operator theory linearly describes nonlinear dynamical systems in a high-dimensional functional space and it allows to apply linear control methods to highly nonlinear systems. However, the Koopman operator does not account for any uncertainty in dynamical systems, causing it to perform poorly in real-world applications.\nTherefore, we propose a deep stochastic Koopman operator (DeSKO) model in a robust learning control framework to guarantee stability of nonlinear stochastic systems. The DeSKO model captures a dynamical system's uncertainty by inferring a distribution of observables. We use the inferred distribution to design a robust, stabilizing closed-loop controller for a dynamical system. Modeling and control experiments on several advanced control benchmarks show that our framework is more robust and scalable than state-of-the-art deep Koopman operators and reinforcement learning methods. Tested control benchmarks include a soft robotic arm, a legged robot, and a biological gene regulatory network. We also demonstrate that this robust control method resists previously unseen uncertainties, such as external disturbances, with a magnitude of up to five times the maximum control input. Our approach opens up new possibilities in learning control for high-dimensional nonlinear systems while robustly managing internal or external uncertainty.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "han|desko_stabilityassured_robust_control_with_a_deep_stochastic_koopman_operator", "pdf": "/pdf/862602026e43c103de39be4295ff8f7288f3acf2.pdf", "one-sentence_summary": "A robust learning control framework with guarantee stability based on deep stochastic Koopman operator models", "supplementary_material": "/attachment/8fd81957080cab01cd427ae09252f3266e67e4e5.zip", "_bibtex": "@inproceedings{\nhan2022desko,\ntitle={De{SKO}: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator},\nauthor={Minghao Han and Jacob Euler-Rolle and Robert K. Katzschmann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hniLRD_XCA}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "oiZJwC_fyS", "original": "5jUw6GFnGOfr", "number": 4300, "cdate": 1632875743879, "mdate": null, "ddate": null, "tcdate": 1632875743879, "tmdate": 1676330457959, "tddate": null, "forum": "oiZJwC_fyS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes", "authorids": ["~Panagiotis_Misiakos1", "~Georgios_Smyrnis1", "~George_Retsinas2", "~Petros_Maragos1"], "authors": ["Panagiotis Misiakos", "Georgios Smyrnis", "George Retsinas", "Petros Maragos"], "keywords": ["Tropical Geometry", "Zonotopes", "Hausdorff Approximation", "Neural Network Compression"], "abstract": "In this work we theoretically contribute to neural network approximation by providing a novel tropical geometrical viewpoint to structured neural network compression. In particular, we show that the approximation error between two neural networks with ReLU activations and one hidden layer depends on the Hausdorff distance of the tropical zonotopes of the networks. This theorem comes as a first step towards a purely geometrical interpretation of neural network approximation. Based on this theoretical contribution, we propose geometrical methods that employ the K-means algorithm to compress the fully connected parts of ReLU activated deep neural networks. We analyze the error bounds of our algorithms theoretically based on our approximation theorem and evaluate them empirically on neural network compression. Our experiments follow a proof-of-concept strategy and indicate that our geometrical tools achieve improved performance over relevant tropical geometry techniques and can be competitive against non-tropical methods. ", "pdf": "/pdf/e09efd74b974abec052126ca4cbb787b04fd3265.pdf", "supplementary_material": "/attachment/e4ebfc4ae844913fb8a8941da483a5e99aebc3cb.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "misiakos|neural_network_approximation_based_on_hausdorff_distance_of_tropical_zonotopes", "data": "", "_bibtex": "@inproceedings{\nmisiakos2022neural,\ntitle={Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes},\nauthor={Panagiotis Misiakos and Georgios Smyrnis and George Retsinas and Petros Maragos},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oiZJwC_fyS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "hqkhcFHOeKD", "original": "ls7TOhql7Q0M", "number": 4295, "cdate": 1632875743549, "mdate": null, "ddate": null, "tcdate": 1632875743549, "tmdate": 1697934531435, "tddate": null, "forum": "hqkhcFHOeKD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Towards The Largest Margins", "authorids": ["~Xiong_Zhou3", "~Xianming_Liu5", "~Deming_Zhai2", "~Junjun_Jiang2", "~Xin_Gao1", "~Xiangyang_Ji1"], "authors": ["Xiong Zhou", "Xianming Liu", "Deming Zhai", "Junjun Jiang", "Xin Gao", "Xiangyang Ji"], "keywords": ["loss function design", "margin-based loss", "classification"], "abstract": "One of the main challenges for feature representation in deep learning-based classification is the design of appropriate loss functions that exhibit strong discriminative power. The classical softmax loss does not explicitly encourage discriminative learning of features. A popular direction of research is to incorporate margins in well-established losses in order to enforce extra intra-class compactness and inter-class separability, which, however, were developed through heuristic means, as opposed to rigorous mathematical principles. In this work, we attempt to address this limitation by formulating the principled optimization objective as learning towards the largest margins. Specifically, we firstly propose to employ the class margin as the measure of inter-class separability, and the sample margin as the measure of intra-class compactness. Accordingly, to encourage discriminative representation of features, the loss function should promote the largest possible margins for both classes and samples. Furthermore, we derive a generalized margin softmax loss to draw general conclusions for the existing margin-based losses. Not only does this principled framework offer new perspectives to understand and interpret existing margin-based losses, but it also provides new insights that can guide the design of new tools, including \\textit{sample margin regularization} and \\textit{largest margin softmax loss} for class balanced cases, and \\textit{zero centroid regularization} for class imbalanced cases. Experimental results demonstrate the effectiveness of our strategy for multiple tasks including visual classification, imbalanced classification, person re-identification, and face verification.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhou|learning_towards_the_largest_margins", "pdf": "/pdf/05f12453b1762c08d54507567f592f91d86425be.pdf", "supplementary_material": "/attachment/27d3497792e0f4eff3040757232400ab271583da.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2206.11589/code)", "_bibtex": "@inproceedings{\nzhou2022learning,\ntitle={Learning Towards The Largest Margins},\nauthor={Xiong Zhou and Xianming Liu and Deming Zhai and Junjun Jiang and Xin Gao and Xiangyang Ji},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hqkhcFHOeKD}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "28ib9tf6zhr", "original": "aIwgJOIOCgy", "number": 4289, "cdate": 1632875743141, "mdate": null, "ddate": null, "tcdate": 1632875743141, "tmdate": 1676330458281, "tddate": null, "forum": "28ib9tf6zhr", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?", "authorids": ["~Yonggan_Fu1", "sz74@rice.edu", "sw99@rice.edu", "~Cheng_Wan2", "~Yingyan_Lin1"], "authors": ["Yonggan Fu", "Shunyao Zhang", "Shang Wu", "Cheng Wan", "Yingyan Lin"], "keywords": ["Vision transformer", "adversarial examples", "robustness"], "abstract": "Vision transformers (ViTs) have recently set off a new wave in neural architecture design thanks to their record-breaking performance in various vision tasks. In parallel, to fulfill the goal of deploying ViTs into real-world vision applications, their robustness against potential malicious attacks has gained increasing attention. In particular, recent works show that ViTs are more robust against adversarial attacks as compared with convolutional neural networks (CNNs), and conjecture that this is because ViTs focus more on capturing global interactions among different input/feature patches, leading to their improved robustness to local perturbations imposed by adversarial attacks. In this work, we ask an intriguing question: \"Under what kinds of perturbations do ViTs become more vulnerable learners compared to CNNs?\" Driven by this question, we first conduct a comprehensive experiment regarding the robustness of both ViTs and CNNs under various existing adversarial attacks to understand the underlying reason favoring their robustness. Based on the drawn insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that fools the self-attention mechanism by attacking its basic component (i.e., a single patch) with a series of attention-aware optimization techniques. Interestingly, our Patch-Fool framework shows for the first time that ViTs are not necessarily more robust than CNNs against adversarial perturbations. In particular, we find that ViTs are more vulnerable learners compared with CNNs against our Patch-Fool attack which is consistent across extensive experiments, and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool, indicate an intriguing insight that the perturbation density and strength on each patch seem to be the key factors that influence the robustness ranking between ViTs and CNNs. It can be expected that our Patch-Fool framework will shed light on both future architecture designs and training schemes for robustifying ViTs towards their real-world deployment. Our codes are available at https://github.com/RICE-EIC/Patch-Fool.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fu|patchfool_are_vision_transformers_always_robust_against_adversarial_perturbations", "pdf": "/pdf/4c7b8d2f80c4ea1bfe11754da2e7c69fc5183754.pdf", "one-sentence_summary": "We propose the Patch-Fool attack to unveil a vulnerability perspective of ViTs.", "_bibtex": "@inproceedings{\nfu2022patchfool,\ntitle={Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?},\nauthor={Yonggan Fu and Shunyao Zhang and Shang Wu and Cheng Wan and Yingyan Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=28ib9tf6zhr}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "Q5uh1Nvv5dm", "original": "EGeYxMp1aJs", "number": 4287, "cdate": 1632875743011, "mdate": null, "ddate": null, "tcdate": 1632875743011, "tmdate": 1676330458448, "tddate": null, "forum": "Q5uh1Nvv5dm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation", "authorids": ["~David_Berthelot1", "~Rebecca_Roelofs1", "~Kihyuk_Sohn1", "~Nicholas_Carlini1", "~Alexey_Kurakin1"], "authors": ["David Berthelot", "Rebecca Roelofs", "Kihyuk Sohn", "Nicholas Carlini", "Alexey Kurakin"], "keywords": ["unsupervised domain adaptation", "semi-supervised learning", "semi-supervised domain adaptation"], "abstract": "We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a unified solution for unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA). In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA and find that AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task. For example, AdaMatch nearly doubles the accuracy compared to that of the prior state-of-the-art on the UDA task for DomainNet and even exceeds the accuracy of the prior state-of-the-art obtained with pre-training by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by providing AdaMatch with just one labeled example per class from the target domain (i.e., the SSDA setting), we increase the target accuracy by an additional 6.1%, and with 5 labeled examples, by 13.6%.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "berthelot|adamatch_a_unified_approach_to_semisupervised_learning_and_domain_adaptation", "pdf": "/pdf/8dd30c7eff2e4f152d2d24368c232baec4e5e974.pdf", "one-sentence_summary": "We introduce AdaMatch, a unified solution that achieves state-of-the-art results for unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA).", "supplementary_material": "", "code": "", "data": "", "_bibtex": "@inproceedings{\nberthelot2022adamatch,\ntitle={AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation},\nauthor={David Berthelot and Rebecca Roelofs and Kihyuk Sohn and Nicholas Carlini and Alexey Kurakin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Q5uh1Nvv5dm}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "l_amHf1oaK", "original": "KzIWzveXoFk", "number": 4286, "cdate": 1632875742943, "mdate": null, "ddate": null, "tcdate": 1632875742943, "tmdate": 1676330458528, "tddate": null, "forum": "l_amHf1oaK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound", "authorids": ["~Claudio_Ferrari2", "~Mark_Niklas_Mueller2", "~Nikola_Jovanovi\u01071", "~Martin_Vechev1"], "authors": ["Claudio Ferrari", "Mark Niklas Mueller", "Nikola Jovanovi\u0107", "Martin Vechev"], "keywords": ["Certified Robustness", "Branch-and-Bound", "Convex Relaxation"], "abstract": "State-of-the-art neural network verifiers are fundamentally based on one of two paradigms: either encoding the whole verification problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging imprecise but fast bounding methods on a large number of easier subproblems. The former can capture complex multi-neuron dependencies but sacrifices completeness due to the inherent limitations of convex relaxations. The latter enables complete verification but becomes increasingly ineffective on larger and more challenging networks. In this work, we present a novel complete verifier which combines the strengths of both paradigms: it leverages multi-neuron relaxations to drastically reduce the number of subproblems generated during the BaB process and an efficient GPU-based dual optimizer to solve the remaining ones. An extensive evaluation demonstrates that our verifier achieves a new state-of-the-art on both established benchmarks as well as networks with significantly higher accuracy than previously considered. The latter result (up to 28% certification gains) indicates meaningful progress towards creating verifiers that can handle practically relevant networks.", "one-sentence_summary": "We obtain a state-of-the-art GPU-based neural network verifier by leveraging tight multi-neuron constraints in a Branch-and-Bound setting.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ferrari|complete_verification_via_multineuron_relaxation_guided_branchandbound", "pdf": "/pdf/fcc20218f5754386cf64f4156a1f41039038b5da.pdf", "data": "", "_bibtex": "@inproceedings{\nferrari2022complete,\ntitle={Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound},\nauthor={Claudio Ferrari and Mark Niklas Mueller and Nikola Jovanovi{\\'c} and Martin Vechev},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=l_amHf1oaK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "VFBjuF8HEp", "original": "e_4Ywe5oI3B", "number": 4276, "cdate": 1632875742271, "mdate": null, "ddate": null, "tcdate": 1632875742271, "tmdate": 1676330459551, "tddate": null, "forum": "VFBjuF8HEp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality", "authorids": ["~Daniel_Watson1", "~William_Chan1", "~Jonathan_Ho1", "~Mohammad_Norouzi1"], "authors": ["Daniel Watson", "William Chan", "Jonathan Ho", "Mohammad Norouzi"], "keywords": [], "abstract": "Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We also present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required.", "one-sentence_summary": "We propose a method to discover fast, high-fidelity samplers for diffusion probabilistic models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "watson|learning_fast_samplers_for_diffusion_models_by_differentiating_through_sample_quality", "pdf": "/pdf/56f0145dd15f32bd53f6dba7efde74914a88f663.pdf", "data": "", "_bibtex": "@inproceedings{\nwatson2022learning,\ntitle={Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality},\nauthor={Daniel Watson and William Chan and Jonathan Ho and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VFBjuF8HEp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "lzupY5zjaU9", "original": "ueXHprnmO6S", "number": 4274, "cdate": 1632875742202, "mdate": null, "ddate": null, "tcdate": 1632875742202, "tmdate": 1697934533775, "tddate": null, "forum": "lzupY5zjaU9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Distribution Compression in Near-Linear Time", "authorids": ["~Abhishek_Shetty1", "~Raaz_Dwivedi1", "~Lester_Mackey1"], "authors": ["Abhishek Shetty", "Raaz Dwivedi", "Lester Mackey"], "keywords": ["Distribution compression", "linear time", "thinning", "i.i.d. sampling", "Markov chain Monte Carlo", "maximum mean discrepancy", "reproducing kernel Hilbert space"], "abstract": "In distribution compression, one aims to accurately summarize a probability distribution $\\mathbb{P}$ using a small number of representative points. Near-optimal thinning procedures achieve this goal by sampling $n$ points from a Markov chain and identifying $\\sqrt{n}$ points with $\\widetilde{\\mathcal{O}}(1/\\sqrt{n})$ discrepancy to $\\mathbb{P}$. Unfortunately, these algorithms suffer from quadratic or super-quadratic runtime in the sample size $n$. To address this deficiency, we introduce Compress++, a simple meta-procedure for speeding up any thinning algorithm while suffering at most a factor of $4$ in error. When combined with the quadratic-time kernel halving and kernel thinning algorithms of Dwivedi and Mackey (2021), Compress++ delivers $\\sqrt{n}$ points with $\\mathcal{O}(\\sqrt{\\log n/n})$ integration error and better-than-Monte-Carlo maximum mean discrepancy in $\\mathcal{O}(n \\log^3 n)$ time and  $\\mathcal{O}( \\sqrt{n} \\log^2 n )$ space. Moreover, Compress++ enjoys the same near-linear runtime given any quadratic-time input and reduces the runtime of super-quadratic algorithms by a square-root factor. In our benchmarks with high-dimensional Monte Carlo samples and Markov chains targeting challenging differential equation posteriors, Compress++ matches or nearly matches the accuracy of its input algorithm in orders of magnitude less time.", "pdf": "/pdf/484f68f97f561be1f3272522336a9a0b1fa84bbc.pdf", "one-sentence_summary": "We introduce a simple algorithm for compressing an $n$-point summary of a probability distribution into a $\\sqrt{n}$-point summary of comparable quality in $O(n \\log^2 n)$ time.", "supplementary_material": "/attachment/d8d1db0c5d670ef21d0b2c6f8f991f806c77a127.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shetty|distribution_compression_in_nearlinear_time", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.07941/code)", "_bibtex": "@inproceedings{\nshetty2022distribution,\ntitle={Distribution Compression in Near-Linear Time},\nauthor={Abhishek Shetty and Raaz Dwivedi and Lester Mackey},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lzupY5zjaU9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "nnU3IUMJmN", "original": "Hk90kJpC8D1", "number": 4271, "cdate": 1632875742002, "mdate": null, "ddate": null, "tcdate": 1632875742002, "tmdate": 1697934534326, "tddate": null, "forum": "nnU3IUMJmN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Capturing Structural Locality in Non-parametric Language Models", "authorids": ["~Frank_F._Xu1", "~Junxian_He1", "~Graham_Neubig1", "~Vincent_Josua_Hellendoorn1"], "authors": ["Frank F. Xu", "Junxian He", "Graham Neubig", "Vincent Josua Hellendoorn"], "keywords": [], "abstract": "Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference retrieved examples from an external source. We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods. Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model efficacy over models without access to these features, with interesting differences. We also perform an analysis of how and where locality features contribute to improving performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure.\n", "one-sentence_summary": " We propose, study the effect of, and incorporate structural locality in non-parametric language models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|capturing_structural_locality_in_nonparametric_language_models", "pdf": "/pdf/05677eb0d7fca88dd7c4c6cbefa73f6ae430ad68.pdf", "supplementary_material": "/attachment/82559872eef8dd03d176c8d5ab48392be1b4929d.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02870/code)", "_bibtex": "@inproceedings{\nxu2022capturing,\ntitle={Capturing Structural Locality in Non-parametric Language Models},\nauthor={Frank F. Xu and Junxian He and Graham Neubig and Vincent Josua Hellendoorn},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nnU3IUMJmN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "9Nk6AJkVYB", "original": "Dl0Z1JMP1P", "number": 4269, "cdate": 1632875741868, "mdate": null, "ddate": null, "tcdate": 1632875741868, "tmdate": 1676330459784, "tddate": null, "forum": "9Nk6AJkVYB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable", "authorids": ["~Shaojin_Ding1", "~Tianlong_Chen1", "~Zhangyang_Wang1"], "authors": ["Shaojin Ding", "Tianlong Chen", "Zhangyang Wang"], "keywords": ["Speech Recognition", "Lottery Ticket Hypothesis"], "abstract": "Lightweight speech recognition models have seen explosive demands owing to a growing amount of speech-interactive features on mobile devices. Since designing such systems from scratch is non-trivial, practitioners typically choose to compress large (pre-trained) speech models. Recently, lottery ticket hypothesis reveals the existence of highly sparse subnetworks that can be trained in isolation without sacrificing the performance of the full models. In this paper, we investigate the tantalizing possibility of using lottery ticket hypothesis to discover lightweight speech recognition models, that are (1) robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and 3) compatible with structured sparsity. We conducted extensive experiments on  CNN-LSTM, RNN-Transducer, and Transformer models, and verified the existence of highly sparse winning tickets that can match the full model performance across those backbones. We obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights. Those winning tickets generalize to structured sparsity with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models, showing the extra bonus of noise robustness by inducing sparsity. Codes are available at https://github.com/VITA-Group/Audio-Lottery.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ding|audio_lottery_speech_recognition_made_ultralightweight_noiserobust_and_transferable", "pdf": "/pdf/3d42ff881f8ec8954935d0f8bbcb2a21d71106ea.pdf", "one-sentence_summary": "We for the first time investigate three unique properties that were rarely studied in previous LTH research but are key to user-interactive ASR devices, bringing new insights to both LTH theory and lightweight ASR research.", "data": "", "_bibtex": "@inproceedings{\nding2022audio,\ntitle={Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable},\nauthor={Shaojin Ding and Tianlong Chen and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9Nk6AJkVYB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 31}}, {"id": "swrMQttr6wN", "original": "7ccqrJXNdVN", "number": 4251, "cdate": 1632875740719, "mdate": null, "ddate": null, "tcdate": 1632875740719, "tmdate": 1697934535279, "tddate": null, "forum": "swrMQttr6wN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Map for Active Semantic Goal Navigation", "authorids": ["~Georgios_Georgakis1", "~Bernadette_Bucher1", "~Karl_Schmeckpeper1", "~Siddharth_Singh5", "~Kostas_Daniilidis1"], "authors": ["Georgios Georgakis", "Bernadette Bucher", "Karl Schmeckpeper", "Siddharth Singh", "Kostas Daniilidis"], "keywords": ["visual navigation", "semantic map", "uncertainty estimation"], "abstract": "We consider the problem of object goal navigation in unseen environments. Solving this problem requires learning of contextual semantic priors, a challenging endeavour given the spatial and semantic variability of indoor environments. Current methods learn to implicitly encode these priors through goal-oriented navigation policy functions operating on spatial representations that are limited to the agent's observable areas. In this work, we propose a novel framework that actively learns to generate semantic maps outside the field of view of the agent and leverages the uncertainty over the semantic classes in the unobserved areas to decide on long term goals. We demonstrate that through this spatial prediction strategy, we are able to learn semantic priors in scenes that can be leveraged in unknown environments. Additionally, we show how different objectives can be defined by balancing exploration with exploitation during searching for semantic targets. Our method is validated in the visually realistic environments of the Matterport3D dataset and show improved results on object goal navigation over competitive baselines.", "one-sentence_summary": "A framework for object goal navigation that actively learns to predict semantic maps and choose long-term goals based on uncertainty measures.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "georgakis|learning_to_map_for_active_semantic_goal_navigation", "pdf": "/pdf/8097afd8a3e6d7c824f59390ca5a9cee0530bbd1.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.15648/code)", "_bibtex": "@inproceedings{\ngeorgakis2022learning,\ntitle={Learning to Map for Active Semantic Goal Navigation},\nauthor={Georgios Georgakis and Bernadette Bucher and Karl Schmeckpeper and Siddharth Singh and Kostas Daniilidis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=swrMQttr6wN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "1W0z96MFEoH", "original": "EKohCRblerU", "number": 4249, "cdate": 1632875740580, "mdate": null, "ddate": null, "tcdate": 1632875740580, "tmdate": 1697934535636, "tddate": null, "forum": "1W0z96MFEoH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Benchmarking the Spectrum of Agent Capabilities", "authorids": ["~Danijar_Hafner1"], "authors": ["Danijar Hafner"], "keywords": ["Evaluation", "Reinforcement Learning", "Environment", "Benchmark", "Unsupervised Reinforcement Learning", "Exploration"], "abstract": "Evaluating the general abilities of intelligent agents requires complex simulation environments. Existing benchmarks typically evaluate only one narrow task per environment, requiring researchers to perform expensive training runs on many different environments. We introduce Crafter, an open world survival game with visual inputs that evaluates a wide range of general abilities within a single environment. Agents either learn from the provided reward signal or through intrinsic objectives and are evaluated by semantically meaningful achievements that can be unlocked during each episode, such as discovering resources and crafting tools. Consistently unlocking all achievements requires strong generalization, deep exploration, and long-term reasoning. We experimentally verify that Crafter is of appropriate difficulty to drive future research and provide baselines scores of reward agents and unsupervised agents. Furthermore, we observe sophisticated behaviors emerging from maximizing the reward signal, such as building tunnel systems, bridges, houses, and plantations. We hope that Crafter will accelerate research progress by quickly evaluating a wide spectrum of abilities.", "pdf": "/pdf/116a18888b3fb460e882ec2b844128223e3b17ca.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hafner|benchmarking_the_spectrum_of_agent_capabilities", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.06780/code)", "_bibtex": "@inproceedings{\nhafner2022benchmarking,\ntitle={Benchmarking the Spectrum of Agent Capabilities},\nauthor={Danijar Hafner},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1W0z96MFEoH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 29}}, {"id": "vqGi8Kp0wM", "original": "dzOb3ecLUjA", "number": 4248, "cdate": 1632875740513, "mdate": null, "ddate": null, "tcdate": 1632875740513, "tmdate": 1676330460229, "tddate": null, "forum": "vqGi8Kp0wM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks", "authorids": ["~Peihao_Zhu1", "~Rameen_Abdal1", "~John_Femiani1", "~Peter_Wonka1"], "authors": ["Peihao Zhu", "Rameen Abdal", "John Femiani", "Peter Wonka"], "keywords": ["GAN", "StyleGAN", "Clip", "Domain Adaptation", "Style Transfer", "Single Shot"], "abstract": "We present a new method for one shot domain adaptation. The input to our method is trained GAN that can produce images in domain A and a single reference image I_B from domain B. The proposed algorithm can translate any output of the trained GAN from domain A to domain B. There are two main advantages of our method compared to the current state of the art: First, our solution achieves higher visual quality, e.g. by noticeably reducing overfitting. Second, our solution allows for more degrees of freedom to control the domain gap, i.e. what aspects of image I_B are used to define the domain B. Technically, we realize the new method by building on a pre-trained StyleGAN generator as GAN and a pre-trained CLIP model for representing the domain gap. We propose several new regularizers for controlling the domain gap to optimize the weights of the pre-trained StyleGAN generator to output images in domain B instead of domain A. The regularizers prevent the optimization from taking on too many attributes of the single reference image. Our results show significant visual improvements over the state of the art as well as multiple applications that highlight improved control.", "one-sentence_summary": "We propose several regularizers to control the domain transfer for single shot domain adaptation in the context of generative adversarial networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|mind_the_gap_domain_gap_control_for_single_shot_domain_adaptation_for_generative_adversarial_networks", "pdf": "/pdf/2f6e593f100fa850ecde50e059aa6b2e73a3f6fe.pdf", "supplementary_material": "/attachment/7622d6fc37313f193ff1062891b66ec77c6cc639.zip", "_bibtex": "@inproceedings{\nzhu2022mind,\ntitle={Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks},\nauthor={Peihao Zhu and Rameen Abdal and John Femiani and Peter Wonka},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vqGi8Kp0wM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "EnwCZixjSh", "original": "xju8EKevlaE", "number": 4244, "cdate": 1632875740242, "mdate": null, "ddate": null, "tcdate": 1632875740242, "tmdate": 1697934536478, "tddate": null, "forum": "EnwCZixjSh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Evaluation Metrics for Graph Generative Models", "authorids": ["~Rylee_Thompson1", "~Boris_Knyazev1", "~Elahe_Ghalebi1", "~Jungtaek_Kim1", "~Graham_W._Taylor1"], "authors": ["Rylee Thompson", "Boris Knyazev", "Elahe Ghalebi", "Jungtaek Kim", "Graham W. Taylor"], "keywords": [], "abstract": "In image generation, generative models can be evaluated naturally by visually inspecting model outputs. However, this is not always the case for graph generative models (GGMs), making their evaluation challenging. Currently, the standard process for evaluating GGMs suffers from three critical limitations: i) it does not produce a single score which makes model selection challenging, ii) in many cases it fails to consider underlying edge and node features, and iii) it is prohibitively slow to perform. In this work, we mitigate these issues by searching for \\emph{scalar, domain-agnostic, and scalable metrics} for evaluating and ranking GGMs. To this end, we study existing GGM metrics and neural-network-based metrics emerging from generative models of images that use embeddings extracted from a task-specific network. Motivated by the power of Graph Neural Networks (GNNs) to extract meaningful graph representations \\emph{without any training}, we introduce several metrics based on the features extracted by an untrained random GNN. We design experiments to thoroughly test and objectively score metrics on their ability to measure the diversity and fidelity of generated graphs, as well as their sample and computational efficiency. Depending on the quantity of samples, we recommend one of two metrics from our collection of random-GNN-based metrics. We show these two metrics to be more expressive than pre-existing and alternative random-GNN-based metrics using our objective scoring. While we focus on applying these metrics to GGM evaluation, in practice this enables the ability to easily compute the dissimilarity between any two sets of graphs \\emph{regardless of domain}. Our code is released at: https://github.com/uoguelph-mlrg/GGM-metrics.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "thompson|on_evaluation_metrics_for_graph_generative_models", "pdf": "/pdf/fcb94055fd54a7db263aab7d0f85b591c34e713e.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.09871/code)", "_bibtex": "@inproceedings{\nthompson2022on,\ntitle={On Evaluation Metrics for Graph Generative Models},\nauthor={Rylee Thompson and Boris Knyazev and Elahe Ghalebi and Jungtaek Kim and Graham W. Taylor},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EnwCZixjSh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "HfUyCRBeQc", "original": "GrzJc8zN8nO8", "number": 4240, "cdate": 1632875739972, "mdate": null, "ddate": null, "tcdate": 1632875739972, "tmdate": 1676330461239, "tddate": null, "forum": "HfUyCRBeQc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Selective Ensembles for Consistent Predictions", "authorids": ["~Emily_Black1", "~Klas_Leino1", "~Matt_Fredrikson1"], "authors": ["Emily Black", "Klas Leino", "Matt Fredrikson"], "keywords": ["consistency", "prediction consistency", "model duplicity", "inconsistent predictions", "deep models", "deep networks", "explanations", "saliency maps", "gradient-based explanations", "fairness", "interpretability"], "abstract": "Recent work has shown that models trained to the same objective, and which achieve similar measures of accuracy on consistent test data, may nonetheless behave very differently on individual predictions. This inconsistency is undesirable in high-stakes contexts, such as medical diagnosis and finance. We show that this duplicitous behavior extends beyond predictions to feature attributions, which may likewise have negative implications for the intelligibility of a model, and one's ability to find recourse for subjects. We then introduce selective ensembles to mitigate such inconsistencies by applying hypothesis testing to the predictions of a set of models trained using randomly-selected starting conditions; importantly, selective ensembles can abstain in cases where a consistent outcome cannot be achieved up to a specified confidence level. We prove that that prediction disagreement between selective ensembles is bounded, and empirically demonstrate that selective ensembles achieve consistent predictions and feature attributions while maintaining low abstention rates. On several benchmark datasets, selective ensembles reach zero inconsistently predicted points, with abstention rates as low as 1.5%.", "one-sentence_summary": "Deep models give inconsistent predictions and explanations over small changes (e.g. random initialization). We can mitigate this by using selective ensemble models, which abstain from prediction if their constituent models do not agree sufficiently.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "black|selective_ensembles_for_consistent_predictions", "pdf": "/pdf/aef96c65d43466af59147df0d990f0b94efbef7a.pdf", "supplementary_material": "/attachment/fa0e74655ac3aaa30612686210e7874379e58615.zip", "_bibtex": "@inproceedings{\nblack2022selective,\ntitle={Selective Ensembles for Consistent Predictions},\nauthor={Emily Black and Klas Leino and Matt Fredrikson},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HfUyCRBeQc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "WLEx3Jo4QaB", "original": "v5Q9KzUUB8l", "number": 4239, "cdate": 1632875739903, "mdate": null, "ddate": null, "tcdate": 1632875739903, "tmdate": 1697934536985, "tddate": null, "forum": "WLEx3Jo4QaB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph Condensation for Graph Neural Networks", "authorids": ["~Wei_Jin4", "~Lingxiao_Zhao1", "~Shichang_Zhang2", "~Yozen_Liu1", "~Jiliang_Tang1", "~Neil_Shah2"], "authors": ["Wei Jin", "Lingxiao Zhao", "Shichang Zhang", "Yozen Liu", "Jiliang Tang", "Neil Shah"], "keywords": ["data-efficient learning", "graph generation", "graph neural networks"], "abstract": "Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs).  Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory  on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In particular, we are able to approximate the original test accuracy by 95.3\\% on Reddit, 99.8\\% on Flickr and 99.0\\% on Citeseer,  while reducing their graph size by more than 99.9\\%, and the condensed graphs can be used to train various GNN architectures. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jin|graph_condensation_for_graph_neural_networks", "pdf": "/pdf/fb904d1d840eb264e6ab2e160ff7322153a1fbb0.pdf", "one-sentence_summary": "We study the problem of graph condensation which targets at condensing a large-real graph into a small-synthetic one while maintaining the performances of GNNs.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.07580/code)", "_bibtex": "@inproceedings{\njin2022graph,\ntitle={Graph Condensation for Graph Neural Networks},\nauthor={Wei Jin and Lingxiao Zhao and Shichang Zhang and Yozen Liu and Jiliang Tang and Neil Shah},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WLEx3Jo4QaB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "bVvMOtLMiw", "original": "V4-eLvV2-iB", "number": 4238, "cdate": 1632875739832, "mdate": null, "ddate": null, "tcdate": 1632875739832, "tmdate": 1676330461357, "tddate": null, "forum": "bVvMOtLMiw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DIVA: Dataset Derivative of a Learning Task", "authorids": ["~Yonatan_Dukler1", "~Alessandro_Achille1", "~Giovanni_Paolini1", "~Avinash_Ravichandran1", "~Marzia_Polito1", "~Stefano_Soatto3"], "authors": ["Yonatan Dukler", "Alessandro Achille", "Giovanni Paolini", "Avinash Ravichandran", "Marzia Polito", "Stefano Soatto"], "keywords": ["Leave one out cross validation", "AutoML", "dataset optimization"], "abstract": "We present a method to compute the derivative of a learning task with respect to a dataset. A learning task is a function from a training set to the validation error, which can be represented by a trained deep neural network (DNN). The ``dataset derivative'' is a linear operator, computed around the trained model, that informs how perturbations of the weight of each training sample affect the validation error, usually computed on a separate validation dataset.  Our method, DIVA (Differentiable Validation) hinges on a closed-form differentiable expression of the leave-one-out cross-validation error around a pre-trained DNN. Such expression constitutes the dataset derivative. DIVA could be used for dataset auto-curation, for example removing samples with faulty annotations, augmenting a dataset with additional relevant samples, or rebalancing. More generally, DIVA can be used to optimize the dataset, along with the parameters of the model, as part of the training process without the need for a separate validation dataset, unlike bi-level optimization methods customary in AutoML. To illustrate the flexibility of DIVA, we report experiments on sample auto-curation tasks such as outlier rejection, dataset extension, and automatic aggregation of multi-modal data.", "one-sentence_summary": "Presents a method to optimize a dataset based on a notion of a dataset derivative that is computed in closed form using linearization", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dukler|diva_dataset_derivative_of_a_learning_task", "pdf": "/pdf/c20ae574c689fe5fbecb96f791b3e678973e0053.pdf", "supplementary_material": "/attachment/d872c8d7c427b934714a7377c281e34e34159490.zip", "_bibtex": "@inproceedings{\ndukler2022diva,\ntitle={{DIVA}: Dataset Derivative of a Learning Task},\nauthor={Yonatan Dukler and Alessandro Achille and Giovanni Paolini and Avinash Ravichandran and Marzia Polito and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bVvMOtLMiw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "sA4qIu3zv6v", "original": "L62IAUUi6zN", "number": 4231, "cdate": 1632875739358, "mdate": null, "ddate": null, "tcdate": 1632875739358, "tmdate": 1676330461466, "tddate": null, "forum": "sA4qIu3zv6v", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards General Function Approximation in Zero-Sum Markov Games", "authorids": ["~Baihe_Huang1", "~Jason_D._Lee1", "~Zhaoran_Wang1", "~Zhuoran_Yang1"], "authors": ["Baihe Huang", "Jason D. Lee", "Zhaoran Wang", "Zhuoran Yang"], "keywords": [], "abstract": "This paper considers two-player zero-sum finite-horizon Markov games with simultaneous moves. The study focuses on the challenging settings where the value\nfunction or the model is parameterized by general function classes. Provably efficient\nalgorithms for both decoupled and coordinated settings are developed. In the decoupled setting where the agent controls a single player and plays against an arbitrary opponent, we propose a new model-free algorithm. The sample complexity is governed by the Minimax Eluder dimension\u2014a new dimension of the function class in Markov games. As a special case, this method improves the state-of-the-art algorithm\nby a $\\sqrt{d}$ factor in the regret when the reward function and transition kernel are parameterized with d-dimensional linear features. In the coordinated setting where both\nplayers are controlled by the agent, we propose a model-based algorithm and a model-free algorithm. In the model-based algorithm, we prove that sample complexity can\nbe bounded by a generalization of Witness rank to Markov games. The model-free\nalgorithm enjoys a  $\\sqrt{K}$-regret upper bound where $K$ is the number of episodes. Our\nalgorithms are based on new techniques of alternate optimism", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|towards_general_function_approximation_in_zerosum_markov_games", "pdf": "/pdf/89164a5698b4ced1396254451108620fc52d5bc1.pdf", "supplementary_material": "/attachment/fbf79664452d7143663daadfb9e759e4a054117d.zip", "_bibtex": "@inproceedings{\nhuang2022towards,\ntitle={Towards General Function Approximation in Zero-Sum Markov Games},\nauthor={Baihe Huang and Jason D. Lee and Zhaoran Wang and Zhuoran Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=sA4qIu3zv6v}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "6PvWo1kEvlT", "original": "SS6i4pzUgHk", "number": 4229, "cdate": 1632875739224, "mdate": null, "ddate": null, "tcdate": 1632875739224, "tmdate": 1676330461599, "tddate": null, "forum": "6PvWo1kEvlT", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings", "authorids": ["~Kartik_Goyal1", "~Chris_Dyer1", "~Taylor_Berg-Kirkpatrick1"], "authors": ["Kartik Goyal", "Chris Dyer", "Taylor Berg-Kirkpatrick"], "keywords": ["Masked Language Models", "Energy-based models", "Metropolis Hastings Monte Carlo", "Bidirectional Sequence models"], "abstract": "While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable from improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, we develop a tractable sampling scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our approach, samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. We validate the effectiveness of the proposed parametrizations by exploring the quality of samples drawn from these energy-based models for both open-ended unconditional generation and a conditional generation task of machine translation. We theoretically and empirically justify our sampling algorithm by showing that the masked conditionals on their own do not yield a Markov chain whose stationary distribution is that of our target distribution, and our approach generates higher quality samples than other recently proposed undirected generation approaches (Wang et al., 2019, Ghazvininejad et al., 2019).", "one-sentence_summary": "We interpret masked language models for sequences as energy based models and propose a tractable scheme inspired by Metropolis--Hasting Monte Carlo to draw samples from these models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "goyal|exposing_the_implicit_energy_networks_behind_masked_language_models_via_metropolishastings", "pdf": "/pdf/dfdc7212f0c035baaec71e0d9d64317aec15492b.pdf", "_bibtex": "@inproceedings{\ngoyal2022exposing,\ntitle={Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings},\nauthor={Kartik Goyal and Chris Dyer and Taylor Berg-Kirkpatrick},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6PvWo1kEvlT}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "EZNOb_uNpJk", "original": "S1N7EmvNdD", "number": 4226, "cdate": 1632875739026, "mdate": null, "ddate": null, "tcdate": 1632875739026, "tmdate": 1676330461769, "tddate": null, "forum": "EZNOb_uNpJk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods", "authorids": ["~Victor_Schmidt2", "~Alexandra_Luccioni1", "~M\u00e9lisande_Teng1", "tianyu.zhang@mila.quebec", "alexia.reynaud@polymtl.ca", "~Sunand_Raghupathi1", "cosne.gautier@gmail.com", "a.juraver@gmail.com", "vardanyan.vahe@gmail.com", "~Alex_Hern\u00e1ndez-Garc\u00eda1", "~Yoshua_Bengio1"], "authors": ["Victor Schmidt", "Alexandra Luccioni", "M\u00e9lisande Teng", "Tianyu Zhang", "Alexia Reynaud", "Sunand Raghupathi", "Gautier Cosne", "Adrien Juraver", "Vahe Vardanyan", "Alex Hern\u00e1ndez-Garc\u00eda", "Yoshua Bengio"], "keywords": ["GAN", "Climate Change", "Domain Adaptation", "Representation Learning", "Computer Vision", "Application"], "abstract": "Climate change is a major threat to humanity and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding its seemingly abstract and distant consequences. Projecting the potential impacts of extreme climate events such as flooding in familiar places can help make the impacts of climate change more concrete and encourage action. As part of a larger initiative to build a website (https://thisclimatedoesnotexist.com) that projects extreme climate events onto user-chosen photos, we present our solution to simulate photo-realistic floods on authentic images. To address this complex task in the absence of suitable data, we propose ClimateGAN, a model that leverages both simulated and real data through unsupervised domain adaptation and conditional image generation. In this paper, we describe the details of our framework, thoroughly evaluate the main components of our architecture and demonstrate that our model is capable of robustly generating photo-realistic flooding on street images.", "one-sentence_summary": "This paper presents a model to robustly produce photo-realistic images of floods for raising climate change awareness, leveraging unsupervised domain adaptation and conditional image generation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "schmidt|climategan_raising_climate_change_awareness_by_generating_images_of_floods", "pdf": "/pdf/ca121d72177c0fb77244bde0b2958681a89d4b98.pdf", "supplementary_material": "/attachment/e35ffdfb3f901392fc63cf210a0634fe93414b4b.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nschmidt2022climategan,\ntitle={Climate{GAN}: Raising Climate Change Awareness by Generating Images of Floods},\nauthor={Victor Schmidt and Alexandra Luccioni and M{\\'e}lisande Teng and Tianyu Zhang and Alexia Reynaud and Sunand Raghupathi and Gautier Cosne and Adrien Juraver and Vahe Vardanyan and Alex Hern{\\'a}ndez-Garc{\\'\\i}a and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EZNOb_uNpJk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "nhN-fqxmNGx", "original": "Mqqa761ll6Df", "number": 4224, "cdate": 1632875738893, "mdate": null, "ddate": null, "tcdate": 1632875738893, "tmdate": 1676330462149, "tddate": null, "forum": "nhN-fqxmNGx", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Comparison of Hamming Errors of Representative Variable Selection Methods", "authorids": ["~Tracy_Ke1", "~Longlin_Wang1"], "authors": ["Tracy Ke", "Longlin Wang"], "keywords": ["Lasso", "Hamming error", "phase diagram", "rare and weak signals", "elastic net", "SCAD", "thresholded Lasso", "forward selection", "forward backward selection"], "abstract": "Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the covariates are moderately or strongly correlated. This motivates alternative approaches such as using a non-convex penalty, adding a ridge regularization, or conducting a post-Lasso thresholding. In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso, and forward backward selection. We measure their performances theoretically by the expected Hamming error, assuming that the regression coefficients are ${\\it iid}$ drawn from a two-point mixture and that the Gram matrix is block-wise diagonal. By deriving the rates of convergence of Hamming errors and the phase diagrams, we obtain useful conclusions about the pros and cons of different methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ke|a_comparison_of_hamming_errors_of_representative_variable_selection_methods", "pdf": "/pdf/ae8e44624ed225194ef2c6ef294ae6d5067515b8.pdf", "one-sentence_summary": "A theoretical comparison of the Hamming errors for 6 different variable selection methods", "supplementary_material": "/attachment/4f16334f29e6421dce68a951e210ea5abf0531db.zip", "_bibtex": "@inproceedings{\nke2022a,\ntitle={A Comparison of Hamming Errors of Representative Variable Selection Methods},\nauthor={Tracy Ke and Longlin Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nhN-fqxmNGx}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "WE4qe9xlnQw", "original": "gCeTN7AdZhM", "number": 4218, "cdate": 1632875738488, "mdate": null, "ddate": null, "tcdate": 1632875738488, "tmdate": 1676330462613, "tddate": null, "forum": "WE4qe9xlnQw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Program to Build E(N)-Equivariant Steerable CNNs ", "authorids": ["~Gabriele_Cesa1", "~Leon_Lang1", "~Maurice_Weiler1"], "authors": ["Gabriele Cesa", "Leon Lang", "Maurice Weiler"], "keywords": ["equivariance", "3D", "geometric deep learning", "isometries", "steerable CNN"], "abstract": "Equivariance is becoming an increasingly popular design choice to build data efficient neural networks by exploiting prior knowledge about the symmetries of the problem at hand. Euclidean steerable CNNs are one of the most common classes of equivariant networks. While the constraints these architectures need to satisfy are understood, existing approaches are tailored to specific (classes of) groups. No generally applicable method that is practical for implementation has been described so far. In this work, we generalize the Wigner-Eckart theorem proposed in Lang & Weiler (2020), which characterizes general $G$-steerable kernel spaces for compact groups $G$ over their homogeneous spaces, to arbitrary $G$-spaces. This enables us to directly parameterize filters in terms of a band-limited basis on the whole space rather than on $G$'s orbits, but also to easily implement steerable CNNs equivariant to a large number of groups. To demonstrate its generality, we instantiate our method on a variety of isometry groups acting on the Euclidean space $\\mathbb{R}^3$. Our framework allows us to build $E(3)$ and $SE(3)$-steerable CNNs like previous works, but also CNNs with arbitrary $G\\leq O(3)$-steerable kernels. For example, we build 3D CNNs equivariant to the symmetries of platonic solids or choose $G=SO(2)$ when working with 3D data having only azimuthal symmetries. We compare these models on 3D shapes and molecular datasets, observing improved performance by matching the model's symmetries to the ones of the data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cesa|a_program_to_build_enequivariant_steerable_cnns", "pdf": "/pdf/6d634b6f1eabc70593f897e223c78025e3029b52.pdf", "one-sentence_summary": "We derive a general method to build G-steerable kernel spaces for equivariant steerable CNNs", "supplementary_material": "/attachment/8e59589617d819e70c68a8002c689047b6750a52.zip", "data": "", "_bibtex": "@inproceedings{\ncesa2022a,\ntitle={A Program to Build E(N)-Equivariant Steerable {CNN}s },\nauthor={Gabriele Cesa and Leon Lang and Maurice Weiler},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WE4qe9xlnQw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "UdxJ2fJx7N0", "original": "qqoU5AdqMzfa", "number": 4217, "cdate": 1632875738421, "mdate": null, "ddate": null, "tcdate": 1632875738421, "tmdate": 1697934538885, "tddate": null, "forum": "UdxJ2fJx7N0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Minimax Optimization with Smooth Algorithmic Adversaries", "authorids": ["~Tanner_Fiez1", "~Chi_Jin1", "~Praneeth_Netrapalli1", "~Lillian_J_Ratliff1"], "authors": ["Tanner Fiez", "Chi Jin", "Praneeth Netrapalli", "Lillian J Ratliff"], "keywords": ["Minimax optimization", "two player zero sum games", "generative adversarial networks", "adversarial training"], "abstract": "This paper considers minimax optimization $\\min_x \\max_y f(x, y)$ in the challenging setting where $f$ can be both nonconvex in $x$ and nonconcave in $y$. Though such optimization problems arise in many machine learning paradigms including training generative adversarial networks (GANs) and adversarially robust models, from a theoretical point of view, two fundamental issues remain: (i) the absence of simple and efficiently computable optimality notions, and (ii) cyclic or diverging behavior of existing algorithms. This paper proposes a new theoretical framework for nonconvex-nonconcave minimax optimization that addresses both of the above issues. The starting point of this paper is the observation that, under a computational budget, the max-player can not fully maximize $f(x,\\cdot)$ since nonconcave maximization is NP-hard in general. So, we propose a new framework, and a corresponding algorithm, for the min-player to play against \\emph{smooth algorithms} deployed by the adversary (i.e., the max-player) instead of against full maximization. Our algorithm is guaranteed to make monotonic progress (thus having no limit cycles or diverging behavior), and to find an appropriate ``stationary point'' in a polynomial number of iterations. Our framework covers practically relevant settings where the smooth algorithms deployed by the adversary are multi-step stochastic gradient ascent, and its accelerated version. We further present experimental results that confirm our theoretical findings and demonstrate the effectiveness of the proposed approach in practice on simple, conceptual settings.", "one-sentence_summary": "We propose a tractable formulation of minimax optimization by modeling the adversary's algorithm, and present new algorithms which are guaranteed to converge and find appropriate stationary points.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fiez|minimax_optimization_with_smooth_algorithmic_adversaries", "pdf": "/pdf/6f978c34600cf6fcf440c6e1bf8d1f93e0afce3d.pdf", "supplementary_material": "/attachment/4e2ba35603ae73bda8e10822707db81c5277ec74.zip", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.01488/code)", "_bibtex": "@inproceedings{\nfiez2022minimax,\ntitle={Minimax Optimization with Smooth Algorithmic Adversaries},\nauthor={Tanner Fiez and Chi Jin and Praneeth Netrapalli and Lillian J Ratliff},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=UdxJ2fJx7N0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "CI-xXX9dg9l", "original": "8SBdOP5ommX", "number": 4202, "cdate": 1632875737422, "mdate": null, "ddate": null, "tcdate": 1632875737422, "tmdate": 1676330463125, "tddate": null, "forum": "CI-xXX9dg9l", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Distributed Adaptive Optimization with Gradient Compression", "authorids": ["~Xiaoyun_Li1", "~Belhal_Karimi1", "~Ping_Li3"], "authors": ["Xiaoyun Li", "Belhal Karimi", "Ping Li"], "keywords": [], "abstract": "We study COMP-AMS, a distributed optimization framework based on gradient averaging and adaptive AMSGrad algorithm. Gradient compression with error feedback is applied to reduce the communication cost in the gradient transmission process. Our convergence analysis of COMP-AMS shows that such compressed gradient averaging strategy yields same convergence rate as standard AMSGrad, and also exhibits the linear speedup effect w.r.t. the number of local workers. Compared with recently proposed protocols on distributed adaptive methods, COMP-AMS is simple and convenient. Numerical experiments are conducted to justify the theoretical findings, and demonstrate that the proposed method can achieve same test accuracy as the full-gradient AMSGrad with substantial communication savings. With its simplicity and efficiency, COMP-AMS can serve as a useful distributed training framework for adaptive methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|on_distributed_adaptive_optimization_with_gradient_compression", "pdf": "/pdf/84313c8e0bf7b65d71addc3b16aba48f161f4092.pdf", "data": "", "_bibtex": "@inproceedings{\nli2022on,\ntitle={On Distributed Adaptive Optimization with Gradient Compression},\nauthor={Xiaoyun Li and Belhal Karimi and Ping Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CI-xXX9dg9l}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "o_HsiMPYh_x", "original": "OjUMTP0Awx", "number": 4190, "cdate": 1632875736617, "mdate": null, "ddate": null, "tcdate": 1632875736617, "tmdate": 1697934541477, "tddate": null, "forum": "o_HsiMPYh_x", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Leveraging unlabeled data to predict out-of-distribution performance", "authorids": ["~Saurabh_Garg3", "~Sivaraman_Balakrishnan1", "~Zachary_Chase_Lipton1", "~Behnam_Neyshabur1", "~Hanie_Sedghi1"], "authors": ["Saurabh Garg", "Sivaraman Balakrishnan", "Zachary Chase Lipton", "Behnam Neyshabur", "Hanie Sedghi"], "keywords": ["Distribution Shift", "OOD error prediction", "Deep Learning"], "abstract": "Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions\nthat may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a \\emph{threshold} on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (\\textsc{Wilds}-FMoW, ImageNet, \\breeds, CIFAR, and MNIST).  In our experiments, ATC estimates target performance $2\\text{--}4\\times$ more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works.\n\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "garg|leveraging_unlabeled_data_to_predict_outofdistribution_performance", "pdf": "/pdf/f94008d1c0cfc4177d8617db211b62b1f85906ea.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.04234/code)", "_bibtex": "@inproceedings{\ngarg2022leveraging,\ntitle={Leveraging unlabeled data to predict out-of-distribution performance},\nauthor={Saurabh Garg and Sivaraman Balakrishnan and Zachary Chase Lipton and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=o_HsiMPYh_x}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "7udZAsEzd60", "original": "07Whlyou2JC", "number": 4186, "cdate": 1632875736348, "mdate": null, "ddate": null, "tcdate": 1632875736348, "tmdate": 1676330464034, "tddate": null, "forum": "7udZAsEzd60", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "VC dimension of partially quantized neural networks in the overparametrized regime", "authorids": ["~Yutong_Wang1", "~Clayton_Scott1"], "authors": ["Yutong Wang", "Clayton Scott"], "keywords": ["VC dimension", "quantized neural networks", "classification", "minimax theory", "overparametrization"], "abstract": "Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we focus on a class of partially quantized networks that we refer to as hyperplane arrangement neural networks (HANNs). Using a sample compression analysis, we show that HANNs can have VC dimension significantly smaller than the number of weights, while being highly expressive. In particular, empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. We further demonstrate the expressivity of HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs are able to match the performance of state-of-the-art full-precision models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|vc_dimension_of_partially_quantized_neural_networks_in_the_overparametrized_regime", "pdf": "/pdf/9760187606b3496a5f4a0fe752a22416bb4a2e21.pdf", "one-sentence_summary": "We apply VC theory to analyze the performance of a neural network in the overparametrized regime and obtain a minimax-optimality result.", "supplementary_material": "/attachment/0a8e41070fc65e398f4f8fb821807fb71ff3378e.zip", "code": "", "_bibtex": "@inproceedings{\nwang2022vc,\ntitle={{VC} dimension of partially quantized neural networks in the overparametrized regime},\nauthor={Yutong Wang and Clayton Scott},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7udZAsEzd60}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "Rf58LPCwJj0", "original": "-njhPrIYcw3c", "number": 4179, "cdate": 1632875735874, "mdate": null, "ddate": null, "tcdate": 1632875735874, "tmdate": 1697934542568, "tddate": null, "forum": "Rf58LPCwJj0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Optimal Representations for Covariate Shift", "authorids": ["~Yangjun_Ruan1", "~Yann_Dubois1", "~Chris_J._Maddison1"], "authors": ["Yangjun Ruan", "Yann Dubois", "Chris J. Maddison"], "keywords": ["distribution shift", "domain generalization", "representation learning", "self-supervised learning", "invariance", "robustness"], "abstract": "Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representation's marginal support needs to be the same across source and target. We make this practical by designing self-supervised objectives that only use unlabelled data and augmentations to train robust representations. \nOur objectives give insights into the robustness of CLIP, and further improve CLIP's representations to achieve SOTA results on DomainBed.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ruan|optimal_representations_for_covariate_shift", "pdf": "/pdf/ddc6369b11aed2bc1a72bc2f493bb2ebd0f65be7.pdf", "one-sentence_summary": "We give a simple variational objective whose optima are exactly the set of representations that are robust under covariate shift", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2201.00057/code)", "_bibtex": "@inproceedings{\nruan2022optimal,\ntitle={Optimal Representations for Covariate Shift},\nauthor={Yangjun Ruan and Yann Dubois and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Rf58LPCwJj0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "ei3SY1_zYsE", "original": "7i9FP3LesvH", "number": 4178, "cdate": 1632875735807, "mdate": null, "ddate": null, "tcdate": 1632875735807, "tmdate": 1697934542788, "tddate": null, "forum": "ei3SY1_zYsE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fortuitous Forgetting in Connectionist Networks", "authorids": ["~Hattie_Zhou1", "~Ankit_Vani1", "~Hugo_Larochelle1", "~Aaron_Courville3"], "authors": ["Hattie Zhou", "Ankit Vani", "Hugo Larochelle", "Aaron Courville"], "keywords": ["Neural Networks", "Generalization", "Iterative Training", "Compositionality", "Iterated Learning"], "abstract": "Forgetting is often seen as an unwanted characteristic in both human and machine learning. However, we propose that forgetting can in fact be favorable to learning. We introduce forget-and-relearn as a powerful paradigm for shaping the learning trajectories of artificial neural networks. In this process, the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The forget-and-relearn framework unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. We leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations. Insights from our analysis provide a coherent view on the dynamics of iterative training in neural networks and offer a clear path towards performance improvements.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhou|fortuitous_forgetting_in_connectionist_networks", "pdf": "/pdf/ca4d5fd0fac40867b797ca356f4056c7cb11fc6a.pdf", "one-sentence_summary": "We introduce \"forget-and-relearn\" as a training paradigm where forgetting removes undesirable information and relearning bolsters useful features towards better generalization and compositionality.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.00155/code)", "_bibtex": "@inproceedings{\nzhou2022fortuitous,\ntitle={Fortuitous Forgetting in Connectionist Networks},\nauthor={Hattie Zhou and Ankit Vani and Hugo Larochelle and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ei3SY1_zYsE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "So6YAqnqgMj", "original": "8dGddB7LRc1X", "number": 4175, "cdate": 1632875735606, "mdate": null, "ddate": null, "tcdate": 1632875735606, "tmdate": 1676330464400, "tddate": null, "forum": "So6YAqnqgMj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "EigenGame Unloaded: When playing games is better than optimizing", "authorids": ["~Ian_Gemp1", "~Brian_McWilliams2", "~Claire_Vernade1", "~Thore_Graepel1"], "authors": ["Ian Gemp", "Brian McWilliams", "Claire Vernade", "Thore Graepel"], "keywords": ["pca", "principal components analysis", "nash", "games", "eigendecomposition", "svd", "singular value decomposition"], "abstract": "We build on the recently proposed EigenGame that views eigendecomposition as a competitive game. EigenGame's updates are biased if computed using minibatches of data, which hinders convergence and more sophisticated parallelism in the stochastic setting. In this work, we propose an unbiased stochastic update that is asymptotically equivalent to EigenGame, enjoys greater parallelism allowing computation on datasets of larger sample sizes, and outperforms EigenGame in experiments. We present applications to finding the principal components of massive datasets and performing spectral clustering of graphs. We analyze and discuss our proposed update in the context of EigenGame and the shift in perspective from optimization to games.", "one-sentence_summary": "We improve the EigenGame algorithm by removing update bias, enabling further parallelism and better performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gemp|eigengame_unloaded_when_playing_games_is_better_than_optimizing", "pdf": "/pdf/cedcb096f43d8f1b1e43c8969cf5b1dd7e83d5ae.pdf", "supplementary_material": "/attachment/1bb9f80ed132a3de78ef901e29fcfe86fe5533aa.zip", "_bibtex": "@inproceedings{\ngemp2022eigengame,\ntitle={EigenGame Unloaded: When playing games is better than optimizing},\nauthor={Ian Gemp and Brian McWilliams and Claire Vernade and Thore Graepel},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=So6YAqnqgMj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "Oh1r2wApbPv", "original": "2-1zPOoichsq", "number": 4167, "cdate": 1632875735066, "mdate": null, "ddate": null, "tcdate": 1632875735066, "tmdate": 1697934543779, "tddate": null, "forum": "Oh1r2wApbPv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Contextualized Scene Imagination for Generative Commonsense Reasoning", "authorids": ["~PeiFeng_Wang1", "jzamoraa@ucsd.edu", "liujunfe@usc.edu", "~Filip_Ilievski1", "~Muhao_Chen1", "~Xiang_Ren1"], "authors": ["PeiFeng Wang", "Jonathan Zamora", "Junfeng Liu", "Filip Ilievski", "Muhao Chen", "Xiang Ren"], "keywords": ["Commonsense reasoning", "constrained text generation", "knowledge representation"], "abstract": "Humans use natural language to compose common concepts from their environment into plausible, day-to-day scene descriptions. However, such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Descriptive sentences about arbitrary concepts generated by neural text generation models (e.g., pre-trained text-to-text Transformers) are often grammatically fluent but may not correspond to human common sense, largely due to their lack of mechanisms to capture concept relations, to identify implicit concepts, and to perform generalizable reasoning about unseen concept compositions. In this paper, we propose an Imagine-and-Verbalize (I\\&V) method, which learns to imagine a relational scene knowledge graph (SKG) with relations between the input concepts, and leverage the SKG as a constraint when generating a plausible scene description. We collect and harmonize a set of knowledge resources from different domains and modalities, providing a rich auxiliary supervision signal for I\\&V. The experiments demonstrate the effectiveness of I\\&V in improving language models on both concept-to-sentence and concept-to-story generation tasks, while enabling the model to learn well from fewer task examples and generate SKGs that make common sense to human annotators.", "one-sentence_summary": "This work aims at tackling generative commonsense reasoning by allowing machines to imagine a reasonable scene before generating text.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|contextualized_scene_imagination_for_generative_commonsense_reasoning", "pdf": "/pdf/a66e1b12b2211131a44463611c8c272c21decbfb.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.06318/code)", "_bibtex": "@inproceedings{\nwang2022contextualized,\ntitle={Contextualized Scene Imagination for Generative Commonsense Reasoning},\nauthor={PeiFeng Wang and Jonathan Zamora and Junfeng Liu and Filip Ilievski and Muhao Chen and Xiang Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Oh1r2wApbPv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "Wm3EA5OlHsG", "original": "FOt3bNmDFnv", "number": 4165, "cdate": 1632875734935, "mdate": null, "ddate": null, "tcdate": 1632875734935, "tmdate": 1690470727117, "tddate": null, "forum": "Wm3EA5OlHsG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Scene Transformer: A unified architecture for predicting future trajectories of multiple agents", "authorids": ["~Jiquan_Ngiam1", "~Vijay_Vasudevan1", "~Benjamin_Caine1", "~Zhengdong_Zhang3", "~Hao-Tien_Lewis_Chiang1", "~Jeffrey_Ling1", "~Rebecca_Roelofs1", "~Alex_Bewley1", "~Chenxi_Liu1", "~Ashish_Venugopal1", "~David_J_Weiss1", "~Benjamin_Sapp3", "~Zhifeng_Chen1", "~Jonathon_Shlens1"], "authors": ["Jiquan Ngiam", "Vijay Vasudevan", "Benjamin Caine", "Zhengdong Zhang", "Hao-Tien Lewis Chiang", "Jeffrey Ling", "Rebecca Roelofs", "Alex Bewley", "Chenxi Liu", "Ashish Venugopal", "David J Weiss", "Benjamin Sapp", "Zhifeng Chen", "Jonathon Shlens"], "keywords": ["trajectory prediction", "motion forecasting", "multi-task learning", "attention", "autonomous vehicles"], "abstract": "Predicting the motion of multiple agents is necessary for planning in dynamic environments. This task is challenging for autonomous driving since agents (e.g., vehicles and pedestrians) and their associated behaviors may be diverse and influence one another. Most prior work have focused on predicting independent futures for each agent based on all past motion, and planning against these independent predictions. However, planning against independent predictions can make it challenging to represent the future interaction possibilities between different agents, leading to sub-optimal planning. In this work, we formulate a model for predicting the behavior of all agents jointly, producing consistent futures that account for interactions between agents. Inspired by recent language modeling approaches, we use a masking strategy as the query to our model, enabling one to invoke a single model to predict agent behavior in many ways, such as potentially conditioned on the goal or full future trajectory of the autonomous vehicle or the behavior of other agents in the environment. Our model architecture employs attention to combine features across road elements, agent interactions, and time steps. We evaluate our approach on autonomous driving datasets for both marginal and joint motion prediction, and achieve state of the art performance across two popular datasets. Through combining a scene-centric approach, agent permutation equivariant model, and a sequence masking strategy, we show that our model can unify a variety of motion prediction tasks from joint motion predictions to conditioned prediction.", "one-sentence_summary": "We introduce a scene-centric masked sequence based motion prediction model that unifies a variety of motion prediction tasks from joint motion predictions to conditioned prediction.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ngiam|scene_transformer_a_unified_architecture_for_predicting_future_trajectories_of_multiple_agents", "pdf": "/pdf/92f191f2cdcf1389ed2d3dce901833dc5fc6deaf.pdf", "data": "", "_bibtex": "@inproceedings{\nngiam2022scene,\ntitle={Scene Transformer: A unified architecture for predicting future trajectories of multiple agents},\nauthor={Jiquan Ngiam and Vijay Vasudevan and Benjamin Caine and Zhengdong Zhang and Hao-Tien Lewis Chiang and Jeffrey Ling and Rebecca Roelofs and Alex Bewley and Chenxi Liu and Ashish Venugopal and David J Weiss and Benjamin Sapp and Zhifeng Chen and Jonathon Shlens},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Wm3EA5OlHsG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "qY79G8jGsep", "original": "tXGC-pZS7m", "number": 4164, "cdate": 1632875734869, "mdate": null, "ddate": null, "tcdate": 1632875734869, "tmdate": 1697934544250, "tddate": null, "forum": "qY79G8jGsep", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals", "authorids": ["~Asma_Ghandeharioun1", "~Been_Kim1", "~Chun-Liang_Li1", "~Brendan_Jou1", "~Brian_Eoff1", "~Rosalind_Picard1"], "authors": ["Asma Ghandeharioun", "Been Kim", "Chun-Liang Li", "Brendan Jou", "Brian Eoff", "Rosalind Picard"], "keywords": ["Explainability", "Interpretability", "Counterfactual generation", "Generative Adversarial Network", "Variational Autoencoder"], "abstract": "Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore \"what-if\" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent \"notion\" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ghandeharioun|dissect_disentangled_simultaneous_explanations_via_concept_traversals", "pdf": "/pdf/8e8a8d5dafd24c9cba49d3671b2ee34d0decdecf.pdf", "one-sentence_summary": "We propose a novel counterfactual explainability method that simultaneously satisfies several desirable qualities where other methods fail by training a generator, a discriminator, and a concept disentangler using the classifier\u2019s signal.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2105.15164/code)", "_bibtex": "@inproceedings{\nghandeharioun2022dissect,\ntitle={{DISSECT}: Disentangled Simultaneous Explanations via Concept Traversals},\nauthor={Asma Ghandeharioun and Been Kim and Chun-Liang Li and Brendan Jou and Brian Eoff and Rosalind Picard},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qY79G8jGsep}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "Az7opqbQE-3", "original": "H5RcWSPfvj", "number": 4163, "cdate": 1632875734805, "mdate": null, "ddate": null, "tcdate": 1632875734805, "tmdate": 1676330465489, "tddate": null, "forum": "Az7opqbQE-3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series", "authorids": ["~Satya_Narayan_Shukla1", "~Benjamin_Marlin1"], "authors": ["Satya Narayan Shukla", "Benjamin Marlin"], "keywords": ["irregular sampling", "uncertainty", "imputation", "interpolation", "multivariate time series", "missing data", "variational autoencoder"], "abstract": "Irregularly sampled time series commonly occur in several domains where they present a significant challenge to standard deep learning models. In this paper, we propose a new deep learning framework for probabilistic interpolation of irregularly sampled time series that we call the Heteroscedastic Temporal Variational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode information about input observation sparsity, a temporal VAE architecture to propagate uncertainty due to input sparsity, and a heteroscedastic output layer to enable variable uncertainty in the output interpolations.  Our results show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models, as well as recently proposed deep latent variable models that use homoscedastic output layers.", "one-sentence_summary": "We present a new deep learning architecture for probabilistic interpolation of irregularly sampled time series.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shukla|heteroscedastic_temporal_variational_autoencoder_for_irregularly_sampled_time_series", "pdf": "/pdf/4a602866528e0ae9511889c65b61991ad9ddfd8b.pdf", "supplementary_material": "", "code": "", "data": "", "_bibtex": "@inproceedings{\nshukla2022heteroscedastic,\ntitle={Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series},\nauthor={Satya Narayan Shukla and Benjamin Marlin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Az7opqbQE-3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "vUH85MOXO7h", "original": "Eepw3MEpF2O9", "number": 4162, "cdate": 1632875734740, "mdate": null, "ddate": null, "tcdate": 1632875734740, "tmdate": 1697934544522, "tddate": null, "forum": "vUH85MOXO7h", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Neural Tangent Kernel Perspective of Infinite Tree Ensembles", "authorids": ["~Ryuichi_Kanoh1", "~Mahito_Sugiyama1"], "authors": ["Ryuichi Kanoh", "Mahito Sugiyama"], "keywords": ["Neural Tangent Kernel", "Tree Ensemble", "Soft Tree"], "abstract": "In practical situations, the tree ensemble is one of the most popular models along with neural networks. A soft tree is a variant of a decision tree. Instead of using a greedy method for searching splitting rules, the soft tree is trained using a gradient method in which the entire splitting operation is formulated in a differentiable form. Although ensembles of such soft trees have been used increasingly in recent years, little theoretical work has been done to understand their behavior. By considering an ensemble of infinite soft trees, this paper introduces and studies the Tree Neural Tangent Kernel (TNTK), which provides new insights into the behavior of the infinite ensemble of soft trees. Using the TNTK, we theoretically identify several non-trivial properties, such as global convergence of the training, the equivalence of the oblivious tree structure, and the degeneracy of the TNTK induced by the deepening of the trees.", "one-sentence_summary": "By considering an ensemble of infinite trees, we introduce and study the Tree Neural Tangent Kernel (TNTK), which provides new insights into the behavior of the infinite ensemble of soft trees.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kanoh|a_neural_tangent_kernel_perspective_of_infinite_tree_ensembles", "pdf": "/pdf/39b3d2b8700abc51932e7eea69ff8d0868dc2be8.pdf", "supplementary_material": "/attachment/4dbd79f04fb1a962d127105279eb63dee09e444f.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.04983/code)", "_bibtex": "@inproceedings{\nkanoh2022a,\ntitle={A Neural Tangent Kernel Perspective of Infinite Tree Ensembles},\nauthor={Ryuichi Kanoh and Mahito Sugiyama},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vUH85MOXO7h}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "nKWjE4QF1hB", "original": "SHRrmv-vJYeB", "number": 4161, "cdate": 1632875734675, "mdate": null, "ddate": null, "tcdate": 1632875734675, "tmdate": 1676330465603, "tddate": null, "forum": "nKWjE4QF1hB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "AlphaZero-based Proof Cost Network to Aid Game Solving", "authorids": ["~Ti-Rong_Wu1", "~Chung-Chin_Shih2", "~Ting_Han_Wei1", "~Meng-Yu_Tsai1", "~Wei-Yuan_Hsu1", "~I-Chen_Wu3"], "authors": ["Ti-Rong Wu", "Chung-Chin Shih", "Ting Han Wei", "Meng-Yu Tsai", "Wei-Yuan Hsu", "I-Chen Wu"], "keywords": ["Monte-Carlo Tree Search", "Solving Games", "AlphaZero", "Deep Reinforcement Learning"], "abstract": "The AlphaZero algorithm learns and plays games without hand-crafted expert knowledge. However, since its objective is to play well, we hypothesize that a better objective can be defined for the related but separate task of solving games. This paper proposes a novel approach to solving problems by modifying the training target of the AlphaZero algorithm, such that it prioritizes solving the game quickly, rather than winning. We train a Proof Cost Network (PCN), where proof cost is a heuristic that estimates the amount of work required to solve problems. This matches the general concept of the so-called proof number from proof number search, which has been shown to be well-suited for game solving. We propose two specific training targets. The first finds the shortest path to a solution, while the second estimates the proof cost. We conduct experiments on solving 15x15 Gomoku and 9x9 Killall-Go problems with both MCTS-based and FDFPN solvers. Comparisons between using AlphaZero networks and PCN as heuristics show that PCN can solve more problems.", "one-sentence_summary": "This paper proposes a novel approach to solving problems by modifying the training target of the AlphaZero algorithm, such that it prioritizes solving the game quickly, rather than winning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|alphazerobased_proof_cost_network_to_aid_game_solving", "pdf": "/pdf/b5c23474ea991857d67e3e750bb82c36a669b2e9.pdf", "supplementary_material": "/attachment/74372572d1441e09d2444d5562c08ab56db2eaa7.zip", "_bibtex": "@inproceedings{\nwu2022alphazerobased,\ntitle={AlphaZero-based Proof Cost Network to Aid Game Solving},\nauthor={Ti-Rong Wu and Chung-Chin Shih and Ting Han Wei and Meng-Yu Tsai and Wei-Yuan Hsu and I-Chen Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nKWjE4QF1hB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "f2lrIbGx3x7", "original": "KwNrBNQrlLs", "number": 4155, "cdate": 1632875734272, "mdate": null, "ddate": null, "tcdate": 1632875734272, "tmdate": 1697934545449, "tddate": null, "forum": "f2lrIbGx3x7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bayesian Framework for Gradient Leakage", "authorids": ["~Mislav_Balunovic1", "~Dimitar_Iliev_Dimitrov2", "~Robin_Staab1", "~Martin_Vechev1"], "authors": ["Mislav Balunovic", "Dimitar Iliev Dimitrov", "Robin Staab", "Martin Vechev"], "keywords": ["federated learning", "privacy", "gradient leakage"], "abstract": "Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "balunovic|bayesian_framework_for_gradient_leakage", "pdf": "/pdf/4e51a98c83f488bc5362a078c71216dab544be00.pdf", "one-sentence_summary": "We propose a theoretical framework for analysis of the Bayes optimal adversary for gradient leakage, and perform evaluation of existing defenses.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.04706/code)", "_bibtex": "@inproceedings{\nbalunovic2022bayesian,\ntitle={Bayesian Framework for Gradient Leakage},\nauthor={Mislav Balunovic and Dimitar Iliev Dimitrov and Robin Staab and Martin Vechev},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=f2lrIbGx3x7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "YpPiNigTzMT", "original": "XPViQg8oD_l", "number": 4154, "cdate": 1632875734203, "mdate": null, "ddate": null, "tcdate": 1632875734203, "tmdate": 1676330465793, "tddate": null, "forum": "YpPiNigTzMT", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Universalizing Weak Supervision", "authorids": ["~Changho_Shin2", "~Winfred_Li1", "~Harit_Vishwakarma1", "~Nicholas_Carl_Roberts1", "~Frederic_Sala1"], "authors": ["Changho Shin", "Winfred Li", "Harit Vishwakarma", "Nicholas Carl Roberts", "Frederic Sala"], "keywords": ["Weak supervision"], "abstract": "Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large datasets for training data-hungry models.\nThese approaches synthesize multiple noisy but cheaply-acquired estimates of labels into a set of high-quality pseudo-labels for downstream training. However, the synthesis technique is specific to a particular kind of label, such as binary labels or sequences, and each new label type requires manually designing a new synthesis algorithm. Instead, we propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees. We apply this technique to important problems previously not tackled by WS frameworks including learning to rank, regression, and learning in hyperbolic space. Theoretically, our synthesis approach produces a consistent estimators for learning some challenging but important generalizations of the exponential family model. Experimentally, we validate our framework and show improvement over baselines in diverse settings including real-world learning-to-rank and regression problems along with learning on hyperbolic manifolds.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shin|universalizing_weak_supervision", "pdf": "/pdf/a2adc08eeb52dcddf2563c7bb42940946813b522.pdf", "one-sentence_summary": "We extend weak supervision frameworks to new settings \u2014 rankings, regression, Riemannian spaces, and more \u2014 with a universal algorithm with theoretical guarantees. ", "supplementary_material": "/attachment/fd1ee4735ecb1725affb1b44878a640320daa2d4.zip", "_bibtex": "@inproceedings{\nshin2022universalizing,\ntitle={Universalizing Weak Supervision},\nauthor={Changho Shin and Winfred Li and Harit Vishwakarma and Nicholas Carl Roberts and Frederic Sala},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YpPiNigTzMT}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "ULfq0qR25dY", "original": "FTsVDQC3Wo9", "number": 4153, "cdate": 1632875734136, "mdate": null, "ddate": null, "tcdate": 1632875734136, "tmdate": 1676330466198, "tddate": null, "forum": "ULfq0qR25dY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Maximum n-times Coverage for Vaccine Design", "authorids": ["~Ge_Liu2", "~Alexander_Dimitrakakis1", "~Brandon_Carter1", "~David_Gifford1"], "authors": ["Ge Liu", "Alexander Dimitrakakis", "Brandon Carter", "David Gifford"], "keywords": ["computational biology", "vaccine design", "COVID-19", "maximum n-times coverage", "combinatorial optimization", "integer linear programming"], "abstract": "We introduce the maximum $n$-times coverage problem that selects $k$ overlays to maximize the summed coverage of weighted elements, where each element must be covered at least $n$ times. We also define the min-cost $n$-times coverage problem where the objective is to select the minimum set of overlays such that the sum of the weights of elements that are covered at least $n$ times is at least $\\tau$. Maximum $n$-times coverage is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. We introduce two new practical solutions for $n$-times coverage based on integer linear programming and sequential greedy optimization. We show that maximum $n$-times coverage is a natural way to frame peptide vaccine design, and find that it produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual's HLA molecules.", "one-sentence_summary": "We introduce the maximum $n$-times coverage problem that selects $k$ overlays to maximize the summed coverage of weighted elements, where each element must be covered at least $n$ times, and show its importance for vaccine design.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|maximum_ntimes_coverage_for_vaccine_design", "pdf": "/pdf/9d61f13ecd3d02a7e3ed6243e5e82f05c5f456cf.pdf", "supplementary_material": "/attachment/5e8db7a31cf8f7eef84a53079c32f0d8e8efb459.zip", "code": "", "_bibtex": "@inproceedings{\nliu2022maximum,\ntitle={Maximum n-times Coverage for Vaccine Design},\nauthor={Ge Liu and Alexander Dimitrakakis and Brandon Carter and David Gifford},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ULfq0qR25dY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "0JzqUlIVVDd", "original": "JeUCSh5Eci-6", "number": 4148, "cdate": 1632875733790, "mdate": null, "ddate": null, "tcdate": 1632875733790, "tmdate": 1697934546314, "tddate": null, "forum": "0JzqUlIVVDd", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "KL Guided Domain Adaptation", "authorids": ["~A._Tuan_Nguyen1", "~Toan_Tran1", "~Yarin_Gal1", "~Philip_Torr1", "~Atilim_Gunes_Baydin1"], "authors": ["A. Tuan Nguyen", "Toan Tran", "Yarin Gal", "Philip Torr", "Atilim Gunes Baydin"], "keywords": ["domain adaptation", "invariant representation"], "abstract": "Domain adaptation is an important problem and often needed for real-world applications. In this problem, instead of i.i.d. training and testing datapoints, we assume that the source (training) data and the target (testing) data have different distributions. With that setting, the empirical risk minimization training procedure often does not perform well, since it does not account for the change in the distribution. A common approach in the domain adaptation literature is to learn a representation of the input that has the same (marginal) distribution over the source and the target domain. However, these approaches often require additional networks and/or optimizing an adversarial (minimax) objective, which can be very expensive or unstable in practice. To improve upon these marginal alignment techniques, in this paper, we first derive a generalization bound for the target loss based on the training loss and the reverse Kullback-Leibler (KL) divergence between the source and the target representation distributions. Based on this bound, we derive an algorithm that minimizes the KL term to obtain a better generalization to the target domain. We show that with a probabilistic representation network, the KL term can be estimated efficiently via minibatch samples without any additional network or a minimax objective. This leads to a theoretically sound alignment method which is also very efficient and stable in practice. Experimental results also suggest that our method outperforms other representation-alignment approaches.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nguyen|kl_guided_domain_adaptation", "pdf": "/pdf/943a05167d50e4a4de4e6c043f7c7e6374502f72.pdf", "one-sentence_summary": "We derive a generalization bound for the domain adaptation problem based on the reversed KL divergence, and propose to regularize the KL term to lower the generalization bound.", "supplementary_material": "/attachment/1db7dd3c325b238240272c2a63bfa0ca9f38dbc8.zip", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.07780/code)", "_bibtex": "@inproceedings{\nnguyen2022kl,\ntitle={{KL} Guided Domain Adaptation},\nauthor={A. Tuan Nguyen and Toan Tran and Yarin Gal and Philip Torr and Atilim Gunes Baydin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0JzqUlIVVDd}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "Mspk_WYKoEH", "original": "v8mZaiCg9ut", "number": 4144, "cdate": 1632875733523, "mdate": null, "ddate": null, "tcdate": 1632875733523, "tmdate": 1676330468510, "tddate": null, "forum": "Mspk_WYKoEH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness", "authorids": ["~Lingxiao_Zhao1", "~Wei_Jin4", "~Leman_Akoglu3", "~Neil_Shah2"], "authors": ["Lingxiao Zhao", "Wei Jin", "Leman Akoglu", "Neil Shah"], "keywords": ["Graph Neural Networks", "Expressiveness", "Message Passing Neural Network", "Graph Classification"], "abstract": "Message Passing Neural Networks (MPNNs) are a common type of Graph Neural Network (GNN), in which each node\u2019s representation is computed recursively by aggregating representations (\u201cmessages\u201d) from its immediate neighbors akin to a star-shaped pattern. MPNNs are appealing for being efficient and scalable, however their expressiveness is upper-bounded by the 1st-order Weisfeiler-Lehman isomorphism test (1-WL). In response, prior works propose highly expressive models at the cost of scalability and sometimes generalization performance. Our work stands between these two regimes: we introduce a general framework to uplift any MPNN to be more expressive, with limited scalability overhead and greatly improved practical performance. We achieve this by extending local aggregation in MPNNs from star patterns to general subgraph patterns (e.g., k-egonets): in our framework, each node representation is computed as the encoding of a surrounding induced subgraph rather than encoding of immediate neighbors only (i.e. a star). We choose the subgraph encoder to be a GNN (mainly MPNNs, considering scalability) to design a general framework that serves as a wrapper to uplift any GNN. We call our proposed method GNN-AK (GNN As Kernel), as the framework resembles a convolutional neural network by replacing the kernel with\nGNNs. Theoretically, we show that our framework is strictly more powerful than 1&2-WL, and is not less powerful than 3-WL. We also design subgraph sampling strategies which greatly reduce memory footprint and improve speed while maintaining performance. Our method sets new state-of-the-art performance by large margins for several well-known graph ML tasks; specifically, 0.08 MAE on ZINC,\n74.79% and 86.887% accuracy on CIFAR10 and PATTERN respectively.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhao|from_stars_to_subgraphs_uplifting_any_gnn_with_local_structure_awareness", "pdf": "/pdf/cc341ac588b917bee10fc4d5bb31b4a119b6108b.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nzhao2022from,\ntitle={From Stars to Subgraphs: Uplifting Any {GNN} with Local Structure Awareness},\nauthor={Lingxiao Zhao and Wei Jin and Leman Akoglu and Neil Shah},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Mspk_WYKoEH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 29}}, {"id": "-8sBpe7rDiV", "original": "TGWCq7it-RV", "number": 4142, "cdate": 1632875733387, "mdate": null, "ddate": null, "tcdate": 1632875733387, "tmdate": 1676330468512, "tddate": null, "forum": "-8sBpe7rDiV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "NETWORK INSENSITIVITY TO PARAMETER NOISE VIA PARAMETER ATTACK DURING TRAINING", "authorids": ["~Julian_B\u00fcchel1", "~Fynn_Firouz_Faber1", "~Dylan_Richard_Muir1"], "authors": ["Julian B\u00fcchel", "Fynn Firouz Faber", "Dylan Richard Muir"], "keywords": ["parameter attack", "adversarial attack", "neural network", "deep learning", "optimisation", "neuromorphic processor"], "abstract": "Neuromorphic neural network processors, in the form of compute-in-memory crossbar arrays of memristors, or in the form of subthreshold analog and mixed-signal ASICs, promise enormous advantages in compute density and energy efficiency for NN-based ML tasks. However, these technologies are prone to computational non-idealities, due to process variation and intrinsic device physics. This degrades the task performance of networks deployed to the processor, by introducing parameter noise into the deployed model. While it is possible to calibrate each device, or train networks individually for each processor, these approaches are expensive and impractical for commercial deployment. Alternative methods are therefore needed to train networks that are inherently robust against parameter variation, as a consequence of network architecture and parameters. We present a new network training algorithm that attacks network parameters during training, and promotes robust performance during inference in the face of random parameter variation. Our approach introduces a loss regularization term that penalizes the susceptibility of a network to weight perturbation. We compare against previous approaches for producing parameter insensitivity such as dropout, weight smoothing and introducing parameter noise during training. We show that our approach produces models that are more robust to random mismatch-induced parameter variation as well as to targeted parameter variation. Our approach finds minima in flatter locations in the weight-loss landscape compared with other approaches, highlighting that the networks found by our technique are less sensitive to parameter perturbation. Our work provides an approach to deploy neural network architectures to inference devices that suffer from computational non-idealities, with minimal loss of performance. This method will enable deployment at scale to novel energy-efficient computational substrates, promoting cheaper and more prevalent edge inference.", "one-sentence_summary": "We flatten the weight loss-landscape by introducing a parameter attack term in the loss function and demonstrate improved network insensitivity to noise common in analog neuromorphic hardware.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "b\u00fcchel|network_insensitivity_to_parameter_noise_via_parameter_attack_during_training", "pdf": "/pdf/b7b77ce8535702dba33084aa20eb08cae53193f4.pdf", "supplementary_material": "/attachment/7ffbeeb53e08296882cdec5ad8afc7050521ca29.zip", "_bibtex": "@inproceedings{\nb{\\\"u}chel2022network,\ntitle={{NETWORK} {INSENSITIVITY} {TO} {PARAMETER} {NOISE} {VIA} {PARAMETER} {ATTACK} {DURING} {TRAINING}},\nauthor={Julian B{\\\"u}chel and Fynn Firouz Faber and Dylan Richard Muir},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-8sBpe7rDiV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "fXHl76nO2AZ", "original": "Bbkp5ZkRU5", "number": 4135, "cdate": 1632875732906, "mdate": null, "ddate": null, "tcdate": 1632875732906, "tmdate": 1676330468615, "tddate": null, "forum": "fXHl76nO2AZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Gradient Importance Learning for Incomplete Observations", "authorids": ["~Qitong_Gao1", "~Dong_Wang2", "~Joshua_David_Amason1", "~Siyang_Yuan1", "~Chenyang_Tao1", "~Ricardo_Henao1", "~Majda_Hadziahmetovic1", "~Lawrence_Carin2", "~Miroslav_Pajic2"], "authors": ["Qitong Gao", "Dong Wang", "Joshua David Amason", "Siyang Yuan", "Chenyang Tao", "Ricardo Henao", "Majda Hadziahmetovic", "Lawrence Carin", "Miroslav Pajic"], "keywords": ["Missing Data", "Reinforcement Learning", "Representation Learning"], "abstract": "Though recent works have developed methods that can generate estimates (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may not align with real-world applications and could suffer from poor performance in subsequent tasks such as classification. This is particularly true if the data have large missingness rates or a small sample size. More importantly, the imputation error could be propagated into the prediction step that follows, which may constrain the capabilities of the prediction model. In this work, we introduce the gradient importance learning (GIL) method to train multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly perform inference from inputs containing missing values without imputation. Specifically, we employ reinforcement learning (RL) to adjust the gradients used to train these models via back-propagation. This allows the model to exploit the underlying information behind missingness patterns. We test the approach on real-world time-series (i.e., MIMIC-III), tabular data obtained from an eye clinic, and a standard dataset (i.e., MNIST), where our imputation-free predictions outperform the traditional two-step imputation-based predictions using state-of-the-art imputation methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gao|gradient_importance_learning_for_incomplete_observations", "pdf": "/pdf/77f82d36ef5cbde5647d6e9f7fb7dd38ce4e2a91.pdf", "supplementary_material": "/attachment/56abd6641e0629e6045cfeb1dd02edc87f195900.zip", "_bibtex": "@inproceedings{\ngao2022gradient,\ntitle={Gradient Importance Learning for Incomplete Observations},\nauthor={Qitong Gao and Dong Wang and Joshua David Amason and Siyang Yuan and Chenyang Tao and Ricardo Henao and Majda Hadziahmetovic and Lawrence Carin and Miroslav Pajic},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fXHl76nO2AZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "v6s3HVjPerv", "original": "yWvQNY096plE", "number": 4125, "cdate": 1632875732231, "mdate": null, "ddate": null, "tcdate": 1632875732231, "tmdate": 1697934549257, "tddate": null, "forum": "v6s3HVjPerv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset", "authorids": ["~Leon_Sixt1", "~Martin_Schuessler1", "~Oana-Iuliana_Popescu1", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Oana-Iuliana Popescu", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable ML", "User Study", "Human Subject Evaluation", "Invertible Neural Networks", "Convolutional Networks"], "abstract": "A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model\u2019s respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies.", "one-sentence_summary": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sixt|do_users_benefit_from_interpretable_vision_a_user_study_baseline_and_dataset", "pdf": "/pdf/49e3023b785924a7159ee756c546ac2ec523e8ea.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.11642/code)", "_bibtex": "@inproceedings{\nsixt2022do,\ntitle={Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset},\nauthor={Leon Sixt and Martin Schuessler and Oana-Iuliana Popescu and Philipp Wei{\\ss} and Tim Landgraf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=v6s3HVjPerv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "Qycd9j5Qp9J", "original": "quySScEQm8_", "number": 4117, "cdate": 1632875731750, "mdate": null, "ddate": null, "tcdate": 1632875731750, "tmdate": 1676330470680, "tddate": null, "forum": "Qycd9j5Qp9J", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding the Variance Collapse of SVGD in High Dimensions", "authorids": ["~Jimmy_Ba1", "~Murat_A_Erdogdu1", "~Marzyeh_Ghassemi2", "~Shengyang_Sun4", "~Taiji_Suzuki1", "~Denny_Wu2", "~Tianzong_Zhang1"], "authors": ["Jimmy Ba", "Murat A Erdogdu", "Marzyeh Ghassemi", "Shengyang Sun", "Taiji Suzuki", "Denny Wu", "Tianzong Zhang"], "keywords": ["Stein Variational Gradient Descent", "Approximate Inference", "Particle-based Variational Inference"], "abstract": "Stein variational gradient descent (SVGD) is a deterministic inference algorithm that evolves a set of particles to fit a target distribution. Despite its computational efficiency, SVGD often underestimates the variance of the target distribution in high dimensions. In this work we attempt to explain the variance collapse in SVGD. On the qualitative side, we compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective; we observe that the variance collapse phenomenon relates to the bias from deterministic updates present in the \"driving force\" of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the quantitative side, we demonstrate that the variance collapse of SVGD can be accurately predicted in the proportional asymptotic limit, i.e., when the number of particles $n$ and dimensions $d$ diverge at the same rate. In particular, for learning high-dimensional isotropic Gaussians, we derive the exact equilibrium variance for both SVGD and MMD-descent under certain near-orthogonality assumption on the converged particles, and confirm that SVGD suffers from the \"curse of dimensionality\".", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ba|understanding_the_variance_collapse_of_svgd_in_high_dimensions", "pdf": "/pdf/71e77dab5447ab6226d0f2e58132575f2217dc3b.pdf", "one-sentence_summary": "Qualitative and quantitative analysis of the variance collapse phenomenon of SVGD in high dimensions. ", "_bibtex": "@inproceedings{\nba2022understanding,\ntitle={Understanding the Variance Collapse of {SVGD} in High Dimensions},\nauthor={Jimmy Ba and Murat A Erdogdu and Marzyeh Ghassemi and Shengyang Sun and Taiji Suzuki and Denny Wu and Tianzong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Qycd9j5Qp9J}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "ZOcX-eybqoL", "original": "_ZQ6F0UmSQ", "number": 4105, "cdate": 1632875730934, "mdate": null, "ddate": null, "tcdate": 1632875730934, "tmdate": 1676330470784, "tddate": null, "forum": "ZOcX-eybqoL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalisation in Lifelong Reinforcement Learning through Logical Composition ", "authorids": ["~Geraud_Nangue_Tasse1", "~Steven_James1", "~Benjamin_Rosman1"], "authors": ["Geraud Nangue Tasse", "Steven James", "Benjamin Rosman"], "keywords": ["Reinforcement Learning", "Lifelong learning", "Multi task learning", "Transfer learning", "Logical composition", "Deep Reinforcement Learning"], "abstract": "We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy. Importantly, we provide two main theoretical results: we bound the performance of the transferred policy on a new task, and we give bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent's lifetime to generalise over a distribution. We verify our approach in a series of experiments, where we perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks. We also demonstrate that, as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task. Finally, we demonstrate our approach in the full lifelong setting where an agent receives tasks from an unknown distribution. Starting from scratch, an agent is able to quickly generalise over the task distribution after learning only a few tasks, which are sub-logarithmic in the size of the task space.", "pdf": "/pdf/89cb79a9b9bb6a9a833a7a8ae73c8c5a87792970.pdf", "one-sentence_summary": "A framework with theoretical guarantees for an agent to quickly generalize over a task space by autonomously determining whether a new task can be solved zero-shot using existing skills, or whether a task-specific skill should be learned few-shot.", "supplementary_material": "/attachment/73712897744101e96675f8da95110567d360b743.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tasse|generalisation_in_lifelong_reinforcement_learning_through_logical_composition", "_bibtex": "@inproceedings{\ntasse2022generalisation,\ntitle={Generalisation in Lifelong Reinforcement Learning through Logical Composition },\nauthor={Geraud Nangue Tasse and Steven James and Benjamin Rosman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZOcX-eybqoL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 40}}, {"id": "gSdSJoenupI", "original": "skLlmxkeHl1", "number": 4102, "cdate": 1632875730732, "mdate": null, "ddate": null, "tcdate": 1632875730732, "tmdate": 1676330470889, "tddate": null, "forum": "gSdSJoenupI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions", "authorids": ["~Zhaoqi_Leng1", "~Mingxing_Tan3", "~Chenxi_Liu1", "~Ekin_Dogus_Cubuk1", "~Jay_Shi1", "~Shuyang_Cheng1", "~Dragomir_Anguelov1"], "authors": ["Zhaoqi Leng", "Mingxing Tan", "Chenxi Liu", "Ekin Dogus Cubuk", "Jay Shi", "Shuyang Cheng", "Dragomir Anguelov"], "keywords": ["classification", "computer vision", "loss"], "abstract": "Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "leng|polyloss_a_polynomial_expansion_perspective_of_classification_loss_functions", "pdf": "/pdf/d1430448cff98fb37273293f39735ba9c6a4313a.pdf", "one-sentence_summary": "In the PolyLoss framework, we propose a simple and effective Poly-1 formulation which outperforms the cross-entropy loss and focal loss on various of tasks.", "data": "", "_bibtex": "@inproceedings{\nleng2022polyloss,\ntitle={PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions},\nauthor={Zhaoqi Leng and Mingxing Tan and Chenxi Liu and Ekin Dogus Cubuk and Jay Shi and Shuyang Cheng and Dragomir Anguelov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gSdSJoenupI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "I2Hw58KHp8O", "original": "xo9fxr0D6wK", "number": 4101, "cdate": 1632875730666, "mdate": null, "ddate": null, "tcdate": 1632875730666, "tmdate": 1676330470940, "tddate": null, "forum": "I2Hw58KHp8O", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Improving Non-Autoregressive Translation Models Without Distillation", "authorids": ["~Xiao_Shi_Huang1", "~Felipe_Perez1", "~Maksims_Volkovs3"], "authors": ["Xiao Shi Huang", "Felipe Perez", "Maksims Volkovs"], "keywords": ["Natural Language Processing", "Deep Learning", "Non-autoregressive Machine Translation", "Transformer", "Distillation"], "abstract": "Transformer-based autoregressive (AR) machine translation models have achieved significant performance improvements, nearing human-level accuracy on some languages. The AR framework translates one token at a time which can be time consuming, especially for long sequences. To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper we investigate possible reasons behind this performance gap, namely, the indistinguishability of tokens, and mismatch between training and inference. We then propose the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Empirically, we show that CMLMC achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets. Full code for this work will be released at the time of publication.", "one-sentence_summary": "Improving the CMLM non-autoregressive machine translation model so it trains without knowledge distillation and achieves SOTA BLEU score on both raw and distilled dataset", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|improving_nonautoregressive_translation_models_without_distillation", "pdf": "/pdf/fe5e18c9939f10295c39693c81d77b03816cad63.pdf", "_bibtex": "@inproceedings{\nhuang2022improving,\ntitle={Improving Non-Autoregressive Translation Models Without Distillation},\nauthor={Xiao Shi Huang and Felipe Perez and Maksims Volkovs},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=I2Hw58KHp8O}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "zzk231Ms1Ih", "original": "_vWu7gLdTrwb", "number": 4095, "cdate": 1632875730339, "mdate": null, "ddate": null, "tcdate": 1632875730339, "tmdate": 1676330471169, "tddate": null, "forum": "zzk231Ms1Ih", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Theory of Tournament Representations", "authorids": ["~Arun_Rajkumar4", "~Vishnu_Veerathu1", "cs20d400@smail.iitm.ac.in"], "authors": ["Arun Rajkumar", "Vishnu Veerathu", "Abdul Bakey Mir"], "keywords": ["tournament", "skew-symmetric", "pairwise ranking"], "abstract": "Real-world tournaments are almost always intransitive. Recent works have noted that parametric models which assume  $d$ dimensional node representations can effectively model intransitive tournaments. However, nothing is known about the structure of the class of tournaments that arise out of any fixed $d$ dimensional representations. In this work, we develop a novel theory for understanding parametric tournament representations. Our first contribution is to structurally characterize the class of tournaments that arise out of $d$ dimensional representations. We do this by showing that these tournament classes have forbidden configurations that must necessarily be a union of flip classes, a novel way to partition the set of all tournaments. We further characterize rank $2$ tournaments completely by showing that the associated forbidden flip class contains just $2$ tournaments. Specifically, we show that the rank $2$ tournaments are equivalent to locally transitive tournaments. This insight allows us to show that the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure. We also exhibit specific forbidden configurations for rank $4$ tournaments. For a general rank $d$ tournament class, we show that the flip class associated with a coned-doubly regular tournament of size $\\mathcal{O}(\\sqrt{d})$ must be a forbidden configuration. To answer a dual question, using a celebrated result of Froster, we show a lower bound of $\\Theta(\\sqrt{n})$ on the minimum dimension needed to represent all tournaments on $n$ nodes. For any given tournament, we show a novel upper bound on the smallest representation dimension that depends on the least size of the number of unique nodes in any feedback arc set of the flip class associated with a tournament. We show how our results also shed light on the upper bound of sign-rank of matrices. ", "one-sentence_summary": "We develop a theory to understand tournament representations i.e. structurally characterise when a tournament graph can be represented in lower dimensions using a skew symmetric matrix. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rajkumar|a_theory_of_tournament_representations", "pdf": "/pdf/a7853d8c301f8a37bc858f4c428d73862dabff26.pdf", "_bibtex": "@inproceedings{\nrajkumar2022a,\ntitle={A Theory of Tournament Representations},\nauthor={Arun Rajkumar and Vishnu Veerathu and Abdul Bakey Mir},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zzk231Ms1Ih}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "OJm3HZuj4r7", "original": "hJHbS1BIfZ71", "number": 4081, "cdate": 1632875729410, "mdate": null, "ddate": null, "tcdate": 1632875729410, "tmdate": 1676330472115, "tddate": null, "forum": "OJm3HZuj4r7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Convergent and Efficient Deep Q Learning Algorithm", "authorids": ["~Zhikang_T._Wang1", "~Masahito_Ueda1"], "authors": ["Zhikang T. Wang", "Masahito Ueda"], "keywords": ["DQN", "reinforcement learning", "convergence"], "abstract": "Despite the empirical success of the deep Q network (DQN) reinforcement learning algorithm and its variants, DQN is still not well understood and it does not guarantee convergence. In this work, we show that DQN can indeed diverge and cease to operate in realistic settings. Although there exist gradient-based convergent methods, we show that they actually have inherent problems in learning dynamics which cause them to fail even for simple tasks. To overcome these problems, we propose a convergent DQN algorithm (C-DQN) that is guaranteed to converge and can work with large discount factors (0.9998). It learns robustly in difficult settings and can learn several difficult games in the Atari 2600 benchmark that DQN fails to solve.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|convergent_and_efficient_deep_q_learning_algorithm", "pdf": "/pdf/d999c3cb704da4722ea5330b5dd48600eb9c4ef4.pdf", "supplementary_material": "/attachment/7414afa6296afdedebef45a3e500076a47622090.zip", "_bibtex": "@inproceedings{\nwang2022convergent,\ntitle={Convergent and Efficient Deep Q Learning Algorithm},\nauthor={Zhikang T. Wang and Masahito Ueda},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OJm3HZuj4r7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "TXsjU8BaibT", "original": "7zqBGZ_qFoV", "number": 4078, "cdate": 1632875729209, "mdate": null, "ddate": null, "tcdate": 1632875729209, "tmdate": 1697934552531, "tddate": null, "forum": "TXsjU8BaibT", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Trigger Hunting with a Topological Prior for Trojan Detection", "authorids": ["~Xiaoling_Hu1", "~Xiao_Lin2", "~Michael_Cogswell1", "~Yi_Yao1", "~Susmit_Jha1", "~Chao_Chen1"], "authors": ["Xiaoling Hu", "Xiao Lin", "Michael Cogswell", "Yi Yao", "Susmit Jha", "Chao Chen"], "keywords": ["Trojan detection", "diversity loss", "topological prior"], "abstract": "Despite their success and popularity, deep neural networks (DNNs) are vulnerable when facing backdoor attacks. This impedes their wider adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models \u2013 models trained with poisoned data. One popular approach is reverse engineering, i.e., recovering the triggers on a clean image by manipulating the model\u2019s prediction. One major challenge of reverse engineering approach is the enormous search space of triggers. To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. Moreover, by encouraging a diverse set of trigger candidates, our method can perform effectively in cases with unknown target labels. We demonstrate that these priors can significantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hu|trigger_hunting_with_a_topological_prior_for_trojan_detection", "pdf": "/pdf/4db1d42d467c296c5ec7fa3f38e37dcb5c140e84.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 7 code implementations](https://www.catalyzex.com/paper/arxiv:2110.08335/code)", "_bibtex": "@inproceedings{\nhu2022trigger,\ntitle={Trigger Hunting with a Topological Prior for Trojan Detection},\nauthor={Xiaoling Hu and Xiao Lin and Michael Cogswell and Yi Yao and Susmit Jha and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TXsjU8BaibT}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "JM2kFbJvvI", "original": "yZON77tdmCqr", "number": 4077, "cdate": 1632875729145, "mdate": null, "ddate": null, "tcdate": 1632875729145, "tmdate": 1676330472277, "tddate": null, "forum": "JM2kFbJvvI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL", "authorids": ["~Yanchao_Sun1", "~Ruijie_Zheng1", "~Yongyuan_Liang1", "~Furong_Huang1"], "authors": ["Yanchao Sun", "Ruijie Zheng", "Yongyuan Liang", "Furong Huang"], "keywords": ["adversarial RL", "robustness of RL", "evasion attack", "optimal attack", "observation perturbation"], "abstract": "Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. \nThis paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named \"actor\" and an RL-based learner named \"director'\". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically optimal and significantly more efficient than prior RL-based works in environments with large state spaces. Empirical results show that our proposed PA-AD universally outperforms state-of-the-art attacking methods in various Atari and MuJoCo environments. By applying PA-AD to adversarial training, we achieve state-of-the-art empirical robustness in multiple tasks under strong adversaries.", "one-sentence_summary": "We theoretically characterize the essence of evasion attacks in RL, and propose a novel attack algorithm for RL agents, which achieves state-of-the-art performance on both attacking and robustifying RL agents in many Atari and MuJoCo tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sun|who_is_the_strongest_enemy_towards_optimal_and_efficient_evasion_attacks_in_deep_rl", "pdf": "/pdf/b11335ea1d1d4ca95531723261e11735e0550bc4.pdf", "supplementary_material": "/attachment/63f167f8a582b919bb82474e697ab07a0d31207c.zip", "_bibtex": "@inproceedings{\nsun2022who,\ntitle={Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep {RL}},\nauthor={Yanchao Sun and Ruijie Zheng and Yongyuan Liang and Furong Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JM2kFbJvvI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "v3aeIsY_vVX", "original": "ANrAFgYWZLV", "number": 4068, "cdate": 1632875728565, "mdate": null, "ddate": null, "tcdate": 1632875728565, "tmdate": 1697934553553, "tddate": null, "forum": "v3aeIsY_vVX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Chunked Autoregressive GAN for Conditional Waveform Synthesis", "authorids": ["~Max_Morrison2", "~Rithesh_Kumar1", "~Kundan_Kumar1", "~Prem_Seetharaman1", "~Aaron_Courville3", "~Yoshua_Bengio1"], "authors": ["Max Morrison", "Rithesh Kumar", "Kundan Kumar", "Prem Seetharaman", "Aaron Courville", "Yoshua Bengio"], "keywords": ["audio generation", "speech synthesis", "deep learning", "generative models", "autoregression", "generative adversarial networks"], "abstract": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast inference speed suitable for real-time or interactive applications, and maintains or improves subjective quality.", "one-sentence_summary": "We improve the state-of-the-art of conditional waveform synthesis by combining the strengths of GANs and autoregression", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "morrison|chunked_autoregressive_gan_for_conditional_waveform_synthesis", "pdf": "/pdf/070239829c83980ec499e2eff346d48eafe3ecb5.pdf", "supplementary_material": "/attachment/ec88c411918d8107aad13556f6a935a5a5d76484.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.10139/code)", "_bibtex": "@inproceedings{\nmorrison2022chunked,\ntitle={Chunked Autoregressive {GAN} for Conditional Waveform Synthesis},\nauthor={Max Morrison and Rithesh Kumar and Kundan Kumar and Prem Seetharaman and Aaron Courville and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=v3aeIsY_vVX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "psh0oeMSBiF", "original": "nOFa1iJCGkS", "number": 4064, "cdate": 1632875728292, "mdate": null, "ddate": null, "tcdate": 1632875728292, "tmdate": 1697934553886, "tddate": null, "forum": "psh0oeMSBiF", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks", "authorids": ["~Fan_Wu6", "~Linyi_Li1", "~Huan_Zhang1", "~Bhavya_Kailkhura1", "~Krishnaram_Kenthapadi1", "~Ding_Zhao1", "~Bo_Li19"], "authors": ["Fan Wu", "Linyi Li", "Huan Zhang", "Bhavya Kailkhura", "Krishnaram Kenthapadi", "Ding Zhao", "Bo Li"], "keywords": ["certified robustness", "poisoning attacks", "reinforcement learning"], "abstract": "As reinforcement learning (RL) has achieved near human-level performance in a variety of tasks, its robustness has raised great attention. While a vast body of research has explored test-time (evasion) attacks in RL and corresponding defenses, its robustness against training-time (poisoning) attacks remains largely unanswered. In this work, we focus on certifying the robustness of of\ufb02ine RL in the presence of poisoning attacks, where a subset of training trajectories could be arbitrarily manipulated. We propose the \ufb01rst certi\ufb01cation framework, COPA, to certify the number of poisoning trajectories that can be tolerated regarding different certi\ufb01cation criteria. Given the complex structure of RL, we propose two certi\ufb01cation criteria: per-state action stability and cumulative reward bound. To further improve the certi\ufb01cation, we propose new partition and aggregation protocols to train robust policies. We further prove that some of the proposed certi\ufb01cation methods are theoretically tight and some are NP-Complete problems. We leverage COPA to certify three RL environments trained with different algorithms and conclude: (1) The proposed robust aggregation protocols such as temporal aggregation can signi\ufb01cantly improve the certi\ufb01cations; (2) Our certi\ufb01cations for both per-state action stability and cumulative reward bound are ef\ufb01cient and tight; (3) The certi\ufb01cation for different training algorithms and environments are different, implying their intrinsic robustness properties. All experimental results are available at https://copa-leaderboard.github.io.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|copa_certifying_robust_policies_for_offline_reinforcement_learning_against_poisoning_attacks", "pdf": "/pdf/0a24a116cb24a1e99cd715566dae243e36472472.pdf", "one-sentence_summary": "We propose the first framework for certifiying robustness of offline reinforcement learning against poisoning attacks.", "supplementary_material": "/attachment/1385e87d991dcb64b971334c8597727cd87c2d4c.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.08398/code)", "_bibtex": "@inproceedings{\nwu2022copa,\ntitle={{COPA}: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks},\nauthor={Fan Wu and Linyi Li and Huan Zhang and Bhavya Kailkhura and Krishnaram Kenthapadi and Ding Zhao and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=psh0oeMSBiF}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "Vzh1BFUCiIX", "original": "V_lMjeui4-2s", "number": 4056, "cdate": 1632875727766, "mdate": null, "ddate": null, "tcdate": 1632875727766, "tmdate": 1697934554618, "tddate": null, "forum": "Vzh1BFUCiIX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning", "authorids": ["~Vamsi_Aribandi1", "~Yi_Tay1", "~Tal_Schuster1", "~Jinfeng_Rao2", "~Huaixiu_Steven_Zheng1", "~Sanket_Vaibhav_Mehta2", "~Honglei_Zhuang1", "~Vinh_Q._Tran1", "~Dara_Bahri1", "~Jianmo_Ni2", "~Jai_Gupta1", "~Kai_Hui1", "~Sebastian_Ruder2", "~Donald_Metzler1"], "authors": ["Vamsi Aribandi", "Yi Tay", "Tal Schuster", "Jinfeng Rao", "Huaixiu Steven Zheng", "Sanket Vaibhav Mehta", "Honglei Zhuang", "Vinh Q. Tran", "Dara Bahri", "Jianmo Ni", "Jai Gupta", "Kai Hui", "Sebastian Ruder", "Donald Metzler"], "keywords": ["Natural Language Processing", "Transfer Learning", "Multi-task Learning"], "abstract": "Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.", "one-sentence_summary": "Using a suite of 107 NLP tasks, we show that massively multi-task pre-training can improve downstream performance on NLP tasks, overcoming trends of negative transfer between tasks while fine-tuning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "aribandi|ext5_towards_extreme_multitask_scaling_for_transfer_learning", "pdf": "/pdf/b64da5c159b90bf56d174fc67459b74928711232.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2111.10952/code)", "_bibtex": "@inproceedings{\naribandi2022ext,\ntitle={ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},\nauthor={Vamsi Aribandi and Yi Tay and Tal Schuster and Jinfeng Rao and Huaixiu Steven Zheng and Sanket Vaibhav Mehta and Honglei Zhuang and Vinh Q. Tran and Dara Bahri and Jianmo Ni and Jai Gupta and Kai Hui and Sebastian Ruder and Donald Metzler},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Vzh1BFUCiIX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "gRCCdgpVZf", "original": "oIfWJ9VtG-c", "number": 4049, "cdate": 1632875727301, "mdate": null, "ddate": null, "tcdate": 1632875727301, "tmdate": 1676330473704, "tddate": null, "forum": "gRCCdgpVZf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Provable Adaptation across Multiway Domains via Representation Learning", "authorids": ["~Zhili_Feng1", "~Shaobo_Han1", "~Simon_Shaolei_Du1"], "authors": ["Zhili Feng", "Shaobo Han", "Simon Shaolei Du"], "keywords": ["Representation learning", "tensor", "statistical learning theory"], "abstract": "This paper studies zero-shot domain adaptation where each domain is indexed on a multi-dimensional array, and we only have data from a small subset of domains. Our goal is to produce predictors that perform well on \\emph{unseen} domains. We propose a model which consists of a domain-invariant latent representation layer and a domain-specific linear prediction layer with a low-rank tensor structure. Theoretically, we present explicit sample complexity bounds to characterize the prediction error on unseen domains in terms of the number of domains with training data and the number of data per domain. To our knowledge, this is the first finite-sample guarantee for zero-shot domain adaptation. In addition, we provide experiments on two-way MNIST and four-way fiber sensing datasets to demonstrate the effectiveness of our proposed model.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "feng|provable_adaptation_across_multiway_domains_via_representation_learning", "pdf": "/pdf/097cce8a39240bc2a614483e1cb4e0314237f10a.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nfeng2022provable,\ntitle={Provable Adaptation across Multiway Domains via Representation Learning},\nauthor={Zhili Feng and Shaobo Han and Simon Shaolei Du},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gRCCdgpVZf}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "EXHG-A3jlM", "original": "UjHsBPC4-Fl", "number": 4047, "cdate": 1632875727165, "mdate": null, "ddate": null, "tcdate": 1632875727165, "tmdate": 1676330473752, "tddate": null, "forum": "EXHG-A3jlM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators", "authorids": ["~John_Guibas1", "~Morteza_Mardani1", "~Zongyi_Li1", "~Andrew_Tao1", "~Anima_Anandkumar1", "~Bryan_Catanzaro1"], "authors": ["John Guibas", "Morteza Mardani", "Zongyi Li", "Andrew Tao", "Anima Anandkumar", "Bryan Catanzaro"], "keywords": ["self attention", "linear complexity", "high-resolution inputs", "operator learning", "Fourier transform"], "abstract": "Vision transformers have delivered tremendous success in representation learning. This is primarily due to effective token mixing through self attention. However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer that learns to mix in the Fourier domain. AFNO is based on a principled foundation of operator learning which allows us to frame token mixing as a continuous global convolution without any dependence on the input resolution. This principle was previously used to design FNO, which solves global convolution efficiently in the Fourier domain and has shown promise in learning challenging PDEs. To handle challenges in visual representation learning such as discontinuities in images and high resolution inputs, we propose principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. The resulting model is highly parallel with a quasi-linear complexity and has linear memory in the sequence size. AFNO outperforms self-attention mechanisms for few-shot segmentation in terms of both efficiency and accuracy. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of 65k and outperforms other efficient self-attention mechanisms.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guibas|efficient_token_mixing_for_transformers_via_adaptive_fourier_neural_operators", "pdf": "/pdf/bec7c123720932f2545dfb12e85bab8ac5cca6ff.pdf", "one-sentence_summary": "We propose Adaptive Fourier Neural Operators (AFNO) for scaling self-attention to high resolution images in vision transformers by establishing a link between operator learning and token mixing.", "data": "", "_bibtex": "@inproceedings{\nguibas2022efficient,\ntitle={Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators},\nauthor={John Guibas and Morteza Mardani and Zongyi Li and Andrew Tao and Anima Anandkumar and Bryan Catanzaro},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EXHG-A3jlM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "xENf4QUL4LW", "original": "MwCqS8r0UFE", "number": 4042, "cdate": 1632875726834, "mdate": null, "ddate": null, "tcdate": 1632875726834, "tmdate": 1676330474073, "tddate": null, "forum": "xENf4QUL4LW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sample Selection with Uncertainty of Losses for Learning with Noisy Labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Mingming_Gong1", "~Jun_Yu3", "~Gang_Niu1", "~Masashi_Sugiyama1"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Mingming Gong", "Jun Yu", "Gang Niu", "Masashi Sugiyama"], "keywords": ["Learning with noisy labels", "Sample selection", "Uncertainty"], "abstract": "In learning with noisy labels, the sample selection approach is very popular, which regards small-loss data as correctly labeled data during training. However, losses are generated on-the-\ufb02y based on the model being trained with noisy labels, and thus large-loss data are likely but not certain to be incorrect. There are actually two possibilities of a large-loss data point: (a) it is mislabeled, and then its loss decreases slower than other data, since deep neural networks learn patterns \ufb01rst; (b) it belongs to an underrepresented group of data and has not been selected yet. In this paper, we incorporate the uncertainty of losses by adopting interval estimation instead of point estimation of losses, where lower bounds of the con\ufb01dence intervals of losses derived from distribution-free concentration inequalities, but not losses themselves, are used for sample selection. In this way, we also give large-loss but less selected data a try; then, we can better distinguish between the cases (a) and (b) by seeing if the losses effectively decrease with the uncertainty after the try. As a result, we can better explore underrepresented data that are correctly labeled but seem to be mislabeled at \ufb01rst glance. Experiments demonstrate that the proposed method is superior to baselines and robust to a broad range of label noise types.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xia|sample_selection_with_uncertainty_of_losses_for_learning_with_noisy_labels", "pdf": "/pdf/0ebab5bba4b36eec025abfd2e21f947e05d6e662.pdf", "supplementary_material": "/attachment/971d5ae740f63bbe9d8adfd2be565cc003f7fc0a.zip", "_bibtex": "@inproceedings{\nxia2022sample,\ntitle={Sample Selection with Uncertainty of Losses for Learning with Noisy Labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Mingming Gong and Jun Yu and Gang Niu and Masashi Sugiyama},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xENf4QUL4LW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "GsH-K1VIyy", "original": "XXt0Jz4UXyt", "number": 4035, "cdate": 1632875726354, "mdate": null, "ddate": null, "tcdate": 1632875726354, "tmdate": 1676330474776, "tddate": null, "forum": "GsH-K1VIyy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Data-Driven Offline Optimization for Architecting Hardware Accelerators", "authorids": ["~Aviral_Kumar2", "~Amir_Yazdanbakhsh1", "~Milad_Hashemi1", "~Kevin_Swersky1", "~Sergey_Levine1"], "authors": ["Aviral Kumar", "Amir Yazdanbakhsh", "Milad Hashemi", "Kevin Swersky", "Sergey Levine"], "keywords": ["computer architecture and systems", "machine learning", "data-driven optimization"], "abstract": "To attain higher efficiency, the industry has gradually reformed towards application-specific hardware accelerators. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a simulation-driven approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a data-driven, offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators---tailored towards both single- and multi-applications---improving performance upon stat-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kumar|datadriven_offline_optimization_for_architecting_hardware_accelerators", "pdf": "/pdf/62fa3ad6648729230b552447a872cf6777743905.pdf", "_bibtex": "@inproceedings{\nkumar2022datadriven,\ntitle={Data-Driven Offline Optimization for Architecting Hardware Accelerators},\nauthor={Aviral Kumar and Amir Yazdanbakhsh and Milad Hashemi and Kevin Swersky and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=GsH-K1VIyy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "H7HDG--DJF0", "original": "2lnrSOzhyM0", "number": 4020, "cdate": 1632875725277, "mdate": null, "ddate": null, "tcdate": 1632875725277, "tmdate": 1697934559342, "tddate": null, "forum": "H7HDG--DJF0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multi-Agent MDP Homomorphic Networks", "authorids": ["~Elise_van_der_Pol1", "~Herke_van_Hoof4", "~Frans_A_Oliehoek1", "~Max_Welling1"], "authors": ["Elise van der Pol", "Herke van Hoof", "Frans A Oliehoek", "Max Welling"], "keywords": ["multiagent systems", "reinforcement learning", "equivariance", "symmetry"], "abstract": "This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different configurations of the agents and their local observations. For example, consider a group of agents navigating: rotating the state globally results in a permutation of the optimal joint policy. Existing work on symmetries in single agent reinforcement learning can only be generalized to the fully centralized setting, because such approaches rely on the global symmetry in the full state-action spaces, and these can result in correspondences across agents. To encode such symmetries while still allowing distributed execution we propose a factorization that decomposes global symmetries into local transformations. Our proposed factorization allows for distributing the computation that enforces global symmetries over local agents and local interactions. We introduce a multi-agent equivariant policy network based on this factorization. We show empirically on symmetric multi-agent problems that globally symmetric distributable policies improve data efficiency compared to non-equivariant baselines.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pol|multiagent_mdp_homomorphic_networks", "pdf": "/pdf/3a8f28592a8f20859b54c37f57cb659f7b0664fa.pdf", "one-sentence_summary": "We introduce globally equivariant multi-agent policy networks with distributed execution.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.04495/code)", "_bibtex": "@inproceedings{\npol2022multiagent,\ntitle={Multi-Agent {MDP} Homomorphic Networks},\nauthor={Elise van der Pol and Herke van Hoof and Frans A Oliehoek and Max Welling},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=H7HDG--DJF0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "yhCp5RcZD7", "original": "pPhCePFBiS_", "number": 4013, "cdate": 1632875724794, "mdate": null, "ddate": null, "tcdate": 1632875724794, "tmdate": 1676330475736, "tddate": null, "forum": "yhCp5RcZD7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields", "authorids": ["~Wang_Yifan1", "~Lukas_Rahmann1", "~Olga_Sorkine-hornung1"], "authors": ["Wang Yifan", "Lukas Rahmann", "Olga Sorkine-hornung"], "keywords": ["implicit functions", "shape reconstruction", "shape representation", "object reconstruction"], "abstract": "We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high-frequency signal is constrained geometrically by the low-frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer\nand demonstrate superior representational power, training stability, and generalizability.", "pdf": "/pdf/55c1560b8382311a7f02b90aaba2fa21e4475e9d.pdf", "one-sentence_summary": "We extend classic displacement mapping to the neural implicit framework. The resulting novel implicit representation demonstrates superior reconstruction accuracy, parameter efficiency and enable implicit shape editing such as detail transfer.", "supplementary_material": "/attachment/db4dd196277d0511907b7f22dc08f56e105d8dfd.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yifan|geometryconsistent_neural_shape_representation_with_implicit_displacement_fields", "code": "", "_bibtex": "@inproceedings{\nyifan2022geometryconsistent,\ntitle={Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields},\nauthor={Wang Yifan and Lukas Rahmann and Olga Sorkine-hornung},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=yhCp5RcZD7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "tyTH9kOxcvh", "original": "gAw_PE_SOS9", "number": 4008, "cdate": 1632875724461, "mdate": null, "ddate": null, "tcdate": 1632875724461, "tmdate": 1676330476112, "tddate": null, "forum": "tyTH9kOxcvh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Modeling Label Space Interactions in Multi-label Classification using Box Embeddings", "authorids": ["~Dhruvesh_Patel1", "~Pavitra_Dangati1", "~Jay-Yoon_Lee1", "~Michael_Boratko1", "~Andrew_McCallum1"], "authors": ["Dhruvesh Patel", "Pavitra Dangati", "Jay-Yoon Lee", "Michael Boratko", "Andrew McCallum"], "keywords": ["Multi-label classification", "Box Embeddings", "Representation Learning", "Embeddings"], "abstract": "Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classification methods do not do so, resulting in label predictions that are inconsistent with the taxonomic constraints, thus failing to accurately represent the fundamentals of problem setting. In this work, we introduce the multi-label box model (MBM), a multi-label classification method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018).  Box embeddings can be understood as trainable Venn-diagrams based on hyper-rectangles.  Representing labels by boxes rather than vectors, MBM is able to capture taxonomic relations among labels.  Furthermore, since box embeddings allow these relations to be learned by stochastic gradient descent from data, and to be read as calibrated conditional probabilities, our model is endowed with a high degree of interpretability. This interpretability also facilitates the injection of partial information about label-label relationships into model training, to further improve its consistency. We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data. Through extensive empirical evaluations on both small and large-scale multi-label classification datasets, we show that BBM can significantly improve taxonomic consistency while preserving or surpassing the state-of-the-art predictive performance.", "one-sentence_summary": "Improving the consistency for multi-label classification by modeling label space interactions using Box Embeddings.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "patel|modeling_label_space_interactions_in_multilabel_classification_using_box_embeddings", "pdf": "/pdf/f5671d43125692a6533d9c7a1996335b8a1cd482.pdf", "_bibtex": "@inproceedings{\npatel2022modeling,\ntitle={Modeling Label Space Interactions in Multi-label Classification using Box Embeddings},\nauthor={Dhruvesh Patel and Pavitra Dangati and Jay-Yoon Lee and Michael Boratko and Andrew McCallum},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tyTH9kOxcvh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "ZKy2X3dgPA", "original": "S11ducpECA1", "number": 4005, "cdate": 1632875724263, "mdate": null, "ddate": null, "tcdate": 1632875724263, "tmdate": 1697934560878, "tddate": null, "forum": "ZKy2X3dgPA", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "It Takes Two to Tango: Mixup for Deep Metric Learning", "authorids": ["~Shashanka_Venkataramanan2", "~Bill_Psomas2", "~Ewa_Kijak1", "~laurent_amsaleg1", "~Konstantinos_Karantzalos1", "~Yannis_Avrithis2"], "authors": ["Shashanka Venkataramanan", "Bill Psomas", "Ewa Kijak", "laurent amsaleg", "Konstantinos Karantzalos", "Yannis Avrithis"], "keywords": [], "abstract": "Metric learning involves learning a discriminative representation such that embeddings of similar classes are encouraged to be close, while embeddings of dissimilar classes are pushed far apart. State-of-the-art methods focus mostly on sophisticated loss functions or mining strategies. On the one hand, metric learning losses consider two or more examples at a time. On the other hand, modern data augmentation methods for classification consider two or more examples at a time. The combination of the two ideas is under-studied.\n\nIn this work, we aim to bridge this gap and improve representations using mixup, which is a powerful data augmentation approach interpolating two or more examples and corresponding target labels at a time. This task is challenging because, unlike classification, the loss functions used in metric learning are not additive over examples, so the idea of interpolating target labels is not straightforward. To the best of our knowledge, we are the first to investigate mixing both examples and target labels for deep metric learning. We develop a generalized formulation that encompasses existing metric learning loss functions and modify it to accommodate for mixup, introducing Metric Mix, or Metrix. We also introduce a new metric---utilization---to demonstrate that by mixing examples during training, we are exploring areas of the embedding space beyond the training classes, thereby improving representations. To validate the effect of improved representations, we show that mixing inputs, intermediate representations or embeddings along with target labels significantly outperforms state-of-the-art metric learning methods on four benchmark deep metric learning datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "venkataramanan|it_takes_two_to_tango_mixup_for_deep_metric_learning", "pdf": "/pdf/1b4683c706bc39fb7b56b3982f8c10166b29773d.pdf", "one-sentence_summary": "We systematically study different mixup strategies in the context of deep metric learning, and study how to, and the effect of, mixing inputs, features, and output embeddings.", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.04990/code)", "_bibtex": "@inproceedings{\nvenkataramanan2022it,\ntitle={It Takes Two to Tango: Mixup for Deep Metric Learning},\nauthor={Shashanka Venkataramanan and Bill Psomas and Ewa Kijak and laurent amsaleg and Konstantinos Karantzalos and Yannis Avrithis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZKy2X3dgPA}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "G89-1yZLFHk", "original": "hOhYWcSJ3lu", "number": 4003, "cdate": 1632875724196, "mdate": null, "ddate": null, "tcdate": 1632875724196, "tmdate": 1676330476367, "tddate": null, "forum": "G89-1yZLFHk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Data Efficient Language-Supervised Zero-Shot Recognition with Optimal Transport Distillation", "authorids": ["~Bichen_Wu1", "~Ruizhe_Cheng1", "~Peizhao_Zhang1", "~Tianren_Gao1", "~Joseph_E._Gonzalez1", "~Peter_Vajda1"], "authors": ["Bichen Wu", "Ruizhe Cheng", "Peizhao Zhang", "Tianren Gao", "Joseph E. Gonzalez", "Peter Vajda"], "keywords": ["Zero shot learning", "contrastive learning", "optimal transport", "vision and language"], "abstract": "Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised \"gold\" labels. Previous works, such as CLIP, use InfoNCE loss to train a model to predict the pairing between images and text captions. CLIP, however, is data hungry and requires more than 400M image-text pairs for training. The inefficiency can be \\textit{partially} attributed to the fact that the image-text pairs are noisy. To address this, we propose OTTER (Optimal TransporT distillation for Efficient zero-shot Recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. Based on pretrained image and text encoders, models trained with OTTER achieve strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero-shot evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes) from Tencent ML-Images. Over 42 evaluations on 7 different dataset/architecture settings x 6 metrics, OTTER outperforms (32) or ties (2) all baselines in 34 of them. Our source code is open sourced at https://github.com/facebookresearch/OTTER.", "one-sentence_summary": "We improve the image-text contrastive learning by augmenting InfoNCE with Optimal Transport", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|data_efficient_languagesupervised_zeroshot_recognition_with_optimal_transport_distillation", "pdf": "/pdf/4692c27fcf85afed7f22e02ea4a1c14104fce2a4.pdf", "supplementary_material": "/attachment/02362d4e7f2d04c71ffc5fbdde9810b855a072fe.zip", "_bibtex": "@inproceedings{\nwu2022data,\ntitle={Data Efficient Language-Supervised Zero-Shot Recognition with Optimal Transport Distillation},\nauthor={Bichen Wu and Ruizhe Cheng and Peizhao Zhang and Tianren Gao and Joseph E. Gonzalez and Peter Vajda},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=G89-1yZLFHk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "Oy9WeuZD51", "original": "8Tftt6NTM5K", "number": 3994, "cdate": 1632875723601, "mdate": null, "ddate": null, "tcdate": 1632875723601, "tmdate": 1676330476785, "tddate": null, "forum": "Oy9WeuZD51", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Networks", "authorids": ["~Matan_Haroush1", "~Tzviel_Frostig1", "ruheller@gmail.com", "~Daniel_Soudry1"], "authors": ["Matan Haroush", "Tzviel Frostig", "Ruth Heller", "Daniel Soudry"], "keywords": ["out of distribution", "DNNs", "p-value", "hypothesis testing", "inductive conformal predictor"], "abstract": "Background.\nCommonly, Deep Neural Networks (DNNs) generalize well on samples drawn from a distribution similar to that of the training set. However, DNNs' predictions are brittle and unreliable when the test samples are drawn from a dissimilar distribution.\nThis is a major concern for deployment in real-world applications, where such behavior may come at a considerable cost, such as industrial production lines, autonomous vehicles, or healthcare applications.\n\nContributions.\nWe frame Out Of Distribution (OOD) detection in DNNs as a statistical hypothesis testing problem. Tests generated within our proposed framework combine evidence from the entire network.\nUnlike previous OOD detection heuristics, this framework returns a $p$-value for each test sample. It is guaranteed to maintain the Type I Error (T1E - incorrectly predicting OOD for an actual in-distribution sample) for test data. Moreover, this allows to combine several detectors while maintaining the T1E.\n\nBuilding on this framework, we suggest a novel OOD procedure based on low-order statistics. Our method achieves comparable or better results than state-of-the-art methods on well-accepted OOD benchmarks, without retraining the network parameters or assuming prior knowledge on the test distribution --- and at a fraction of the computational cost.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "haroush|a_statistical_framework_for_efficient_out_of_distribution_detection_in_deep_neural_networks", "pdf": "/pdf/8ab4fc0f10bb1b17497961ee8ff9912af8ed2cc3.pdf", "data": "", "code": "", "_bibtex": "@inproceedings{\nharoush2022a,\ntitle={A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Networks},\nauthor={Matan Haroush and Tzviel Frostig and Ruth Heller and Daniel Soudry},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Oy9WeuZD51}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "HuaYQfggn5u", "original": "nFhew1QMc8I", "number": 3993, "cdate": 1632875723535, "mdate": null, "ddate": null, "tcdate": 1632875723535, "tmdate": 1676330476893, "tddate": null, "forum": "HuaYQfggn5u", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FedBABU: Toward Enhanced Representation for Federated Image Classification", "authorids": ["~Jaehoon_Oh1", "~SangMook_Kim1", "~Se-Young_Yun1"], "authors": ["Jaehoon Oh", "SangMook Kim", "Se-Young Yun"], "keywords": ["Federated Learning", "Representation Learning"], "abstract": "Federated learning has evolved to improve a single global model under data heterogeneity (as a curse) or to develop multiple personalized models using data heterogeneity (as a blessing). However, little research has considered both directions simultaneously. In this paper, we first investigate the relationship between them by analyzing Federated Averaging at the client level and determine that a better federated global model performance does not constantly improve personalization. To elucidate the cause of this personalization performance degradation problem, we decompose the entire network into the body (extractor), which is related to universality, and the head (classifier), which is related to personalization. We then point out that this problem stems from training the head. Based on this observation, we propose a novel federated learning algorithm, coined FedBABU, which only updates the body of the model during federated training (i.e., the head is randomly initialized and never updated), and the head is fine-tuned for personalization during the evaluation process. Extensive experiments show consistent performance improvements and an efficient personalization of FedBABU. The code is available at https://github.com/jhoon-oh/FedBABU.", "one-sentence_summary": "We propose a novel algorithm, FedBABU, which updates and aggregates only the body during federated training for enhanced representation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "oh|fedbabu_toward_enhanced_representation_for_federated_image_classification", "pdf": "/pdf/09e0b377fa4e3200e80d267b3e1df94235e10a45.pdf", "data": "", "_bibtex": "@inproceedings{\noh2022fedbabu,\ntitle={Fed{BABU}: Toward Enhanced Representation for Federated Image Classification},\nauthor={Jaehoon Oh and SangMook Kim and Se-Young Yun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HuaYQfggn5u}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "AP1MKT37rJ", "original": "Y5nIFGb1ds", "number": 3990, "cdate": 1632875723335, "mdate": null, "ddate": null, "tcdate": 1632875723335, "tmdate": 1676330477048, "tddate": null, "forum": "AP1MKT37rJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Should I Run Offline Reinforcement Learning or Behavioral Cloning?", "authorids": ["~Aviral_Kumar2", "~Joey_Hong2", "~Anikait_Singh1", "~Sergey_Levine1"], "authors": ["Aviral Kumar", "Joey Hong", "Anikait Singh", "Sergey Levine"], "keywords": ["offline RL"], "abstract": "Offline reinforcement learning (RL) algorithms can acquire effective policies by utilizing only previously collected experience, without any online interaction.  While it is widely understood that offline RL is able to extract good policies even from highly suboptimal data, in practice offline RL is often used with data that resembles demonstrations. In this case, one can also use behavioral cloning (BC) algorithms, which mimic a subset of the dataset via supervised learning. It seems natural to ask: When should we prefer offline RL over BC? In this paper, our goal is to characterize environments and dataset compositions where offline RL leads to better performance than BC.  In particular, we characterize the properties of environments that allow offline RL methods to perform better than BC methods even when only provided with expert data. Additionally, we show that policies trained on suboptimal data that is sufficiently noisy can attain better performance than even BC algorithms with expert data, especially on long-horizon problems. We validate our theoretical results via extensive experiments on both diagnostic and high-dimensional domains including robot manipulation, maze navigation and Atari games, when learning from a variety of data sources. We observe that modern offline RL methods trained on suboptimal, noisy data in sparse reward domains outperform cloning the expert data in several practical problems.", "one-sentence_summary": "Characterization of scenarios where offline reinforcement learning outperforms behavioral cloning", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kumar|should_i_run_offline_reinforcement_learning_or_behavioral_cloning", "pdf": "/pdf/ab91050974b19858a9a241236b4d69019903de0e.pdf", "_bibtex": "@inproceedings{\nkumar2022should,\ntitle={Should I Run Offline Reinforcement Learning or Behavioral Cloning?},\nauthor={Aviral Kumar and Joey Hong and Anikait Singh and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AP1MKT37rJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "CLpxpXqqBV", "original": "-0tnC7PkWvE", "number": 3988, "cdate": 1632875723195, "mdate": null, "ddate": null, "tcdate": 1632875723195, "tmdate": 1697934562661, "tddate": null, "forum": "CLpxpXqqBV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning State Representations via Retracing in Reinforcement Learning", "authorids": ["~Changmin_Yu1", "~Dong_Li10", "~Jianye_HAO1", "~Jun_Wang2", "~Neil_Burgess1"], "authors": ["Changmin Yu", "Dong Li", "Jianye HAO", "Jun Wang", "Neil Burgess"], "keywords": ["Representation learning", "model-based reinforcement learning"], "abstract": "We propose learning via retracing, a novel self-supervised approach for learning the state representation (and the associated dynamics model) for reinforcement learning tasks. In addition to the predictive (reconstruction) supervision in the forward direction, we propose to include \"retraced\" transitions for representation/model learning, by enforcing the cycle-consistency constraint between the original and retraced states, hence improve upon the sample efficiency of learning. Moreover, learning via retracing explicitly propagates information about future transitions backward for inferring previous states, thus facilitates stronger representation learning for the downstream reinforcement learning tasks. We introduce Cycle-Consistency World Model (CCWM), a concrete model-based instantiation of learning via retracing. Additionally we propose a novel adaptive \"truncation\" mechanism for counteracting the negative impacts brought by \"irreversible\" transitions such that learning via retracing can be maximally effective. Through extensive empirical studies on visual-based continuous control benchmarks, we demonstrate that CCWM achieves state-of-the-art performance in terms of sample efficiency and asymptotic performance, whilst exhibiting behaviours that are indicative of stronger representation learning. ", "one-sentence_summary": "We introduce Learning via Retracing, a novel self-supervised framework based on temporal cycle-consistency assumption of the transition dynamics, for improved learning of the representation (and the dynamics model) in RL tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|learning_state_representations_via_retracing_in_reinforcement_learning", "pdf": "/pdf/04d24e2870546f3dcff312162e1b4006ecd641b7.pdf", "supplementary_material": "/attachment/e39030f397b3ff1339770b1966f14b3366dd2a78.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2111.12600/code)", "_bibtex": "@inproceedings{\nyu2022learning,\ntitle={Learning State Representations via Retracing in Reinforcement Learning},\nauthor={Changmin Yu and Dong Li and Jianye HAO and Jun Wang and Neil Burgess},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CLpxpXqqBV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "O-r8LOR-CCA", "original": "1bi-bG_Auhj3", "number": 3983, "cdate": 1632875722861, "mdate": null, "ddate": null, "tcdate": 1632875722861, "tmdate": 1697934563332, "tddate": null, "forum": "O-r8LOR-CCA", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Open-World Semi-Supervised Learning", "authorids": ["~Kaidi_Cao1", "~Maria_Brbic1", "~Jure_Leskovec1"], "authors": ["Kaidi Cao", "Maria Brbic", "Jure Leskovec"], "keywords": ["deep learning", "semi-supervised learning", "novel class discovery", "clustering"], "abstract": "A fundamental limitation of applying semi-supervised learning in real-world settings is the assumption that unlabeled test data contains only classes previously encountered in the labeled training data. However, this assumption rarely holds for data in-the-wild, where instances belonging to novel classes may appear at testing time. Here, we introduce a novel open-world semi-supervised learning setting that formalizes the notion that novel classes may appear in the unlabeled test data. In this novel setting, the goal is to solve the class distribution mismatch problem between labeled and unlabeled data, where at the test time every input instance either needs to be classified into one of the existing classes or a new unseen class needs to be initialized and the instance assigned to it. To tackle this challenging problem, we propose ORCA, an end-to-end approach that assigns instances to previously seen classes or  forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. In this way, ORCA gradually increases the discriminability of the model during the training and reduces the gap between intra-class variance of seen with respect to novel classes. Extensive experiments on image classification datasets and a single-cell dataset demonstrate that ORCA consistently outperforms alternative baselines, achieving 25% improvement on seen and 96% improvement on novel classes of the ImageNet dataset. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cao|openworld_semisupervised_learning", "pdf": "/pdf/e5ffbb438b307d601bd7794c87fae3c23950a63f.pdf", "one-sentence_summary": "We propose a pipeline that recognizes previously seen classes and discovers novel, never-before-seen classes at the same time.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2102.03526/code)", "_bibtex": "@inproceedings{\ncao2022openworld,\ntitle={Open-World Semi-Supervised Learning},\nauthor={Kaidi Cao and Maria Brbic and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=O-r8LOR-CCA}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "af1eUDdUVz", "original": "u_4EeiLaAs3", "number": 3968, "cdate": 1632875721844, "mdate": null, "ddate": null, "tcdate": 1632875721844, "tmdate": 1676330478362, "tddate": null, "forum": "af1eUDdUVz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent", "authorids": ["~Oliver_Bryniarski1", "~Nabeel_Hingun1", "~Pedro_Pachuca1", "~Vincent_Wang1", "~Nicholas_Carlini1"], "authors": ["Oliver Bryniarski", "Nabeel Hingun", "Pedro Pachuca", "Vincent Wang", "Nicholas Carlini"], "keywords": ["Adversarial examples", "adversarial attacks"], "abstract": "Evading adversarial example detection defenses requires finding adversarial examples that must simultaneously (a) be misclassified by the model and (b) be detected as non-adversarial. We find that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. We introduce Selective Projected Gradient Descent and Orthogonal Projected Gradient Descent, improved attack techniques to generate adversarial examples that avoid this problem by orthogonalizing the gradients when running standard gradient-based attacks. We use our technique to evade four state-of-the-art detection defenses, reducing their accuracy to 0% while maintaining a 0% detection rate.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bryniarski|evading_adversarial_example_detection_defenses_with_orthogonal_projected_gradient_descent", "pdf": "/pdf/3d2eb96b012475581aa80cda16373c217e28c087.pdf", "one-sentence_summary": "We break four defenses that detect adversarial examples by introducing an improved attack technique that modifies the gradient before applying PGD.", "code": "", "data": "", "_bibtex": "@inproceedings{\nbryniarski2022evading,\ntitle={Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent},\nauthor={Oliver Bryniarski and Nabeel Hingun and Pedro Pachuca and Vincent Wang and Nicholas Carlini},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=af1eUDdUVz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "Azh9QBQ4tR7", "original": "2rjO6u3Wzx", "number": 3963, "cdate": 1632875721509, "mdate": null, "ddate": null, "tcdate": 1632875721509, "tmdate": 1676330478667, "tddate": null, "forum": "Azh9QBQ4tR7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off", "authorids": ["~Rahul_Rade1", "~Seyed-Mohsen_Moosavi-Dezfooli1"], "authors": ["Rahul Rade", "Seyed-Mohsen Moosavi-Dezfooli"], "keywords": ["adversarial training", "robustness"], "abstract": "While adversarial training has become the de facto approach for training robust classifiers, it leads to a drop in accuracy. This has led to prior works postulating that accuracy is inherently at odds with robustness. Yet, the phenomenon remains inexplicable. In this paper, we closely examine the changes induced in the decision boundary of a deep network during adversarial training. We find that adversarial training leads to unwarranted increase in the margin along certain adversarial directions, thereby hurting accuracy. Motivated by this observation, we present a novel algorithm, called Helper-based Adversarial Training (HAT), to reduce this effect by incorporating additional wrongly labelled examples during training. Our proposed method provides a notable improvement in accuracy without compromising robustness. It achieves a better trade-off between accuracy and robustness in comparison to existing defenses. Code is available at https://github.com/imrahulr/hat.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rade|reducing_excessive_margin_to_achieve_a_better_accuracy_vs_robustness_tradeoff", "pdf": "/pdf/c2a72787c4e6f0d24586b17eab7ca97027346386.pdf", "supplementary_material": "/attachment/a0c1b26035db8ee49a4270005489c63c6a99ddea.zip", "data": "", "_bibtex": "@inproceedings{\nrade2022reducing,\ntitle={Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off},\nauthor={Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Azh9QBQ4tR7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "WxuE_JWxjkW", "original": "NvWZVXuY1iE", "number": 3959, "cdate": 1632875721238, "mdate": null, "ddate": null, "tcdate": 1632875721238, "tmdate": 1676330478965, "tddate": null, "forum": "WxuE_JWxjkW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability", "authorids": ["~Shangmin_Guo1", "~Yi_Ren6", "~Kory_Wallace_Mathewson1", "simon.kirby@ed.ac.uk", "~Stefano_V_Albrecht1", "~Kenny_Smith1"], "authors": ["Shangmin Guo", "Yi Ren", "Kory Wallace Mathewson", "Simon Kirby", "Stefano V Albrecht", "Kenny Smith"], "keywords": ["Emergent Language", "Expressivity"], "abstract": "Researchers are using deep learning models to explore the emergence of language in various language games, where agents interact and develop an emergent language to solve tasks. We focus on the factors that determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. We measure the expressivity of emergent languages based on the generalisation performance across different games, and demonstrate that the expressivity of emergent languages is a trade-off between the complexity and unpredictability of the context those languages emerged from. Another contribution of this work is the discovery of message type collapse, i.e. the number of unique messages is lower than that of inputs. We also show that using the contrastive loss proposed by Chen et al. (2020) can alleviate this problem.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guo|expressivity_of_emergent_languages_is_a_tradeoff_between_contextual_complexity_and_unpredictability", "pdf": "/pdf/be46689741877d2b59dc56c09443500af7dd2941.pdf", "one-sentence_summary": "We demonstrate that the expressivity of emergent languages is a trade-off between the complexity and unpredictability of the context those languages are used in.", "supplementary_material": "/attachment/9d25f73d43130742bdd6e4b09e1654d9e93e7c5d.zip", "_bibtex": "@inproceedings{\nguo2022expressivity,\ntitle={Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability},\nauthor={Shangmin Guo and Yi Ren and Kory Wallace Mathewson and Simon Kirby and Stefano V Albrecht and Kenny Smith},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WxuE_JWxjkW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "hcoswsDHNAW", "original": "jgkLjJzYxkf", "number": 3955, "cdate": 1632875720966, "mdate": null, "ddate": null, "tcdate": 1632875720966, "tmdate": 1697934565624, "tddate": null, "forum": "hcoswsDHNAW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fast AdvProp", "authorids": ["~Jieru_Mei2", "~Yucheng_Han1", "~Yutong_Bai1", "~Yixiao_Zhang1", "~Yingwei_Li4", "~Xianhang_Li1", "~Alan_Yuille1", "~Cihang_Xie3"], "authors": ["Jieru Mei", "Yucheng Han", "Yutong Bai", "Yixiao Zhang", "Yingwei Li", "Xianhang Li", "Alan Yuille", "Cihang Xie"], "keywords": ["Adversarial examples", "efficient training", "generalization"], "abstract": "Adversarial Propagation (AdvProp) is an effective way to improve recognition models, leveraging adversarial examples. Nonetheless, AdvProp suffers from the extremely slow training speed, mainly because: a) extra forward and backward passes are required for generating adversarial examples; b) both original samples and their adversarial counterparts are used for training (i.e., 2X data).  In this paper, we introduce Fast AdvProp, which aggressively revamps AdvProp's costly training components, rendering the method nearly as cheap as the vanilla training. Specifically, our modifications in Fast AdvProp are guided by the hypothesis that disentangled learning with adversarial examples is the key for performance improvements, while other training recipes (e.g., paired clean and adversarial training samples, multi-step adversarial attackers) could be largely simplified.   \n\nOur empirical results show that, compared to the vanilla training baseline, Fast AdvProp is able to further model performance on a spectrum of visual benchmarks, without incurring extra training cost. Additionally, our ablations find Fast AdvProp scales better if larger models are used, is compatible with existing data augmentation methods (i.e., Mixup and CutMix), and can be easily adapted to other recognition tasks like object detection.  The code is available here: https://github.com/meijieru/fast_advprop.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mei|fast_advprop", "pdf": "/pdf/12e365a996eeb801b2173df149f6f8bc69ec02fa.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.09838/code)", "_bibtex": "@inproceedings{\nmei2022fast,\ntitle={Fast AdvProp},\nauthor={Jieru Mei and Yucheng Han and Yutong Bai and Yixiao Zhang and Yingwei Li and Xianhang Li and Alan Yuille and Cihang Xie},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hcoswsDHNAW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "8in_5gN9I0", "original": "3iI_jbNxhijQ", "number": 3944, "cdate": 1632875720230, "mdate": null, "ddate": null, "tcdate": 1632875720230, "tmdate": 1697934566581, "tddate": null, "forum": "8in_5gN9I0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Triangle and Four Cycle Counting with Predictions in Graph Streams", "authorids": ["~Justin_Y_Chen1", "~Talya_Eden1", "~Piotr_Indyk1", "~Honghao_Lin1", "~Shyam_Narayanan1", "~Ronitt_Rubinfeld1", "~Sandeep_Silwal1", "~Tal_Wagner1", "~David_Woodruff1", "~Michael_Zhang8"], "authors": ["Justin Y Chen", "Talya Eden", "Piotr Indyk", "Honghao Lin", "Shyam Narayanan", "Ronitt Rubinfeld", "Sandeep Silwal", "Tal Wagner", "David Woodruff", "Michael Zhang"], "keywords": ["learning augmented", "streaming", "graph streaming", "data driven", "cycle counting", "triangle counting"], "abstract": "We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recently, Hsu et al. (2019) and Jiang et al. (2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict certain properties of the stream elements to improve on prior \u201cclassical\u201d algorithms that did not use oracles. In this paper, we explore the power of a \u201cheavy edge\u201d oracle in multiple graph edge streaming models. In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle. In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal. We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs. Our methodology expands upon prior work on \u201cclassical\u201d streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms.", "one-sentence_summary": "We propose algorithms for counting triangles and four cycles in graph streams with the aid of ML predictors.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|triangle_and_four_cycle_counting_with_predictions_in_graph_streams", "pdf": "/pdf/25b70c42018200ce5f79c1f1dfc16f4c95ff9304.pdf", "supplementary_material": "/attachment/14b95db6a480d2864710b7276908d21056779dac.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.09572/code)", "_bibtex": "@inproceedings{\nchen2022triangle,\ntitle={Triangle and Four Cycle Counting with Predictions in Graph Streams},\nauthor={Justin Y Chen and Talya Eden and Piotr Indyk and Honghao Lin and Shyam Narayanan and Ronitt Rubinfeld and Sandeep Silwal and Tal Wagner and David Woodruff and Michael Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8in_5gN9I0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "js62_xuLDDv", "original": "no1mSjEsOT6", "number": 3937, "cdate": 1632875719750, "mdate": null, "ddate": null, "tcdate": 1632875719750, "tmdate": 1676330479979, "tddate": null, "forum": "js62_xuLDDv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning", "authorids": ["~Natalie_Dullerud1", "~Karsten_Roth1", "~Kimia_Hamidieh1", "~Nicolas_Papernot1", "~Marzyeh_Ghassemi2"], "authors": ["Natalie Dullerud", "Karsten Roth", "Kimia Hamidieh", "Nicolas Papernot", "Marzyeh Ghassemi"], "keywords": ["deep metric learning", "fairness", "representation learning"], "abstract": "Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving  generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space -- inter-class alignment, intra-class alignment, and uniformity -- and propose \\textit{\\textbf{finDML}}, the \\textit{\\textbf{f}}airness \\textit{\\textbf{i}}n \\textit{\\textbf{n}}on-balanced \\textit{\\textbf{DML}} benchmark to characterize representation fairness. Utilizing \\textit{finDML}, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (\\textit{\\textbf{\\pad}}) to disentangle feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics.", "one-sentence_summary": "We provide a benchmark for fairness in the scope of deep metric learning; investigate fairness impacts of learned representations on downstream classification; and provide a novel method for reducing subgroup gaps in deep metric learning methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dullerud|is_fairness_only_metric_deep_evaluating_and_addressing_subgroup_gaps_in_deep_metric_learning", "pdf": "/pdf/f404cf882e197b2c86f3e62a769c3cbf9024a9b5.pdf", "supplementary_material": "/attachment/9c2d8e567434fbb161734f50533b624e1ae94b8d.zip", "data": "", "_bibtex": "@inproceedings{\ndullerud2022is,\ntitle={Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning},\nauthor={Natalie Dullerud and Karsten Roth and Kimia Hamidieh and Nicolas Papernot and Marzyeh Ghassemi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=js62_xuLDDv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "xMJWUKJnFSw", "original": "2mRyh-asQ5T", "number": 3931, "cdate": 1632875719347, "mdate": null, "ddate": null, "tcdate": 1632875719347, "tmdate": 1676330480672, "tddate": null, "forum": "xMJWUKJnFSw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs", "authorids": ["~Mikhail_Galkin1", "~Etienne_Denis1", "~Jiapeng_Wu1", "~William_L._Hamilton1"], "authors": ["Mikhail Galkin", "Etienne Denis", "Jiapeng Wu", "William L. Hamilton"], "keywords": ["knowledge graphs", "graph representation learning", "tokenization", "link prediction", "node classification"], "abstract": "Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. \nSuch a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs of working with real-world KGs.\nDrawing parallels with subword tokenization commonly used in NLP, we explore the landscape of more parameter-efficient node embedding strategies with possibly sublinear memory requirements. \nTo this end, we propose NodePiece, an anchor-based approach to learn a fixed-size entity vocabulary. \nIn NodePiece, a vocabulary of subword/sub-entity units is constructed from anchor nodes in a graph with known relation types. Given such a fixed-size vocabulary, it is possible to bootstrap an encoding and embedding for any entity, including those unseen during training.\nExperiments show that NodePiece performs competitively in node classification, link prediction, and relation prediction tasks retaining less than 10% of explicit nodes in a graph as anchors and often having 10x fewer parameters. To this end, we show that a NodePiece-enabled model outperforms existing shallow models on a large OGB WikiKG 2 graph having 70x fewer parameters.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "galkin|nodepiece_compositional_and_parameterefficient_representations_of_large_knowledge_graphs", "pdf": "/pdf/6eb641d163812ce838dbad1b8e7fddebb2c72c12.pdf", "one-sentence_summary": "Node hashing in graphs for 10-100x embedding size reduction without significant performance losses on many tasks and inductive out of the box.", "supplementary_material": "/attachment/53d96e699eba373dd4e613264db035ae53de6037.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\ngalkin2022nodepiece,\ntitle={NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs},\nauthor={Mikhail Galkin and Etienne Denis and Jiapeng Wu and William L. Hamilton},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xMJWUKJnFSw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "e42KbIw6Wb", "original": "A0RVHP-IpbE", "number": 3930, "cdate": 1632875719277, "mdate": null, "ddate": null, "tcdate": 1632875719277, "tmdate": 1697934567949, "tddate": null, "forum": "e42KbIw6Wb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pix2seq: A Language Modeling Framework for Object Detection", "authorids": ["~Ting_Chen1", "srbs@google.com", "~Lala_Li1", "~David_J._Fleet1", "~Geoffrey_Hinton1"], "authors": ["Ting Chen", "Saurabh Saxena", "Lala Li", "David J. Fleet", "Geoffrey Hinton"], "keywords": ["language modeling", "object detection"], "abstract": "We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|pix2seq_a_language_modeling_framework_for_object_detection", "pdf": "/pdf/1f7291d96e3b195bdf0664dfb0f5313b0eab7a04.pdf", "one-sentence_summary": "We demonstrated that object detection can be tackled by simply training a language model conditioned on pixel inputs.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.10852/code)", "_bibtex": "@inproceedings{\nchen2022pixseq,\ntitle={Pix2seq: A Language Modeling Framework for Object Detection},\nauthor={Ting Chen and Saurabh Saxena and Lala Li and David J. Fleet and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=e42KbIw6Wb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "PQQp7AJwz3", "original": "3x2qMqhBg-3T", "number": 3928, "cdate": 1632875719137, "mdate": null, "ddate": null, "tcdate": 1632875719137, "tmdate": 1676330481084, "tddate": null, "forum": "PQQp7AJwz3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization", "authorids": ["~Kazusato_Oko1", "~Taiji_Suzuki1", "~Atsushi_Nitanda1", "~Denny_Wu2"], "authors": ["Kazusato Oko", "Taiji Suzuki", "Atsushi Nitanda", "Denny Wu"], "keywords": ["Neural Network Optimization", "Mean field Regime", "Overparameterization"], "abstract": "We introduce Particle-SDCA, a gradient-based optimization algorithm for two-layer neural networks in the mean field regime that achieves exponential convergence rate in regularized empirical risk minimization. The proposed algorithm can be regarded as an infinite dimensional extension of Stochastic Dual Coordinate Ascent (SDCA) in the probability space: we exploit the convexity of the dual problem, for which the coordinate-wise proximal gradient method can be applied. Our proposed method inherits advantages of the original SDCA, including (i) exponential convergence (with respect to the outer iteration steps), and (ii) better dependency on the sample size and condition number than the full-batch gradient method. One technical challenge in implementing the SDCA update is the intractable integral over the entire parameter space at every step. To overcome this limitation, we propose a tractable \\textit{particle method} that approximately solves the dual problem, and an importance re-weighted technique to reduce the computational cost. The convergence rate of our method is verified by numerical experiments.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "oko|particle_stochastic_dual_coordinate_ascent_exponential_convergent_algorithm_for_mean_field_neural_network_optimization", "pdf": "/pdf/b6a0af59072ab41c5553c6952e5a786b25d0adde.pdf", "one-sentence_summary": "Proposed a new algorithm for optimizing two-layer neural network in the mean field regime that achieves exponential convergence in regularized empirical risk minimization (w.r.t. outer loop iterations).", "supplementary_material": "/attachment/f26eeda2e85718aa3a423b67a8ce2f90a1e99ae4.zip", "_bibtex": "@inproceedings{\noko2022particle,\ntitle={Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization},\nauthor={Kazusato Oko and Taiji Suzuki and Atsushi Nitanda and Denny Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PQQp7AJwz3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "7_JR7WpwKV1", "original": "DS6pZLM-WtZ", "number": 3921, "cdate": 1632875718649, "mdate": null, "ddate": null, "tcdate": 1632875718649, "tmdate": 1676330481422, "tddate": null, "forum": "7_JR7WpwKV1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders ", "authorids": ["~Divyansh_Pareek1", "~Andrej_Risteski2"], "authors": ["Divyansh Pareek", "Andrej Risteski"], "keywords": ["variational autoencoders", "encoder", "representational complexity", "Langevin", "invertibility", "deep learning theory"], "abstract": "Training and using modern neural-network based latent-variable generative models (like Variational Autoencoders) often require simultaneously training a generative direction along with an inferential (encoding) direction, which approximates the posterior distribution over the latent variables. Thus, the question arises: how complex does the inferential model need to be, in order to be able to accurately model the posterior distribution of a given generative model?  In this paper, we identify an important property of the generative map impacting the required size of the encoder. We show that if the generative map is ``strongly invertible\" (in a sense we suitably formalize), the inferential model need not be much more complex. Conversely, we prove that there exist non-invertible generative maps, for which the encoding direction needs to be exponentially larger (under standard assumptions in computational complexity). Importantly, we do not require the generative model to be layerwise invertible, which a lot of the related literature assumes and isn't satisfied by many architectures used in practice (e.g. convolution and pooling based networks). Thus, we provide theoretical support for the empirical wisdom that learning deep generative models is harder when data lies on a low-dimensional manifold.", "one-sentence_summary": "VAEs with invertible mean map have small approximate encoders; non-invertible maps can result in large encoders. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pareek|the_effects_of_invertibility_on_the_representational_complexity_of_encoders_in_variational_autoencoders", "pdf": "/pdf/4116475bedc76111284bad627cb9a8fbaec2059b.pdf", "_bibtex": "@inproceedings{\npareek2022the,\ntitle={The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders },\nauthor={Divyansh Pareek and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7_JR7WpwKV1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "Ro_zAjZppv", "original": "13D_pgsJhlW", "number": 3920, "cdate": 1632875718578, "mdate": null, "ddate": null, "tcdate": 1632875718578, "tmdate": 1676330481469, "tddate": null, "forum": "Ro_zAjZppv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Tracking the risk of a deployed model and detecting harmful distribution shifts", "authorids": ["~Aleksandr_Podkopaev1", "~Aaditya_Ramdas2"], "authors": ["Aleksandr Podkopaev", "Aaditya Ramdas"], "keywords": ["Distribution shift", "sequential testing"], "abstract": "When deployed in the real world, machine learning models inevitably encounter changes in the data distribution, and certain---but not all---distribution shifts could result in significant performance degradation. In practice, it may make sense to ignore benign shifts, under which the performance of a deployed model does not degrade substantially, making interventions by a human expert (or model retraining) unnecessary.  While several works have developed tests for distribution shifts, these typically either use non-sequential methods, or detect arbitrary shifts (benign or harmful), or both. We argue that a sensible method for firing off a warning has to both (a) detect harmful shifts while ignoring benign ones, and (b) allow continuous monitoring of model performance without increasing the false alarm rate. In this work, we design simple sequential tools for testing if the difference between source (training) and target (test) distributions leads to a significant increase in a risk function of interest, like accuracy or calibration. Recent advances in constructing time-uniform confidence sequences allow efficient aggregation of statistical evidence accumulated during the tracking process. The designed framework is applicable in settings where (some) true labels are revealed after the prediction is performed, or when batches of labels become available in a delayed fashion. We demonstrate the efficacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "podkopaev|tracking_the_risk_of_a_deployed_model_and_detecting_harmful_distribution_shifts", "pdf": "/pdf/f763a5271b61d98bca4127ab14ce483150d152c4.pdf", "supplementary_material": "/attachment/526060ad4ef0325751f4b35f06aea4f0213c9925.zip", "_bibtex": "@inproceedings{\npodkopaev2022tracking,\ntitle={Tracking the risk of a deployed model and detecting harmful distribution shifts},\nauthor={Aleksandr Podkopaev and Aaditya Ramdas},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Ro_zAjZppv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "BmJV7kyAmg", "original": "THSoSr_LfJ", "number": 3912, "cdate": 1632875718017, "mdate": null, "ddate": null, "tcdate": 1632875718017, "tmdate": 1676330481715, "tddate": null, "forum": "BmJV7kyAmg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Understanding the Robustness Against Evasion Attack on Categorical Data", "authorids": ["~Hongyan_Bao1", "~Yufei_Han1", "~Yujun_Zhou1", "~Yun_Shen2", "~Xiangliang_Zhang1"], "authors": ["Hongyan Bao", "Yufei Han", "Yujun Zhou", "Yun Shen", "Xiangliang Zhang"], "keywords": ["robustness certification", "adversarial learning", "categorical data"], "abstract": "Characterizing and assessing the adversarial vulnerability of classification models with categorical input has been a practically important, while rarely explored research problem. Our work echoes the challenge by first unveiling the impact factors of adversarial vulnerability of classification models with categorical data based on an information-theoretic adversarial risk analysis about the targeted classifier. Though certifying the robustness of such classification models is intrinsically an NP-hard combinatorial problem, our study shows that the robustness certification can be solved via an efficient greedy exploration of the discrete attack space for any measurable classifiers with a mild smoothness constraint. Our proposed robustness certification framework is instantiated with deep neural network models applied on real-world safety-critic data sources. Our empirical observations confirm the impact of the key adversarial risk factors with categorical input.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bao|towards_understanding_the_robustness_against_evasion_attack_on_categorical_data", "pdf": "/pdf/b599972b615dea56e3cd777bb3c09e18b73ba736.pdf", "one-sentence_summary": "This paper explores the characterization and certification of the robustness against evasion attack with categorical input.", "_bibtex": "@inproceedings{\nbao2022towards,\ntitle={Towards Understanding the Robustness Against Evasion Attack on Categorical Data},\nauthor={Hongyan Bao and Yufei Han and Yujun Zhou and Yun Shen and Xiangliang Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BmJV7kyAmg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 4}}, {"id": "WPI2vbkAl3Q", "original": "MseZ1jZofoD", "number": 3911, "cdate": 1632875717941, "mdate": null, "ddate": null, "tcdate": 1632875717941, "tmdate": 1697934569435, "tddate": null, "forum": "WPI2vbkAl3Q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Curves for SGD on Structured Features", "authorids": ["~Blake_Bordelon1", "~Cengiz_Pehlevan2"], "authors": ["Blake Bordelon", "Cengiz Pehlevan"], "keywords": ["Stochastic Gradient Descent", "Generalization"], "abstract": "The generalization performance of a machine learning algorithm such as a neural network depends in a non-trivial way on the structure of the data distribution. To analyze the influence of data structure on test loss dynamics, we study an exactly solveable model of stochastic gradient descent (SGD) on the square loss which predicts test error when training on features with arbitrary covariance structure. We solve the theory exactly for both Gaussian features and arbitrary features and we show that the simpler Gaussian model accurately predicts test loss of nonlinear random-feature models and neural networks in the kernel regime trained with SGD on real datasets such as MNIST and CIFAR-10. We show that the optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure, demonstrating the computational benefits of SGD with small batch sizes. Lastly, we extend our theory to the more usual setting of stochastic gradient descent on a fixed subsampled training set, showing that both training and test error can be accurately predicted in our framework on real data.", "one-sentence_summary": "The average case test risk for stochastic gradient descent on mean square error is computed in terms of feature covariance structure.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bordelon|learning_curves_for_sgd_on_structured_features", "pdf": "/pdf/05e1bd43845bd2321a0ab8593b8960931a65e24e.pdf", "supplementary_material": "/attachment/1d264da6817d352d660eb5ca22cfc7c51991e4a2.zip", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.02713/code)", "_bibtex": "@inproceedings{\nbordelon2022learning,\ntitle={Learning Curves for {SGD} on Structured Features},\nauthor={Blake Bordelon and Cengiz Pehlevan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WPI2vbkAl3Q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "Qaw16njk6L", "original": "oslWUGLsJ0ys", "number": 3910, "cdate": 1632875717875, "mdate": null, "ddate": null, "tcdate": 1632875717875, "tmdate": 1676330482110, "tddate": null, "forum": "Qaw16njk6L", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", "authorids": ["~Chengyue_Gong1", "~Dilin_Wang1", "~Meng_Li1", "~Xinlei_Chen1", "~Zhicheng_Yan2", "~Yuandong_Tian1", "~qiang_liu4", "~Vikas_Chandra2"], "authors": ["Chengyue Gong", "Dilin Wang", "Meng Li", "Xinlei Chen", "Zhicheng Yan", "Yuandong Tian", "qiang liu", "Vikas Chandra"], "keywords": ["vision transformer", "gradient conflict", "neural architecture search"], "abstract": "Designing accurate and efficient vision transformers (ViTs) is a highly important but challenging task. Supernet-based one-shot neural architecture search (NAS) enables fast architecture optimization and has achieved state-of-the-art (SOTA) results on convolutional neural networks (CNNs). However, directly applying the supernet-based NAS to optimize ViTs leads to poor performance - even worse compared to training single ViTs. In this work, we observe that the poor performance is due to a gradient conflict issue: the gradients of different sub-networks conflict with that of the supernet more severely in ViTs than CNNs, which leads to early saturation in training and inferior convergence. To alleviate this issue, we propose a series of techniques, including a gradient projection algorithm, a switchable layer scaling design, and a simplified data augmentation and regularization training recipe. The proposed techniques significantly improve the convergence and the performance of all sub-networks. Our discovered hybrid ViT model family, dubbed NASViT, achieves top-1 accuracy from 78.2% to 81.8% on ImageNet from 200M to 800M FLOPs, and outperforms all the prior art CNNs and ViTs, including AlphaNet and LeViT, etc. When transferred to semantic segmentation tasks, NASViTs also outperform previous backbones on both Cityscape and ADE20K datasets, achieving 73.2% and 37.9% mIoU with only 5G FLOPs, respectively. Code is available at\nhttps://github.com/facebookresearch/NASViT.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gong|nasvit_neural_architecture_search_for_efficient_vision_transformers_with_gradient_conflict_aware_supernet_training", "pdf": "/pdf/a6df48abb7e0bb493e7c343c46beb7b365cdc788.pdf", "one-sentence_summary": "we identify one key issue of ViT supernet training that the supernet gradients and the sub-network gradients are likely to disagree with each other, and propose gradient conflict aware training.", "data": "", "_bibtex": "@inproceedings{\ngong2022nasvit,\ntitle={{NASV}iT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training},\nauthor={Chengyue Gong and Dilin Wang and Meng Li and Xinlei Chen and Zhicheng Yan and Yuandong Tian and qiang liu and Vikas Chandra},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Qaw16njk6L}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "sTNHCrIKDQc", "original": "pi8flZw0xxm", "number": 3907, "cdate": 1632875717666, "mdate": null, "ddate": null, "tcdate": 1632875717666, "tmdate": 1676330482570, "tddate": null, "forum": "sTNHCrIKDQc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graphon based Clustering and Testing of Networks: Algorithms and Theory", "authorids": ["~Mahalakshmi_Sabanayagam1", "~Leena_Chennuru_Vankadara2", "~Debarghya_Ghoshdastidar1"], "authors": ["Mahalakshmi Sabanayagam", "Leena Chennuru Vankadara", "Debarghya Ghoshdastidar"], "keywords": ["Clustering", "Networks", "Graphs", "Two-sample testing", "Graphon"], "abstract": "Network-valued data are encountered in a wide range of applications, and pose challenges in learning due to their complex structure and absence of vertex correspondence. Typical examples of such problems include classification or grouping of protein structures and social networks. Various methods, ranging from graph kernels to graph neural networks, have been proposed that achieve some success in graph classification problems. However, most methods have limited theoretical justification, and their applicability beyond classification remains unexplored. In this work, we propose methods for clustering multiple graphs, without vertex correspondence, that are inspired by the recent literature on estimating graphons---symmetric functions corresponding to infinite vertex limit of graphs. We propose a novel graph distance based on sorting-and-smoothing graphon estimators. Using the proposed graph distance, we present two clustering algorithms and show that they achieve state-of-the-art results. We prove the statistical consistency of both algorithms under Lipschitz assumptions on the graph degrees. We further study the applicability of the proposed distance for graph two-sample testing problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sabanayagam|graphon_based_clustering_and_testing_of_networks_algorithms_and_theory", "pdf": "/pdf/bc3a82e090f7f3cfaa9a92ef69181887e0348ede.pdf", "supplementary_material": "/attachment/679208f0f9bf5cf9289f95cf9ed33c705e8d5409.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nsabanayagam2022graphon,\ntitle={Graphon based Clustering and Testing of Networks: Algorithms and Theory},\nauthor={Mahalakshmi Sabanayagam and Leena Chennuru Vankadara and Debarghya Ghoshdastidar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=sTNHCrIKDQc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "TYw3-OlrRm-", "original": "z6h-x1tk2AK", "number": 3904, "cdate": 1632875717470, "mdate": null, "ddate": null, "tcdate": 1632875717470, "tmdate": 1698766273651, "tddate": null, "forum": "TYw3-OlrRm-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Network Augmentation for Tiny Deep Learning", "authorids": ["~Han_Cai1", "~Chuang_Gan1", "~Ji_Lin1", "~Song_Han5"], "authors": ["Han Cai", "Chuang Gan", "Ji Lin", "Song Han"], "keywords": ["Tiny Deep Learning"], "abstract": "We introduce Network Augmentation (NetAug), a new training method for improving the performance of tiny neural networks. Existing regularization techniques (e.g., data augmentation, dropout) have shown much success on large neural networks by adding noise to overcome over-fitting. However, we found these techniques hurt the performance of tiny neural networks. We argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model. At test time, only the tiny model is used for inference, incurring zero inference overhead. We demonstrate the effectiveness of NetAug on image classification and object detection. NetAug consistently improves the performance of tiny models, achieving up to 2.2% accuracy improvement on ImageNet. On object detection, achieving the same level of performance, NetAug requires 41% fewer MACs on Pascal VOC and 38% fewer MACs on COCO than the baseline.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cai|network_augmentation_for_tiny_deep_learning", "pdf": "/pdf/484496875b902e745fc4d6514abb817e7be477c2.pdf", "one-sentence_summary": "a training method for tiny neural networks", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.08890/code)", "_bibtex": "@inproceedings{\ncai2022network,\ntitle={Network Augmentation for Tiny Deep Learning},\nauthor={Han Cai and Chuang Gan and Ji Lin and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TYw3-OlrRm-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "o-1v9hdSult", "original": "Bz8E52qVmpaP", "number": 3903, "cdate": 1632875717404, "mdate": null, "ddate": null, "tcdate": 1632875717404, "tmdate": 1676330482967, "tddate": null, "forum": "o-1v9hdSult", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations", "authorids": ["~Sarath_Sreedharan1", "~Utkarsh_Soni1", "~Mudit_Verma2", "~Siddharth_Srivastava2", "~Subbarao_Kambhampati1"], "authors": ["Sarath Sreedharan", "Utkarsh Soni", "Mudit Verma", "Siddharth Srivastava", "Subbarao Kambhampati"], "keywords": ["Explanations", "XAI", "Post-hoc explanations"], "abstract": "As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to contest these decisions. A significant hurdle to allowing for such explanatory dialogue could be the {\\em vocabulary mismatch} between the user and the AI system. This paper introduces methods for providing contrastive explanations in terms of user-specified concepts for sequential decision-making settings where the system's model of the task may be best represented as an inscrutable model. We do this by building partial symbolic models of a local approximation of the task that can be leveraged to answer the user queries. We test these methods on a popular Atari game (Montezuma's Revenge) and variants of Sokoban (a well-known planning benchmark) and report the results of user studies to evaluate whether people find explanations generated in this form useful.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sreedharan|bridging_the_gap_providing_posthoc_symbolic_explanations_for_sequential_decisionmaking_problems_with_inscrutable_representations", "pdf": "/pdf/2558c3735ba361f65aac84ecf8e9f4624e87dec8.pdf", "supplementary_material": "/attachment/cadc687dfa3160fd9f9298c433a8db107c10c110.zip", "_bibtex": "@inproceedings{\nsreedharan2022bridging,\ntitle={Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations},\nauthor={Sarath Sreedharan and Utkarsh Soni and Mudit Verma and Siddharth Srivastava and Subbarao Kambhampati},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=o-1v9hdSult}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "C8Ltz08PtBp", "original": "7rHvWA3wI6_", "number": 3902, "cdate": 1632875717339, "mdate": null, "ddate": null, "tcdate": 1632875717339, "tmdate": 1676330483081, "tddate": null, "forum": "C8Ltz08PtBp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Distributional Reinforcement Learning with Monotonic Splines", "authorids": ["~Yudong_Luo1", "~Guiliang_Liu1", "~Haonan_Duan2", "~Oliver_Schulte1", "~Pascal_Poupart2"], "authors": ["Yudong Luo", "Guiliang Liu", "Haonan Duan", "Oliver Schulte", "Pascal Poupart"], "keywords": ["Distributional RL"], "abstract": "Distributional Reinforcement Learning (RL) differs from traditional RL by estimating the distribution over returns to capture the intrinsic uncertainty of MDPs. One key challenge in distributional RL lies in how to parameterize the quantile function when minimizing the Wasserstein metric of temporal differences. Existing algorithms use step functions or piecewise linear functions. In this paper, we propose to learn smooth continuous quantile functions represented by monotonic rational-quadratic splines, which also naturally solve the quantile crossing problem. Experiments in stochastic environments show that a dense estimation for quantile functions enhances distributional RL in terms of faster empirical convergence and higher rewards in most cases.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "luo|distributional_reinforcement_learning_with_monotonic_splines", "pdf": "/pdf/376a906de470631ee01098610befe6addc3d72de.pdf", "supplementary_material": "/attachment/da455063f7268429bbaa204c0f3826a1de46e40f.zip", "_bibtex": "@inproceedings{\nluo2022distributional,\ntitle={Distributional Reinforcement Learning with Monotonic Splines},\nauthor={Yudong Luo and Guiliang Liu and Haonan Duan and Oliver Schulte and Pascal Poupart},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=C8Ltz08PtBp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "R79ZGjHhv6p", "original": "4bog_f7rCG", "number": 3890, "cdate": 1632875716523, "mdate": null, "ddate": null, "tcdate": 1632875716523, "tmdate": 1676330483541, "tddate": null, "forum": "R79ZGjHhv6p", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Toward Faithful Case-based Reasoning through Learning Prototypes in a Nearest Neighbor-friendly Space.", "authorids": ["~Seyed_Omid_Davoudi1", "~Majid_Komeili1"], "authors": ["Seyed Omid Davoudi", "Majid Komeili"], "keywords": ["case-based reasoning", "interpretable machine learning", "explainable artificial intelligence", "xai", "prototype learning"], "abstract": "Recent advances in machine learning have brought opportunities for the ever-increasing use of AI in the real world. This has created concerns about the black-box nature of many of the most recent machine learning approaches. In this work, we propose an interpretable neural network that leverages metric and prototype learning for classification tasks. It encodes its own explanations and provides an improved case-based reasoning through learning prototypes in an embedding space learned by a probabilistic nearest neighbor rule. Through experiments, we demonstrated the effectiveness of the proposed method in both performance and the accuracy of the explanations provided.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "davoudi|toward_faithful_casebased_reasoning_through_learning_prototypes_in_a_nearest_neighborfriendly_space", "pdf": "/pdf/6d0714a184aa752df631ed2df558e8cfee0d4bb9.pdf", "one-sentence_summary": "Offering better prototype explanations using a nearest-neighbor friendly embedding space.", "data": "", "_bibtex": "@inproceedings{\ndavoudi2022toward,\ntitle={Toward Faithful Case-based Reasoning through Learning Prototypes in a Nearest Neighbor-friendly Space.},\nauthor={Seyed Omid Davoudi and Majid Komeili},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=R79ZGjHhv6p}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "iMqTLyfwnOO", "original": "YSyRCpumOep", "number": 3889, "cdate": 1632875716453, "mdate": null, "ddate": null, "tcdate": 1632875716453, "tmdate": 1697934572013, "tddate": null, "forum": "iMqTLyfwnOO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Augmented Sliced Wasserstein Distances", "authorids": ["~Xiongjie_Chen1", "~Yongxin_Yang1", "~Yunpeng_Li1"], "authors": ["Xiongjie Chen", "Yongxin Yang", "Yunpeng Li"], "keywords": [], "abstract": "While theoretically appealing, the application of the Wasserstein distance to large-scale machine learning problems has been hampered by its prohibitive computational cost. The sliced Wasserstein distance and its variants improve the computational efficiency through the random projection, yet they suffer from low accuracy if the number of projections is not sufficiently large, because the majority of projections result in trivially small values. In this work, we propose a new family of distance metrics, called augmented sliced Wasserstein distances (ASWDs), constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear projections of samples residing on these hypersurfaces would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. We show that the hypersurfaces can be optimized by gradient ascent efficiently. We provide the condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture. Numerical results demonstrate that the ASWD significantly outperforms other Wasserstein variants for both synthetic and real-world problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|augmented_sliced_wasserstein_distances", "pdf": "/pdf/d09a765a0ca6e8fe66e61db6af5518d089814c41.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2006.08812/code)", "_bibtex": "@inproceedings{\nchen2022augmented,\ntitle={Augmented Sliced Wasserstein Distances},\nauthor={Xiongjie Chen and Yongxin Yang and Yunpeng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iMqTLyfwnOO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "Az-7gJc6lpr", "original": "3s3VFg6E5e8L", "number": 3887, "cdate": 1632875716386, "mdate": null, "ddate": null, "tcdate": 1632875716386, "tmdate": 1676330483750, "tddate": null, "forum": "Az-7gJc6lpr", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Relational Learning with Variational Bayes", "authorids": ["~Kuang-Hung_Liu1"], "authors": ["Kuang-Hung Liu"], "keywords": ["Relational learning", "psychology", "unsupervised learning", "variational inference", "probabilistic graphical model."], "abstract": "In psychology, relational learning refers to the ability to recognize and respond to relationship among objects irrespective of the nature of those objects. Relational learning has long been recognized as a hallmark of human cognition and a key question in artificial intelligence research. In this work, we propose an unsupervised learning method for addressing the relational learning problem where we learn the underlying relationship between a pair of data irrespective of the nature of those data. The central idea of the proposed method is to encapsulate the relational learning problem with a probabilistic graphical model in which we perform inference to learn about data relationship and other relational processing tasks.", "one-sentence_summary": "Propose an unsupervised learning method for addressing the relational learning problem where we learn the underlying relationship between a pair of data irrespective of the nature of those data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|relational_learning_with_variational_bayes", "pdf": "/pdf/9d3dfe42360aa203adb14bacece6acbb08064ac0.pdf", "_bibtex": "@inproceedings{\nliu2022relational,\ntitle={Relational Learning with Variational Bayes},\nauthor={Kuang-Hung Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Az-7gJc6lpr}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "UMfhoMtIaP5", "original": "GJH3_uu681a", "number": 3882, "cdate": 1632875716050, "mdate": null, "ddate": null, "tcdate": 1632875716050, "tmdate": 1676330484468, "tddate": null, "forum": "UMfhoMtIaP5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Provably Robust Adversarial Examples", "authorids": ["~Dimitar_Iliev_Dimitrov2", "~Gagandeep_Singh1", "~Timon_Gehr1", "~Martin_Vechev1"], "authors": ["Dimitar Iliev Dimitrov", "Gagandeep Singh", "Timon Gehr", "Martin Vechev"], "keywords": ["Adversarial attacks", "Robustness Certification", "Abstract Interpretation", "Deep Learning"], "abstract": "We introduce the concept of provably robust adversarial examples for deep neural networks \u2013 connected input regions constructed from standard adversarial examples which are guaranteed to be robust to a set of real-world perturbations (such as changes in pixel intensity and geometric transformations). We present a novel method called PARADE for generating these regions in a scalable manner which works by iteratively refining the region initially obtained via sampling until a refined region is certified to be adversarial with existing state-of-the-art verifiers. At each step, a novel optimization procedure is applied to maximize the region's volume under the constraint that the convex relaxation of the network behavior with respect to the region implies a chosen bound on the certification objective. Our experimental evaluation shows the effectiveness of PARADE: it successfully finds large provably robust regions including ones containing $\\approx 10^{573}$ adversarial examples for pixel intensity and $\\approx 10^{599}$ for geometric perturbations. The provability enables our robust examples to be significantly more effective against state-of-the-art defenses based on randomized smoothing than the individual attacks used to construct the regions.", "one-sentence_summary": "We introduce the concept of provably robust adversarial examples $-$ connected input regions constructed from standard adversarial examples and guaranteed to be provably robust to a set of perturbations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dimitrov|provably_robust_adversarial_examples", "pdf": "/pdf/3b8eb27fbc166f48033673d3fadc49a86ef0b79f.pdf", "supplementary_material": "/attachment/124d5d0906843e2d2ca88b3100ea79ddf6d90df2.zip", "data": "", "_bibtex": "@inproceedings{\ndimitrov2022provably,\ntitle={Provably Robust Adversarial Examples},\nauthor={Dimitar Iliev Dimitrov and Gagandeep Singh and Timon Gehr and Martin Vechev},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=UMfhoMtIaP5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "vcUmUvQCloe", "original": "bxQPENI3ybdc", "number": 3879, "cdate": 1632875715846, "mdate": null, "ddate": null, "tcdate": 1632875715846, "tmdate": 1697934572649, "tddate": null, "forum": "vcUmUvQCloe", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Joint Shapley values: a measure of joint feature importance", "authorids": ["chrisharriscjh@gmail.com", "~Richard_Pymar1", "~Colin_Rowat1"], "authors": ["Chris Harris", "Richard Pymar", "Colin Rowat"], "keywords": ["explainable AI", "Shapley value", "interaction index", "cooperative game theory"], "abstract": "The Shapley value is one of the most widely used measures of feature importance partly as it measures a feature's average effect on a model's prediction.  We introduce joint Shapley values, which directly extend Shapley's axioms and intuitions: joint Shapley values measure a set of features' average effect on a model's prediction.  We prove the uniqueness of joint Shapley values, for any order of explanation.  Results for games show that joint Shapley values present different insights from existing interaction indices, which assess the effect of a feature within a set of features.  The joint Shapley values seem to provide sensible results in ML attribution problems.  With binary features, we present a presence-adjusted global value that is more consistent with local intuitions than the usual approach.", "one-sentence_summary": "We present a direct extension of Shapley's value to sets of features, thus extending the Shapley value's intuition: a set of feature's average effect on a model's prediction", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "harris|joint_shapley_values_a_measure_of_joint_feature_importance", "pdf": "/pdf/7d8a95bb048b3b204b4a1c9a95e93486a12439a1.pdf", "supplementary_material": "/attachment/c2c971b23f3cf858b09d283af3ced660356cd2fb.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.11357/code)", "_bibtex": "@inproceedings{\nharris2022joint,\ntitle={Joint Shapley values: a measure of joint feature importance},\nauthor={Chris Harris and Richard Pymar and Colin Rowat},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vcUmUvQCloe}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "v8OlxjGn23S", "original": "3QNmdp4BqcRh", "number": 3877, "cdate": 1632875715708, "mdate": null, "ddate": null, "tcdate": 1632875715708, "tmdate": 1676330484654, "tddate": null, "forum": "v8OlxjGn23S", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach", "authorids": ["~Rafid_Mahmood1", "~Sanja_Fidler1", "~Marc_T_Law1"], "authors": ["Rafid Mahmood", "Sanja Fidler", "Marc T Law"], "keywords": ["active learning", "integer optimization"], "abstract": "Active learning is the process of training a model with limited labeled data by selecting a core subset of an unlabeled data pool to label. The large scale of data sets used in deep learning forces most sample selection strategies to employ efficient heuristics. This paper introduces an integer optimization problem for selecting a core set that minimizes the discrete Wasserstein distance from the unlabeled pool. We demonstrate that this problem can be tractably solved with a Generalized Benders Decomposition algorithm. Our strategy uses high-quality latent features that can be obtained by unsupervised learning on the unlabeled pool. Numerical results on several data sets show that our optimization approach is competitive with baselines and particularly outperforms them in the low budget regime where less than one percent of the data set is labeled. ", "one-sentence_summary": "We propose an integer optimization problem for active learning and demonstrate how optimally selecting which points to label can significantly improve classifiers under low labeling budgets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mahmood|lowbudget_active_learning_via_wasserstein_distance_an_integer_programming_approach", "pdf": "/pdf/9dac127c30d4567d8dde179f21749b9ca5494686.pdf", "data": "", "_bibtex": "@inproceedings{\nmahmood2022lowbudget,\ntitle={Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach},\nauthor={Rafid Mahmood and Sanja Fidler and Marc T Law},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=v8OlxjGn23S}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "fVu3o-YUGQK", "original": "N383wlf8FC", "number": 3876, "cdate": 1632875715641, "mdate": null, "ddate": null, "tcdate": 1632875715641, "tmdate": 1676330484703, "tddate": null, "forum": "fVu3o-YUGQK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient Self-supervised Vision Transformers for Representation Learning", "authorids": ["~Chunyuan_Li1", "~Jianwei_Yang1", "~Pengchuan_Zhang1", "~Mei_Gao1", "~Bin_Xiao2", "~Xiyang_Dai2", "~Lu_Yuan1", "~Jianfeng_Gao1"], "authors": ["Chunyuan Li", "Jianwei Yang", "Pengchuan Zhang", "Mei Gao", "Bin Xiao", "Xiyang Dai", "Lu Yuan", "Jianfeng Gao"], "keywords": ["self-supervised learning", "vision transformers", "non-contrastive region-matching task"], "abstract": "This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task, non-contrastive region-matching, which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and pre-trained models are released at: https://github.com/microsoft/esvit", "one-sentence_summary": "Achieving SoTA ImageNet linear probe task with 10 times higher throughput, using the synergy of a multi-stage Transformer architecture and a non-contrastive region-matching pre-training task. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|efficient_selfsupervised_vision_transformers_for_representation_learning", "pdf": "/pdf/e7b63dccef8ad598db1c36a2386c8d8a63058e8e.pdf", "supplementary_material": "/attachment/46af8eba5e24f1e74e43db2667abbb8ee5d70ee0.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nli2022efficient,\ntitle={Efficient Self-supervised Vision Transformers for Representation Learning},\nauthor={Chunyuan Li and Jianwei Yang and Pengchuan Zhang and Mei Gao and Bin Xiao and Xiyang Dai and Lu Yuan and Jianfeng Gao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fVu3o-YUGQK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "9RUHPlladgh", "original": "XWGZqttkdEe", "number": 3872, "cdate": 1632875715371, "mdate": null, "ddate": null, "tcdate": 1632875715371, "tmdate": 1676330484805, "tddate": null, "forum": "9RUHPlladgh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain", "authorids": ["~Lukas_Schott2", "~Julius_Von_K\u00fcgelgen1", "~Frederik_Tr\u00e4uble1", "~Peter_Vincent_Gehler1", "~Chris_Russell3", "~Matthias_Bethge1", "~Bernhard_Sch\u00f6lkopf1", "~Francesco_Locatello1", "~Wieland_Brendel1"], "authors": ["Lukas Schott", "Julius Von K\u00fcgelgen", "Frederik Tr\u00e4uble", "Peter Vincent Gehler", "Chris Russell", "Matthias Bethge", "Bernhard Sch\u00f6lkopf", "Francesco Locatello", "Wieland Brendel"], "keywords": ["Generalization", "Composition", "Out of distribution", "Disentanglement"], "abstract": "An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world.\nIn this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. \nIn contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark.\nIn total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets.\nDespite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization.", "one-sentence_summary": "We study and benchmark the inductive biases for generalization in visual representation learning on systematic out-of-distribution settings. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "schott|visual_representation_learning_does_not_generalize_strongly_within_the_same_domain", "pdf": "/pdf/775e024ab2e9ce40e6b2f7608d5b1eb2c1136e75.pdf", "supplementary_material": "/attachment/1afb08ed0c165f2166e4314f0d76693796a29268.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nschott2022visual,\ntitle={Visual Representation Learning Does Not Generalize Strongly Within the Same Domain},\nauthor={Lukas Schott and Julius Von K{\\\"u}gelgen and Frederik Tr{\\\"a}uble and Peter Vincent Gehler and Chris Russell and Matthias Bethge and Bernhard Sch{\\\"o}lkopf and Francesco Locatello and Wieland Brendel},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9RUHPlladgh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "e2Lle5cij9D", "original": "EInN8o2tvHwO", "number": 3868, "cdate": 1632875715096, "mdate": null, "ddate": null, "tcdate": 1632875715096, "tmdate": 1697934574791, "tddate": null, "forum": "e2Lle5cij9D", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions", "authorids": ["~Arda_Sahiner1", "~Tolga_Ergen1", "~Batu_Ozturkler1", "~Burak_Bartan1", "~John_M._Pauly1", "~Morteza_Mardani1", "~Mert_Pilanci3"], "authors": ["Arda Sahiner", "Tolga Ergen", "Batu Ozturkler", "Burak Bartan", "John M. Pauly", "Morteza Mardani", "Mert Pilanci"], "keywords": ["Wasserstein GAN", "convex-concave game", "saddle points", "generative models", "quadratic", "polynomial activation", "convex duality"], "abstract": "Generative Adversarial Networks (GANs) are commonly used for modeling complex distributions of data. Both the generators and discriminators of GANs are often modeled by neural networks, posing a non-transparent optimization problem which is non-convex and non-concave over the generator and discriminator, respectively. Such networks are often heuristically optimized with gradient descent-ascent (GDA), but it is unclear whether the optimization problem contains any saddle points, or whether heuristic methods can find them in practice. In this work, we analyze the training of Wasserstein GANs with two-layer neural network discriminators through the lens of convex duality, and for a variety of generators expose the conditions under which Wasserstein GANs can be solved exactly with convex optimization approaches, or can be represented as convex-concave games. Using this convex duality interpretation, we further demonstrate the impact of different activation functions of the discriminator. Our observations are verified with numerical results demonstrating the power of the convex interpretation, with an application in progressive training of convex architectures corresponding to linear generators and quadratic-activation discriminators for CelebA image generation. The code for our experiments is available at https://github.com/ardasahiner/ProCoGAN.", "one-sentence_summary": "We demonstrate that Wasserstein GANs with two-layer discriminators and a variety of generators are equivalent to convex optimization problems or convex-concave games, allowing for global optimization in polynomial time and improved interpretability.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sahiner|hidden_convexity_of_wasserstein_gans_interpretable_generative_models_with_closedform_solutions", "pdf": "/pdf/733796fc142ddb063afc1a0818ecba208aef1465.pdf", "supplementary_material": "/attachment/170a0d7587ed2af69529a8c7742c9ddf9460c884.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.05680/code)", "_bibtex": "@inproceedings{\nsahiner2022hidden,\ntitle={Hidden Convexity of Wasserstein {GAN}s: Interpretable Generative Models with Closed-Form Solutions},\nauthor={Arda Sahiner and Tolga Ergen and Batu Ozturkler and Burak Bartan and John M. Pauly and Morteza Mardani and Mert Pilanci},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=e2Lle5cij9D}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "NRX9QZ6yqt", "original": "hylN6jeTVSCf", "number": 3866, "cdate": 1632875714962, "mdate": null, "ddate": null, "tcdate": 1632875714962, "tmdate": 1697934574778, "tddate": null, "forum": "NRX9QZ6yqt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Memory Augmented Optimizers for Deep Learning", "authorids": ["~Paul-Aymeric_Martin_McRae1", "~Prasanna_Parthasarathi2", "~Mido_Assran1", "~Sarath_Chandar1"], "authors": ["Paul-Aymeric Martin McRae", "Prasanna Parthasarathi", "Mido Assran", "Sarath Chandar"], "keywords": ["Optimization for Deep learning", "Memory augmented Optimizers"], "abstract": "Popular approaches for minimizing loss in data-driven learning often involve an abstraction or an explicit retention of the history of gradients for efficient parameter updates. \nThe aggregated history of gradients nudges the parameter updates in the right direction even when the gradients at any given step are not informative. \nAlthough the history of gradients summarized in meta-parameters or explicitly stored in memory has been shown effective in theory and practice, the question of whether $all$ or only a subset of the gradients in the history are sufficient in deciding the parameter updates remains unanswered. \nIn this paper, we propose a framework of memory-augmented gradient descent optimizers that retain a limited view of their gradient history in their internal memory. \nSuch optimizers scale well to large real-life datasets, and our experiments show that the memory augmented extensions of standard optimizers enjoy accelerated convergence and improved performance on a majority of computer vision and language tasks that we considered.\nAdditionally, we prove that the proposed class of optimizers with fixed-size memory converge under assumptions of strong convexity, regardless of which gradients are selected or how they are linearly combined to form the update step.", "one-sentence_summary": "We propose a framework of memory augmented optimizers and empirically show that the class of optimizers provide accelerated convergence and even better test performance. We show the proposed optimizers converge in smooth strongly convex setting.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mcrae|memory_augmented_optimizers_for_deep_learning", "pdf": "/pdf/874e2c95385be68f564d4d96107e652253f10706.pdf", "supplementary_material": "/attachment/9f4edb820d39b8c6dfcf0a140b6cb7cce18cf7b8.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2106.10708/code)", "_bibtex": "@inproceedings{\nmcrae2022memory,\ntitle={Memory Augmented Optimizers for Deep Learning},\nauthor={Paul-Aymeric Martin McRae and Prasanna Parthasarathi and Mido Assran and Sarath Chandar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NRX9QZ6yqt}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "c87d0TS4yX", "original": "-qJQjNE1BwH", "number": 3856, "cdate": 1632875714275, "mdate": null, "ddate": null, "tcdate": 1632875714275, "tmdate": 1697934576183, "tddate": null, "forum": "c87d0TS4yX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Orchestrated Value Mapping for Reinforcement Learning", "authorids": ["~Mehdi_Fatemi1", "~Arash_Tavakoli1"], "authors": ["Mehdi Fatemi", "Arash Tavakoli"], "keywords": ["Reinforcement Learning", "Value Mapping", "Reward Decomposition"], "abstract": "We present a general convergent class of reinforcement learning algorithms that is founded on two distinct principles: (1) mapping value estimates to a different space using arbitrary functions from a broad class, and (2) linearly decomposing the reward signal into multiple channels. The first principle enables incorporating specific properties into the value estimator that can enhance learning. The second principle, on the other hand, allows for the value function to be represented as a composition of multiple utility functions. This can be leveraged for various purposes, e.g. dealing with highly varying reward scales, incorporating a priori knowledge about the sources of reward, and ensemble learning. Combining the two principles yields a general blueprint for instantiating convergent algorithms by orchestrating diverse mapping functions over multiple reward channels. This blueprint generalizes and subsumes algorithms such as Q-Learning, Log Q-Learning, and Q-Decomposition. In addition, our convergence proof for this general class relaxes certain required assumptions in some of these algorithms. Based on our theory, we discuss several interesting configurations as special cases. Finally, to illustrate the potential of the design space that our theory opens up, we instantiate a particular algorithm and evaluate its performance on the Atari suite.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fatemi|orchestrated_value_mapping_for_reinforcement_learning", "pdf": "/pdf/9ef3cef089b9f45f5bdb93fddb0ed8ccfa9e3268.pdf", "one-sentence_summary": "We present a general convergent class of RL algorithms based on combining arbitrary value mappings and reward decomposition. ", "supplementary_material": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.07171/code)", "_bibtex": "@inproceedings{\nfatemi2022orchestrated,\ntitle={Orchestrated Value Mapping for Reinforcement Learning},\nauthor={Mehdi Fatemi and Arash Tavakoli},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=c87d0TS4yX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "CIaQKbTBwtU", "original": "82mpRJMHwOa", "number": 3853, "cdate": 1632875714068, "mdate": null, "ddate": null, "tcdate": 1632875714068, "tmdate": 1697934576176, "tddate": null, "forum": "CIaQKbTBwtU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Generalize across Domains on Single Test Samples", "authorids": ["~Zehao_Xiao1", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Zehao Xiao", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["domain generalization", "single test sample generalization", "meta learning", "variational inference"], "abstract": "We strive to learn a model from a set of source domains that generalizes well to unseen target domains. The main challenge in such a domain generalization scenario is the unavailability of any target domain data during training, resulting in the learned model not being explicitly adapted to the unseen target domains. We propose learning to generalize across domains on single test samples. We leverage a meta-learning paradigm to learn our model to acquire the ability of adaptation with single samples at training time so as to further adapt itself to each single test sample at test time. We formulate the adaptation to the single test sample as a variational Bayesian inference problem, which incorporates the test sample as a conditional into the generation of model parameters. The adaptation to each test sample requires only one feed-forward computation at test time without any fine-tuning or self-supervised training on additional data from the unseen domains.  Extensive ablation studies demonstrate that our model learns the ability to adapt models to each single sample by mimicking domain shifts during training. Further, our model achieves at least comparable -- and often better -- performance than state-of-the-art methods on multiple benchmarks for domain generalization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xiao|learning_to_generalize_across_domains_on_single_test_samples", "pdf": "/pdf/4fcc67594340f12c1beb7e4f1ce64c7be6f70c0a.pdf", "one-sentence_summary": "We leverage a meta-learning paradigm to learn our model to acquire the ability of adaptation with single samples at training time so as to further adapt itself to each single test sample at test time.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.08045/code)", "_bibtex": "@inproceedings{\nxiao2022learning,\ntitle={Learning to Generalize across Domains on Single Test Samples},\nauthor={Zehao Xiao and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CIaQKbTBwtU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "lY0-7bj0Vfz", "original": "h_-GLbcj-O_", "number": 3839, "cdate": 1632875713104, "mdate": null, "ddate": null, "tcdate": 1632875713104, "tmdate": 1676330486242, "tddate": null, "forum": "lY0-7bj0Vfz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Prototype memory and attention mechanisms for few shot image generation", "authorids": ["~Tianqin_Li2", "~Zijie_Li2", "~Andrew_Luo2", "~Harold_Rockwell1", "~Amir_Barati_Farimani2", "~Tai_Sing_Lee1"], "authors": ["Tianqin Li", "Zijie Li", "Andrew Luo", "Harold Rockwell", "Amir Barati Farimani", "Tai Sing Lee"], "keywords": ["neuroscience", "deep learning"], "abstract": "Recent discoveries indicate that the neural codes in the primary visual cortex (V1) of macaque monkeys are complex, diverse and sparse. This leads us to ponder the computational advantages and functional role of these \u201cgrandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing within the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized via a memory-based attention operation, which we define as Memory Concept Attention (MoCA). To test our proposal, we show in a few-shot image generation task, that having a prototype memory during attention can improve image synthesis quality, learn interpretable visual concept clusters, as well as improve the robustness of the model. Interestingly, we also find that our attentional memory mechanism can implicitly modify the horizontal connections by updating the transformation into the prototype embedding space for self-attention. Insofar as GANs can be seen as plausible models for reasoning about the top-down synthesis in the analysis-by-synthesis loop of the hierarchical visual cortex, our findings demonstrate a plausible computational role for these \u201cprototype concept\" neurons in visual processing in the brain.", "one-sentence_summary": "computational role for \u201cprototype concept neurons\u201d in top-down synthesis path", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|prototype_memory_and_attention_mechanisms_for_few_shot_image_generation", "pdf": "/pdf/c2a4a72f1bd5890c4beeb93de11cac4746eae2c1.pdf", "data": "", "_bibtex": "@inproceedings{\nli2022prototype,\ntitle={Prototype memory and attention mechanisms for few shot image generation},\nauthor={Tianqin Li and Zijie Li and Andrew Luo and Harold Rockwell and Amir Barati Farimani and Tai Sing Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lY0-7bj0Vfz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "FEBFJ98FKx", "original": "XvogUj5gVZ", "number": 3832, "cdate": 1632875712623, "mdate": null, "ddate": null, "tcdate": 1632875712623, "tmdate": 1676330487123, "tddate": null, "forum": "FEBFJ98FKx", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "TPU-GAN: Learning temporal coherence from dynamic point cloud sequences", "authorids": ["~Zijie_Li2", "~Tianqin_Li2", "~Amir_Barati_Farimani2"], "authors": ["Zijie Li", "Tianqin Li", "Amir Barati Farimani"], "keywords": ["Point cloud super resolution", "Temporal learning", "Generative Adversarial Networks"], "abstract": "Point cloud sequence is an important data representation that provides flexible shape and motion information. Prior work demonstrates that incorporating scene flow information into loss can make model learn temporally coherent feature spaces. However, it is prohibitively expensive to acquire point correspondence information across frames in real-world environments. In this work, we propose a super-resolution generative adversarial network (GAN) for upsampling dynamic point cloud sequences, which does not require point correspondence annotation.  Our model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, we propose a learnable masking module to adapt upsampling ratio according to the point distribution. We conduct extensive experiments on point cloud sequences from two different domains: particles in the fluid dynamical system and human action scanned data. The quantitative and qualitative evaluation demonstrates the effectiveness of our method on upsampling tasks as well as learning temporal coherence from irregular point cloud sequences.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|tpugan_learning_temporal_coherence_from_dynamic_point_cloud_sequences", "pdf": "/pdf/52569840ae5698d2203efde4f8f06d012fa7868a.pdf", "one-sentence_summary": "We propose a GAN framework for super-resolution task on dynamic point cloud sequences.", "_bibtex": "@inproceedings{\nli2022tpugan,\ntitle={{TPU}-{GAN}: Learning temporal coherence from dynamic point cloud sequences},\nauthor={Zijie Li and Tianqin Li and Amir Barati Farimani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FEBFJ98FKx}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "JBAZe2yN6Ub", "original": "Bux9pzNtRdw", "number": 3831, "cdate": 1632875712554, "mdate": null, "ddate": null, "tcdate": 1632875712554, "tmdate": 1676330487196, "tddate": null, "forum": "JBAZe2yN6Ub", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A First-Occupancy Representation for Reinforcement Learning", "authorids": ["~Ted_Moskovitz1", "~Spencer_R_Wilson1", "~Maneesh_Sahani1"], "authors": ["Ted Moskovitz", "Spencer R Wilson", "Maneesh Sahani"], "keywords": ["successor representation", "successor features", "generalized policy improvement", "GPI"], "abstract": "Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states.  The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to different reward structures in an otherwise constant Markovian environment and has been hypothesized to underlie aspects of biological behavior and neural activity.  However, in the real world, rewards may only be available for consumption once, may shift location, or agents may simply aim to reach goal states as rapidly as possible without the constraint of artificially imposed task horizons. In such cases, the most behaviorally-relevant representation would carry information about when the agent was likely to first reach states of interest, rather than how often it should expect to visit them over a potentially infinite time span.  To reflect such demands, we introduce the first-occupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed.  We demonstrate that the FR facilitates exploration, the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "moskovitz|a_firstoccupancy_representation_for_reinforcement_learning", "pdf": "/pdf/46abdff2d131f44012d855cdd93c0fa7034d601a.pdf", "one-sentence_summary": "We introduce the first-occupancy representation, a modification of the successor representation which enables agents to perform rapid policy evaluation and planning for a class of ethologically important non-Markovian reward functions.", "supplementary_material": "/attachment/378bd6631f271e490141e223fce76b4742da14df.zip", "_bibtex": "@inproceedings{\nmoskovitz2022a,\ntitle={A First-Occupancy Representation for Reinforcement Learning},\nauthor={Ted Moskovitz and Spencer R Wilson and Maneesh Sahani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JBAZe2yN6Ub}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "ci7LBzDn2Q", "original": "fCruAI-ZOEY", "number": 3830, "cdate": 1632875712486, "mdate": null, "ddate": null, "tcdate": 1632875712486, "tmdate": 1676330487289, "tddate": null, "forum": "ci7LBzDn2Q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Deep ReLU Networks Preserve Expected Length", "authorids": ["~Boris_Hanin1", "~Ryan_S_Jeong1", "~David_Rolnick1"], "authors": ["Boris Hanin", "Ryan S Jeong", "David Rolnick"], "keywords": ["deep learning theory", "random ReLU networks", "length distortion", "initialization", "expressivity"], "abstract": "Assessing the complexity of functions computed by a neural network helps us understand how the network will learn and generalize. One natural measure of complexity is how the network distorts length - if the network takes a unit-length curve as input, what is the length of the resulting curve of outputs? It has been widely believed that this length grows exponentially in network depth. We prove that in fact this is not the case: the expected length distortion does not grow with depth, and indeed shrinks slightly, for ReLU networks with standard random initialization. We also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher-dimensional volumes. These theoretical results are corroborated by our experiments.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hanin|deep_relu_networks_preserve_expected_length", "pdf": "/pdf/726f7b1d7efcb38a8f1685099dbfc32c938b1267.pdf", "one-sentence_summary": "This article proves that, both on average and with high probability, randomly initialized ReLU networks with width larger than depth do not distort lengths and volumes. ", "_bibtex": "@inproceedings{\nhanin2022deep,\ntitle={Deep Re{LU} Networks Preserve Expected Length},\nauthor={Boris Hanin and Ryan S Jeong and David Rolnick},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ci7LBzDn2Q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "lTqGXfn9Tv", "original": "vW6gFrVrX1p", "number": 3828, "cdate": 1632875712357, "mdate": null, "ddate": null, "tcdate": 1632875712357, "tmdate": 1697934579292, "tddate": null, "forum": "lTqGXfn9Tv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Phenomenology of Double Descent in Finite-Width Neural Networks", "authorids": ["~Sidak_Pal_Singh1", "~Aurelien_Lucchi1", "~Thomas_Hofmann1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Sidak Pal Singh", "Aurelien Lucchi", "Thomas Hofmann", "Bernhard Sch\u00f6lkopf"], "keywords": ["double descent", "generalization", "neural networks", "hessian", "flatness"], "abstract": "`Double descent' delineates the generalization behaviour of models depending on the regime they belong to: under- or over-parameterized. The current theoretical understanding behind the occurrence of this phenomenon is primarily based on linear and kernel regression models --- with informal parallels to neural networks via the Neural Tangent Kernel. Therefore such analyses do not adequately capture the mechanisms behind double descent in finite-width neural networks, as well as, disregard crucial components --- such as the choice of the loss function. We address these shortcomings by leveraging influence functions in order to derive suitable expressions of the population loss and its lower bound, while imposing minimal assumptions on the form of the parametric model. Our derived bounds bear an intimate connection with the spectrum of the Hessian at the optimum, and importantly, exhibit a double descent behaviour at the interpolation threshold. Building on our analysis, we further investigate how the loss function affects double descent --- and thus uncover interesting properties of neural networks and their Hessian spectra near the interpolation threshold.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "singh|phenomenology_of_double_descent_in_finitewidth_neural_networks", "pdf": "/pdf/692a8cdd6b0dd0b5c63c485191d55432b21ad442.pdf", "one-sentence_summary": "We provide a theoretical analysis of double descent that applies for finite-width neural networks and delivers insights into the properties of neural networks (and its Hessian spectrum) near the interpolation threshold.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.07337/code)", "_bibtex": "@inproceedings{\nsingh2022phenomenology,\ntitle={Phenomenology of Double Descent in Finite-Width Neural Networks},\nauthor={Sidak Pal Singh and Aurelien Lucchi and Thomas Hofmann and Bernhard Sch{\\\"o}lkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lTqGXfn9Tv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "F72ximsx7C1", "original": "JN51cIvjPN0", "number": 3825, "cdate": 1632875712151, "mdate": null, "ddate": null, "tcdate": 1632875712151, "tmdate": 1697934579287, "tddate": null, "forum": "F72ximsx7C1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How Attentive are Graph Attention Networks? ", "authorids": ["~Shaked_Brody1", "~Uri_Alon1", "~Eran_Yahav1"], "authors": ["Shaked Brody", "Uri Alon", "Eran Yahav"], "keywords": ["graph attention networks", "dynamic attention", "GAT", "GNN"], "abstract": "Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query.\nHowever, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention.\nBecause GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. \nTo remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 12 OGB and other benchmarks while we match their parametric costs. \nOur code is available at https://github.com/tech-srl/how_attentive_are_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library.", "one-sentence_summary": "We identify that Graph Attention Networks (GAT) compute a very weak form of attention. We show its empirical implications and propose a fix.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "brody|how_attentive_are_graph_attention_networks", "pdf": "/pdf/10878ac1155ddeeada5fd384fbe0cf15747d06bf.pdf", "supplementary_material": "/attachment/19dbb6aeb236683cd556af10c1238bfb14434f2b.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2105.14491/code)", "_bibtex": "@inproceedings{\nbrody2022how,\ntitle={How Attentive are Graph Attention Networks? },\nauthor={Shaked Brody and Uri Alon and Eran Yahav},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=F72ximsx7C1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "92tYQiil17", "original": "IVfcYQ1LWqN", "number": 3810, "cdate": 1632875711141, "mdate": null, "ddate": null, "tcdate": 1632875711141, "tmdate": 1676330488151, "tddate": null, "forum": "92tYQiil17", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Transferable Reward for Query Object Localization with Policy Adaptation", "authorids": ["~Tingfeng_Li3", "~Shaobo_Han1", "~Martin_Renqiang_Min1", "~Dimitris_N._Metaxas1"], "authors": ["Tingfeng Li", "Shaobo Han", "Martin Renqiang Min", "Dimitris N. Metaxas"], "keywords": [], "abstract": "We propose a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning. Our proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. In addition, the transferable reward allows repurposing the trained agent from one specific class to another class. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of our approach.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|learning_transferable_reward_for_query_object_localization_with_policy_adaptation", "pdf": "/pdf/5b72a5cbe8d019baa19ccef469da73414589de18.pdf", "data": "", "_bibtex": "@inproceedings{\nli2022learning,\ntitle={Learning Transferable Reward for Query Object Localization with Policy Adaptation},\nauthor={Tingfeng Li and Shaobo Han and Martin Renqiang Min and Dimitris N. Metaxas},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=92tYQiil17}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "8FhxBtXSl0", "original": "I4RDyOeBrfyg", "number": 3800, "cdate": 1632875710469, "mdate": null, "ddate": null, "tcdate": 1632875710469, "tmdate": 1697934582382, "tddate": null, "forum": "8FhxBtXSl0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CKConv: Continuous Kernel Convolution For Sequential Data", "authorids": ["~David_W._Romero1", "~Anna_Kuzina1", "~Erik_J_Bekkers1", "~Jakub_Mikolaj_Tomczak1", "~Mark_Hoogendoorn2"], "authors": ["David W. Romero", "Anna Kuzina", "Erik J Bekkers", "Jakub Mikolaj Tomczak", "Mark Hoogendoorn"], "keywords": ["Convolutional Networks", "Continuous kernel Convolutions", "Continuous Convolutional Kernels", "Implicit Neural Representations", "Sequential Data."], "abstract": "Conventional neural architectures for sequential data present important limitations. Recurrent neural networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional neural networks cannot handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that these problems can be solved by formulating the convolutional kernels of CNNs as continuous functions. The resulting Continuous Kernel Convolution (CKConv) handles arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularly-sampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a faster and simpler manner.", "one-sentence_summary": "We provide  a continuous parameterization to convolutional kernels, with which several advantages upon conventional (discrete) parameterizations are obtained.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "romero|ckconv_continuous_kernel_convolution_for_sequential_data", "pdf": "/pdf/eb7ec6afc6fd671f4c62e8ae61ac22465ac362ab.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2102.02611/code)", "_bibtex": "@inproceedings{\nromero2022ckconv,\ntitle={{CKC}onv: Continuous Kernel Convolution For Sequential Data},\nauthor={David W. Romero and Anna Kuzina and Erik J Bekkers and Jakub Mikolaj Tomczak and Mark Hoogendoorn},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8FhxBtXSl0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "H4PmOqSZDY", "original": "6n0hv9KYdDT", "number": 3796, "cdate": 1632875710188, "mdate": null, "ddate": null, "tcdate": 1632875710188, "tmdate": 1697934583021, "tddate": null, "forum": "H4PmOqSZDY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Empirical Sandwich Bounds on the Rate-Distortion Function", "authorids": ["~Yibo_Yang1", "~Stephan_Mandt1"], "authors": ["Yibo Yang", "Stephan Mandt"], "keywords": ["information theory", "deep generative modeling", "lossy data compression"], "abstract": "Rate-distortion (R-D) function, a key quantity in information theory, characterizes the fundamental limit of how much a data source can be compressed subject to a fidelity criterion, by any compression algorithm. As researchers push for ever-improving compression performance, establishing the R-D function of a given data source is not only of scientific interest, but also reveals the possible room for improvement in existing compression algorithms. Previous work on this problem relied on distributional assumptions on the data source (Gibson, 2017) or only applied to discrete data (Blahut, 1972; Arimoto, 1972). By contrast, this paper makes the first attempt at an algorithm for sandwiching the R-D function of a general (not necessarily discrete) source requiring only i.i.d. data samples. We estimate R-D sandwich bounds for a variety of artificial and real-world data sources, in settings far beyond the feasibility of any known method, and shed light on the optimality of neural data compression (Ball\u00e9 et al., 2021; Yang et al., 2022). Our R-D upper bound on natural images indicates theoretical room for improving state-of-the-art image compression methods by at least one dB in PSNR at various bitrates. Our data and code can be found at https://github.com/mandt-lab/RD-sandwich.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|towards_empirical_sandwich_bounds_on_the_ratedistortion_function", "pdf": "/pdf/68b5094de29bac048c6f775fd6ae524a866caf89.pdf", "one-sentence_summary": "We make a first attempt at an algorithm for sandwiching the rate-distortion function of a general data sources requiring only i.i.d. samples.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2111.12166/code)", "_bibtex": "@inproceedings{\nyang2022towards,\ntitle={Towards Empirical Sandwich Bounds on the Rate-Distortion Function},\nauthor={Yibo Yang and Stephan Mandt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=H4PmOqSZDY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "wfZGut6e09", "original": "nPkCLX9nmDu", "number": 3794, "cdate": 1632875710050, "mdate": null, "ddate": null, "tcdate": 1632875710050, "tmdate": 1676330489404, "tddate": null, "forum": "wfZGut6e09", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pareto Policy Adaptation", "authorids": ["~Panagiotis_Kyriakis1", "~Jyotirmoy_Deshmukh2", "~Paul_Bogdan1"], "authors": ["Panagiotis Kyriakis", "Jyotirmoy Deshmukh", "Paul Bogdan"], "keywords": ["multi-objective reinforcement learning", "policy gradient", "pareto optimality", "policy adaptation"], "abstract": "We present a policy gradient method for Multi-Objective Reinforcement Learning under unknown, linear preferences. By enforcing Pareto stationarity, a first-order condition for Pareto optimality, we are able to design a simple policy gradient algorithm that approximates the Pareto front and infers the unknown preferences. Our method relies on a projected gradient descent solver that identifies common ascent directions for all objectives. Leveraging the solution of that solver, we introduce Pareto Policy Adaptation (PPA), a loss function that adapts the policy to be optimal with respect to any distribution over preferences. PPA uses implicit differentiation to back-propagate the loss gradient bypassing the operations of the projected gradient descent solver. Our approach is straightforward, easy to implement and can be used with all existing policy gradient and actor-critic methods. We evaluate our method in a series of reinforcement learning tasks", "one-sentence_summary": "We propose a policy gradient method for multi-objective reinforcement learning under unknown, linear preferences. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kyriakis|pareto_policy_adaptation", "pdf": "/pdf/20c222c6c34c93ec1b453f5c07d51bd1827ec7bd.pdf", "supplementary_material": "/attachment/413b411f4c4ec43d9502b5a6c43f9b246a0f7ba8.zip", "_bibtex": "@inproceedings{\nkyriakis2022pareto,\ntitle={Pareto Policy Adaptation},\nauthor={Panagiotis Kyriakis and Jyotirmoy Deshmukh and Paul Bogdan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wfZGut6e09}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "BrFIKuxrZE", "original": "fpmlq4zQ74s", "number": 3788, "cdate": 1632875709635, "mdate": null, "ddate": null, "tcdate": 1632875709635, "tmdate": 1697934583892, "tddate": null, "forum": "BrFIKuxrZE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fair Normalizing Flows", "authorids": ["~Mislav_Balunovic1", "~Anian_Ruoss1", "~Martin_Vechev1"], "authors": ["Mislav Balunovic", "Anian Ruoss", "Martin Vechev"], "keywords": ["fairness", "fair representation learning", "adversarial fairness", "trustworthy machine learning"], "abstract": "Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "balunovic|fair_normalizing_flows", "pdf": "/pdf/609e4c482621a5208ae4ebb3b311369c3b04689f.pdf", "one-sentence_summary": "We propose a new fair representation learning method based on normalizing flows which can bound the accuracy of any adversary trying to predict sensitive attributes. ", "supplementary_material": "/attachment/51959f7876af18e13ea9452aec7febd2d8e30434.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.05937/code)", "_bibtex": "@inproceedings{\nbalunovic2022fair,\ntitle={Fair Normalizing Flows},\nauthor={Mislav Balunovic and Anian Ruoss and Martin Vechev},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BrFIKuxrZE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 33}}, {"id": "5QhUE1qiVC6", "original": "VhpE8S8K4B", "number": 3787, "cdate": 1632875709568, "mdate": null, "ddate": null, "tcdate": 1632875709568, "tmdate": 1676330489702, "tddate": null, "forum": "5QhUE1qiVC6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program", "authorids": ["~Yifei_Wang2", "~Mert_Pilanci3"], "authors": ["Yifei Wang", "Mert Pilanci"], "keywords": ["Two-layer ReLU networks", "convex optimization", "convex duality", "gradient flow"], "abstract": "We study non-convex subgradient flows for training two-layer ReLU neural networks from a convex geometry and duality perspective. We characterize the implicit bias of unregularized non-convex gradient flow as convex regularization of an equivalent convex model. We then show that the limit points of non-convex subgradient flows can be identified via primal-dual correspondence in this convex optimization problem.  Moreover, we derive a sufficient condition on the dual variables which ensures that the stationary points of the non-convex objective are the KKT points of the convex objective, thus proving convergence of non-convex gradient flows to the global optimum. For a class of regular training data distributions such as orthogonal separable data, we show that this sufficient condition holds. Therefore, non-convex gradient flows in fact converge to optimal solutions of a convex optimization problem. We present numerical results verifying the predictions of our theory for non-convex subgradient descent.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|the_convex_geometry_of_backpropagation_neural_network_gradient_flows_converge_to_extreme_points_of_the_dual_convex_program", "pdf": "/pdf/4d29755fe3cd56f6093aa8e79892bc79392b8c0d.pdf", "supplementary_material": "/attachment/04bee031dbd55f17f67a6d239aea255d5ec906d5.zip", "_bibtex": "@inproceedings{\nwang2022the,\ntitle={The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program},\nauthor={Yifei Wang and Mert Pilanci},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5QhUE1qiVC6}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "5MLb3cLCJY", "original": "2r7undbrrUay", "number": 3786, "cdate": 1632875709497, "mdate": null, "ddate": null, "tcdate": 1632875709497, "tmdate": 1676330489863, "tddate": null, "forum": "5MLb3cLCJY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Adaptive Wavelet Transformer Network for 3D Shape Representation Learning", "authorids": ["~Hao_Huang3", "~Yi_Fang2"], "authors": ["Hao Huang", "Yi Fang"], "keywords": [], "abstract": "We present a novel method for 3D shape representation learning using multi-scale wavelet decomposition. Previous works often decompose 3D shapes into complementary components in spatial domain at a single scale. In this work, we study to decompose 3D shapes into sub-bands components in frequency domain at multiple scales, resulting in a hierarchical decomposition tree in a principled manner rooted in multi-resolution wavelet analysis. Specifically, we propose Adaptive Wavelet Transformer Network (AWT-Net) that firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, AWT-Net exploits Transformer to enhance the original shape features by querying and fusing features from different but integrated sub-bands. The wavelet coefficients can be learned without direct supervision on coefficients, and AWT-Net is fully differentiable and can be learned in an end-to-end fashion. Extensive experiments demonstrate that AWT-Net achieves competitive performance on 3D shape classification and segmentation benchmarks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|adaptive_wavelet_transformer_network_for_3d_shape_representation_learning", "pdf": "/pdf/b460e9efd8a892dfa306a2d12f830a63074ab5dd.pdf", "data": "", "_bibtex": "@inproceedings{\nhuang2022adaptive,\ntitle={Adaptive Wavelet Transformer Network for 3D Shape Representation Learning},\nauthor={Hao Huang and Yi Fang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5MLb3cLCJY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "g5tANwND04i", "original": "z8Qu9BJFXRd", "number": 3772, "cdate": 1632875708608, "mdate": null, "ddate": null, "tcdate": 1632875708608, "tmdate": 1676330490682, "tddate": null, "forum": "g5tANwND04i", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Convergence of mSGD and AdaGrad for Stochastic Optimization", "authorids": ["~ruinan_Jin2", "~Yu_Xing3", "~Xingkang_He1"], "authors": ["ruinan Jin", "Yu Xing", "Xingkang He"], "keywords": ["stochastic gradient descent", "adaptive gradient algorithm", "asymptotic convergence"], "abstract": "As one of the most fundamental stochastic optimization algorithms, stochastic gradient descent (SGD) has been intensively developed and extensively applied in machine learning in the past decade. There have been some modified SGD-type algorithms, which outperform the SGD in many competitions and applications in terms of convergence rate and accuracy, such as momentum-based SGD (mSGD) and adaptive gradient algorithm (AdaGrad). Despite these empirical successes, the theoretical properties of these algorithms have not been well established due to technical difficulties. With this motivation, we focus on convergence analysis of mSGD and AdaGrad for any smooth (possibly non-convex) loss functions in stochastic optimization. First, we prove that the iterates of mSGD are asymptotically convergent to a connected set of stationary points with probability one, which is more general than existing works on subsequence convergence or convergence of time averages. Moreover, we prove that the loss function of mSGD decays at a certain rate faster than that of SGD. In addition, we prove the iterates of AdaGrad are asymptotically convergent to a connected set of stationary points with probability one. Also, this result extends the results from the literature on subsequence convergence and the convergence of time averages. Despite the generality of the above convergence results, we have relaxed some assumptions of gradient noises, convexity of loss functions, as well as boundedness of iterates.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jin|on_the_convergence_of_msgd_and_adagrad_for_stochastic_optimization", "pdf": "/pdf/b4ec8da613b96b5a0a6fa0fdf588173e801abfc8.pdf", "one-sentence_summary": "A theoretical paper focusing on the investigation for the convergence of mSGD and AdaGrad optimization algorithms.", "_bibtex": "@inproceedings{\njin2022on,\ntitle={On the Convergence of m{SGD} and AdaGrad for Stochastic Optimization},\nauthor={ruinan Jin and Yu Xing and Xingkang He},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=g5tANwND04i}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "nioAdKCEdXB", "original": "yQcOTmkYC8c", "number": 3762, "cdate": 1632875708046, "mdate": null, "ddate": null, "tcdate": 1632875708046, "tmdate": 1676330490979, "tddate": null, "forum": "nioAdKCEdXB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Likelihood Training of Schr\u00f6dinger Bridge using Forward-Backward SDEs Theory", "authorids": ["~Tianrong_Chen1", "~Guan-Horng_Liu1", "~Evangelos_Theodorou1"], "authors": ["Tianrong Chen", "Guan-Horng Liu", "Evangelos Theodorou"], "keywords": ["Schr\u00f6dinger Bridge", "score-based generative model", "optimal transport", "forward-backward stochastic differential equations", "stochastic optimal control"], "abstract": "Schr\u00f6dinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory \u2013 a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimization principle that inherits the same SB optimality yet without losing applications of modern generative training techniques, and we show that the resulting training algorithm achieves comparable results on generating realistic images on MNIST, CelebA, and CIFAR10. Our code is available at https://github.com/ghliu/SB-FBSDE.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|likelihood_training_of_schr\u00f6dinger_bridge_using_forwardbackward_sdes_theory", "pdf": "/pdf/88f4662fe55ad470a87f305792547280c33c6e1b.pdf", "one-sentence_summary": "We present a new computational framework, grounded on Forward-Backward SDEs theory, for the log-likelihood training of Schr\u00f6dinger Bridge and provide theoretical connections to score-baesd generative models.", "_bibtex": "@inproceedings{\nchen2022likelihood,\ntitle={Likelihood Training of Schr\\\"odinger Bridge using Forward-Backward {SDE}s Theory},\nauthor={Tianrong Chen and Guan-Horng Liu and Evangelos Theodorou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nioAdKCEdXB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 33}}, {"id": "twv2QlJhXzo", "original": "qMANySZXGKT", "number": 3761, "cdate": 1632875707980, "mdate": null, "ddate": null, "tcdate": 1632875707980, "tmdate": 1697934586527, "tddate": null, "forum": "twv2QlJhXzo", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Imitation Learning from Observations under Transition Model Disparity", "authorids": ["~Tanmay_Gangwani1", "~Yuan_Zhou1", "~Jian_Peng1"], "authors": ["Tanmay Gangwani", "Yuan Zhou", "Jian Peng"], "keywords": ["Imitation Learning", "Deep Reinforcement Learning"], "abstract": "Learning to perform tasks by leveraging a dataset of expert observations, also known as imitation learning from observations (ILO), is an important paradigm for learning skills without access to the expert reward function or the expert actions. We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of the discrepancy being the transition dynamics model. Recent methods for scalable ILO utilize adversarial learning to match the state-transition distributions of the expert and the learner, an approach that becomes challenging when the dynamics are dissimilar. In this work, we propose an algorithm that trains an intermediary policy in the learner environment and uses it as a surrogate expert for the learner. The intermediary policy is learned such that the state transitions generated by it are close to the state transitions in the expert dataset. To derive a practical and scalable algorithm, we employ concepts from prior work on estimating the support of a probability distribution. Experiments using MuJoCo locomotion tasks highlight that our method compares favorably to the baselines for ILO with transition dynamics mismatch.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gangwani|imitation_learning_from_observations_under_transition_model_disparity", "pdf": "/pdf/7fd85a0997ef411d5948afe26129e044734df91a.pdf", "one-sentence_summary": "Imitation learning from observations when the expert and the learner agents operate in environments with dissimilar transition dynamics models.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.11446/code)", "_bibtex": "@inproceedings{\ngangwani2022imitation,\ntitle={Imitation Learning from Observations under Transition Model Disparity},\nauthor={Tanmay Gangwani and Yuan Zhou and Jian Peng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=twv2QlJhXzo}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "4C93Qvn-tz", "original": "CM-juPHkOb52", "number": 3736, "cdate": 1632875706290, "mdate": null, "ddate": null, "tcdate": 1632875706290, "tmdate": 1676330492113, "tddate": null, "forum": "4C93Qvn-tz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MCMC Should Mix: Learning Energy-Based Model with Neural Transport Latent Space MCMC", "authorids": ["~Erik_Nijkamp2", "~Ruiqi_Gao1", "~Pavel_Sountsov2", "~Srinivas_Vasudevan1", "~Bo_Pang1", "~Song-Chun_Zhu1", "~Ying_Nian_Wu1"], "authors": ["Erik Nijkamp", "Ruiqi Gao", "Pavel Sountsov", "Srinivas Vasudevan", "Bo Pang", "Song-Chun Zhu", "Ying Nian Wu"], "keywords": ["Generative models", "energy-based models", "MCMC"], "abstract": "Learning energy-based model (EBM) requires MCMC sampling of the learned model as an inner loop of the learning algorithm. However, MCMC sampling of EBMs in high-dimensional data space is generally not mixing, because the energy function, which is usually parametrized by deep network, is highly multi-modal in the data space. This is a serious handicap for both theory and practice of EBMs. In this paper, we propose to learn EBM with a flow-based model (or in general latent variable model) serving as a backbone, so that the EBM is a correction or an exponential tilting of the flow-based model. We show that the model has a particularly simple form in the space of the latent variables of the generative model, and MCMC sampling of the EBM in the latent space mixes well and traverses modes in the data space. This enables proper sampling and learning of EBMs.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nijkamp|mcmc_should_mix_learning_energybased_model_with_neural_transport_latent_space_mcmc", "pdf": "/pdf/e59fd3452037dfc60c95270a1328f0a3077b10f9.pdf", "one-sentence_summary": "Learning energy-based models with mixing Markov chains.", "data": "", "_bibtex": "@inproceedings{\nnijkamp2022mcmc,\ntitle={{MCMC} Should Mix: Learning Energy-Based Model with Neural Transport Latent Space {MCMC}},\nauthor={Erik Nijkamp and Ruiqi Gao and Pavel Sountsov and Srinivas Vasudevan and Bo Pang and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=4C93Qvn-tz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "rrWeE9ZDw_", "original": "-8KIKckmoaZ", "number": 3728, "cdate": 1632875705882, "mdate": null, "ddate": null, "tcdate": 1632875705882, "tmdate": 1676330492237, "tddate": null, "forum": "rrWeE9ZDw_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Autonomous Learning of Object-Centric Abstractions for High-Level Planning", "authorids": ["~Steven_James1", "~Benjamin_Rosman1", "~George_Konidaris1"], "authors": ["Steven James", "Benjamin Rosman", "George Konidaris"], "keywords": ["reinforcement learning", "planning", "multitask", "transfer", "objects"], "abstract": "We propose a method for autonomously learning an object-centric representation of a continuous and high-dimensional environment that is suitable for planning. Such representations can immediately be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. We first demonstrate our approach on a 2D crafting domain consisting of numerous objects where the agent learns a compact, lifted representation that generalises across objects. We then apply it to a series of Minecraft tasks to learn object-centric representations and object types - directly from pixel data - that can be leveraged to solve new tasks quickly. The resulting learned representations enable the use of a task-level planner, resulting in an agent capable of transferring learned representations to form complex, long-term plans.", "one-sentence_summary": "We learn object-centric PDDL representations directly from raw observation data", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "james|autonomous_learning_of_objectcentric_abstractions_for_highlevel_planning", "pdf": "/pdf/a9a9310c7615055c5fd767dd5c8bde623331a28b.pdf", "supplementary_material": "/attachment/d22e396b03b9c2f0a52724f663c8d254fe53f06f.zip", "_bibtex": "@inproceedings{\njames2022autonomous,\ntitle={Autonomous Learning of Object-Centric Abstractions for High-Level Planning},\nauthor={Steven James and Benjamin Rosman and George Konidaris},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rrWeE9ZDw_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "fCSq8yrDkc", "original": "jgW5JcMDqXX", "number": 3723, "cdate": 1632875705610, "mdate": null, "ddate": null, "tcdate": 1632875705610, "tmdate": 1676330492432, "tddate": null, "forum": "fCSq8yrDkc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A fast and accurate splitting method for optimal transport: analysis and implementation", "authorids": ["~Vien_V._Mai2", "jlindbac@kth.se", "~Mikael_Johansson3"], "authors": ["Vien V. Mai", "Jacob Lindb\u00e4ck", "Mikael Johansson"], "keywords": ["Optimal transport", "Operator splitting", "Douglas-Rachford", "ADMM", "GPUs"], "abstract": "We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. Built on the celebrated Douglas-Rachford splitting technique, our method tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows us to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. The proposed method enjoys an iteration complexity $O(1/\\epsilon)$ compared to the best-known $O(1/\\epsilon^2)$ of the Sinkhorn method. In addition, we establish a linear convergence rate for our formulation of the OT problem. We detail an efficient GPU implementation of the proposed method that maintains a primal-dual stopping criterion at no extra cost. Substantial experiments demonstrate the effectiveness of our method, both in terms of computation times and robustness.", "one-sentence_summary": "We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mai|a_fast_and_accurate_splitting_method_for_optimal_transport_analysis_and_implementation", "pdf": "/pdf/c466eee883ed6f6c6b6dad8f7dc4d5f36092bb25.pdf", "supplementary_material": "/attachment/e3e662eed75085f17bddd7cc0809b5be2d68f7b7.zip", "_bibtex": "@inproceedings{\nmai2022a,\ntitle={A fast and accurate splitting method for optimal transport: analysis and implementation},\nauthor={Vien V. Mai and Jacob Lindb{\\\"a}ck and Mikael Johansson},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fCSq8yrDkc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "VLgmhQDVBV", "original": "ptM68KQiXgs", "number": 3719, "cdate": 1632875705342, "mdate": null, "ddate": null, "tcdate": 1632875705342, "tmdate": 1676330492610, "tddate": null, "forum": "VLgmhQDVBV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks", "authorids": ["~Benjamin_Bowman1", "~Guido_Montufar1"], "authors": ["Benjamin Bowman", "Guido Montufar"], "keywords": ["underparameterized regime", "spectral bias", "neural tangent kernel", "implicit bias", "implicit regularization", "gradient flow"], "abstract": "We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow.  We show that in the underparameterized regime the network learns eigenfunctions of an integral operator $T_K$ determined by the Neural Tangent Kernel at rates corresponding to their eigenvalues.  For example, for uniformly distributed data on the sphere $S^{d - 1}$ and rotation invariant weight distributions, the eigenfunctions of $T_K$ are the spherical harmonics.  Our results can be understood as describing a spectral bias in the underparameterized regime. The proofs use the concept of ``Damped Deviations'' where deviations of the NTK matter less for eigendirections with large eigenvalues.  Aside from the underparameterized regime, the damped deviations point-of-view allows us to extend certain results in the literature in the overparameterized setting. ", "one-sentence_summary": "Underparameterized networks optimizing MSE learn eigenfunctions of an NTK integral operator at rates corresponding to their eigenvalues.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bowman|implicit_bias_of_mse_gradient_optimization_in_underparameterized_neural_networks", "pdf": "/pdf/ccf082acfc5365ae3951682ff517392f46042ab1.pdf", "_bibtex": "@inproceedings{\nbowman2022implicit,\ntitle={Implicit Bias of {MSE} Gradient Optimization in Underparameterized Neural Networks},\nauthor={Benjamin Bowman and Guido Montufar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VLgmhQDVBV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "POTMtpYI1xH", "original": "ePWM0s5MV_A", "number": 3712, "cdate": 1632875704863, "mdate": null, "ddate": null, "tcdate": 1632875704863, "tmdate": 1697934590118, "tddate": null, "forum": "POTMtpYI1xH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Discovering Latent Concepts Learned in BERT", "authorids": ["~Fahim_Dalvi1", "~Abdul_Rafae_Khan1", "~Firoj_Alam1", "~Nadir_Durrani1", "jxu70@stevens.edu", "~Hassan_Sajjad1"], "authors": ["Fahim Dalvi", "Abdul Rafae Khan", "Firoj Alam", "Nadir Durrani", "Jia Xu", "Hassan Sajjad"], "keywords": ["interpretation", "BERT", "NLP"], "abstract": "A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-defined concepts that reinforce the traditional linguistic knowledge and do not reflect on how novel concepts are learned by the model. We address this limitation by discovering and analyzing latent concepts learned in neural network models in an unsupervised fashion and provide interpretations from the model's perspective. In this work, we study: i) what latent concepts exist in the pre-trained BERT model, ii) how the discovered latent concepts align or diverge from classical linguistic hierarchy and iii) how the latent concepts evolve across layers. \nOur findings show: i) a model learns novel concepts (e.g. animal categories and demographic groups), which do not strictly adhere to any pre-defined categorization (e.g. POS, semantic tags), ii) several latent concepts are based on multiple properties which may include semantics, syntax, and  morphology, iii) the lower layers in the model dominate in learning shallow lexical concepts while the higher layers learn semantic relations and iv) the discovered  latent concepts highlight potential biases learned in the model. We also release a novel BERT ConceptNet dataset consisting of 174 concept labels and 1M annotated instances.", "one-sentence_summary": "We approach interpretability from a model\u2019s perspective by discovering and analyzing latent concepts learned in pre-trained models in an unsupervised fashion. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dalvi|discovering_latent_concepts_learned_in_bert", "pdf": "/pdf/f96833144308f5ae7999c4c5e5f0dc8c6208fe67.pdf", "supplementary_material": "/attachment/a7fb817b68305b71d585b86e4d4fc7ab8460d008.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2205.07237/code)", "_bibtex": "@inproceedings{\ndalvi2022discovering,\ntitle={Discovering Latent Concepts Learned in {BERT}},\nauthor={Fahim Dalvi and Abdul Rafae Khan and Firoj Alam and Nadir Durrani and Jia Xu and Hassan Sajjad},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=POTMtpYI1xH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "dNigytemkL", "original": "PJK6LnK63z", "number": 3694, "cdate": 1632875703702, "mdate": null, "ddate": null, "tcdate": 1632875703702, "tmdate": 1676330493738, "tddate": null, "forum": "dNigytemkL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks", "authorids": ["~Rahim_Entezari1", "~Hanie_Sedghi1", "~Olga_Saukh1", "~Behnam_Neyshabur1"], "authors": ["Rahim Entezari", "Hanie Sedghi", "Olga Saukh", "Behnam Neyshabur"], "keywords": ["Permutation", "Invariance", "Mode Connectivity", "Energy Barrier", "Loss landscape", "Deep Learning"], "abstract": "In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them. Although it is a bold conjecture, we show how extensive empirical attempts fall short of refuting it. We further provide a preliminary theoretical result to support our conjecture. Our conjecture has implications for the lottery ticket hypothesis, distributed training, and ensemble methods. The source code is available at \\url{https://github.com/rahimentezari/PermutationInvariance}.", "one-sentence_summary": "We conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "entezari|the_role_of_permutation_invariance_in_linear_mode_connectivity_of_neural_networks", "pdf": "/pdf/a575dfe6923ea65c17895fd63d13fc299132536f.pdf", "_bibtex": "@inproceedings{\nentezari2022the,\ntitle={The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},\nauthor={Rahim Entezari and Hanie Sedghi and Olga Saukh and Behnam Neyshabur},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dNigytemkL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "B5XahNLmna", "original": "eSfwDut0bd", "number": 3665, "cdate": 1632875701801, "mdate": null, "ddate": null, "tcdate": 1632875701801, "tmdate": 1676330495089, "tddate": null, "forum": "B5XahNLmna", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Data Poisoning Won\u2019t Save You From Facial Recognition", "authorids": ["~Evani_Radiya-Dixit1", "~Sanghyun_Hong1", "~Nicholas_Carlini1", "~Florian_Tramer1"], "authors": ["Evani Radiya-Dixit", "Sanghyun Hong", "Nicholas Carlini", "Florian Tramer"], "keywords": ["Poisoning attacks", "adversarial examples", "facial recognition", "arms race", "defenses"], "abstract": "Data poisoning has been proposed as a compelling defense against facial recognition models trained on Web-scraped pictures. Users can perturb images they post online, so that models will misclassify future (unperturbed) pictures.\n  \n We demonstrate that this strategy provides a false sense of security, as it ignores an inherent asymmetry between the parties: users' pictures are perturbed once and for all before being published (at which point they are scraped) and must thereafter fool all future models---including models trained adaptively against the users' past attacks, or models that use new technologies discovered after the attack.\n  \nWe evaluate two systems for poisoning attacks against large-scale facial recognition, Fawkes (500,000+ downloads) and LowKey. We demonstrate how an \"oblivious\" model trainer can simply wait for future developments in computer vision to nullify the protection of pictures collected in the past. We further show that an adversary with black-box access to the attack can (i) train a robust model that resists the perturbations of collected pictures and (ii) detect poisoned pictures uploaded online.\n  \nWe caution that facial recognition poisoning will not admit an \"arms race\" between attackers and defenders. Once perturbed pictures are scraped, the attack cannot be changed so any future successful defense irrevocably undermines users' privacy.", "one-sentence_summary": "Data poisoning and adversarial examples won't protect users from facial recognition", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "radiyadixit|data_poisoning_wont_save_you_from_facial_recognition", "pdf": "/pdf/664ff8f2d58700ae00821a00773fdedbf383c737.pdf", "_bibtex": "@inproceedings{\nradiya-dixit2022data,\ntitle={Data Poisoning Won{\\textquoteright}t Save You From Facial Recognition},\nauthor={Evani Radiya-Dixit and Sanghyun Hong and Nicholas Carlini and Florian Tramer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=B5XahNLmna}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "Opmqtk_GvYL", "original": "izQtbbjdu1W", "number": 3664, "cdate": 1632875701736, "mdate": null, "ddate": null, "tcdate": 1632875701736, "tmdate": 1697934593992, "tddate": null, "forum": "Opmqtk_GvYL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MetaMorph: Learning Universal Controllers with Transformers", "authorids": ["~Agrim_Gupta1", "~Linxi_Fan2", "~Surya_Ganguli1", "~Li_Fei-Fei1"], "authors": ["Agrim Gupta", "Linxi Fan", "Surya Ganguli", "Li Fei-Fei"], "keywords": ["RL", "Modular Robots", "Transformers"], "abstract": "Multiple domains like vision, natural language, and audio are witnessing tremendous progress by leveraging Transformers for large scale pre-training followed by task specific fine tuning. In contrast, in robotics we primarily train a single robot for a single task. However, modular robot systems now allow for the flexible combination of general-purpose building blocks into task optimized morphologies. However, given the exponentially large number of possible robot morphologies, training a controller for each new design is impractical. In this work, we propose MetaMorph, a Transformer based approach to learn a universal controller over a modular robot design space. MetaMorph is based on the insight that robot morphology is just another modality on which we can condition the output of a Transformer. Through extensive experiments we demonstrate that large scale pre-training on a variety of robot morphologies results in policies with combinatorial generalization capabilities, including zero shot generalization to unseen robot morphologies. We further demonstrate that our pre-trained policy can be used for sample-efficient transfer to completely new robot morphologies and tasks.", "one-sentence_summary": "We learn a transformer based general purpose controller for a modular robot design space which can zero-shot generalize to unseen variations in dynamics, kinematics, new morphologies and tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gupta|metamorph_learning_universal_controllers_with_transformers", "pdf": "/pdf/7ef00fd81bdb696532e182f6073e6e6d9cb15e98.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.11931/code)", "_bibtex": "@inproceedings{\ngupta2022metamorph,\ntitle={MetaMorph: Learning Universal Controllers with Transformers},\nauthor={Agrim Gupta and Linxi Fan and Surya Ganguli and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Opmqtk_GvYL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "P-pPW1nxf1r", "original": "GC6OKy62qVM", "number": 3655, "cdate": 1632875701196, "mdate": null, "ddate": null, "tcdate": 1632875701196, "tmdate": 1697934594972, "tddate": null, "forum": "P-pPW1nxf1r", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models", "authorids": ["~Armen_Aghajanyan1", "oxo@fb.com", "~Mike_Lewis1", "~Mandar_Joshi1", "huxu@fb.com", "gghosh@fb.com", "~Luke_Zettlemoyer1"], "authors": ["Armen Aghajanyan", "Dmytro Okhonko", "Mike Lewis", "Mandar Joshi", "Hu Xu", "Gargi Ghosh", "Luke Zettlemoyer"], "keywords": ["prompting", "nlp", "representational learning", "priming"], "abstract": "We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. 'class' and 'id' attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling '<title>' tags for a webpage that contains the input text).  We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research. ", "one-sentence_summary": "We unlock new state-of-the-art ways of priming and automatically generating prompts by pre-training on simplified HTML.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "aghajanyan|htlm_hypertext_pretraining_and_prompting_of_language_models", "pdf": "/pdf/565914339be7ddb2453523852e836e1fe3ce3b8b.pdf", "supplementary_material": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.06955/code)", "_bibtex": "@inproceedings{\naghajanyan2022htlm,\ntitle={{HTLM}: Hyper-Text Pre-Training and Prompting of Language Models},\nauthor={Armen Aghajanyan and Dmytro Okhonko and Mike Lewis and Mandar Joshi and Hu Xu and Gargi Ghosh and Luke Zettlemoyer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=P-pPW1nxf1r}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "h0OYV0We3oh", "original": "G_gf7kF3OYD", "number": 3647, "cdate": 1632875700654, "mdate": null, "ddate": null, "tcdate": 1632875700654, "tmdate": 1697934595680, "tddate": null, "forum": "h0OYV0We3oh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Illiterate DALL-E Learns to Compose", "authorids": ["~Gautam_Singh3", "~Fei_Deng1", "~Sungjin_Ahn1"], "authors": ["Gautam Singh", "Fei Deng", "Sungjin Ahn"], "keywords": ["Zero-Shot Image Generation", "Compositional Representation", "Object-Centric Representation", "Out-of-Distribution Generalization", "Image Transformers"], "abstract": "Although DALL-E has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL-E, its ability to systematically generalize for zero-shot generation is significantly limited. In this paper, we propose a simple but novel slot-based autoencoding architecture, called SLATE, for combining the best of both worlds: learning object-centric representations that allow systematic generalization in zero-shot image generation without text. As such, this model can also be seen as an illiterate DALL-E model. Unlike the pixel-mixture decoders of existing object-centric representation models, we propose to use the Image GPT decoder conditioned on the slots for capturing complex interactions among the slots and pixels. In experiments, we show that this simple and easy-to-implement architecture not requiring a text prompt achieves significant improvement in in-distribution and out-of-distribution (zero-shot) image generation and qualitatively comparable or better slot-attention structure than the models based on mixture decoders.", "one-sentence_summary": "To learn compositional slot-based representation of an image and perform slot composition for zero-shot novel image generation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "singh|illiterate_dalle_learns_to_compose", "pdf": "/pdf/bcb5e847c6cefafd402be4829f49227cf6b457c7.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.11405/code)", "_bibtex": "@inproceedings{\nsingh2022illiterate,\ntitle={Illiterate {DALL}-E Learns to Compose},\nauthor={Gautam Singh and Fei Deng and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=h0OYV0We3oh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "JYtwGwIL7ye", "original": "boZvHnKn6MkG", "number": 3639, "cdate": 1632875700120, "mdate": null, "ddate": null, "tcdate": 1632875700120, "tmdate": 1676330496118, "tddate": null, "forum": "JYtwGwIL7ye", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models", "authorids": ["~Alexander_Pan1", "~Kush_Bhatia3", "~Jacob_Steinhardt1"], "authors": ["Alexander Pan", "Kush Bhatia", "Jacob Steinhardt"], "keywords": ["reward misspecification", "reinforcement learning", "reward hacking", "alignment", "ml safety"], "abstract": "Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of \\emph{phase transitions}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.", "one-sentence_summary": "We map out trends in reward misspecification and how to mitigate their impact.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pan|the_effects_of_reward_misspecification_mapping_and_mitigating_misaligned_models", "pdf": "/pdf/772b3ddc867fd13a9c98fb15c99f94d5b68ae558.pdf", "supplementary_material": "/attachment/495a17da018002a191c5ceb4ad2cc2d563d40c64.zip", "_bibtex": "@inproceedings{\npan2022the,\ntitle={The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models},\nauthor={Alexander Pan and Kush Bhatia and Jacob Steinhardt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JYtwGwIL7ye}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "J_2xNmVcY4", "original": "Oqo33S0moHF", "number": 3638, "cdate": 1632875700049, "mdate": null, "ddate": null, "tcdate": 1632875700049, "tmdate": 1676330496321, "tddate": null, "forum": "J_2xNmVcY4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Optimizing Neural Networks with Gradient Lexicase Selection", "authorids": ["~Li_Ding3", "~Lee_Spector1"], "authors": ["Li Ding", "Lee Spector"], "keywords": ["deep learning", "lexicase selection", "optimization", "evolutionary algorithms"], "abstract": "One potential drawback of using aggregated performance measurement in machine learning is that models may learn to accept higher errors on some training cases as compromises for lower errors on others, with the lower errors actually being instances of overfitting. This can lead both to stagnation at local optima and to poor generalization. Lexicase selection is an uncompromising method developed in evolutionary computation, which selects models on the basis of sequences of individual training case errors instead of using aggregated metrics such as loss and accuracy. In this paper, we investigate how the general idea of lexicase selection can fit into the context of deep learning to improve generalization. We propose Gradient Lexicase Selection, an optimization framework that combines gradient descent and lexicase selection in an evolutionary fashion. Experimental results show that the proposed method improves the generalization performance of various popular deep neural network architectures on three image classification benchmarks. Qualitative analysis also indicates that our method helps the networks learn more diverse representations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ding|optimizing_neural_networks_with_gradient_lexicase_selection", "pdf": "/pdf/769eeb9706c07b44bcffed8ceeb0c8f027af3caf.pdf", "one-sentence_summary": "We propose Gradient Lexicase Selection, an evolutionary optimization method that improves the generalization of deep neural networks.", "supplementary_material": "/attachment/0b767483a08145204f50ab702ea0425e41fe3d0f.zip", "data": "", "_bibtex": "@inproceedings{\nding2022optimizing,\ntitle={Optimizing Neural Networks with Gradient Lexicase Selection},\nauthor={Li Ding and Lee Spector},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=J_2xNmVcY4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "68n2s9ZJWF8", "original": "U8U9MvysoIz", "number": 3609, "cdate": 1632875698510, "mdate": null, "ddate": null, "tcdate": 1632875698510, "tmdate": 1697934598453, "tddate": null, "forum": "68n2s9ZJWF8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Offline Reinforcement Learning with Implicit Q-Learning", "authorids": ["~Ilya_Kostrikov1", "~Ashvin_Nair1", "~Sergey_Levine1"], "authors": ["Ilya Kostrikov", "Ashvin Nair", "Sergey Levine"], "keywords": ["Deep Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Continuous Control"], "abstract": "Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This tradeoff is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose a new offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function, without any explicit policy. Then, we extract the policy via advantage-weighted behavioral cloning, which also avoids querying out-of-sample actions. We dub our method Implicit Q-learning (IQL).  IQL is easy to implement, computationally efficient, and only requires fitting an additional critic with an asymmetric L2 loss. IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kostrikov|offline_reinforcement_learning_with_implicit_qlearning", "pdf": "/pdf/40ae9547c1a21998c591513e30da199c204553ac.pdf", "one-sentence_summary": "Offline RL method with only dataset actions.", "supplementary_material": "/attachment/552bdbad7a77b911d3142c29adfb9c9c08a5f121.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 7 code implementations](https://www.catalyzex.com/paper/arxiv:2110.06169/code)", "_bibtex": "@inproceedings{\nkostrikov2022offline,\ntitle={Offline Reinforcement Learning with Implicit Q-Learning},\nauthor={Ilya Kostrikov and Ashvin Nair and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=68n2s9ZJWF8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "To-R742x7se", "original": "Kjp80Cp_to", "number": 3608, "cdate": 1632875698442, "mdate": null, "ddate": null, "tcdate": 1632875698442, "tmdate": 1676330497478, "tddate": null, "forum": "To-R742x7se", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Distributionally Robust Models at Scale via Composite Optimization", "authorids": ["~Farzin_Haddadpour1", "~Mohammad_Mahdi_Kamani2", "~Mehrdad_Mahdavi2", "~amin_karbasi1"], "authors": ["Farzin Haddadpour", "Mohammad Mahdi Kamani", "Mehrdad Mahdavi", "amin karbasi"], "keywords": ["Composite Optimization", "Distributionally Robust Optimization"], "abstract": "To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective. However, the existing approaches to learning a distributionally robust model either require solving complex optimization problems such as semidefinite programming or a first-order method whose convergence scales linearly with the number of data samples-- which hinders their scalability to large datasets.  In this paper, we show how different variants of DRO are simply instances of a finite-sum composite optimization for which we provide scalable methods.  We also provide empirical results that demonstrate the effectiveness of our proposed algorithm with respect to the prior art in order to learn robust models from very large datasets. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "haddadpour|learning_distributionally_robust_models_at_scale_via_composite_optimization", "pdf": "/pdf/df930de07a8cc14a26729682ca5b2e909c56cfbd.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nhaddadpour2022learning,\ntitle={Learning Distributionally Robust Models at Scale via Composite Optimization},\nauthor={Farzin Haddadpour and Mohammad Mahdi Kamani and Mehrdad Mahdavi and amin karbasi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=To-R742x7se}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "noaG7SrPVK0", "original": "XngZMfDnuFIG", "number": 3605, "cdate": 1632875698234, "mdate": null, "ddate": null, "tcdate": 1632875698234, "tmdate": 1697934599059, "tddate": null, "forum": "noaG7SrPVK0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Counterfactual Plans under Distributional Ambiguity", "authorids": ["~Ngoc_Bui1", "~Duy_Nguyen2", "~Viet_Anh_Nguyen2"], "authors": ["Ngoc Bui", "Duy Nguyen", "Viet Anh Nguyen"], "keywords": ["Counterfactual explanations", "Robust optimization"], "abstract": "Counterfactual explanations are attracting significant attention due to the flourishing applications of machine learning models in consequential domains. A counterfactual plan consists of multiple possibilities to modify a given instance so that the model's prediction will be altered. As the predictive model can be updated subject to the future arrival of new data, a counterfactual plan may become ineffective or infeasible, with respect to the future values of the model parameters. In this work, we study the counterfactual plans under model uncertainty, in which the distribution of the model parameters is partially prescribed using only the first- and second-moment information. First, we propose an uncertainty quantification tool to compute the lower and upper bounds of the probability of feasibility for any given counterfactual plan. We then provide corrective methods to adjust the counterfactual plan to improve the feasibility measure. The numerical experiments validate our bounds and demonstrate that our correction increases the robustness of the counterfactual plans in different real-world datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bui|counterfactual_plans_under_distributional_ambiguity", "pdf": "/pdf/c3f5f4075890ebfe659f86fbf42ec54a78759f3f.pdf", "one-sentence_summary": "This study the counterfactual plans under model uncertainty, in which the distribution of the model parameters is partially prescribed using only the first- and second-moment information.", "supplementary_material": "/attachment/b6f840ca34d2f311344041ff18b0e210ded1151e.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.12487/code)", "_bibtex": "@inproceedings{\nbui2022counterfactual,\ntitle={Counterfactual Plans under Distributional Ambiguity},\nauthor={Ngoc Bui and Duy Nguyen and Viet Anh Nguyen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=noaG7SrPVK0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "srtIXtySfT4", "original": "Z2wuXswv8Qb", "number": 3599, "cdate": 1632875697897, "mdate": null, "ddate": null, "tcdate": 1632875697897, "tmdate": 1676330498140, "tddate": null, "forum": "srtIXtySfT4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Parameter Allocation Search", "authorids": ["~Bryan_A._Plummer1", "~Nikoli_Dryden1", "~Julius_Frost1", "~Torsten_Hoefler1", "~Kate_Saenko1"], "authors": ["Bryan A. Plummer", "Nikoli Dryden", "Julius Frost", "Torsten Hoefler", "Kate Saenko"], "keywords": ["efficient training methods", "cross-layer parameter sharing"], "abstract": "Training neural networks requires increasing amounts of memory. Parameter sharing can reduce memory and communication costs, but existing methods assume networks have many identical layers and utilize hand-crafted sharing strategies that fail to generalize. We introduce Neural Parameter Allocation Search (NPAS), a novel task where the goal is to train a neural network given an arbitrary, fixed parameter budget. NPAS covers both low-budget regimes, which produce compact networks, as well as a novel high-budget regime, where additional capacity can be added to boost performance without increasing inference FLOPs.  To address NPAS, we introduce Shapeshifter Networks (SSNs), which automatically learn where and how to share parameters in a network to support any parameter budget without requiring any changes to the architecture or loss function. NPAS and SSNs provide a complete framework for addressing generalized parameter sharing, and can also be combined with prior work for additional performance gains. We demonstrate the effectiveness of our approach using nine network architectures across four diverse tasks, including ImageNet classification and transformers.", "one-sentence_summary": "An efficient approach for searching for the optimal allocation of parameters to layers across any neural network", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "plummer|neural_parameter_allocation_search", "pdf": "/pdf/41098cac197bacfbb30bd46f809e88c6d1cf5d76.pdf", "supplementary_material": "", "data": "", "code": "", "_bibtex": "@inproceedings{\nplummer2022neural,\ntitle={Neural Parameter Allocation Search},\nauthor={Bryan A. Plummer and Nikoli Dryden and Julius Frost and Torsten Hoefler and Kate Saenko},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=srtIXtySfT4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "d2TT6gK9qZn", "original": "94l1OdKbFJ7V", "number": 3595, "cdate": 1632875697638, "mdate": null, "ddate": null, "tcdate": 1632875697638, "tmdate": 1676330498550, "tddate": null, "forum": "d2TT6gK9qZn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Non-Linear Operator Approximations for Initial Value Problems", "authorids": ["~Gaurav_Gupta2", "~Xiongye_Xiao1", "~Radu_Balan1", "~Paul_Bogdan1"], "authors": ["Gaurav Gupta", "Xiongye Xiao", "Radu Balan", "Paul Bogdan"], "keywords": ["exponential operators", "initial value problem", "pade approximation", "multiwavelets", "partial differential equations"], "abstract": "Time-evolution of partial differential equations is the key to model several dynamical processes, events forecasting but the operators associated with such problems are non-linear. We propose a Pad\u00e9 approximation based exponential neural operator scheme for efficiently learning the map between a given initial condition and activities at a later time. The multiwavelets bases are used for space discretization. By explicitly embedding the exponential operators in the model, we reduce the training parameters and make it more data-efficient which is essential in dealing with scarce real-world datasets. The Pad\u00e9 exponential operator uses a $\\textit{recurrent structure with shared parameters}$ to model the non-linearity compared to recent neural operators that rely on using multiple linear operator layers in succession. We show theoretically that the gradients associated with the recurrent Pad\u00e9 network are bounded across the recurrent horizon. We perform experiments on non-linear systems such as Korteweg-de Vries (KdV) and Kuramoto\u2013Sivashinsky (KS) equations to show that the proposed approach achieves the best performance and at the same time is data-efficient. We also show that urgent real-world problems like Epidemic forecasting (for example, COVID-19) can be formulated as a 2D time-varying operator problem. The proposed Pad\u00e9 exponential operators yield better prediction results ($\\textbf{53\\%} (\\textbf{52\\%})$ better MAE than best neural operator (non-neural operator deep learning model)) compared to state-of-the-art forecasting models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gupta|nonlinear_operator_approximations_for_initial_value_problems", "pdf": "/pdf/64034193e19bb69648d9e3ed9301e225d08d08fe.pdf", "one-sentence_summary": "A Pad\u00e9 approximation based exponential operator module is proposed for working with the Initial Value Problems. The compactness of model yields data-efficiency and better performance which is demonstrated on scarce real-world dataset.", "supplementary_material": "/attachment/4efb22d2272d9a77a6ba7b24001ee48778f337e7.zip", "_bibtex": "@inproceedings{\ngupta2022nonlinear,\ntitle={Non-Linear Operator Approximations for Initial Value Problems},\nauthor={Gaurav Gupta and Xiongye Xiao and Radu Balan and Paul Bogdan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=d2TT6gK9qZn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "7IWGzQ6gZ1D", "original": "gA_uuG9ORO11", "number": 3584, "cdate": 1632875696946, "mdate": null, "ddate": null, "tcdate": 1632875696946, "tmdate": 1676330498847, "tddate": null, "forum": "7IWGzQ6gZ1D", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates", "authorids": ["~Safa_Alver1", "~Doina_Precup1"], "authors": ["Safa Alver", "Doina Precup"], "keywords": ["reinforcement learning", "lifelong learning", "transfer learning", "successor features"], "abstract": "We study the problem of learning a good set of policies, so that when combined together, they can solve a wide variety of unseen reinforcement learning tasks with no or very little new data. Specifically, we consider the framework of generalized policy evaluation and improvement, in which the rewards for all tasks of interest are assumed to be expressible as a linear combination of a fixed set of features. We show theoretically that, under certain assumptions, having access to a specific set of diverse policies, which we call a set of independent policies, can allow for instantaneously achieving high-level performance on all possible downstream tasks which are typically more complex than the ones on which the agent was trained. Based on this theoretical analysis, we propose a simple algorithm that iteratively constructs this set of policies. In addition to empirically validating our theoretical results, we compare our approach with recently proposed diverse policy set construction methods and show that, while others fail, our approach is able to build a behavior basis that enables instantaneous transfer to all possible downstream tasks. We also show empirically that having access to a set of independent policies can better bootstrap the learning process on downstream tasks where the new reward function cannot be described as a linear combination of the features. Finally, we demonstrate how this policy set can be useful in a lifelong reinforcement learning setting.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "alver|constructing_a_good_behavior_basis_for_transfer_using_generalized_policy_updates", "pdf": "/pdf/99cd11d061fb209a0dffb177bc34b326b5d4d7a6.pdf", "_bibtex": "@inproceedings{\nalver2022constructing,\ntitle={Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates},\nauthor={Safa Alver and Doina Precup},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7IWGzQ6gZ1D}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "7TZeCsNOUB_", "original": "94ay61Ii6QuA", "number": 3582, "cdate": 1632875696815, "mdate": null, "ddate": null, "tcdate": 1632875696815, "tmdate": 1697934601899, "tddate": null, "forum": "7TZeCsNOUB_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Collapse by Conditioning: Training Class-conditional GANs with Limited Data", "authorids": ["~Mohamad_Shahbazi1", "~Martin_Danelljan4", "~Danda_Pani_Paudel1", "~Luc_Van_Gool1"], "authors": ["Mohamad Shahbazi", "Martin Danelljan", "Danda Pani Paudel", "Luc Van Gool"], "keywords": ["Generative Adversarial Network", "GAN", "Conditional GAN", "limited data"], "abstract": "Class-conditioning offers a direct means to control a Generative Adversarial Network (GAN) based on a discrete input variable. While necessary in many applications, the additional information provided by the class labels could even be expected to benefit the training of the GAN itself. On the contrary, we observe that class-conditioning causes mode collapse in limited data settings, where unconditional learning leads to satisfactory generative ability. Motivated by this observation, we propose a training strategy for class-conditional GANs (cGANs) that effectively prevents the observed mode-collapse by leveraging unconditional learning. Our training strategy starts with an unconditional GAN and gradually injects the class conditioning into the generator and the objective function. The proposed method for training cGANs with limited data results not only in stable training but also in generating high-quality images, thanks to the early-stage exploitation of the shared information across classes. We analyze the observed mode collapse problem in comprehensive experiments on four datasets. Our approach demonstrates outstanding results compared with state-of-the-art methods and established baselines. The code is available at https://github.com/mshahbazi72/transitional-cGAN", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shahbazi|collapse_by_conditioning_training_classconditional_gans_with_limited_data", "pdf": "/pdf/3fccdf773fc5217d7e02e135a75df9f5ac0e7de9.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.06578/code)", "_bibtex": "@inproceedings{\nshahbazi2022collapse,\ntitle={Collapse by Conditioning: Training Class-conditional {GAN}s with Limited Data},\nauthor={Mohamad Shahbazi and Martin Danelljan and Danda Pani Paudel and Luc Van Gool},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7TZeCsNOUB_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "dSw0QtRMJkO", "original": "TwIUWq8mRiMy", "number": 3567, "cdate": 1632875695853, "mdate": null, "ddate": null, "tcdate": 1632875695853, "tmdate": 1676330499923, "tddate": null, "forum": "dSw0QtRMJkO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize", "authorids": ["~Ali_Kavis1", "~Kfir_Yehuda_Levy1", "~Volkan_Cevher1"], "authors": ["Ali Kavis", "Kfir Yehuda Levy", "Volkan Cevher"], "keywords": ["adaptive methods", "nonconvex optimization", "stochastic optimization", "high probability bounds"], "abstract": "In this paper, we propose a new, simplified high probability analysis of AdaGrad for smooth, non-convex problems. \nMore specifically, we focus on a particular accelerated gradient (AGD) template (Lan, 2020), through which we recover the original AdaGrad and its variant with averaging, and prove a convergence rate of $\\mathcal O (1/ \\sqrt{T})$ with high probability without the knowledge of smoothness and variance. \nWe use a particular version of Freedman's concentration bound for martingale difference sequences (Kakade & Tewari, 2008) which enables us to achieve the best-known dependence of $\\log (1 / \\delta )$ on the probability margin $\\delta$. \nWe present our analysis in a modular way and obtain a complementary $\\mathcal O (1 / T)$ convergence rate in the deterministic setting. \nTo the best of our knowledge, this is the first high probability result for AdaGrad with a truly adaptive scheme, i.e., completely oblivious to the knowledge of smoothness and uniform variance bound, which simultaneously has best-known dependence of $\\log( 1/ \\delta)$. \nWe further prove noise adaptation property of AdaGrad under additional noise assumptions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kavis|high_probability_bounds_for_a_class_of_nonconvex_algorithms_with_adagrad_stepsize", "pdf": "/pdf/11cf98dd0d3e5fe19f0cfc472913b4778244ba26.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nkavis2022high,\ntitle={High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize},\nauthor={Ali Kavis and Kfir Yehuda Levy and Volkan Cevher},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dSw0QtRMJkO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "1NUsBU-7HAL", "original": "1abZW4spSWJ", "number": 3559, "cdate": 1632875695297, "mdate": null, "ddate": null, "tcdate": 1632875695297, "tmdate": 1676330499965, "tddate": null, "forum": "1NUsBU-7HAL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Map Induction: Compositional spatial submap learning for efficient exploration in novel environments", "authorids": ["~Sugandha_Sharma1", "~Aidan_Curtis2", "~Marta_Kryven1", "~Joshua_B._Tenenbaum1", "~Ila_R_Fiete1"], "authors": ["Sugandha Sharma", "Aidan Curtis", "Marta Kryven", "Joshua B. Tenenbaum", "Ila R Fiete"], "keywords": ["Cognitive Science", "Bayesian Framework", "Program Induction", "Spatial Navigation", "Planning", "Map Learning"], "abstract": "Humans are expert explorers and foragers. Understanding the computational cognitive mechanisms that support this capability can advance the study of the human mind and enable more efficient exploration algorithms. We hypothesize that humans explore new environments by inferring the structure of unobserved spaces through re-use of spatial information collected from previously explored spaces. Taking inspiration from the neuroscience of repeating map fragments and ideas about program induction, we present a novel ``Map Induction'' framework, which involves the generation of novel map proposals for unseen environments based on compositions of already-seen spaces in a Hierarchical Bayesian framework. The model thus explicitly reasons about unseen spaces through a distribution of strong spatial priors. We introduce a new behavioral Map Induction Task (MIT) that involves foraging for rewards to compare human performance with state-of-the-art existing models and Map Induction. We show that Map Induction better predicts human behavior than the non-inductive baselines. We also show that Map Induction, when used to augment state-of-the-art approximate planning algorithms, improves their performance.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sharma|map_induction_compositional_spatial_submap_learning_for_efficient_exploration_in_novel_environments", "pdf": "/pdf/e711160279e8d24176f9ee29e77240aab6849830.pdf", "one-sentence_summary": "Modelling Map Induction for efficient exploration in novel environments. ", "_bibtex": "@inproceedings{\nsharma2022map,\ntitle={Map Induction: Compositional spatial submap learning for efficient exploration in novel environments},\nauthor={Sugandha Sharma and Aidan Curtis and Marta Kryven and Joshua B. Tenenbaum and Ila R Fiete},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1NUsBU-7HAL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "gFDFKC4gHL4", "original": "9HsqPVHnlWF", "number": 3558, "cdate": 1632875695229, "mdate": null, "ddate": null, "tcdate": 1632875695229, "tmdate": 1676330500104, "tddate": null, "forum": "gFDFKC4gHL4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How Did the Model Change? Efficiently Assessing Machine Learning API Shifts ", "authorids": ["~Lingjiao_Chen1", "~Matei_Zaharia1", "~James_Zou1"], "authors": ["Lingjiao Chen", "Matei Zaharia", "James Zou"], "keywords": ["ML API performance shifts", "ML as a service", "ML monitoring", "ML performance evaluation"], "abstract": "ML prediction APIs from providers like Amazon and Google have made it simple to use ML in applications. A challenge for users is that such APIs continuously change over time as the providers update models, and changes can happen silently without users knowing. It is thus important to monitor when and how much the MLAPIs\u2019 performance shifts. To provide detailed change assessment, we model MLAPI shifts as confusion matrix differences, and propose a principled algorithmic framework, MASA, to provably assess these shifts efficiently given a sample budget constraint.MASAemploys an upper-confidence bound based approach to adaptively determine on which data point to query the ML API to estimate shifts. Empirically, we observe significant ML API shifts from 2020 to 2021 among 12 out of 36 applications using commercial APIs from Google, Microsoft, Amazon, and other providers. These real-world shifts include both improvements and reductions in accuracy. Extensive experiments show that MASA can estimate such API shifts more accurately than standard approaches given the same budget", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|how_did_the_model_change_efficiently_assessing_machine_learning_api_shifts", "pdf": "/pdf/acadda3ff8e7686ab4e0b73fd64ba0ef89f2134a.pdf", "one-sentence_summary": "We systematically study real world ML APIs' performance shifts due to API updates/retraining and propose a framework to efficiently estimate those shifts.  ", "supplementary_material": "/attachment/3865c217a2dfb2f67e202e03c1c3515b86524215.zip", "data": "", "_bibtex": "@inproceedings{\nchen2022how,\ntitle={How Did the Model Change? Efficiently Assessing Machine Learning {API} Shifts },\nauthor={Lingjiao Chen and Matei Zaharia and James Zou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gFDFKC4gHL4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "31d5RLCUuXC", "original": "MQfl-nOB0rj", "number": 3555, "cdate": 1632875695023, "mdate": null, "ddate": null, "tcdate": 1632875695023, "tmdate": 1676330500130, "tddate": null, "forum": "31d5RLCUuXC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model", "authorids": ["~Jianwen_Xie1", "~Yaxuan_Zhu1", "~Jun_Li13", "~Ping_Li3"], "authors": ["Jianwen Xie", "Yaxuan Zhu", "Jun Li", "Ping Li"], "keywords": ["Langevin dynamics", "energy-based model", "normalizing flow", "cooperative learning", "short-run MCMC"], "abstract": "This paper studies the cooperative learning of two generative flow models, in which the two models are iteratively updated based on the jointly synthesized examples. The first flow model is a normalizing flow that transforms an initial simple density to a target density by applying a sequence of invertible transformations. The second flow model is a Langevin flow that runs finite steps of gradient-based MCMC toward an energy-based model. We start from proposing a generative framework that trains an energy-based model with a normalizing flow as an amortized sampler to initialize the MCMC chains of the energy-based model. In each learning iteration, we generate synthesized examples by using a normalizing flow initialization followed by a short-run Langevin flow revision toward the current energy-based model. Then we treat the synthesized examples as fair samples from the energy-based model and update the model parameters with the maximum likelihood learning gradient, while the normalizing flow directly learns from the synthesized examples by maximizing the tractable likelihood. Under the short-run non-mixing MCMC scenario, the estimation of the energy-based model  is shown to follow the perturbation of maximum likelihood, and the short-run Langevin flow and the normalizing flow form a two-flow generator that we call CoopFlow. We provide an  understating of the CoopFlow algorithm by information geometry and show that it is a valid generator as it converges to a moment matching estimator. We demonstrate that the trained CoopFlow is capable of synthesizing realistic images, reconstructing images, and interpolating between images.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xie|a_tale_of_two_flows_cooperative_learning_of_langevin_flow_and_normalizing_flow_toward_energybased_model", "pdf": "/pdf/64ee4e9d385df72d0c9c8052fdd6ef8cc2b78e7a.pdf", "one-sentence_summary": "Joint learning of a short-run MCMC generator and a normalizing flow in the context of energy-based model for image representation and generation. ", "data": "", "_bibtex": "@inproceedings{\nxie2022a,\ntitle={A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model},\nauthor={Jianwen Xie and Yaxuan Zhu and Jun Li and Ping Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=31d5RLCUuXC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 42}}, {"id": "WVX0NNVBBkV", "original": "eru7hdNdgj", "number": 3553, "cdate": 1632875694894, "mdate": null, "ddate": null, "tcdate": 1632875694894, "tmdate": 1676330500367, "tddate": null, "forum": "WVX0NNVBBkV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?", "authorids": ["~Vikash_Sehwag1", "~Saeed_Mahloujifar1", "~Tinashe_Handina1", "~Sihui_Dai1", "~Chong_Xiang1", "~Mung_Chiang2", "~Prateek_Mittal1"], "authors": ["Vikash Sehwag", "Saeed Mahloujifar", "Tinashe Handina", "Sihui Dai", "Chong Xiang", "Mung Chiang", "Prateek Mittal"], "keywords": ["adversarial robustness", "certified adversarial robustness", "adversarial attacks", "generative models", "proxy distribution"], "abstract": "While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced  generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to $7.5$% and $6.7$% in $\\ell_{\\infty}$ and $\\ell_2$ threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by $7.6$% on the CIFAR-10 dataset. We further demonstrate that different generative models brings a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact and further provide a deeper understanding of why diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sehwag|robust_learning_meets_generative_models_can_proxy_distributions_improve_adversarial_robustness", "pdf": "/pdf/de688f13dda6db1dc9ca524f5de9b302f9b5261d.pdf", "one-sentence_summary": "We leverage proxy distributions to significantly improve the robustness of deep neural network. ", "supplementary_material": "/attachment/ddf8f50e25ae557f137880f544fba3f294cb3f3f.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nsehwag2022robust,\ntitle={Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?},\nauthor={Vikash Sehwag and Saeed Mahloujifar and Tinashe Handina and Sihui Dai and Chong Xiang and Mung Chiang and Prateek Mittal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WVX0NNVBBkV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "ECvgmYVyeUz", "original": "UCUM_ilLgSa_", "number": 3550, "cdate": 1632875694674, "mdate": null, "ddate": null, "tcdate": 1632875694674, "tmdate": 1697934604543, "tddate": null, "forum": "ECvgmYVyeUz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap", "authorids": ["~Yifei_Wang1", "zhangq327@mail2.sysu.edu.cn", "~Yisen_Wang1", "yjs@math.pku.edu.cn", "~Zhouchen_Lin1"], "authors": ["Yifei Wang", "Qi Zhang", "Yisen Wang", "Jiansheng Yang", "Zhouchen Lin"], "keywords": ["Contrastive Learning", "Representation Learning", "Self-supervised Learning"], "abstract": "Recently, contrastive learning has risen to be a promising approach for large-scale self-supervised learning. However, theoretical understanding of how it works is still unclear. In this paper, we propose a new guarantee on the downstream performance without resorting to the conditional independence assumption that is widely adopted in previous work but hardly holds in practice. Our new theory hinges on the insight that the support of different intra-class samples will become more overlapped under aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. Based on this augmentation overlap perspective, theoretically, we obtain asymptotically closed bounds for downstream performance under weaker assumptions, and empirically, we propose an unsupervised model selection metric ARC that aligns well with downstream accuracy. Our theory suggests an alternative understanding of contrastive learning: the role of aligning positive samples is more like a surrogate task than an ultimate goal, and the overlapped augmented views (i.e., the chaos) create a ladder for contrastive learning to gradually learn class-separated representations. The code for computing ARC is available at https://github.com/zhangq327/ARC.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|chaos_is_a_ladder_a_new_theoretical_understanding_of_contrastive_learning_via_augmentation_overlap", "pdf": "/pdf/69ce3c69488d43f1e8d28d5759b1cd8a9c731519.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.13457/code)", "_bibtex": "@inproceedings{\nwang2022chaos,\ntitle={Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap},\nauthor={Yifei Wang and Qi Zhang and Yisen Wang and Jiansheng Yang and Zhouchen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ECvgmYVyeUz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "xNO7OEIcJc6", "original": "T7diWXR1X6s", "number": 3549, "cdate": 1632875694606, "mdate": null, "ddate": null, "tcdate": 1632875694606, "tmdate": 1676330500657, "tddate": null, "forum": "xNO7OEIcJc6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Language-biased image classification: evaluation based on semantic representations", "authorids": ["~Yoann_Lemesle1", "~Masataka_Sawayama1", "~Guillermo_Valle-Perez1", "~Maxime_Adolphe1", "~H\u00e9l\u00e8ne_Sauz\u00e9on1", "~Pierre-Yves_Oudeyer1"], "authors": ["Yoann Lemesle", "Masataka Sawayama", "Guillermo Valle-Perez", "Maxime Adolphe", "H\u00e9l\u00e8ne Sauz\u00e9on", "Pierre-Yves Oudeyer"], "keywords": ["interpretation of learned representations", "language and visual processing", "language-biased image classification", "cognitive science"], "abstract": "Humans show language-biased image recognition for a word-embedded image, known as picture-word interference. Such interference depends on hierarchical semantic categories and reflects that human language processing highly interacts with visual processing. Similar to humans, recent artificial models jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased image classification. Exploring whether the bias leads to interference similar to those observed in humans can contribute to understanding how much the model acquires hierarchical semantic representations from joint learning of language and vision. The present study introduces methodological tools from the cognitive science literature to assess the biases of artificial models. Specifically, we introduce a benchmark task to test whether words superimposed on images can distort the image classification across different category levels and, if it can, whether the perturbation is due to the shared semantic representation between language and vision. Our dataset is a set of word-embedded images and consists of a mixture of natural image datasets and hierarchical word labels with superordinate/basic category levels. Using this benchmark test, we evaluate the CLIP model. We show that presenting words distorts the image classification by the model across different category levels, but the effect does not depend on the semantic relationship between images and embedded words. This suggests that the semantic word representation in the CLIP visual processing is not shared with the image representation, although the word representation strongly dominates for word-embedded images.", "one-sentence_summary": "We developed a benchmark test based on hierarchical semantic compositionality to evaluate the language-biased image classification of artificial models  and evaluated the CLIP model using it. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lemesle|languagebiased_image_classification_evaluation_based_on_semantic_representations", "pdf": "/pdf/b094eb383e555c4e4fa0c02dc8416aabb50bcc00.pdf", "supplementary_material": "/attachment/f77fde7c56861bfd764f13cb7d229809229f0327.zip", "data": "", "_bibtex": "@inproceedings{\nlemesle2022languagebiased,\ntitle={Language-biased image classification: evaluation based on semantic representations},\nauthor={Yoann Lemesle and Masataka Sawayama and Guillermo Valle-Perez and Maxime Adolphe and H{\\'e}l{\\`e}ne Sauz{\\'e}on and Pierre-Yves Oudeyer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xNO7OEIcJc6}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "fwzUgo0FM9v", "original": "yZBZLCRXvwP4", "number": 3547, "cdate": 1632875694472, "mdate": null, "ddate": null, "tcdate": 1632875694472, "tmdate": 1676330500796, "tddate": null, "forum": "fwzUgo0FM9v", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Robbing the Fed:  Directly Obtaining Private Data in Federated Learning with Modified Models", "authorids": ["~Liam_H_Fowl1", "~Jonas_Geiping1", "~Wojciech_Czaja1", "~Micah_Goldblum1", "~Tom_Goldstein1"], "authors": ["Liam H Fowl", "Jonas Geiping", "Wojciech Czaja", "Micah Goldblum", "Tom Goldstein"], "keywords": ["Privacy", "Federated Learning", "Gradient Inversion"], "abstract": "Federated learning has quickly gained popularity with its promises of increased user privacy and efficiency.  Previous works have shown that federated gradient updates contain information that can be used to approximately recover user data in some situations.  These previous attacks on user privacy have been limited in scope and do not scale to gradient updates aggregated over even a handful of  data  points,  leaving  some  to  conclude  that  data  privacy  is  still  intact  for realistic training regimes.  In this work, we introduce a new threat model based on minimal but malicious modifications of the shared model architecture which enable the server to directly obtain a verbatim copy of user data from gradient updates without solving difficult inverse problems.  Even user data aggregated over large batches \u2013 where previous methods fail to extract meaningful content \u2013 can be reconstructed by these minimally modified models.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fowl|robbing_the_fed_directly_obtaining_private_data_in_federated_learning_with_modified_models", "pdf": "/pdf/f001474d4a74672028ea06af6f68b4e7c71bca47.pdf", "supplementary_material": "/attachment/b1757dfe33925977b460c665a138cc681a0ebe2e.zip", "_bibtex": "@inproceedings{\nfowl2022robbing,\ntitle={Robbing the Fed:  Directly Obtaining Private Data in Federated Learning with Modified Models},\nauthor={Liam H Fowl and Jonas Geiping and Wojciech Czaja and Micah Goldblum and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fwzUgo0FM9v}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "3pugbNqOh5m", "original": "KslY5sGIdyB", "number": 3542, "cdate": 1632875694141, "mdate": null, "ddate": null, "tcdate": 1632875694141, "tmdate": 1676330501000, "tddate": null, "forum": "3pugbNqOh5m", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Practical Conditional Neural Process Via Tractable Dependent Predictions", "authorids": ["~Stratis_Markou1", "~James_Requeima1", "~Wessel_Bruinsma1", "av555@cam.ac.uk", "~Richard_E_Turner1"], "authors": ["Stratis Markou", "James Requeima", "Wessel Bruinsma", "Anna Vaughan", "Richard E Turner"], "keywords": ["conditional neural processes", "neural processes", "meta-learning", "convolutional conditional neural processes", "Gaussian neural processes"], "abstract": "Conditional Neural Processes (CNPs; Garnelo et al., 2018a) are meta-learning models which leverage the flexibility of deep learning to produce well-calibrated predictions and naturally handle off-the-grid and missing data. CNPs scale to large datasets and train with ease. Due to these features, CNPs appear well-suited to tasks from environmental sciences or healthcare. Unfortunately, CNPs do not produce correlated predictions, making them fundamentally inappropriate for many estimation and decision making tasks. Predicting heat waves or floods, for example, requires modelling dependencies in temperature or precipitation over time and space. Existing approaches which model output dependencies, such as Neural Processes (NPs; Garnelo et al., 2018b) or the FullConvGNP (Bruinsma et al., 2021), are either complicated to train or prohibitively expensive. What is needed is an approach which provides dependent predictions, but is simple to train and computationally tractable. In this work, we present a new class of Neural Process models that make correlated predictions and support exact maximum likelihood training that is simple and scalable. We extend the proposed models by using invertible output transformations, to capture non-Gaussian output distributions. Our models can be used in downstream estimation tasks which require dependent function samples. By accounting for output dependencies, our models show improved predictive performance on a range of experiments with synthetic and real data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "markou|practical_conditional_neural_process_via_tractable_dependent_predictions", "pdf": "/pdf/55352f18a5f52efe2957b212d6bdcf7771277fd7.pdf", "_bibtex": "@inproceedings{\nmarkou2022practical,\ntitle={Practical Conditional Neural Process Via Tractable Dependent Predictions},\nauthor={Stratis Markou and James Requeima and Wessel Bruinsma and Anna Vaughan and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3pugbNqOh5m}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "OWZVD-l-ZrC", "original": "SnLUoI8f9dp", "number": 3536, "cdate": 1632875693812, "mdate": null, "ddate": null, "tcdate": 1632875693812, "tmdate": 1676330501231, "tddate": null, "forum": "OWZVD-l-ZrC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning", "authorids": ["~Xinran_Liang1", "~Katherine_Shu1", "~Kimin_Lee1", "~Pieter_Abbeel2"], "authors": ["Xinran Liang", "Katherine Shu", "Kimin Lee", "Pieter Abbeel"], "keywords": [], "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains as a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efficiency in RL. We present an exploration method specifically for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Specifically, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reflects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that reward uncertainty exploration improves both feedback- and sample-efficiency of preference-based RL algorithms on complex robot manipulation tasks from Meta-World benchmarks, compared with other existing exploration methods that measure the novelty of state visitation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liang|reward_uncertainty_for_exploration_in_preferencebased_reinforcement_learning", "pdf": "/pdf/96609cdcc6871a3da572808bab08da66cc38cb2c.pdf", "_bibtex": "@inproceedings{\nliang2022reward,\ntitle={Reward Uncertainty for Exploration in Preference-based Reinforcement Learning},\nauthor={Xinran Liang and Katherine Shu and Kimin Lee and Pieter Abbeel},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OWZVD-l-ZrC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "oj2yn1Q4Ett", "original": "f2W2WPzLB4mz", "number": 3527, "cdate": 1632875693200, "mdate": null, "ddate": null, "tcdate": 1632875693200, "tmdate": 1676330501780, "tddate": null, "forum": "oj2yn1Q4Ett", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Decentralized Learning for Overparameterized Problems: A Multi-Agent Kernel Approximation Approach", "authorids": ["~Prashant_Khanduri1", "~Haibo_Yang1", "~Mingyi_Hong1", "~Jia_Liu1", "~Hoi_To_Wai1", "~Sijia_Liu1"], "authors": ["Prashant Khanduri", "Haibo Yang", "Mingyi Hong", "Jia Liu", "Hoi To Wai", "Sijia Liu"], "keywords": ["distributed optimization", "over-parameterized optimization", "kernel learning"], "abstract": "This work develops a novel framework for communication-efficient distributed learning where the models to be learned are overparameterized. We focus on a class of kernel learning problems (which includes the popular neural tangent kernel (NTK) learning as a special case) and propose a novel {\\it multi-agent kernel approximation} technique that allows the agents to distributedly estimate the full kernel function, and subsequently perform decentralized optimization, without directly exchanging any local data or parameters. The proposed framework is a significant departure from the classical consensus-based approaches, because the agents do not exchange problem parameters, and no consensus is required. We analyze the optimization and the generalization performance of the proposed framework for the $\\ell_2$ loss. We show that with $M$ agents and $N$ total samples when certain generalized inner-product kernels (resp. the random features kernel) are used, each agent needs to communicate $\\mathcal{O}\\big({N^2}/{M}\\big)$ bits (resp. $\\mathcal{O}\\big(N \\sqrt{N}/M \\big)$ real values) to achieve minimax optimal generalization performance. We validate the theoretical results on 90 UCI benchmarking datasets (with average data size $N \\approx 1000$) and show that each agent needs to share a total of $200N/M$ bits (resp. $3N/M$ real values) to closely match the performance of the centralized algorithms, and these numbers are independent of parameter and feature dimensions. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "khanduri|decentralized_learning_for_overparameterized_problems_a_multiagent_kernel_approximation_approach", "pdf": "/pdf/d2d0106daf104b8b9c7f3868ca69a3a81cd09902.pdf", "one-sentence_summary": "Decentralized optimization for overparameterized kernel learning", "supplementary_material": "/attachment/875eb6b57775c3f8d2a374fa8f984ff55905bab6.zip", "_bibtex": "@inproceedings{\nkhanduri2022decentralized,\ntitle={Decentralized Learning for Overparameterized Problems: A Multi-Agent Kernel Approximation Approach},\nauthor={Prashant Khanduri and Haibo Yang and Mingyi Hong and Jia Liu and Hoi To Wai and Sijia Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oj2yn1Q4Ett}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "YiBa9HKTyXE", "original": "tUKVZzShApC", "number": 3523, "cdate": 1632875692926, "mdate": null, "ddate": null, "tcdate": 1632875692926, "tmdate": 1697934606892, "tddate": null, "forum": "YiBa9HKTyXE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Permutation-Based SGD: Is Random Optimal?", "authorids": ["~Shashank_Rajput1", "~Kangwook_Lee1", "~Dimitris_Papailiopoulos1"], "authors": ["Shashank Rajput", "Kangwook Lee", "Dimitris Papailiopoulos"], "keywords": ["Convex Optimization", "Stochastic Optimization", "Large Scale Learning"], "abstract": "A recent line of ground-breaking results for permutation-based SGD  has corroborated a widely observed phenomenon: random permutations offer faster convergence than with-replacement sampling. However, is random optimal? We show that this depends heavily on what functions we are optimizing, and the convergence gap between optimal and random permutations can vary from exponential to nonexistent. We first show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist optimal permutations that offer exponentially faster convergence compared to random. However, for general strongly convex functions, random permutations are optimal. Finally, we show that for quadratic, strongly-convex functions, there are easy-to-construct permutations that lead to accelerated convergence compared to random. Our results suggest that a general convergence characterization of optimal permutations cannot capture the nuances of individual function classes, and can  mistakenly indicate that one cannot do much better than random.", "one-sentence_summary": "We show that the question of whether random permutations are optimal for permutation-based SGD is nuanced, and depends on the family of functions one is trying to optimize.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rajput|permutationbased_sgd_is_random_optimal", "pdf": "/pdf/38cb93486fb7896b831a82484bdf43c375bd5b9e.pdf", "supplementary_material": "/attachment/ed23add48779386139f866e6ee6e8a15a7c4a141.zip", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2102.09718/code)", "_bibtex": "@inproceedings{\nrajput2022permutationbased,\ntitle={Permutation-Based {SGD}: Is Random Optimal?},\nauthor={Shashank Rajput and Kangwook Lee and Dimitris Papailiopoulos},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YiBa9HKTyXE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "4p6_5HBWPCw", "original": "odJzwajw6a", "number": 3517, "cdate": 1632875692527, "mdate": null, "ddate": null, "tcdate": 1632875692527, "tmdate": 1676330502309, "tddate": null, "forum": "4p6_5HBWPCw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation", "authorids": ["~Shichang_Zhang2", "~Yozen_Liu1", "~Yizhou_Sun1", "~Neil_Shah2"], "authors": ["Shichang Zhang", "Yozen Liu", "Yizhou Sun", "Neil Shah"], "keywords": ["Graph Neural Networks", "Distillation", "Node Classification", "Model Inference Acceleration"], "abstract": "Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|graphless_neural_networks_teaching_old_mlps_new_tricks_via_distillation", "pdf": "/pdf/96f207794ab92fa350a4284d8d40ad874c3d9d06.pdf", "one-sentence_summary": "Distill knowledge from GNNs to MLPs to accelerate model inference and facilitate deployment for large-scale applications", "supplementary_material": "/attachment/8f427cff78ced924a327c553f9c963658ed20a3f.zip", "_bibtex": "@inproceedings{\nzhang2022graphless,\ntitle={Graph-less Neural Networks: Teaching Old {MLP}s New Tricks Via Distillation},\nauthor={Shichang Zhang and Yozen Liu and Yizhou Sun and Neil Shah},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=4p6_5HBWPCw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "B8DVo9B1YE0", "original": "AL-_P4xbrtCI", "number": 3482, "cdate": 1632875690471, "mdate": null, "ddate": null, "tcdate": 1632875690471, "tmdate": 1676330502890, "tddate": null, "forum": "B8DVo9B1YE0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Relating transformers to models and neural representations of the hippocampal formation", "authorids": ["~James_C._R._Whittington1", "joseph.warren@ucl.ac.uk", "behrens@fmrib.ox.ac.uk"], "authors": ["James C. R. Whittington", "Joseph Warren", "Tim E.J. Behrens"], "keywords": ["Neuroscience", "representation learning", "hippocampus", "cortex", "transformers"], "abstract": "Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "whittington|relating_transformers_to_models_and_neural_representations_of_the_hippocampal_formation", "pdf": "/pdf/684a0f0eef15279b5b52184a954e3ca24a2217ac.pdf", "one-sentence_summary": "Transformers learn brain representatations and they are algorithmically related to models of the hippocampal formation.", "_bibtex": "@inproceedings{\nwhittington2022relating,\ntitle={Relating transformers to models and neural representations of the hippocampal formation},\nauthor={James C. R. Whittington and Joseph Warren and Tim E.J. Behrens},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=B8DVo9B1YE0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 32}}, {"id": "ChMLTGRjFcU", "original": "9adNUJ35XMrB", "number": 3475, "cdate": 1632875690008, "mdate": null, "ddate": null, "tcdate": 1632875690008, "tmdate": 1676330503122, "tddate": null, "forum": "ChMLTGRjFcU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How many degrees of freedom do we need to train deep networks: a loss landscape perspective", "authorids": ["~Brett_W_Larsen1", "~Stanislav_Fort1", "~Nic_Becker1", "~Surya_Ganguli1"], "authors": ["Brett W Larsen", "Stanislav Fort", "Nic Becker", "Surya Ganguli"], "keywords": ["loss landscape", "high-dimensional geometry", "random hyperplanes", "optimization"], "abstract": "A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We analyze this phenomenon for random subspaces by first examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality.  We find a sharp phase transition in the success probability from $0$ to $1$ as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of properties of the high-dimensional geometry of the loss landscape.  In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sublevel set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large.  In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total parameters, implying by our theory that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sublevel sets is very large. Moreover, we compare this threshold training dimension to more sophisticated ways of reducing training degrees of freedom, including lottery tickets as well as a new, analogous method: lottery subspaces. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "larsen|how_many_degrees_of_freedom_do_we_need_to_train_deep_networks_a_loss_landscape_perspective", "pdf": "/pdf/0801cd7dac7a5c8a0c9d6202ed146b1662b1ccdc.pdf", "supplementary_material": "/attachment/54854bf5a8a6f71e2bc06b9d150b440470715d92.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nlarsen2022how,\ntitle={How many degrees of freedom do we need to train deep networks: a loss landscape perspective},\nauthor={Brett W Larsen and Stanislav Fort and Nic Becker and Surya Ganguli},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ChMLTGRjFcU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "uqBOne3LUKy", "original": "Do4sDO2dvoiB", "number": 3473, "cdate": 1632875689874, "mdate": null, "ddate": null, "tcdate": 1632875689874, "tmdate": 1697934609605, "tddate": null, "forum": "uqBOne3LUKy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Is Importance Weighting Incompatible with Interpolating Classifiers?", "authorids": ["~Ke_Alexander_Wang1", "~Niladri_Shekhar_Chatterji1", "~Saminul_Haque1", "~Tatsunori_Hashimoto1"], "authors": ["Ke Alexander Wang", "Niladri Shekhar Chatterji", "Saminul Haque", "Tatsunori Hashimoto"], "keywords": ["overparameterization", "distribution shifts", "importance weighting", "implicit bias", "generalization analysis", "interpolation"], "abstract": "Importance weighting is a classic technique to handle distribution shifts. However, prior work has presented strong empirical and theoretical evidence demonstrating that importance weights can have little to no effect on overparameterized neural networks. \\emph{Is importance weighting truly incompatible with the training of overparameterized neural networks?} Our paper answers this in the negative. We show that importance weighting fails not because of the overparameterization, but instead, as a result of using exponentially-tailed losses like the logistic or cross-entropy loss. As a remedy, we show that polynomially-tailed losses restore the effects of importance reweighting in correcting distribution shift in overparameterized models. We characterize the behavior of gradient descent on importance weighted polynomially-tailed losses with overparameterized linear models, and theoretically demonstrate the advantage of using polynomially-tailed losses in a label shift setting. Surprisingly, our theory shows that using weights that are obtained by exponentiating the classical unbiased importance weights can improve performance. Finally, we demonstrate the practical value of our analysis with neural network experiments on a subpopulation shift and a label shift dataset. When reweighted, our loss function can outperform reweighted cross-entropy by as much as 9\\% in test accuracy. Our loss function also gives test accuracies comparable to, or even exceeding, well-tuned state-of-the-art methods for correcting distribution shifts.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|is_importance_weighting_incompatible_with_interpolating_classifiers", "pdf": "/pdf/a1dcd84e1b3a313fb6b6f62958fef613ebefa9d5.pdf", "one-sentence_summary": "We theoretically and empirically demonstrate that importance weighting can be effective in handling distribution shifts in overparameterized classifiers.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.12986/code)", "_bibtex": "@inproceedings{\nwang2022is,\ntitle={Is Importance Weighting Incompatible with Interpolating Classifiers?},\nauthor={Ke Alexander Wang and Niladri Shekhar Chatterji and Saminul Haque and Tatsunori Hashimoto},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=uqBOne3LUKy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "ibrUkC-pbis", "original": "j5dMl13XgGz", "number": 3461, "cdate": 1632875689065, "mdate": null, "ddate": null, "tcdate": 1632875689065, "tmdate": 1697934610459, "tddate": null, "forum": "ibrUkC-pbis", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Models for Output-Space Invariance in Combinatorial Problems", "authorids": ["~Yatin_Nandwani1", "~Vidit_Jain2", "~Mausam_.1", "~Parag_Singla1"], "authors": ["Yatin Nandwani", "Vidit Jain", "Mausam .", "Parag Singla"], "keywords": ["neural reasoning", "output space invariance"], "abstract": "Recently many neural models have been proposed to solve combinatorial puzzles by implicitly learning underlying constraints using their solved instances, such as sudoku or graph coloring (GCP). One drawback of the proposed architectures, which are often based on Graph Neural Networks (GNN) (Zhou et al., 2020), is that they cannot generalize across the size of the output space from which variables are assigned a value, for example, set of colors in a GCP, or board-size in sudoku. We call the output space for the variables as \u2018value-set\u2019. While many works have demonstrated generalization of GNNs across graph size, there has been no study on how to design a GNN for achieving value-set invariance for problems that come from the same domain. For example, learning to solve 16 x 16 sudoku after being trained on only 9 x 9 sudokus, or coloring a 7 colorable graph after training on 4 colorable graphs.  In this work, we propose novel methods to extend GNN based architectures to achieve value-set invariance. Specifically, our model builds on recently proposed Recurrent Relational Networks (RRN) (Palm et al., 2018). Our first approach exploits the graph-size invariance of GNNs by converting a multi-class node classification problem into a binary node classification problem. Our second approach works directly with multiple classes by adding multiple nodes corresponding to the values in the value-set, and then connecting variable nodes to value nodes depending on the problem initialization. Our experimental evaluation on three different combinatorial problems demonstrates that both our models perform well on our novel problem, compared to a generic neural reasoner. Between two of our models, we observe an inherent trade-off: while the binarized model gives better performance when trained on smaller value-sets, multi-valued model is much more memory efficient, resulting in improved performance when trained on larger value-sets, where binarized model fails to train.", "one-sentence_summary": "We design GNN based neural models to achieve output space invariance in combinatorial problems, e.g. solving 16 x 16 sudoku after training on 9 x 9 sudoku", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nandwani|neural_models_for_outputspace_invariance_in_combinatorial_problems", "pdf": "/pdf/372d1714883eb775a0b6579118d157fcb97f83bc.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.03229/code)", "_bibtex": "@inproceedings{\nnandwani2022neural,\ntitle={Neural Models for Output-Space Invariance in Combinatorial Problems},\nauthor={Yatin Nandwani and Vidit Jain and Mausam . and Parag Singla},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ibrUkC-pbis}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "iUuzzTMUw9K", "original": "DuHY4kPceW_", "number": 3458, "cdate": 1632875688860, "mdate": null, "ddate": null, "tcdate": 1632875688860, "tmdate": 1676330503832, "tddate": null, "forum": "iUuzzTMUw9K", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis", "authorids": ["~Jiatao_Gu1", "~Lingjie_Liu1", "~Peng_Wang17", "~Christian_Theobalt2"], "authors": ["Jiatao Gu", "Lingjie Liu", "Peng Wang", "Christian Theobalt"], "keywords": ["Neural Radiance Field", "StyleGAN", "high resolution image generation"], "abstract": "We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view  consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize high-resolution images with fine details or yield clearly noticeable 3D-inconsistent artifacts. In addition, many of them lack control on style attributes and explicit 3D camera poses. To address these issues, StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. To address the first issue, we perform volume rendering only to produce a low-resolution feature map, and progressively apply upsampling in 2D. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs including a better upsampler choice and a new regularization loss to enforce 3D consistency. With these designs, StyleNeRF is able to synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks such as style mixing, inversion and simple semantic edits. \n", "one-sentence_summary": "We present StyleNeRF, a 3D-aware generative model that can synthesize high-resolution images with high multi-view consistency.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gu|stylenerf_a_stylebased_3d_aware_generator_for_highresolution_image_synthesis", "pdf": "/pdf/ac9a5093dd4cf1624afa5fa1f5fea74d822061c6.pdf", "supplementary_material": "/attachment/dce26819b324a6fdf2e161a5fcd4982721e7cb9a.zip", "data": "", "_bibtex": "@inproceedings{\ngu2022stylenerf,\ntitle={StyleNe{RF}: A Style-based 3D Aware Generator for High-resolution Image Synthesis},\nauthor={Jiatao Gu and Lingjie Liu and Peng Wang and Christian Theobalt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iUuzzTMUw9K}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "8eb12UQYxrG", "original": "haa7I-J6aX_", "number": 3452, "cdate": 1632875688465, "mdate": null, "ddate": null, "tcdate": 1632875688465, "tmdate": 1676330504146, "tddate": null, "forum": "8eb12UQYxrG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Role of Pretrained Representations for the OOD Generalization of RL Agents", "authorids": ["~Frederik_Tr\u00e4uble1", "~Andrea_Dittadi1", "~Manuel_Wuthrich1", "~Felix_Widmaier1", "~Peter_Vincent_Gehler1", "~Ole_Winther1", "~Francesco_Locatello1", "~Olivier_Bachem1", "~Bernhard_Sch\u00f6lkopf1", "~Stefan_Bauer1"], "authors": ["Frederik Tr\u00e4uble", "Andrea Dittadi", "Manuel Wuthrich", "Felix Widmaier", "Peter Vincent Gehler", "Ole Winther", "Francesco Locatello", "Olivier Bachem", "Bernhard Sch\u00f6lkopf", "Stefan Bauer"], "keywords": ["representations", "out-of-distribution", "generalization", "deep learning", "reinforcement learning"], "abstract": "Building sample-efficient agents that generalize out-of-distribution (OOD) in real-world settings remains a fundamental unsolved problem on the path towards achieving higher-level cognition. One particularly promising approach is to begin with low-dimensional, pretrained representations of our world, which should facilitate efficient downstream learning and generalization. By training 240 representations and over 10,000 reinforcement learning (RL) policies on a simulated robotic setup, we evaluate to what extent different properties of pretrained VAE-based representations affect the OOD generalization of downstream agents. We observe that many agents are surprisingly robust to realistic distribution shifts, including the challenging sim-to-real case. In addition, we find that the generalization performance of a simple downstream proxy task reliably predicts the generalization performance of our RL agents under a wide range of OOD settings. Such proxy tasks can thus be used to select pretrained representations that will lead to agents that generalize.", "one-sentence_summary": "We study the role of pretrained representations for the out-of-distribution generalization of RL agents.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tr\u00e4uble|the_role_of_pretrained_representations_for_the_ood_generalization_of_rl_agents", "pdf": "/pdf/21a137cbbf57e242ed59de220b1d44db251017d9.pdf", "_bibtex": "@inproceedings{\ntr{\\\"a}uble2022the,\ntitle={The Role of Pretrained Representations for the {OOD} Generalization of {RL} Agents},\nauthor={Frederik Tr{\\\"a}uble and Andrea Dittadi and Manuel Wuthrich and Felix Widmaier and Peter Vincent Gehler and Ole Winther and Francesco Locatello and Olivier Bachem and Bernhard Sch{\\\"o}lkopf and Stefan Bauer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8eb12UQYxrG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "rhOiUS8KQM9", "original": "CzPB8HdAYU", "number": 3432, "cdate": 1632875687121, "mdate": null, "ddate": null, "tcdate": 1632875687121, "tmdate": 1676330504911, "tddate": null, "forum": "rhOiUS8KQM9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Enabling Arbitrary Translation Objectives with Adaptive Tree Search", "authorids": ["~Wang_Ling1", "wstokowiec@google.com", "~Domenic_Donato1", "~Chris_Dyer1", "~Lei_Yu4", "~Laurent_Sartran1", "armatthe@gmail.com"], "authors": ["Wang Ling", "Wojciech Stokowiec", "Domenic Donato", "Chris Dyer", "Lei Yu", "Laurent Sartran", "Austin Matthews"], "keywords": ["Machine Translation", "Decoding", "MCTS", "Beam Search"], "abstract": "We introduce an adaptive tree search algorithm, which is a deterministic variant of Monte Carlo tree search, that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, our algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in autoregressive models. Empirically, we show that our adaptive tree search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in models whose scores do not decompose additively with respect to the words in the output. We also characterise the correlation of several translation model objectives with respect to BLEU. We find that while some standard models are poorly calibrated and benefit from the beam search bias, other often more robust models (autoregressive models tuned to maximize expected automatic metric scores, the noisy channel model and a newly proposed objective) benefit from increasing amounts of search using our proposed decoder, whereas the beam search bias limits the improvements obtained from such objectives. Thus, we argue that as models improve, the improvements may be masked by over-reliance on beam search or reranking based methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ling|enabling_arbitrary_translation_objectives_with_adaptive_tree_search", "pdf": "/pdf/db1b6e5bc8b72c86683c9aae725f36f188279740.pdf", "one-sentence_summary": "MCTS is used as a decoder for autoregressive and non-autoregressive machine translation models.", "_bibtex": "@inproceedings{\nling2022enabling,\ntitle={Enabling Arbitrary Translation Objectives with Adaptive Tree Search},\nauthor={Wang Ling and Wojciech Stokowiec and Domenic Donato and Chris Dyer and Lei Yu and Laurent Sartran and Austin Matthews},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rhOiUS8KQM9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "rpxJc9j04U", "original": "SJpADyfKj_Z", "number": 3407, "cdate": 1632875685392, "mdate": null, "ddate": null, "tcdate": 1632875685392, "tmdate": 1697934615756, "tddate": null, "forum": "rpxJc9j04U", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Proof Artifact Co-Training for Theorem Proving with Language Models", "authorids": ["~Jesse_Michael_Han1", "~Jason_Rute1", "~Yuhuai_Wu1", "e.w.ayers@maths.cam.ac.uk", "~Stanislas_Polu1"], "authors": ["Jesse Michael Han", "Jason Rute", "Yuhuai Wu", "Edward Ayers", "Stanislas Polu"], "keywords": ["self-supervised learning", "mathematics", "reasoning", "theorem proving", "language modeling"], "abstract": "Labeled data for imitation learning of theorem proving in large libraries of formalized mathematics is scarce as such libraries require years of concentrated effort by human specialists to be built. This is particularly challenging when applying large Transformer language models to tactic prediction, because the scaling of performance with respect to model size is quickly disrupted in the data-scarce, easily-overfitted regime.  We propose PACT (Proof Artifact Co-Training), a general methodology for extracting abundant self-supervised data from kernel-level proof terms for joint training alongside the usual tactic prediction objective.  We apply this methodology to Lean,an interactive proof assistant which hosts some of the most sophisticated formalized mathematics to date. We instrument Lean with a neural theorem prover driven by a Transformer language model and show that PACT improves theorem proving success rate on a held-out suite of test theorems from 32% to 48%.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "han|proof_artifact_cotraining_for_theorem_proving_with_language_models", "pdf": "/pdf/452a6f034546f7c5332394391a04b08aafdba0f7.pdf", "one-sentence_summary": "Co-training on low-level proof tasks in a Transformer theorem prover improves success from 32% to 48% on Lean theorems.", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2102.06203/code)", "_bibtex": "@inproceedings{\nhan2022proof,\ntitle={Proof Artifact Co-Training for Theorem Proving with Language Models},\nauthor={Jesse Michael Han and Jason Rute and Yuhuai Wu and Edward Ayers and Stanislas Polu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rpxJc9j04U}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "aBO5SvgSt1", "original": "U-ry9vSwd5oM", "number": 3405, "cdate": 1632875685260, "mdate": null, "ddate": null, "tcdate": 1632875685260, "tmdate": 1697934615784, "tddate": null, "forum": "aBO5SvgSt1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Mirror Descent Policy Optimization", "authorids": ["~Manan_Tomar1", "~Lior_Shani2", "~Yonathan_Efroni2", "~Mohammad_Ghavamzadeh2"], "authors": ["Manan Tomar", "Lior Shani", "Yonathan Efroni", "Mohammad Ghavamzadeh"], "keywords": ["Reinforcement Learning", "Policy Optimization"], "abstract": "Mirror descent (MD), a well-known first-order method in constrained convex optimization, has recently been shown as an important tool to analyze trust-region algorithms in reinforcement learning (RL). However, there remains a considerable gap between such theoretically analyzed algorithms and the ones used in practice. Inspired by this, we propose an efficient RL algorithm, called {\\em mirror descent policy optimization} (MDPO). MDPO iteratively updates the policy by {\\em approximately} solving a trust-region problem, whose objective function consists of two terms: a linearization of the standard RL objective and a proximity term that restricts two consecutive policies to be close to each other. Each update performs this approximation by taking multiple gradient steps on this objective function. We derive {\\em on-policy} and {\\em off-policy} variants of MDPO, while emphasizing important design choices motivated by the existing theory of MD in RL. We highlight the connections between on-policy MDPO and two popular trust-region RL algorithms: TRPO and PPO, and show that explicitly enforcing the trust-region constraint is in fact {\\em not} a necessity for high performance gains in TRPO. We then show how the popular soft actor-critic (SAC) algorithm can be derived by slight modifications of off-policy MDPO. Overall, MDPO is derived from the MD principles, offers a unified approach to viewing a number of popular RL algorithms, and performs better than or on-par with TRPO, PPO, and SAC in a number of continuous and discrete control tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tomar|mirror_descent_policy_optimization", "pdf": "/pdf/03dfa6176ca266c8fa73ec81fc9a35d808fa7ad3.pdf", "one-sentence_summary": "A theory-grounded practical algorithm for policy optimization in RL, which is conceptually simpler and performs better or on par to SOTA.", "supplementary_material": "/attachment/02907fadef589f40ff66fa22a7710ef314c56951.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2005.09814/code)", "_bibtex": "@inproceedings{\ntomar2022mirror,\ntitle={Mirror Descent Policy Optimization},\nauthor={Manan Tomar and Lior Shani and Yonathan Efroni and Mohammad Ghavamzadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aBO5SvgSt1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "OcKMT-36vUs", "original": "OvVc8Uc5WLxi", "number": 3385, "cdate": 1632875683996, "mdate": null, "ddate": null, "tcdate": 1632875683996, "tmdate": 1676330507429, "tddate": null, "forum": "OcKMT-36vUs", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Loss Curvature Perspective on Training Instabilities of Deep Learning Models", "authorids": ["~Justin_Gilmer1", "~Behrooz_Ghorbani1", "~Ankush_Garg1", "~Sneha_Kudugunta1", "~Behnam_Neyshabur1", "dcardoze@google.com", "~George_Edward_Dahl1", "~Zachary_Nado1", "~Orhan_Firat1"], "authors": ["Justin Gilmer", "Behrooz Ghorbani", "Ankush Garg", "Sneha Kudugunta", "Behnam Neyshabur", "David Cardoze", "George Edward Dahl", "Zachary Nado", "Orhan Firat"], "keywords": ["Optimization", "Deep Learning", "Training Instability", "Curvature", "Loss Landscape", "Hessian"], "abstract": "In this work, we study the evolution of the loss Hessian across many classification tasks in order to understand the effect the curvature of the loss has on the training dynamics. Whereas prior work has focused on how different learning rates affect the loss Hessian observed during training, we also analyze the effects of model initialization, architectural choices, and common training heuristics such as gradient clipping and learning rate warmup. Our results demonstrate that successful model and hyperparameter choices allow the early optimization trajectory to either avoid---or navigate out of---regions of high curvature and into flatter regions that tolerate a higher learning rate. Our results suggest a unifying perspective on how disparate mitigation strategies for training instability ultimately address the same underlying failure mode of neural network optimization, namely poor conditioning. Inspired by the conditioning perspective, we show that learning rate warmup can improve training stability just as much as batch normalization, layer normalization, MetaInit, GradInit, and Fixup initialization.", "one-sentence_summary": "Our results suggest a unifying perspective on how disparate mitigation strategies for training instability ultimately address poor conditioning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gilmer|a_loss_curvature_perspective_on_training_instabilities_of_deep_learning_models", "pdf": "/pdf/0e01902abf6d0c88e31b87ec9ca863b66ac6e85c.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\ngilmer2022a,\ntitle={A Loss Curvature Perspective on Training Instabilities of Deep Learning Models},\nauthor={Justin Gilmer and Behrooz Ghorbani and Ankush Garg and Sneha Kudugunta and Behnam Neyshabur and David Cardoze and George Edward Dahl and Zachary Nado and Orhan Firat},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OcKMT-36vUs}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "xP3cPq2hQC", "original": "HrLgMfCXV9f", "number": 3382, "cdate": 1632875683790, "mdate": null, "ddate": null, "tcdate": 1632875683790, "tmdate": 1697934617790, "tddate": null, "forum": "xP3cPq2hQC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Cross-Domain Imitation Learning via Optimal Transport", "authorids": ["~Arnaud_Fickinger1", "~Samuel_Cohen1", "~Stuart_Russell1", "~Brandon_Amos1"], "authors": ["Arnaud Fickinger", "Samuel Cohen", "Stuart Russell", "Brandon Amos"], "keywords": ["optimal transportation", "imitation learning", "cross-domain imitation learning", "gromov-Wasserstein"], "abstract": "Cross-domain imitation learning studies how to leverage expert demonstrations of one agent to train an imitation agent with a different embodiment or morphology. Comparing trajectories and stationary distributions between the expert and imitation agents is challenging because they live on different systems that may not even have the same dimensionality. We propose Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Our theory formally characterizes the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. We demonstrate the effectiveness of GWIL in non-trivial continuous control domains ranging from simple rigid transformation of the expert domain to arbitrary transformation of the state-action space.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fickinger|crossdomain_imitation_learning_via_optimal_transport", "pdf": "/pdf/0e1aa9f9ddcbcd903dbecbe6f034779d158d2260.pdf", "one-sentence_summary": "We study the use of Gromov-Wasserstein for cross-domain imitation learning", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.03684/code)", "_bibtex": "@inproceedings{\nfickinger2022crossdomain,\ntitle={Cross-Domain Imitation Learning via Optimal Transport},\nauthor={Arnaud Fickinger and Samuel Cohen and Stuart Russell and Brandon Amos},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xP3cPq2hQC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "0UXT6PpRpW", "original": "iyrehUN6O7B", "number": 3379, "cdate": 1632875683659, "mdate": null, "ddate": null, "tcdate": 1632875683659, "tmdate": 1676330507790, "tddate": null, "forum": "0UXT6PpRpW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Large-Scale Representation Learning on Graphs via Bootstrapping", "authorids": ["~Shantanu_Thakoor5", "~Corentin_Tallec2", "~Mohammad_Gheshlaghi_Azar1", "~Mehdi_Azabou2", "~Eva_L_Dyer1", "~Remi_Munos1", "~Petar_Veli\u010dkovi\u01071", "~Michal_Valko1"], "authors": ["Shantanu Thakoor", "Corentin Tallec", "Mohammad Gheshlaghi Azar", "Mehdi Azabou", "Eva L Dyer", "Remi Munos", "Petar Veli\u010dkovi\u0107", "Michal Valko"], "keywords": [], "abstract": "Self-supervised learning provides a promising path towards eliminating the need for costly label information in representation learning on graphs.  However, to achieve state-of-the-art performance, methods often need large numbers of negative examples and rely on complex augmentations.  This can be prohibitively expensive, especially for large graphs. To address these challenges, we introduce Bootstrapped Graph Latents (BGRL) - a graph representation learning method that learns by predicting alternative augmentations of the input. BGRL uses only simple augmentations and alleviates the need for contrasting with negative examples, and thus is scalable by design. BGRL outperforms or matches prior methods on several established benchmarks, while achieving a 2-10x reduction in memory costs. Furthermore, we show that BGRL can be scaled up to extremely large graphs with hundreds of millions of nodes in the semi-supervised regime, achieving state-of-the-art performance and improving over supervised baselines where representations are shaped only through label information.  In particular, our solution centered on BGRL constituted one of the winning entries to the Open Graph Benchmark -Large Scale Challenge at KDD Cup 2021, on a graph orders of magnitudes larger than all previously available benchmarks, thus demonstrating the scalability and effectiveness of our approach.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "thakoor|largescale_representation_learning_on_graphs_via_bootstrapping", "pdf": "/pdf/5e487c083a8ccf77a044bed028e5f4bc7d61c314.pdf", "supplementary_material": "/attachment/16a5c61c7ca86ff6547b499d5134965d271317fc.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nthakoor2022largescale,\ntitle={Large-Scale Representation Learning on Graphs via Bootstrapping},\nauthor={Shantanu Thakoor and Corentin Tallec and Mohammad Gheshlaghi Azar and Mehdi Azabou and Eva L Dyer and Remi Munos and Petar Veli{\\v{c}}kovi{\\'c} and Michal Valko},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0UXT6PpRpW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "xZ6H7wydGl", "original": "Ow2zXyKQFTi", "number": 3377, "cdate": 1632875683532, "mdate": null, "ddate": null, "tcdate": 1632875683532, "tmdate": 1676330507932, "tddate": null, "forum": "xZ6H7wydGl", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Robust and Scalable SDE Learning: A Functional Perspective", "authorids": ["~Scott_Alexander_Cameron1", "~Tyron_Luke_Cameron1", "~Arnu_Pretorius1", "~Stephen_J._Roberts1"], "authors": ["Scott Alexander Cameron", "Tyron Luke Cameron", "Arnu Pretorius", "Stephen J. Roberts"], "keywords": ["SDE Learning", "Parallelization", "Importance Sampling"], "abstract": "Stochastic differential equations provide a rich class of flexible generative\nmodels, capable of describing a wide range of spatio-temporal processes. A host\nof recent work looks to learn data-representing SDEs, using neural networks and\nother flexible function approximators. Despite these advances, learning remains\ncomputationally expensive due to the sequential nature of SDE integrators. In\nthis work, we propose an importance-sampling estimator for probabilities of\nobservations of SDEs for the purposes of learning. Crucially, the approach we\nsuggest does not rely on such integrators. The proposed method produces\nlower-variance gradient estimates compared to algorithms based on SDE\nintegrators and has the added advantage of being embarrassingly parallelizable.\nThis facilitates the effective use of large-scale parallel hardware for massive\ndecreases in computation time.\n", "one-sentence_summary": "We provide an algorithm for estimating the probability of observations of a stochastic process which is significantly faster and more stable than those based on standard integration schemes", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cameron|robust_and_scalable_sde_learning_a_functional_perspective", "pdf": "/pdf/59f33538897ccebf6c85feda2739b570f0445ebd.pdf", "supplementary_material": "/attachment/f2767b17918a44b2576fe830a79535c7d8c4a5a5.zip", "_bibtex": "@inproceedings{\ncameron2022robust,\ntitle={Robust and Scalable {SDE} Learning: A Functional Perspective},\nauthor={Scott Alexander Cameron and Tyron Luke Cameron and Arnu Pretorius and Stephen J. Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xZ6H7wydGl}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "JPkQwEdYn8", "original": "kZ9OR4Mx8ED", "number": 3373, "cdate": 1632875683259, "mdate": null, "ddate": null, "tcdate": 1632875683259, "tmdate": 1676330508253, "tddate": null, "forum": "JPkQwEdYn8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Processes with Stochastic Attention: Paying more attention to the context dataset", "authorids": ["~Mingyu_Kim2", "~Kyeong_Ryeol_Go1", "~Se-Young_Yun1"], "authors": ["Mingyu Kim", "Kyeong Ryeol Go", "Se-Young Yun"], "keywords": ["neural processes", "stochastic attention", "variational inference", "information theory"], "abstract": "Neural processes (NPs) aim to stochastically complete unseen data points based on a given context dataset. NPs essentially leverage a given dataset as a context representation to derive a suitable identifier for a novel task. To improve the prediction accuracy, many variants of NPs have investigated context embedding approaches that generally design novel network architectures and aggregation functions satisfying permutation invariant. In this work, we propose a stochastic attention mechanism for NPs to capture appropriate context information. From the perspective of information theory, we demonstrate that the proposed method encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in a target dataset and context embedding independently. We observe that the proposed method can appropriately capture context embedding even under noisy data sets and restricted task distributions, where typical NPs suffer from a lack of context embeddings. We empirically show that our approach substantially outperforms conventional NPs in various domains through 1D regression, predator-prey model, and image completion. Moreover, the proposed method is also validated by MovieLens-10k dataset, a real-world problem.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kim|neural_processes_with_stochastic_attention_paying_more_attention_to_the_context_dataset", "pdf": "/pdf/f1bb535788b750cfd30f0aa321b7bcfa2a50924b.pdf", "one-sentence_summary": " This paper extends the attentive neural process (ANP), replacing the deterministic weights in the cross-attention module of ANP with latent weights.", "supplementary_material": "", "_bibtex": "@inproceedings{\nkim2022neural,\ntitle={Neural Processes with Stochastic Attention: Paying more attention to the context dataset},\nauthor={Mingyu Kim and Kyeong Ryeol Go and Se-Young Yun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JPkQwEdYn8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "SLz5sZjacp", "original": "SwgEuMklPBrO", "number": 3371, "cdate": 1632875683118, "mdate": null, "ddate": null, "tcdate": 1632875683118, "tmdate": 1676330508299, "tddate": null, "forum": "SLz5sZjacp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Evaluating Disentanglement of Structured Representations", "authorids": ["~Rapha\u00ebl_Dang-Nhu2"], "authors": ["Rapha\u00ebl Dang-Nhu"], "keywords": [], "abstract": "We introduce the first metric for evaluating disentanglement at individual hierarchy levels of a structured latent representation. Applied to object-centric generative models, this offers a systematic, unified approach to evaluating (i) object separation between latent slots (ii) disentanglement of object properties inside individual slots (iii) disentanglement of intrinsic and extrinsic object properties. We theoretically show that our framework gives stronger guarantees of selecting a good model than previous disentanglement metrics. Experimentally, we demonstrate that viewing object compositionality as a disentanglement problem addresses several issues with prior visual metrics of object separation. As a core technical component, we present the first representation probing algorithm handling slot permutation invariance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dangnhu|evaluating_disentanglement_of_structured_representations", "pdf": "/pdf/c64b7f2fadb664b509c8b2e80def8ea967bb95a3.pdf", "one-sentence_summary": "We introduce the first metric for evaluating disentanglement at individual hierarchy levels of a structured latent representation, and apply it to object-centric generative models.", "data": "", "_bibtex": "@inproceedings{\ndang-nhu2022evaluating,\ntitle={Evaluating Disentanglement of Structured Representations},\nauthor={Rapha{\\\"e}l Dang-Nhu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SLz5sZjacp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "CS4463zx6Hi", "original": "5Zo76bhYHGk", "number": 3368, "cdate": 1632875682912, "mdate": null, "ddate": null, "tcdate": 1632875682912, "tmdate": 1697934619908, "tddate": null, "forum": "CS4463zx6Hi", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Geometric Transformers for Protein Interface Contact Prediction", "authorids": ["~Alex_Morehead1", "~Chen_Chen31", "~Jianlin_Cheng1"], "authors": ["Alex Morehead", "Chen Chen", "Jianlin Cheng"], "keywords": ["Geometric Deep Learning", "Graph Transformers", "Protein Bioinformatics", "Invariance"], "abstract": "Computational methods for predicting the interface contacts between proteins come highly sought after for drug discovery as they can significantly advance the accuracy of alternative approaches, such as protein-protein docking, protein function analysis tools, and other computational methods for protein bioinformatics. In this work, we present the Geometric Transformer, a novel geometry-evolving graph transformer for rotation and translation-invariant protein interface contact prediction, packaged within DeepInteract, an end-to-end prediction pipeline. DeepInteract predicts partner-specific protein interface contacts (i.e., inter-protein residue-residue contacts) given the 3D tertiary structures of two proteins as input. In rigorous benchmarks, DeepInteract, on challenging protein complex targets from the 13th and 14th CASP-CAPRI experiments as well as Docking Benchmark 5, achieves 14% and 1.1% top L/5 precision (L: length of a protein unit in a complex), respectively. In doing so, DeepInteract, with the Geometric Transformer as its graph-based backbone, outperforms existing methods for interface contact prediction in addition to other graph-based neural network backbones compatible with DeepInteract, thereby validating the effectiveness of the Geometric Transformer for learning rich relational-geometric features for downstream tasks on 3D protein structures.", "one-sentence_summary": "We introduce a geometry-evolving graph transformer for 3D protein structures and employ it to achieve state-of-the-art precision for predicting inter-protein residue-residue contacts in challenging protein complex targets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "morehead|geometric_transformers_for_protein_interface_contact_prediction", "pdf": "/pdf/539393e93bb75c9c05221839904c4d0b83d67adf.pdf", "supplementary_material": "/attachment/aa045d03845c89621853ad3cf7ca962d8d47ad60.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02423/code)", "_bibtex": "@inproceedings{\nmorehead2022geometric,\ntitle={Geometric Transformers for Protein Interface Contact Prediction},\nauthor={Alex Morehead and Chen Chen and Jianlin Cheng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CS4463zx6Hi}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "E4EE_ohFGz", "original": "F-1NHMT1bGv", "number": 3365, "cdate": 1632875682709, "mdate": null, "ddate": null, "tcdate": 1632875682709, "tmdate": 1676330508526, "tddate": null, "forum": "E4EE_ohFGz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Diurnal or Nocturnal? Federated Learning of Multi-branch Networks from Periodically Shifting Distributions", "authorids": ["~Chen_Zhu2", "~Zheng_Xu2", "~Mingqing_Chen1", "~Jakub_Kone\u010dn\u00fd1", "~Andrew_Hard1", "~Tom_Goldstein1"], "authors": ["Chen Zhu", "Zheng Xu", "Mingqing Chen", "Jakub Kone\u010dn\u00fd", "Andrew Hard", "Tom Goldstein"], "keywords": ["Federated Learning", "Peroredical Distribution Shift"], "abstract": "Federated learning has been deployed to train machine learning models from decentralized client data on mobile devices in practice. The clients available for training are observed to have periodically shifting distributions changing with the time of day, which can cause instability in training and degrade the model performance. In this paper, instead of modeling the distribution shift with a block-cyclic pattern as previous works, we model it with a mixture of distributions that gradually shifts between daytime and nighttime modes, and find this intuitive model to better match the observations in practical federated learning systems. \nFurthermore, we propose to jointly train a clustering model and a multi-branch network to allocate lightweight specialized branches to clients from different modes. A temporal prior is used to significantly boost the training performance.\nExperiments for image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show that the proposed algorithm can counter the effects of the distribution shift and significantly improve the final model performance. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|diurnal_or_nocturnal_federated_learning_of_multibranch_networks_from_periodically_shifting_distributions", "pdf": "/pdf/9b5ecb00ea582bf0bc094c59b0c48d7c18be9def.pdf", "one-sentence_summary": "We study a better modeling assumption for the periodical distribution shift in FL, and propose algorithms that learn better from the shifting distribution.", "_bibtex": "@inproceedings{\nzhu2022diurnal,\ntitle={Diurnal or Nocturnal? Federated Learning of Multi-branch Networks from Periodically Shifting Distributions},\nauthor={Chen Zhu and Zheng Xu and Mingqing Chen and Jakub Kone{\\v{c}}n{\\'y} and Andrew Hard and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=E4EE_ohFGz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "5kq11Tl1z4", "original": "o-mW78vCqlEF", "number": 3363, "cdate": 1632875682574, "mdate": null, "ddate": null, "tcdate": 1632875682574, "tmdate": 1697934620726, "tddate": null, "forum": "5kq11Tl1z4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "IGLU: Efficient GCN Training via Lazy Updates", "authorids": ["~S_Deepak_Narayanan1", "~Aditya_Sinha1", "~Prateek_Jain1", "~Purushottam_Kar1", "~SUNDARARAJAN_SELLAMANICKAM2"], "authors": ["S Deepak Narayanan", "Aditya Sinha", "Prateek Jain", "Purushottam Kar", "SUNDARARAJAN SELLAMANICKAM"], "keywords": ["Graph convolutional networks", "Graph neural networks", "Optimization", "Lazy updates"], "abstract": "Training multi-layer Graph Convolution Networks (GCN) using standard SGD techniques scales poorly as each descent step ends up updating node embeddings for a large portion of the graph. Recent attempts to remedy this sub-sample the graph that reduces compute but introduce additional variance and may offer suboptimal performance. This paper develops the IGLU method that caches intermediate computations at various GCN layers thus enabling lazy updates that significantly reduce the compute cost of descent. IGLU introduces bounded bias into the gradients but nevertheless converges to a first-order saddle point under standard assumptions such as objective smoothness. Benchmark experiments show that IGLU offers up to 1.2% better accuracy despite requiring up to 88% less compute.", "one-sentence_summary": "IGLU is a novel lazy update-based optimization technique for accelerated GCN training with provable convergence guarantees", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "narayanan|iglu_efficient_gcn_training_via_lazy_updates", "pdf": "/pdf/c65e33e9b21b066dc663b67740ec61e057d414c8.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 13 code implementations](https://www.catalyzex.com/paper/arxiv:2109.13995/code)", "_bibtex": "@inproceedings{\nnarayanan2022iglu,\ntitle={{IGLU}: Efficient {GCN} Training via Lazy Updates},\nauthor={S Deepak Narayanan and Aditya Sinha and Prateek Jain and Purushottam Kar and SUNDARARAJAN SELLAMANICKAM},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5kq11Tl1z4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "FmBegXJToY", "original": "Rlpsj59oRGd", "number": 3357, "cdate": 1632875682173, "mdate": null, "ddate": null, "tcdate": 1632875682173, "tmdate": 1676330509165, "tddate": null, "forum": "FmBegXJToY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Procedural generalization by planning with self-supervised world models", "authorids": ["~Ankesh_Anand1", "~Jacob_C_Walker1", "~Yazhe_Li2", "~Eszter_V\u00e9rtes1", "~Julian_Schrittwieser1", "~Sherjil_Ozair1", "~Theophane_Weber1", "~Jessica_B_Hamrick1"], "authors": ["Ankesh Anand", "Jacob C Walker", "Yazhe Li", "Eszter V\u00e9rtes", "Julian Schrittwieser", "Sherjil Ozair", "Theophane Weber", "Jessica B Hamrick"], "keywords": ["Self-Supervised Learning", "Model-Based RL", "Generalization in RL"], "abstract": "One of the key promises of model-based reinforcement learning is the ability to generalize using an internal model of the world to make predictions in novel environments and tasks. However, the generalization ability of model-based agents is not well understood because existing work has focused on model-free agents when benchmarking generalization. Here, we explicitly measure the generalization ability of model-based agents in comparison to their model-free counterparts. We focus our analysis on MuZero (Schrittwieser et al., 2020), a powerful model-based agent, and evaluate its performance on both procedural and task generalization. We identify three factors of procedural generalization---planning, self-supervised representation learning, and procedural data diversity---and show that by combining these techniques, we achieve state-of-the art generalization performance and data efficiency on Procgen (Cobbe et al., 2019). However, we find that these factors do not always provide the same benefits for the task generalization benchmarks in Meta-World (Yu et al., 2019), indicating that transfer remains a challenge and may require different approaches than procedural generalization. Overall, we suggest that building generalizable agents requires moving beyond the single-task, model-free paradigm and towards self-supervised model-based agents that are trained in rich, procedural, multi-task environments.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "anand|procedural_generalization_by_planning_with_selfsupervised_world_models", "pdf": "/pdf/aff2220dd1aea0f7b326e525fa1ed3f25efe2eb8.pdf", "one-sentence_summary": " We study generalization in model-based agents and find that they excel at procedural generalization, with planning, self-supervision and data-diversity combining to yield SoTA results on Procgen; however, task generalization is more challenging.", "_bibtex": "@inproceedings{\nanand2022procedural,\ntitle={Procedural generalization by planning with self-supervised world models},\nauthor={Ankesh Anand and Jacob C Walker and Yazhe Li and Eszter V{\\'e}rtes and Julian Schrittwieser and Sherjil Ozair and Theophane Weber and Jessica B Hamrick},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FmBegXJToY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "-Gk_IPJWvk", "original": "s_zzMOBj8V7a", "number": 3355, "cdate": 1632875682033, "mdate": null, "ddate": null, "tcdate": 1632875682033, "tmdate": 1697934621658, "tddate": null, "forum": "-Gk_IPJWvk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Top-N: Equivariant Set and Graph Generation without Exchangeability", "authorids": ["~Clement_Vignac1", "~Pascal_Frossard1"], "authors": ["Clement Vignac", "Pascal Frossard"], "keywords": ["set generation", "graph generation", "permutation equivariance", "generative models", "Top-N"], "abstract": "This work addresses one-shot set and graph generation, and, more specifically, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then processing these points along with the prior vector using Transformer layers or Graph Neural Networks. \nThis architecture is designed to generate exchangeable distributions, i.e., all permutations of the generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence lower bound, which makes it hard to train. We then study equivariance in generative settings and show that non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce Top-n creation, a differentiable generation mechanism that uses the latent vector to select the most relevant points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational Autoencoder or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15% at SetMNIST reconstruction, by 33% at object detection on CLEVR, generates sets that are 74% closer to the true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9. ", "one-sentence_summary": "We propose the Top-N method for one-shot set and graph probabilistic decoders as a replacement for i.i.d. generation in the first layer.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "vignac|topn_equivariant_set_and_graph_generation_without_exchangeability", "pdf": "/pdf/8dd83e84eec6c2706847d33a5777775604829ad9.pdf", "supplementary_material": "/attachment/7999da100924b2edc37b348a879c2d32838b2708.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.02096/code)", "_bibtex": "@inproceedings{\nvignac2022topn,\ntitle={Top-N: Equivariant Set and Graph Generation without Exchangeability},\nauthor={Clement Vignac and Pascal Frossard},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-Gk_IPJWvk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "P7FLfMLTSEX", "original": "eRhD3IB6Ip6", "number": 3346, "cdate": 1632875681412, "mdate": null, "ddate": null, "tcdate": 1632875681412, "tmdate": 1676330509866, "tddate": null, "forum": "P7FLfMLTSEX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Spectral Bias of Polynomial Neural Networks", "authorids": ["~Moulik_Choraria1", "~Leello_Tadesse_Dadi1", "~Grigorios_Chrysos1", "~Julien_Mairal1", "~Volkan_Cevher1"], "authors": ["Moulik Choraria", "Leello Tadesse Dadi", "Grigorios Chrysos", "Julien Mairal", "Volkan Cevher"], "keywords": ["Deep Neural Networks", "Polynomials", "Spectral Bias", "Neural Tangent Kernel", "Deep Image Prior", "Infinite Width", "Mercer Decomposition"], "abstract": "Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\\text{\\it{spectral bias}}$ towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. \nWe verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials. \n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "choraria|the_spectral_bias_of_polynomial_neural_networks", "pdf": "/pdf/e0444281519be29bf9b0bf62909377028d4cb017.pdf", "one-sentence_summary": "We study the spectral bias of polynomial networks and compare it with the spectral bias of standard neural nets using kernel approximations", "_bibtex": "@inproceedings{\nchoraria2022the,\ntitle={The Spectral Bias of Polynomial Neural Networks},\nauthor={Moulik Choraria and Leello Tadesse Dadi and Grigorios Chrysos and Julien Mairal and Volkan Cevher},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=P7FLfMLTSEX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "-e4EXDWXnSn", "original": "ptzhhnMCEDFq", "number": 3344, "cdate": 1632875681275, "mdate": null, "ddate": null, "tcdate": 1632875681275, "tmdate": 1676330509913, "tddate": null, "forum": "-e4EXDWXnSn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Invariant Causal Representation Learning for Out-of-Distribution Generalization", "authorids": ["~Chaochao_Lu1", "~Yuhuai_Wu1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Chaochao Lu", "Yuhuai Wu", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Bernhard Sch\u00f6lkopf"], "keywords": [], "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant relationship with the target. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features and build an invariant predictor. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. We propose invariant Causal Representation Learning (iCaRL), an approach that enables out-of-distribution (OOD) generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: the prior over the data representation (i.e., a set of latent variables encoding the data) given the target and the environment belongs to general exponential family distributions, i.e., a more flexible conditionally non-factorized prior that can actually capture complicated dependences between the latent variables. Based on this, we show that it is possible to identify the data representation up to simple transformations. We also show that all direct causes of the target can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Experiments on both synthetic and real-world datasets demonstrate that our approach outperforms a variety of baseline methods. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lu|invariant_causal_representation_learning_for_outofdistribution_generalization", "pdf": "/pdf/131aed68985b1886dbcb75f29a9399e1da984948.pdf", "_bibtex": "@inproceedings{\nlu2022invariant,\ntitle={Invariant Causal Representation Learning for Out-of-Distribution Generalization},\nauthor={Chaochao Lu and Yuhuai Wu and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato and Bernhard Sch{\\\"o}lkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-e4EXDWXnSn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "HCRVf71PMF", "original": "klYSo6-B6yn", "number": 3331, "cdate": 1632875680375, "mdate": null, "ddate": null, "tcdate": 1632875680375, "tmdate": 1676330510638, "tddate": null, "forum": "HCRVf71PMF", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5", "authorids": ["~Chengwei_Qin1", "~Shafiq_Joty1"], "authors": ["Chengwei Qin", "Shafiq Joty"], "keywords": ["lifelong few-shot language Learning", "prompt tuning", "pseudo samples", "knowledge distillation"], "abstract": "Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we define this more challenging yet practical problem as Lifelong Few-shot Language Learning (LFLL) and propose a unified framework for it based on prompt tuning of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot learning ability, and simultaneously trains the model as a task solver and a data generator. Before learning a new domain of the same task type, LFPT5 generates pseudo (labeled) samples of previously learned domains, and later gets trained on those samples to alleviate forgetting of previous knowledge as it learns the new domain. In addition, a KL divergence loss is minimized to achieve label consistency between the previous and the current model. While adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task. With extensive experiments, we demonstrate that LFPT5 can be applied to various different types of tasks and significantly outperform previous methods in different LFLL settings.", "one-sentence_summary": "We define a challenging yet practical problem as Lifelong Few-shot Language Learning and propose a unified framework for it based on prompt tuning of T5.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "qin|lfpt5_a_unified_framework_for_lifelong_fewshot_language_learning_based_on_prompt_tuning_of_t5", "pdf": "/pdf/a61cfe769dd99962e037c723c20da960f1378949.pdf", "supplementary_material": "/attachment/6a30a20920e8e38269c21b347da89d31a77d241f.zip", "_bibtex": "@inproceedings{\nqin2022lfpt,\ntitle={{LFPT}5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5},\nauthor={Chengwei Qin and Shafiq Joty},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HCRVf71PMF}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 30}}, {"id": "6yVvwR9H9Oj", "original": "CbK1lPfTUW", "number": 3328, "cdate": 1632875680180, "mdate": null, "ddate": null, "tcdate": 1632875680180, "tmdate": 1697934624373, "tddate": null, "forum": "6yVvwR9H9Oj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Non-Random Missing Labels in Semi-Supervised Learning", "authorids": ["~Xinting_Hu1", "~Yulei_Niu1", "~Chunyan_Miao1", "~Xian-Sheng_Hua1", "~Hanwang_Zhang3"], "authors": ["Xinting Hu", "Yulei Niu", "Chunyan Miao", "Xian-Sheng Hua", "Hanwang Zhang"], "keywords": ["Semi-Supervised Learning", "Missing Not At Random", "Image Classification"], "abstract": "Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in which the label Missing Not At Random (MNAR) problem is more realistic and challenging, compared to the widely-adopted yet naive Missing Completely At Random assumption where both labeled and unlabeled data share the same class distribution. Different from existing SSL solutions that overlook the role of  ''class'' in causing the non-randomness, e.g., users are more likely to label popular classes, we explicitly incorporate ''class'' into SSL. Our method is three-fold: 1) We propose Class-Aware Propensity (CAP) that exploits the unlabeled data to train an improved classifier using the biased labeled data. 2) To encourage rare class training, whose model is low-recall but high-precision that discards too many pseudo-labeled data, we propose Class-Aware Imputation (CAI) that dynamically decreases (or increases) the pseudo-label assignment threshold for rare (or frequent) classes. 3) Overall, we integrate CAP and CAI into a Class-Aware Doubly Robust (CADR) estimator for training an unbiased SSL model. Under various MNAR settings and ablations, our method not only significantly outperforms existing baselines, but also surpasses other label bias removal SSL methods.\n", "pdf": "/pdf/5b4b2c13eef5b39bb9f1edf005023f268f76c88d.pdf", "one-sentence_summary": "We presented a principled class-aware doubly robust solution to handle the non-random missing labels in semi-supervised learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hu|on_nonrandom_missing_labels_in_semisupervised_learning", "data": "", "supplementary_material": "/attachment/b5903a5ec9797f00763d55740ae6bbf0d2d7ff86.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2206.14923/code)", "_bibtex": "@inproceedings{\nhu2022on,\ntitle={On Non-Random Missing Labels in Semi-Supervised Learning},\nauthor={Xinting Hu and Yulei Niu and Chunyan Miao and Xian-Sheng Hua and Hanwang Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6yVvwR9H9Oj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "sPfB2PI87BZ", "original": "wpH2avg2sP1", "number": 3312, "cdate": 1632875679163, "mdate": null, "ddate": null, "tcdate": 1632875679163, "tmdate": 1676330511516, "tddate": null, "forum": "sPfB2PI87BZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Mapping conditional distributions for domain adaptation under generalized target shift", "authorids": ["~Matthieu_Kirchmeyer1", "~Alain_Rakotomamonjy1", "~Emmanuel_de_Bezenac2", "~patrick_gallinari1"], "authors": ["Matthieu Kirchmeyer", "Alain Rakotomamonjy", "Emmanuel de Bezenac", "patrick gallinari"], "keywords": ["Unsupervised domain adaptation", "generalized target shift"], "abstract": "We consider the problem of unsupervised domain adaptation (UDA) between a source and a target domain under conditional and label shift a.k.a Generalized Target Shift (GeTarS). Unlike simpler UDA settings, few works have addressed this challenging problem. Recent approaches learn domain-invariant representations, yet they have practical limitations and rely on strong assumptions that may not hold in practice. In this paper, we explore a novel and general approach to align pretrained representations, which circumvents existing drawbacks. Instead of constraining representation invariance, it learns an optimal transport map, implemented as a NN, which maps source representations onto target ones. Our approach is flexible and scalable, it preserves the problem's structure and it has strong theoretical guarantees under mild assumptions. In particular, our solution is unique, matches conditional distributions across domains, recovers target proportions and explicitly controls the target generalization risk. Through an exhaustive comparison on several datasets, we challenge the state-of-the-art in GeTarS.", "one-sentence_summary": "We propose a novel theoretically grounded approach for domain adaptation under Generalized Target Shift; it learns a map between pretrained source and target representations that matches conditional distributions and recovers target proportions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kirchmeyer|mapping_conditional_distributions_for_domain_adaptation_under_generalized_target_shift", "pdf": "/pdf/57e7b71f45b2a39eb5f81670fb7e8fd080744a11.pdf", "_bibtex": "@inproceedings{\nkirchmeyer2022mapping,\ntitle={Mapping conditional distributions for domain adaptation under generalized target shift},\nauthor={Matthieu Kirchmeyer and Alain Rakotomamonjy and Emmanuel de Bezenac and patrick gallinari},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=sPfB2PI87BZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "oWZsQ8o5EA", "original": "H-x8PFzav2JM", "number": 3311, "cdate": 1632875679096, "mdate": null, "ddate": null, "tcdate": 1632875679096, "tmdate": 1676330511549, "tddate": null, "forum": "oWZsQ8o5EA", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Generalization of Models Trained with SGD: Information-Theoretic Bounds and Implications", "authorids": ["~Ziqiao_Wang1", "~Yongyi_Mao2"], "authors": ["Ziqiao Wang", "Yongyi Mao"], "keywords": ["deep learning", "generalization", "information theory", "learning bound", "regularization"], "abstract": "This paper follows up on a recent work of Neu et al. (2021) and presents some new information-theoretic upper bounds for the generalization error of machine learning models, such as neural networks, trained with SGD. We apply these bounds to analyzing the generalization behaviour of linear and two-layer ReLU networks. Experimental study of these bounds provide some insights on the SGD training of neural networks. They also point to a new and simple regularization scheme which we show performs comparably to the current state of the art. ", "one-sentence_summary": "We derived new information-theoretic generalization bounds for SGD and we also proposed a new regularization scheme.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|on_the_generalization_of_models_trained_with_sgd_informationtheoretic_bounds_and_implications", "pdf": "/pdf/2de81438aaadab6bf2476214305bff6725392d71.pdf", "supplementary_material": "/attachment/78bdf7df1a4f031f74b220da6a9caa3a6be4fa49.zip", "data": "", "_bibtex": "@inproceedings{\nwang2022on,\ntitle={On the Generalization of Models Trained with {SGD}: Information-Theoretic Bounds and Implications},\nauthor={Ziqiao Wang and Yongyi Mao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oWZsQ8o5EA}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "3PN4iyXBeF", "original": "oYFSt431SuA3", "number": 3305, "cdate": 1632875678691, "mdate": null, "ddate": null, "tcdate": 1632875678691, "tmdate": 1697934626218, "tddate": null, "forum": "3PN4iyXBeF", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Amortized Implicit Differentiation for Stochastic Bilevel Optimization", "authorids": ["~Michael_Arbel1", "~Julien_Mairal1"], "authors": ["Michael Arbel", "Julien Mairal"], "keywords": ["bilevel optimization", "stochastic optimization"], "abstract": "We study a class of algorithms for solving bilevel optimization problems in both stochastic and deterministic settings when the inner-level objective is strongly convex. Specifically, we consider  algorithms based on inexact implicit differentiation and we exploit a warm-start strategy to amortize the estimation of the exact gradient. We then introduce a unified theoretical framework inspired by the study of singularly perturbed systems to analyze such amortized algorithms. By using this framework, our analysis shows these algorithms to match the computational complexity of oracle methods that have access to an unbiased estimate of the gradient, thus outperforming many existing results for bilevel optimization.\nWe illustrate these findings on synthetic experiments and demonstrate the efficiency of these algorithms on hyper-parameter optimization experiments involving several thousands of variables. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "arbel|amortized_implicit_differentiation_for_stochastic_bilevel_optimization", "pdf": "/pdf/53d397803e076505ad052eb9a7267b6d0805c0dc.pdf", "one-sentence_summary": "We provide a unified framework for analyzing bilevel optimization algorithm based on approximate implicit differentiation with a warm-start strategy", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.14580/code)", "_bibtex": "@inproceedings{\narbel2022amortized,\ntitle={Amortized Implicit Differentiation for Stochastic Bilevel Optimization},\nauthor={Michael Arbel and Julien Mairal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3PN4iyXBeF}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "FlwzVjfMryn", "original": "JBWHne7KGrd", "number": 3304, "cdate": 1632875678627, "mdate": null, "ddate": null, "tcdate": 1632875678627, "tmdate": 1676330511800, "tddate": null, "forum": "FlwzVjfMryn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multi-objective Optimization by Learning Space Partition", "authorids": ["~Yiyang_Zhao1", "~Linnan_Wang2", "~Kevin_Yang2", "~Tianjun_Zhang1", "~Tian_Guo3", "~Yuandong_Tian1"], "authors": ["Yiyang Zhao", "Linnan Wang", "Kevin Yang", "Tianjun Zhang", "Tian Guo", "Yuandong Tian"], "keywords": ["Optimization", "Machine Learning"], "abstract": "In contrast to single-objective optimization (SOO), multi-objective optimization (MOO) requires an optimizer to find the Pareto frontier, a subset of feasible solutions that are not dominated by other feasible solutions. In this paper, we propose LaMOO, a novel multi-objective optimizer that learns a model from observed samples to partition the search space and then focus on promising regions that are likely to contain a subset of the Pareto frontier. The partitioning is based on the dominance number, which measures \"how close'' a data point is to the Pareto frontier among existing samples. To account for possible partition errors due to limited samples and model mismatch, we leverage Monte Carlo Tree Search (MCTS) to exploit promising regions while exploring suboptimal regions that may turn out to contain good solutions later. Theoretically, we prove the efficacy of learning space partitioning via LaMOO under certain assumptions. Empirically, on the HyperVolume (HV) benchmark, a popular MOO metric, LaMOO substantially outperforms strong baselines on multiple real-world MOO tasks, by up to 225% in sample efficiency for neural architecture search on Nasbench201, and up to 10% for molecular design.", "one-sentence_summary": "Multi-objective Optimization by Learning Space Partition", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhao|multiobjective_optimization_by_learning_space_partition", "pdf": "/pdf/9de2fff362decf573ae08e0c8975856a05f47a3d.pdf", "supplementary_material": "/attachment/33d50adbebba7e7d1f11bb94dadbff6bcb243266.zip", "data": "", "_bibtex": "@inproceedings{\nzhao2022multiobjective,\ntitle={Multi-objective Optimization by Learning Space Partition},\nauthor={Yiyang Zhao and Linnan Wang and Kevin Yang and Tianjun Zhang and Tian Guo and Yuandong Tian},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FlwzVjfMryn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "gJcEM8sxHK", "original": "_fRrj82rJyD", "number": 3303, "cdate": 1632875678560, "mdate": null, "ddate": null, "tcdate": 1632875678560, "tmdate": 1676330511902, "tddate": null, "forum": "gJcEM8sxHK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Mapping Language Models to Grounded Conceptual Spaces", "authorids": ["~Roma_Patel1", "~Ellie_Pavlick1"], "authors": ["Roma Patel", "Ellie Pavlick"], "keywords": [], "abstract": "A fundamental criticism of text-only language models (LMs) is their lack of grounding---that is, the ability to tie a word for which they have learned a representation, to its actual use in the world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate fluent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed reflects the conceptual structure of the non-linguistic world---which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word ``left\" means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, the word ``right\", in a similar grid world. We investigate a range of generative language models of varying sizes (including GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations ``from scratch'', it is possible that large text-only models learn a sufficiently rich conceptual structure that could allow them to be grounded in a data-efficient way.", "one-sentence_summary": "Mapping text-only pre-trained language models to grounded conceptual worlds.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "patel|mapping_language_models_to_grounded_conceptual_spaces", "pdf": "/pdf/e21987f87f4a8c8ec30cd2613232d4f27014f121.pdf", "data": "", "_bibtex": "@inproceedings{\npatel2022mapping,\ntitle={Mapping Language Models to Grounded Conceptual Spaces},\nauthor={Roma Patel and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gJcEM8sxHK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "iulEMLYh1uR", "original": "B4zs7Zl6upm", "number": 3298, "cdate": 1632875678217, "mdate": null, "ddate": null, "tcdate": 1632875678217, "tmdate": 1676330512284, "tddate": null, "forum": "iulEMLYh1uR", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Efficiency Misnomer", "authorids": ["~Mostafa_Dehghani1", "~Yi_Tay1", "~Anurag_Arnab1", "~Lucas_Beyer1", "~Ashish_Vaswani1"], "authors": ["Mostafa Dehghani", "Yi Tay", "Anurag Arnab", "Lucas Beyer", "Ashish Vaswani"], "keywords": ["Efficiency in Machine Learning", "FLOPs", "Number of Parameters", "Throughput"], "abstract": "Model efficiency is a critical aspect of developing and deploying machine learning models. \nInference time and latency directly affect the user experience, and some applications have hard requirements. In addition to inference costs, model training also have direct financial and environmental impacts.\nAlthough there are numerous well-established metrics (cost indicators) for measuring model efficiency, researchers and practitioners often assume that these metrics are correlated with each other and report only a few of them.\nIn this paper, we thoroughly discuss common cost indicators, their advantages and disadvantages, and how they can contradict each other.\nWe demonstrate how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models. We further present suggestions to improve reporting of efficiency metrics.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dehghani|the_efficiency_misnomer", "pdf": "/pdf/ccde7c84b026b78d9ecf3609e60b0ea1e279d302.pdf", "_bibtex": "@inproceedings{\ndehghani2022the,\ntitle={The Efficiency Misnomer},\nauthor={Mostafa Dehghani and Yi Tay and Anurag Arnab and Lucas Beyer and Ashish Vaswani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iulEMLYh1uR}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "auOPcdAcoy", "original": "qraVJs7WWCK", "number": 3295, "cdate": 1632875678015, "mdate": null, "ddate": null, "tcdate": 1632875678015, "tmdate": 1676330512602, "tddate": null, "forum": "auOPcdAcoy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface", "authorids": ["~Tuan_Anh_Le1", "~Katherine_M._Collins1", "~Luke_Hewitt1", "~Kevin_Ellis1", "~Siddharth_N1", "~Samuel_Gershman1", "~Joshua_B._Tenenbaum1"], "authors": ["Tuan Anh Le", "Katherine M. Collins", "Luke Hewitt", "Kevin Ellis", "Siddharth N", "Samuel Gershman", "Joshua B. Tenenbaum"], "keywords": ["wake-sleep", "variational inference", "neuro-symbolic generative models"], "abstract": "Modeling complex phenomena typically involves the use of both discrete and continuous variables. Such a setting applies across a wide range of problems, from identifying trends in time-series data to performing effective compositional scene understanding in images. Here, we propose Hybrid Memoised Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid discrete-continuous models. Prior approaches to learning suffer as they need to perform repeated expensive inner-loop discrete inference. We build on a recent approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by memoising discrete variables, and extend it to allow for a principled and effective way to handle continuous variables by learning a separate recognition model used for importance-sampling based approximate inference and marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene understanding domains, and show that it outperforms current state-of-the-art inference methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "le|hybrid_memoised_wakesleep_approximate_inference_at_the_discretecontinuous_interface", "pdf": "/pdf/4406e9491128f790573cb34e530a68b400ba3b6a.pdf", "_bibtex": "@inproceedings{\nle2022hybrid,\ntitle={Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface},\nauthor={Tuan Anh Le and Katherine M. Collins and Luke Hewitt and Kevin Ellis and Siddharth N and Samuel Gershman and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=auOPcdAcoy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "MR7XubKUFB", "original": "_bjaHO1rwzf", "number": 3293, "cdate": 1632875677873, "mdate": null, "ddate": null, "tcdate": 1632875677873, "tmdate": 1697934627311, "tddate": null, "forum": "MR7XubKUFB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Adversarial Retriever-Ranker for Dense Text Retrieval", "authorids": ["~Hang_Zhang6", "~Yeyun_Gong2", "~Yelong_Shen2", "~Jiancheng_Lv2", "~Nan_Duan1", "~Weizhu_Chen1"], "authors": ["Hang Zhang", "Yeyun Gong", "Yelong Shen", "Jiancheng Lv", "Nan Duan", "Weizhu Chen"], "keywords": [], "abstract": "Current dense text retrieval models face two typical challenges. First, it adopts a siamese dual-encoder architecture to encode query and document independently for fast indexing and searching, whereas neglecting the finer-grained term-wise interactions. This results in a sub-optimal recall performance. Second, it highly relies on a negative sampling technique to build up the negative documents in its contrastive loss. To address these challenges, we present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder retriever plus a cross-encoder ranker. The two models are jointly optimized according to a minimax adversarial objective: the retriever learns to retrieve negative documents to cheat the ranker, while the ranker learns to rank a collection of candidates including both the ground-truth and the retrieved ones, as well as providing progressive direct feedback to the dual-encoder retriever. Through this adversarial game, the retriever gradually produces harder negative documents to train a better ranker, whereas the cross-encoder ranker provides progressive feedback to improve retriever. We evaluate AR2 on three benchmarks. Experimental results show that AR2 consistently and significantly outperforms existing dense retriever methods and achieves new state-of-the-art results on all of them. This includes the improvements on Natural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and MS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data publicly available. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|adversarial_retrieverranker_for_dense_text_retrieval", "pdf": "/pdf/f136ae88d6299eb072ca8a5c70978e10cf5559f4.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2110.03611/code)", "_bibtex": "@inproceedings{\nzhang2022adversarial,\ntitle={Adversarial Retriever-Ranker for Dense Text Retrieval},\nauthor={Hang Zhang and Yeyun Gong and Yelong Shen and Jiancheng Lv and Nan Duan and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MR7XubKUFB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "t5s-hd1bqLk", "original": "49PIrSdU1zOC", "number": 3287, "cdate": 1632875677543, "mdate": null, "ddate": null, "tcdate": 1632875677543, "tmdate": 1676330513384, "tddate": null, "forum": "t5s-hd1bqLk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Conditioning Sequence-to-sequence Networks with Learned Activations", "authorids": ["~Alberto_Gil_Couto_Pimentel_Ramos1", "~Abhinav_Mehrotra1", "~Nicholas_Donald_Lane1", "~Sourav_Bhattacharya1"], "authors": ["Alberto Gil Couto Pimentel Ramos", "Abhinav Mehrotra", "Nicholas Donald Lane", "Sourav Bhattacharya"], "keywords": ["Conditional Neural Networks", "Sound Enhancement", "Personalized ASR"], "abstract": "Conditional neural networks play an important role in a number of sequence-to-sequence modeling tasks, including personalized sound enhancement (PSE), speaker dependent automatic speech recognition (ASR), and generative modeling such as text-to-speech synthesis. In conditional neural networks, the output of a model is often influenced by a conditioning vector, in addition to the input. Common approaches of conditioning include input concatenation or modulation with the conditioning vector, which comes at a cost of increased model size. In this work, we introduce a novel approach of neural network conditioning by learning intermediate layer activations based on the conditioning vector. We systematically explore and show that learned activation functions can produce conditional models with comparable or better quality, while decreasing model sizes, thus making them ideal candidates for resource-efficient on-device deployment. As exemplary target use-cases we consider (i) the task of PSE as a pre-processing technique for improving telephony or pre-trained ASR performance under noise, and (ii) personalized ASR in single speaker scenarios. We find that conditioning via activation function learning is an effective modeling strategy, suggesting a broad applicability of the proposed technique across a number of application domains.", "one-sentence_summary": "Conditioning neural networks by learning the layer activations based on the conditioning vector", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ramos|conditioning_sequencetosequence_networks_with_learned_activations", "pdf": "/pdf/61c3a9f19dc0bf8c4ab041dd9340f0c73d2c2a8b.pdf", "supplementary_material": "/attachment/9f037e606072f0946fa374e369cc3154d38e33f3.zip", "data": "", "_bibtex": "@inproceedings{\nramos2022conditioning,\ntitle={Conditioning Sequence-to-sequence Networks with Learned Activations},\nauthor={Alberto Gil Couto Pimentel Ramos and Abhinav Mehrotra and Nicholas Donald Lane and Sourav Bhattacharya},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=t5s-hd1bqLk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "BRFWxcZfAdC", "original": "dJ8iofJO8d8", "number": 3285, "cdate": 1632875677407, "mdate": null, "ddate": null, "tcdate": 1632875677407, "tmdate": 1676330513389, "tddate": null, "forum": "BRFWxcZfAdC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "LOSSY COMPRESSION WITH DISTRIBUTION SHIFT AS ENTROPY CONSTRAINED OPTIMAL TRANSPORT", "authorids": ["~Huan_Liu4", "~George_Zhang1", "~Jun_Chen8", "~Ashish_J_Khisti1"], "authors": ["Huan Liu", "George Zhang", "Jun Chen", "Ashish J Khisti"], "keywords": ["Image Compression", "Image Restoration", "Optimal Transport", "Deep Learning"], "abstract": "We study an extension of lossy compression where the reconstruction distribution is different from the source distribution in order to account for distributional shift due to processing. We formulate this as a generalization of optimal transport with an entropy bottleneck to account for the rate constraint due to compression. We provide expressions for the tradeoff between  compression rate and the achievable distortion with and without shared common randomness between the encoder and decoder.  We study the examples of binary, uniform and Gaussian sources (in an asymptotic setting) in detail and  demonstrate that shared randomness can strictly improve the tradeoff. For the case without common randomness and squared-Euclidean distortion, we show that the optimal solution partially decouples into the problem of optimal compression and transport and also characterize the penalty associated with fully decoupling them. We provide experimental results by training deep learning end-to-end compression systems for performing denoising on SVHN and super-resolution on MNIST suggesting consistency with our theoretical results.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|lossy_compression_with_distribution_shift_as_entropy_constrained_optimal_transport", "pdf": "/pdf/78a4014e3770c6c6eea378ae2f81625b2241af8e.pdf", "one-sentence_summary": "We consider the novel task of cross-distribution lossy compression and characterize it as an optimal transport problem under an entropy constraint, then provide experimental results to demonstrate the principles suggested by our theory.", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nliu2022lossy,\ntitle={{LOSSY} {COMPRESSION} {WITH} {DISTRIBUTION} {SHIFT} {AS} {ENTROPY} {CONSTRAINED} {OPTIMAL} {TRANSPORT}},\nauthor={Huan Liu and George Zhang and Jun Chen and Ashish J Khisti},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BRFWxcZfAdC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "gKLAAfiytI", "original": "oZRHZ27CkWn", "number": 3275, "cdate": 1632875676802, "mdate": null, "ddate": null, "tcdate": 1632875676802, "tmdate": 1676330513768, "tddate": null, "forum": "gKLAAfiytI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Equivariant Self-Supervised Learning: Encouraging Equivariance in Representations", "authorids": ["~Rumen_Dangovski1", "~Li_Jing1", "~Charlotte_Loh1", "~Seungwook_Han1", "~Akash_Srivastava1", "~Brian_Cheung1", "~Pulkit_Agrawal1", "~Marin_Soljacic1"], "authors": ["Rumen Dangovski", "Li Jing", "Charlotte Loh", "Seungwook Han", "Akash Srivastava", "Brian Cheung", "Pulkit Agrawal", "Marin Soljacic"], "keywords": ["self-supervised learning", "contrastive learning", "photonics science"], "abstract": "In state-of-the-art self-supervised learning (SSL) pre-training produces semantically good representations by encouraging them to be invariant under meaningful transformations prescribed from human knowledge. In fact, the property of invariance is a trivial instance of a broader class called equivariance, which can be intuitively understood as the property that representations transform according to the way the inputs transform. Here, we show that rather than using only invariance, pre-training that encourages non-trivial equivariance to some transformations, while maintaining invariance to other transformations, can be used to improve the semantic quality of representations. Specifically, we extend popular SSL methods to a more general framework which we name Equivariant Self-Supervised Learning (E-SSL). In E-SSL, a simple additional pre-training objective encourages equivariance by predicting the transformations applied to the input. We demonstrate E-SSL\u2019s effectiveness empirically on several popular computer vision benchmarks, e.g. improving SimCLR to 72.5% linear probe accuracy on ImageNet. Furthermore, we demonstrate usefulness of E-SSL for applications beyond computer vision; in particular, we show its utility on regression problems in photonics science. Our code, datasets and pre-trained models are available at https://github.com/rdangovs/essl to aid further research in E-SSL.", "one-sentence_summary": "Imposing invariance to certain transformations (e.g. random resized cropping) and sensitivity to other transformations (e.g. four-fold rotations) learns better features.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dangovski|equivariant_selfsupervised_learning_encouraging_equivariance_in_representations", "pdf": "/pdf/4b06d5ed7734b3d8e6ca71f132d90bf8a5b910e6.pdf", "supplementary_material": "/attachment/3a397d7c2ec44e7256103f9d03c9f82aedb15e7c.zip", "data": "", "_bibtex": "@inproceedings{\ndangovski2022equivariant,\ntitle={Equivariant Self-Supervised Learning: Encouraging Equivariance in Representations},\nauthor={Rumen Dangovski and Li Jing and Charlotte Loh and Seungwook Han and Akash Srivastava and Brian Cheung and Pulkit Agrawal and Marin Soljacic},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gKLAAfiytI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "25kzAhUB1lz", "original": "uclBi_UfbHM", "number": 3261, "cdate": 1632875675921, "mdate": null, "ddate": null, "tcdate": 1632875675921, "tmdate": 1676330514812, "tddate": null, "forum": "25kzAhUB1lz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching", "authorids": ["~Pierre-Alexandre_Kamienny1", "~Jean_Tarbouriech1", "~sylvain_lamprier1", "~Alessandro_Lazaric2", "~Ludovic_Denoyer1"], "authors": ["Pierre-Alexandre Kamienny", "Jean Tarbouriech", "sylvain lamprier", "Alessandro Lazaric", "Ludovic Denoyer"], "keywords": ["unsupervised reinforcement learning", "skill discovery", "mutual information"], "abstract": "Learning meaningful behaviors in the absence of reward is a difficult problem in reinforcement learning. A desirable and challenging unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. In this paper, we build on the mutual information framework for skill discovery and introduce UPSIDE, which addresses the coverage-directedness trade-off in the following ways: 1) We design policies with a decoupled structure of a directed skill, trained to reach a specific region, followed by a diffusing part that induces a local coverage. 2) We optimize policies by  maximizing their number under the constraint that each of them reaches distinct regions of the environment (i.e., they are sufficiently discriminable) and prove that this serves as a lower bound to the original mutual information objective. 3) Finally, we compose the learned directed skills into a growing tree that adaptively covers the environment. We illustrate in several navigation and control environments how the skills learned by UPSIDE solve sparse-reward downstream tasks better than existing baselines.", "pdf": "/pdf/cc127fb35d524312500353fa6195db8fc4418285.pdf", "supplementary_material": "", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kamienny|direct_then_diffuse_incremental_unsupervised_skill_discovery_for_state_covering_and_goal_reaching", "_bibtex": "@inproceedings{\nkamienny2022direct,\ntitle={Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching},\nauthor={Pierre-Alexandre Kamienny and Jean Tarbouriech and Alessandro Lazaric and Ludovic Denoyer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=25kzAhUB1lz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "Nh7CtbyoqV5", "original": "tRNA5waTwUR9", "number": 3254, "cdate": 1632875675524, "mdate": null, "ddate": null, "tcdate": 1632875675524, "tmdate": 1676330515102, "tddate": null, "forum": "Nh7CtbyoqV5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Normalization of Language Embeddings for Cross-Lingual Alignment", "authorids": ["~Prince_Osei_Aboagye1", "~Yan_Zheng2", "~Chin-Chia_Michael_Yeh1", "~Junpeng_Wang1", "~Wei_Zhang52", "~Liang_Wang11", "~Hao_Yang8", "~Jeff_Phillips1"], "authors": ["Prince Osei Aboagye", "Yan Zheng", "Chin-Chia Michael Yeh", "Junpeng Wang", "Wei Zhang", "Liang Wang", "Hao Yang", "Jeff Phillips"], "keywords": ["cross-lingual word embeddings", "natural language processing"], "abstract": "Learning a good transfer function to map the word vectors from two languages into a shared cross-lingual word vector space plays a crucial role in cross-lingual NLP. It is useful in translation tasks and important in allowing complex models built on a high-resource language like English to be directly applied on an aligned low resource language.  While Procrustes and other techniques can align language models with some success, it has recently been identified that structural differences (for instance, due to differing word frequency) create different profiles for various monolingual embedding. When these profiles differ across languages, it correlates with how well languages can align and their performance on cross-lingual downstream tasks.  In this work, we develop a very general language embedding normalization procedure, building and subsuming various previous approaches, which removes these structural profiles across languages without destroying their intrinsic meaning.  We demonstrate that meaning is retained and alignment is improved on similarity, translation, and cross-language classification tasks.  Our proposed normalization clearly outperforms all prior approaches like centering and vector normalization on each task and with each alignment approach. ", "pdf": "/pdf/1cdf6da2d049db967f45c6454ba03f40aa1e3849.pdf", "one-sentence_summary": "Our embedding normalization subsumes existing approaches and consistently improves cross-lingual alignment. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "aboagye|normalization_of_language_embeddings_for_crosslingual_alignment", "_bibtex": "@inproceedings{\naboagye2022normalization,\ntitle={Normalization of Language Embeddings for Cross-Lingual Alignment},\nauthor={Prince Osei Aboagye and Jeff Phillips and Yan Zheng and Junpeng Wang and Chin-Chia Michael Yeh and Wei Zhang and Liang Wang and Hao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Nh7CtbyoqV5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "Q76Y7wkiji", "original": "IlaqlIjOWka", "number": 3247, "cdate": 1632875675050, "mdate": null, "ddate": null, "tcdate": 1632875675050, "tmdate": 1697934632249, "tddate": null, "forum": "Q76Y7wkiji", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Boosting the Certified Robustness of L-infinity Distance Nets", "authorids": ["~Bohang_Zhang1", "tlzmybm@gmail.com", "~Di_He1", "~Liwei_Wang1"], "authors": ["Bohang Zhang", "Du Jiang", "Di He", "Liwei Wang"], "keywords": ["Adversarial Robustness", "Certified Defense", "Lipschitz Network"], "abstract": "Recently, Zhang et al. (2021) developed a new neural network architecture based on $\\ell_\\infty$-distance functions, which naturally possesses certified $\\ell_\\infty$ robustness by its construction. Despite the novel design and theoretical foundation, so far the model only achieved comparable performance to conventional networks. In this paper, we make the following two contributions: $\\mathrm{(i)}$ We demonstrate that $\\ell_\\infty$-distance nets enjoy a fundamental advantage in certified robustness over conventional networks (under typical certification approaches); $\\mathrm{(ii)}$ With an improved training process we are able to significantly boost the certified accuracy of $\\ell_\\infty$-distance nets. Our training approach largely alleviates the optimization problem that arose in the previous training scheme, in particular, the unexpected large Lipschitz constant due to the use of a crucial trick called \\textit{$\\ell_p$-relaxation}. The core of our training approach is a novel objective function that combines scaled cross-entropy loss and clipped hinge loss with a decaying mixing coefficient. Experiments show that using the proposed training strategy, the certified accuracy of $\\ell_\\infty$-distance net can be dramatically improved from 33.30% to 40.06% on CIFAR-10 ($\\epsilon=8/255$), meanwhile outperforming other approaches in this area by a large margin. Our results clearly demonstrate the effectiveness and potential of $\\ell_\\infty$-distance net for certified robustness. Codes are available at https://github.com/zbh2047/L_inf-dist-net-v2.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|boosting_the_certified_robustness_of_linfinity_distance_nets", "pdf": "/pdf/64139e9e640cc111e35419ac12e0f222b55ecd9b.pdf", "one-sentence_summary": "We design a new training strategy that significantly boosts the performance of $\\ell_\\infty$-distance nets and establishes new state-of-the-art certified robustness.", "supplementary_material": "/attachment/a9cf8b11614a3670161a19ff40037d41d9549ba2.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.06850/code)", "_bibtex": "@inproceedings{\nzhang2022boosting,\ntitle={Boosting the Certified Robustness of L-infinity Distance Nets},\nauthor={Bohang Zhang and Du Jiang and Di He and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Q76Y7wkiji}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "ZBESeIUB5k", "original": "yKHosQFOYWSJ", "number": 3217, "cdate": 1632875673020, "mdate": null, "ddate": null, "tcdate": 1632875673020, "tmdate": 1697934635432, "tddate": null, "forum": "ZBESeIUB5k", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Stochastic Training is Not Necessary for Generalization", "authorids": ["~Jonas_Geiping1", "~Micah_Goldblum1", "~Phil_Pope1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Micah Goldblum", "Phil Pope", "Michael Moeller", "Tom Goldstein"], "keywords": ["Optimization", "Generalization", "Stochasticity", "SGD", "full-batch", "implicit regularization", "implicit bias"], "abstract": "It is widely believed that the implicit regularization of SGD is fundamental to the impressive generalization behavior we observe in neural networks.  In this work, we demonstrate that non-stochastic full-batch training can achieve comparably strong performance to SGD on CIFAR-10 using modern architectures. To this end, we show that the implicit regularization of SGD can be completely replaced with explicit regularization. Our observations indicate that the perceived difficulty of full-batch training may be the result of its optimization properties and the disproportionate time and effort spent by the ML community tuning optimizers and hyperparameters for small-batch training.", "one-sentence_summary": "Models trained with full-batch gradient descent and explicit regularization can match the generalization performance of models trained with stochastic minibatching.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "geiping|stochastic_training_is_not_necessary_for_generalization", "pdf": "/pdf/63b51516e9311da290bf727bbd4e9376f88c0a48.pdf", "supplementary_material": "/attachment/9c5f0533736ab60237cf287e95c4f57d56c96fe9.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2109.14119/code)", "_bibtex": "@inproceedings{\ngeiping2022stochastic,\ntitle={Stochastic Training is Not Necessary for Generalization},\nauthor={Jonas Geiping and Micah Goldblum and Phil Pope and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZBESeIUB5k}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "7KdAoOsI81C", "original": "PK_r8xKLdzM", "number": 3216, "cdate": 1632875672951, "mdate": null, "ddate": null, "tcdate": 1632875672951, "tmdate": 1676330516832, "tddate": null, "forum": "7KdAoOsI81C", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Transfer RL across Observation Feature Spaces via Model-Based Regularization", "authorids": ["~Yanchao_Sun1", "~Ruijie_Zheng1", "~Xiyao_Wang1", "~Andrew_E_Cohen1", "~Furong_Huang1"], "authors": ["Yanchao Sun", "Ruijie Zheng", "Xiyao Wang", "Andrew E Cohen", "Furong Huang"], "keywords": ["transfer reinforcement learning", "representation learning", "observation space change", "latent dynamics model"], "abstract": "In many reinforcement learning (RL) applications, the observation space is specified by human developers and restricted by physical realizations, and may thus be subject to dramatic changes over time (e.g. increased number of observable features). However, when the observation space changes, the previous policy will likely fail due to the mismatch of input features, and another policy must be trained from scratch, which is inefficient in terms of computation and sample complexity. Following theoretical insights, we propose a novel algorithm which extracts the latent-space dynamics in the source task, and transfers the dynamics model to the target task to use as a model-based regularizer. Our algorithm works for drastic changes of observation space (e.g. from vector-based observation to image-based observation), without any inter-task mapping or any prior knowledge of the target task. Empirical results show that our algorithm significantly improves the efficiency and stability of learning in the target task.", "one-sentence_summary": "We propose a model-based transfer learning algorithm that transfers knowledge across tasks with drastically different observation spaces, without any prior knowledge of the inter-task mapping.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sun|transfer_rl_across_observation_feature_spaces_via_modelbased_regularization", "pdf": "/pdf/8b952d72e26efd12a72d49901beab7dc48d28ce9.pdf", "supplementary_material": "/attachment/dccc4bf7b5ed9bbb43e86e6aa370597dd1891b6c.zip", "_bibtex": "@inproceedings{\nsun2022transfer,\ntitle={Transfer {RL} across Observation Feature Spaces via Model-Based Regularization},\nauthor={Yanchao Sun and Ruijie Zheng and Xiyao Wang and Andrew E Cohen and Furong Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7KdAoOsI81C}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "kR1hC6j48Tp", "original": "a9lwLzccQ8De", "number": 3210, "cdate": 1632875672548, "mdate": null, "ddate": null, "tcdate": 1632875672548, "tmdate": 1697934636122, "tddate": null, "forum": "kR1hC6j48Tp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GATSBI: Generative Adversarial Training for Simulation-Based Inference", "authorids": ["~Poornima_Ramesh1", "~Jan-Matthis_Lueckmann2", "~Jan_Boelts1", "~\u00c1lvaro_Tejero-Cantero1", "~David_S._Greenberg1", "~Pedro_J._Goncalves1", "~Jakob_H._Macke1"], "authors": ["Poornima Ramesh", "Jan-Matthis Lueckmann", "Jan Boelts", "\u00c1lvaro Tejero-Cantero", "David S. Greenberg", "Pedro J. Goncalves", "Jakob H. Macke"], "keywords": ["Machine Learning", "simulation-based inference", "generative adversarial networks", "approximate bayesian computation", "data-driven modelling", "GANs", "SBI", "likelihood-free inference", "implicit models"], "abstract": "Simulation-based inference (SBI) refers to statistical inference on stochastic models for which we can generate samples, but not compute likelihoods.\nLike SBI algorithms, generative adversarial networks (GANs) do not require explicit likelihoods. We study the relationship between SBI and GANs, and introduce GATSBI, an adversarial approach to SBI. GATSBI reformulates the variational objective in an adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, works in high-dimensional posterior spaces and supports implicit priors. We evaluate GATSBI on two common SBI benchmark problems and on two high-dimensional simulators. On a model for wave propagation on the surface of a shallow water body, we show that GATSBI can return well-calibrated posterior estimates even in high dimensions. \nOn a model of camera optics, it infers a high-dimensional posterior given an implicit prior, and performs better than a\nstate-of-the-art SBI approach. We also show how GATSBI can be extended to perform sequential posterior estimation to focus on individual observations.\nOverall, GATSBI opens up opportunities for leveraging advances in GANs to perform Bayesian inference on high-dimensional simulation-based models.", "one-sentence_summary": "Using generative adversarial networks for simulation-based inference", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ramesh|gatsbi_generative_adversarial_training_for_simulationbased_inference", "pdf": "/pdf/7dfeefe58037c9529799e1abf5323514fa066f4e.pdf", "supplementary_material": "/attachment/10d9479b5540c60f2de396ab3347eba4a2780237.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.06481/code)", "_bibtex": "@inproceedings{\nramesh2022gatsbi,\ntitle={{GATSBI}: Generative Adversarial Training for Simulation-Based Inference},\nauthor={Poornima Ramesh and Jan-Matthis Lueckmann and Jan Boelts and {\\'A}lvaro Tejero-Cantero and David S. Greenberg and Pedro J. Goncalves and Jakob H. Macke},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kR1hC6j48Tp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "AwgtcUAhBq", "original": "rbXd3hrJmF5", "number": 3201, "cdate": 1632875671936, "mdate": null, "ddate": null, "tcdate": 1632875671936, "tmdate": 1697934637482, "tddate": null, "forum": "AwgtcUAhBq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Domain Adversarial Training: A Game Perspective", "authorids": ["~David_Acuna1", "~Marc_T_Law1", "~Guojun_Zhang1", "~Sanja_Fidler1"], "authors": ["David Acuna", "Marc T Law", "Guojun Zhang", "Sanja Fidler"], "keywords": ["Domain Adversarial Training", "Domain Adaptation", "Neural Networks Optimization", "Game Theory"], "abstract": "The dominant line of work in domain adaptation has focused on learning invariant representations using domain-adversarial training. In this paper, we interpret this approach from a game theoretical perspective. Defining optimal solutions in domain-adversarial training as a local Nash equilibrium, we show that gradient descent in domain-adversarial training can violate the asymptotic convergence guarantees of the optimizer, oftentimes hindering the transfer performance. Our analysis leads us to replace gradient descent with high-order ODE solvers (i.e., Runge\u2013Kutta), for which we derive asymptotic convergence guarantees. This family of optimizers is significantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers. Our experiments show that in conjunction with state-of-the-art domain-adversarial methods, we achieve up to 3.5% improvement with less than of half training iterations. Our optimizers are easy to implement, free of additional parameters, and can be plugged into any domain-adversarial framework.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "acuna|domain_adversarial_training_a_game_perspective", "pdf": "/pdf/e7f03a4c2a754cc0f8a114d7a24b3d00814adfcd.pdf", "one-sentence_summary": "A novel perspective on domain-adversarial training that leads to more stable and performant optimizers.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.05352/code)", "_bibtex": "@inproceedings{\nacuna2022domain,\ntitle={Domain Adversarial Training: A Game Perspective},\nauthor={David Acuna and Marc T Law and Guojun Zhang and Sanja Fidler},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AwgtcUAhBq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "MXdFBmHT4C", "original": "xCST9FIRCys", "number": 3197, "cdate": 1632875671657, "mdate": null, "ddate": null, "tcdate": 1632875671657, "tmdate": 1676330518198, "tddate": null, "forum": "MXdFBmHT4C", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Differentiable Expectation-Maximization for Set Representation Learning", "authorids": ["~Minyoung_Kim2"], "authors": ["Minyoung Kim"], "keywords": ["Representation learning", "Bayesian models", "Mixture estimation", "Optimal transport", "Attention"], "abstract": "We tackle the set2vec problem, the task of extracting a vector representation from an input set comprised of a variable number of feature vectors. Although recent approaches based on self attention such as (Set)Transformers were very successful due to the capability of capturing complex interaction between set elements, the  computational overhead is the well-known downside. The inducing-point attention and the latest optimal transport kernel embedding (OTKE) are promising remedies that attain comparable or better performance with reduced computational cost, by incorporating a fixed number of learnable queries in attention. In this paper we approach the set2vec problem from a completely different perspective. The elements of an input set are considered as i.i.d.~samples from a mixture distribution, and we define our set embedding feed-forward network as the  maximum-a-posterior (MAP) estimate of the mixture which is approximately attained by a few Expectation-Maximization (EM) steps. The whole MAP-EM steps are differentiable operations with a fixed number of mixture parameters, allowing efficient auto-diff back-propagation for any given downstream task. Furthermore, the proposed mixture set data fitting framework allows unsupervised set representation learning naturally via marginal likelihood maximization aka the empirical Bayes. Interestingly, we also find that OTKE can be seen as a special case of our framework, specifically a single-step EM with extra balanced assignment constraints on the E-step. Compared to OTKE, our approach provides more flexible set embedding as well as prior-induced model regularization. We evaluate our approach on various tasks demonstrating improved performance over the state-of-the-arts.", "one-sentence_summary": "We propose a novel set embedding function, a feed-forward network defined as the (differentiable) maximum-a-posterior estimate of the mixture, approximately attained by a few Expectation-Maximization steps.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kim|differentiable_expectationmaximization_for_set_representation_learning", "pdf": "/pdf/7042a234575b76e9959702f6b853e880419fa525.pdf", "supplementary_material": "/attachment/52d44551cdcc3e5908885e2fc289003de778c2cd.zip", "data": "", "_bibtex": "@inproceedings{\nkim2022differentiable,\ntitle={Differentiable Expectation-Maximization for Set Representation Learning},\nauthor={Minyoung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MXdFBmHT4C}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "vIC-xLFuM6", "original": "mO4TjY50PQ-", "number": 3181, "cdate": 1632875670706, "mdate": null, "ddate": null, "tcdate": 1632875670706, "tmdate": 1697934638873, "tddate": null, "forum": "vIC-xLFuM6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Overcoming The Spectral Bias of Neural Value Approximation", "authorids": ["~Ge_Yang1", "~Anurag_Ajay1", "~Pulkit_Agrawal1"], "authors": ["Ge Yang", "Anurag Ajay", "Pulkit Agrawal"], "keywords": ["spectral bias", "neural value approximation", "Q learning", "reinforcement learning", "neural tangent kernels", "kernel regression"], "abstract": "Value approximation using deep neural networks is at the heart of off-policy deep reinforcement learning, and is often the primary module that provides learning signals to the rest of the algorithm.  While multi-layer perceptron networks are universal function approximators, recent works in neural kernel regression suggest the presence of a \\textit{spectral bias}, where fitting high-frequency components of the value function requires exponentially more gradient update steps than the low-frequency ones. In this work, we re-examine off-policy reinforcement learning through the lens of kernel regression and propose to overcome such bias via a composite neural tangent kernel. With just a single line-change, our approach, the Fourier feature networks (FFN) produce state-of-the-art performance on challenging continuous control domains with only a fraction of the compute. Faster convergence and better off-policy stability also make it possible to remove the target network without suffering catastrophic divergences, which further reduces TD(0)'s estimation bias on a few tasks. Code and analysis available at https://geyang.github.io/ffn.", "one-sentence_summary": "Overcoming the spectral bias of neural value approximation via fourier features", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|overcoming_the_spectral_bias_of_neural_value_approximation", "pdf": "/pdf/aaba9521b9c66ad9d524bcc064cd282ae41b5137.pdf", "supplementary_material": "/attachment/906aecf6951719108ef78b5579857b00d2aef206.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2206.04672/code)", "_bibtex": "@inproceedings{\nyang2022overcoming,\ntitle={Overcoming The Spectral Bias of Neural Value Approximation},\nauthor={Ge Yang and Anurag Ajay and Pulkit Agrawal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vIC-xLFuM6}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "AIgn9uwfcD1", "original": "71-Y24dbia6", "number": 3166, "cdate": 1632875669825, "mdate": null, "ddate": null, "tcdate": 1632875669825, "tmdate": 1676330519358, "tddate": null, "forum": "AIgn9uwfcD1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients", "authorids": ["~Milad_Alizadeh1", "~Shyam_A._Tailor1", "~Luisa_M_Zintgraf1", "~Joost_van_Amersfoort1", "~Sebastian_Farquhar1", "~Nicholas_Donald_Lane1", "~Yarin_Gal1"], "authors": ["Milad Alizadeh", "Shyam A. Tailor", "Luisa M Zintgraf", "Joost van Amersfoort", "Sebastian Farquhar", "Nicholas Donald Lane", "Yarin Gal"], "keywords": ["pruning", "lottery ticket hypothesis", "pruning at initialization"], "abstract": "Pruning neural networks at initialization would enable us to find sparse models that retain the accuracy of the original network while consuming fewer computational resources for training and inference. However, current methods are insufficient to enable this optimization and lead to a large degradation in model performance. In this paper, we identify a fundamental limitation in the formulation of current methods, namely that their saliency criteria look at a single step at the start of training without taking into account the trainability of the network. While pruning iteratively and gradually has been shown to improve pruning performance, explicit consideration of the training stage that will immediately follow pruning has so far been absent from the computation of the saliency criterion. To overcome the short-sightedness of existing methods, we propose Prospect Pruning (ProsPr), which uses meta-gradients through the first few steps of optimization to determine which weights to prune. ProsPr combines an estimate of the higher-order effects of pruning on the loss and the optimization trajectory to identify the trainable sub-network. Our method achieves state-of-the-art pruning performance on a variety of vision classification tasks, with less data and in a single shot compared to existing pruning-at-initialization methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "alizadeh|prospect_pruning_finding_trainable_weights_at_initialization_using_metagradients", "pdf": "/pdf/1dfea35c91657067e141f26d76aa2c2c7e96bcc6.pdf", "one-sentence_summary": "We use meta-gradients to prune neural networks at initialization based on \"trainability\" of weights instead of their impact on the loss at a single step.", "_bibtex": "@inproceedings{\nalizadeh2022prospect,\ntitle={Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients},\nauthor={Milad Alizadeh and Shyam A. Tailor and Luisa M Zintgraf and Joost van Amersfoort and Sebastian Farquhar and Nicholas Donald Lane and Yarin Gal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AIgn9uwfcD1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "PVJ6j87gOHz", "original": "lwuwhaHRGCGk", "number": 3155, "cdate": 1632875669147, "mdate": null, "ddate": null, "tcdate": 1632875669147, "tmdate": 1676330519715, "tddate": null, "forum": "PVJ6j87gOHz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CoMPS: Continual Meta Policy Search", "authorids": ["~Glen_Berseth1", "~Zhiwei_Zhang5", "grace.zhang@berkeley.edu", "~Chelsea_Finn1", "~Sergey_Levine1"], "authors": ["Glen Berseth", "Zhiwei Zhang", "Grace Zhang", "Chelsea Finn", "Sergey Levine"], "keywords": ["Reinforcement Learning"], "abstract": "We develop a new continual meta-learning method to address challenges in sequential multi-task learning. In this setting, the agent's goal is to achieve high reward over any sequence of tasks quickly. Prior meta-reinforcement learning algorithms have demonstrated promising results in accelerating the acquisition of new tasks. However, they require access to all tasks during training. Beyond simply transferring past experience to new tasks, our goal is to devise continual reinforcement learning algorithms that learn to learn, using their experience on previous tasks to learn new tasks more quickly. We introduce a new method, continual meta-policy search (CoMPS), that removes this limitation by meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta-learning to prepare for subsequent task learning. We find that CoMPS outperforms prior continual learning and off-policy meta-reinforcement methods on several sequences of challenging continuous control tasks.", "one-sentence_summary": "Cotinual meta-reinforcement learning accelerates task learning, via repeated meta off-policy search.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "berseth|comps_continual_meta_policy_search", "pdf": "/pdf/3ca19a998e5306a18adb13bd58c7b521611ff1f2.pdf", "_bibtex": "@inproceedings{\nberseth2022comps,\ntitle={Co{MPS}: Continual Meta Policy Search},\nauthor={Glen Berseth and Zhiwei Zhang and Grace Zhang and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PVJ6j87gOHz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 37}}, {"id": "ziRLU3Y2PN_", "original": "biFEM8KoWC", "number": 3141, "cdate": 1632875668206, "mdate": null, "ddate": null, "tcdate": 1632875668206, "tmdate": 1697934642038, "tddate": null, "forum": "ziRLU3Y2PN_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalized rectifier wavelet covariance models for texture synthesis", "authorids": ["~Antoine_Brochard1", "~Sixin_Zhang2", "~St\u00e9phane_Mallat1"], "authors": ["Antoine Brochard", "Sixin Zhang", "St\u00e9phane Mallat"], "keywords": ["texture synthesis", "generative models", "wavelets"], "abstract": "State-of-the-art maximum entropy models for texture synthesis are built from statistics relying on image representations defined by convolutional neural networks (CNN). Such representations capture rich structures in texture images, outperforming wavelet-based representations in this regard. However, conversely to neural networks, wavelets offer meaningful representations, as they are known to detect structures at multiple scales (e.g. edges) in images. In this work, we propose a family of statistics built upon non-linear wavelet based representations, that can be viewed as a particular instance of a one-layer CNN, using a generalized rectifier non-linearity. These statistics significantly improve the visual quality of previous classical wavelet-based models, and allow one to produce syntheses of similar quality to state-of-the-art models, on both gray-scale and color textures. We further provide insights on memorization effects in these models. \n", "pdf": "/pdf/db6c8615f4a7a3851946620b267f795166d259df.pdf", "one-sentence_summary": "This paper presents a model for texture synthesis, built on a wavelet-based representation of images.", "supplementary_material": "/attachment/7e21ac9cd6b43093299dc7d2417cf78f7adc2ae2.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "brochard|generalized_rectifier_wavelet_covariance_models_for_texture_synthesis", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2203.07902/code)", "_bibtex": "@inproceedings{\nbrochard2022generalized,\ntitle={Generalized rectifier wavelet covariance models for texture synthesis},\nauthor={Antoine Brochard and Sixin Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ziRLU3Y2PN_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "_5js_8uTrx1", "original": "xY9fx8HcKyXb", "number": 3133, "cdate": 1632875667675, "mdate": null, "ddate": null, "tcdate": 1632875667675, "tmdate": 1676330520851, "tddate": null, "forum": "_5js_8uTrx1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Evaluating the Robustness of Neural Networks Learned by Transduction", "authorids": ["~Jiefeng_Chen2", "~Xi_Wu1", "~Yang_Guo4", "~Yingyu_Liang1", "~Somesh_Jha1"], "authors": ["Jiefeng Chen", "Xi Wu", "Yang Guo", "Yingyu Liang", "Somesh Jha"], "keywords": ["adversarial robustness", "transductive learning", "test-time defense", "dynamic defense", "attacking model spaces"], "abstract": "There has been emerging interest in using transductive learning for adversarial robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020; Wang et al., ArXiv 2021). Compared to traditional defenses, these defense mechanisms \"dynamically learn\" the model based on test-time input; and theoretically, attacking these defenses reduces to solving a bilevel optimization problem, which poses difficulty in crafting adaptive attacks. In this paper, we examine these defense mechanisms from a principled threat analysis perspective. We formulate and analyze threat models for transductive-learning based defenses, and point out important subtleties. We propose the principle of attacking model space for solving bilevel attack objectives, and present Greedy Model Space Attack (GMSA), an attack framework that can serve as a new baseline for evaluating transductive-learning based defenses. Through systematic evaluation, we show that GMSA, even with weak instantiations, can break previous transductive-learning based defenses, which were resilient to previous attacks, such as AutoAttack (Croce and Hein, ICML 2020). On the positive side, we report a somewhat surprising empirical result of \"transductive adversarial training\": Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks we consider.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|towards_evaluating_the_robustness_of_neural_networks_learned_by_transduction", "pdf": "/pdf/3ca51d07268ca31c7f5ccb9d049d08f06a99f051.pdf", "one-sentence_summary": "Exploring evaluating the adversarial robustness of transductive-learning based defenses. ", "supplementary_material": "/attachment/e8d0e1b51f2f9e01fb2986f13326f11fa0b11c05.zip", "_bibtex": "@inproceedings{\nchen2022towards,\ntitle={Towards Evaluating the Robustness of Neural Networks Learned by Transduction},\nauthor={Jiefeng Chen and Xi Wu and Yang Guo and Yingyu Liang and Somesh Jha},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_5js_8uTrx1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "oJGDYQFKL3i", "original": "1iWoD04yVZU", "number": 3129, "cdate": 1632875667416, "mdate": null, "ddate": null, "tcdate": 1632875667416, "tmdate": 1676330520947, "tddate": null, "forum": "oJGDYQFKL3i", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "OBJECT DYNAMICS DISTILLATION FOR SCENE DECOMPOSITION AND REPRESENTATION", "authorids": ["~Qu_Tang1", "~Xiangyu_Zhu3", "~Zhen_Lei2", "~Zhaoxiang_Zhang3"], "authors": ["Qu Tang", "Xiangyu Zhu", "Zhen Lei", "Zhaoxiang Zhang"], "keywords": [], "abstract": "The ability to perceive scenes in terms of abstract entities is crucial for us to\nachieve higher-level intelligence. Recently, several methods have been proposed\nto learn object-centric representations of scenes with multiple objects, yet most\nof which focus on static scenes. In this paper, we work on object dynamics and\npropose Object Dynamics Distillation Network (ODDN), a framework that distillates explicit object dynamics (e.g., velocity) from sequential static representations. ODDN also builds a relation module to model object interactions. We verify\nour approach on tasks of video reasoning and video prediction, which are two important evaluations for video understanding. The results show that the reasoning\nmodel with visual representations of ODDN performs better in answering reasoning questions around physical events in a video compared to the previous state-of-the-art methods. The distilled object dynamics also could be used to predict\nfuture video frames given two input frames, involving occlusion and objects collision. In addition, our architecture brings better segmentation quality and higher\nreconstruction accuracy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tang|object_dynamics_distillation_for_scene_decomposition_and_representation", "pdf": "/pdf/0ad29c6ff610a0ba752b90b3599e1008193b012f.pdf", "_bibtex": "@inproceedings{\ntang2022object,\ntitle={{OBJECT} {DYNAMICS} {DISTILLATION} {FOR} {SCENE} {DECOMPOSITION} {AND} {REPRESENTATION}},\nauthor={Qu Tang and Xiangyu Zhu and Zhen Lei and Zhaoxiang Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oJGDYQFKL3i}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "NlObxR0rosG", "original": "9kWuJLnPRJE3", "number": 3128, "cdate": 1632875667353, "mdate": null, "ddate": null, "tcdate": 1632875667353, "tmdate": 1676330521050, "tddate": null, "forum": "NlObxR0rosG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Practical Integration via Separable Bijective Networks", "authorids": ["~Christopher_M_Bender1", "~Patrick_Emmanuel1", "~Michael_K._Reiter1", "~Junier_Oliva1"], "authors": ["Christopher M Bender", "Patrick Emmanuel", "Michael K. Reiter", "Junier Oliva"], "keywords": ["integration", "flow", "likelihood", "classification", "regression", "out of distribution", "regularization"], "abstract": "Neural networks have enabled learning over examples that contain thousands of dimensions.\nHowever, most of these models are limited to training and evaluating on a finite collection of \\textit{points} and do not consider the hypervolume in which the data resides.\nAny analysis of the model's local or global behavior is therefore limited to very expensive or imprecise estimators.\nWe propose to formulate neural networks as a composition of a bijective (flow) network followed by a learnable, separable network.\nThis construction allows for learning (or assessing) over full hypervolumes with precise estimators at tractable computational cost via integration over the \\textit{input space}.\nWe develop the necessary machinery, propose several practical integrals to use during training, and demonstrate their utility.", "one-sentence_summary": "We explore a method that enables learning over hypervolumes within the data space.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bender|practical_integration_via_separable_bijective_networks", "pdf": "/pdf/95fa2183421df6d966e9da9063e2c757245f7d49.pdf", "data": "", "_bibtex": "@inproceedings{\nbender2022practical,\ntitle={Practical Integration via Separable Bijective Networks},\nauthor={Christopher M Bender and Patrick Emmanuel and Michael K. Reiter and Junier Oliva},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NlObxR0rosG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "zuqcmNVK4c2", "original": "LC1zzwgqdrA", "number": 3122, "cdate": 1632875666960, "mdate": null, "ddate": null, "tcdate": 1632875666960, "tmdate": 1676330521478, "tddate": null, "forum": "zuqcmNVK4c2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Self-Joint Supervised Learning", "authorids": ["~Navid_Kardan1", "~Mubarak_Shah3", "~Mitch_Hill1"], "authors": ["Navid Kardan", "Mubarak Shah", "Mitch Hill"], "keywords": [], "abstract": "Supervised learning is a fundamental framework used to train machine learning systems. A supervised learning problem is often formulated using an i.i.d. assumption that restricts model attention to a single relevant signal at a time when predicting. This contrasts with the human ability to actively use related samples as reference when making decisions. We hypothesize that the restriction to a single signal for each prediction in the standard i.i.d. framework contributes to well-known drawbacks of supervised learning: making overconfident predictions and vulnerability to overfitting, adversarial attacks, and out-of-distribution data. To address these limitations, we propose a new supervised learning paradigm called self-joint learning that generalizes the standard approach by modeling the joint conditional distribution of two observed samples, where each sample is an image and its label. Rather than assuming samples are independent, our models explicitly learn the sample-to-sample relation of conditional independence. Our framework can naturally incorporate auxiliary unlabeled data to further improve the performance. Experiments on benchmark image datasets show our method offers significant improvement over standard supervised learning in terms of accuracy, robustness against adversarial attacks, out-of-distribution detection, and overconfidence mitigation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kardan|selfjoint_supervised_learning", "pdf": "/pdf/960211bfb6a9d646a3fe04a45ebc7a4670bbe0ec.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nkardan2022selfjoint,\ntitle={Self-Joint Supervised Learning},\nauthor={Navid Kardan and Mubarak Shah and Mitch Hill},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zuqcmNVK4c2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "Jjcv9MTqhcq", "original": "6mZzBOouffjK", "number": 3117, "cdate": 1632875666696, "mdate": null, "ddate": null, "tcdate": 1632875666696, "tmdate": 1697934645333, "tddate": null, "forum": "Jjcv9MTqhcq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Rethinking Supervised Pre-Training for Better Downstream Transferring", "authorids": ["~Yutong_Feng2", "~Jianwen_Jiang2", "~Mingqian_Tang1", "~Rong_Jin1", "~Yue_Gao4"], "authors": ["Yutong Feng", "Jianwen Jiang", "Mingqian Tang", "Rong Jin", "Yue Gao"], "keywords": ["Pre-Training", "Contrastive Learning", "Representation Learning", "Downstream Transferring"], "abstract": "The pretrain-finetune paradigm has shown outstanding performance on many applications of deep learning, where a model is pre-trained on an upstream large dataset (e.g. ImageNet), and is then fine-tuned to different downstream tasks. Though for most cases, the pre-training stage is conducted based on supervised methods, recent works on self-supervised pre-training have shown powerful transferability and even outperform supervised pre-training on multiple downstream tasks. It thus remains an open question how to better generalize supervised pre- training model to downstream tasks. In this paper, we argue that the worse transferability of existing supervised pre-training methods arise from the negligence of valuable intra-class semantic difference. This is because these methods tend to push images from the same class close to each other despite of the large diversity in their visual contents, a problem to which referred as \u201coverfit of upstream tasks\u201d. To alleviate this problem, we propose a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and consequentially preserving part of intra-class difference for better transferring to downstream tasks. We developed efficient implementation of the proposed method that scales well to large datasets. Experimental studies on multiple downstream tasks show that LOOK outperforms other state-of-the-art methods for supervised and self-supervised pre-training.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "feng|rethinking_supervised_pretraining_for_better_downstream_transferring", "pdf": "/pdf/a343ccb79799f29dc7462f132c0c658e9714c2b3.pdf", "one-sentence_summary": "We propose a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, which relieves the problem of overfitting upstream tasks  and preserving part of intra-class difference for better transferring to downstream tasks.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.06014/code)", "_bibtex": "@inproceedings{\nfeng2022rethinking,\ntitle={Rethinking Supervised Pre-Training for Better Downstream Transferring},\nauthor={Yutong Feng and Jianwen Jiang and Mingqian Tang and Rong Jin and Yue Gao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Jjcv9MTqhcq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "OUz_9TiTv9j", "original": "Lt_QatLxlAU", "number": 3112, "cdate": 1632875666368, "mdate": null, "ddate": null, "tcdate": 1632875666368, "tmdate": 1676330522231, "tddate": null, "forum": "OUz_9TiTv9j", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Zest of LIME: Towards Architecture-Independent Model Distances", "authorids": ["~Hengrui_Jia1", "hy.chen@mail.utoronto.ca", "jonas@cs.toronto.edu", "~Ali_Shahin_Shamsabadi1", "~Nicolas_Papernot1"], "authors": ["Hengrui Jia", "Hongyu Chen", "Jonas Guan", "Ali Shahin Shamsabadi", "Nicolas Papernot"], "keywords": ["model distance", "model stealing", "machine unlearning", "fairwashing"], "abstract": "Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights. While similarity of weights is attractive because it implies similarity of predictions in the limit, it suffers from being inapplicable to comparing models with different architectures. On the other hand, the similarity of predictions is broadly applicable but depends heavily on the choice of model inputs during comparison. In this paper, we instead propose to compute distance between black-box models by comparing their Local Interpretable Model-Agnostic Explanations (LIME). To compare two models, we take a reference dataset, and locally approximate the models on each reference point with linear models trained by LIME. We then compute the cosine distance between the concatenated weights of the linear models. This yields an approach that is both architecture-independent and possesses the benefits of comparing models in weight space. We empirically show that our method, which we call Zest, can be applied to two problems that require measurements of model similarity: detecting model stealing and machine unlearning.", "one-sentence_summary": "We propose an architecture-independent distance metric that measures the similarity between ML models by comparing their global behaviors, approximated using LIME.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jia|a_zest_of_lime_towards_architectureindependent_model_distances", "pdf": "/pdf/01c303fb27c1a38f990545d60c831f0fa8f0f2b9.pdf", "data": "", "_bibtex": "@inproceedings{\njia2022a,\ntitle={A Zest of {LIME}: Towards Architecture-Independent Model Distances},\nauthor={Hengrui Jia and Hongyu Chen and Jonas Guan and Ali Shahin Shamsabadi and Nicolas Papernot},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OUz_9TiTv9j}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "KTPuIsx4pmo", "original": "zwsQy6Yxjmln", "number": 3104, "cdate": 1632875665966, "mdate": null, "ddate": null, "tcdate": 1632875665966, "tmdate": 1676330522440, "tddate": null, "forum": "KTPuIsx4pmo", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Meta-Imitation Learning by Watching Video Demonstrations", "authorids": ["~Jiayi_Li1", "~Tao_Lu6", "caoxiaoge2020@ia.ac.cn", "yinghao.cai@ia.ac.cn", "shuo.wang@ia.ac.cn"], "authors": ["Jiayi Li", "Tao Lu", "Xiaoge Cao", "Yinghao Cai", "Shuo Wang"], "keywords": ["Meta-imitation Learning", "One-shot Learning", "Learning by Watching", "Generative Adversarial Networks"], "abstract": "Meta-Imitation Learning is a promising technique for the robot to learn a new task from observing one or a few human demonstrations. However, it usually requires a significant number of demonstrations both from humans and robots during the meta-training phase, which is a laborious and hard work for data collection, especially in recording the actions and specifying the correspondence between human and robot. In this work, we present an approach of meta-imitation learning by watching video demonstrations from humans. In comparison to prior works, our approach is able to translate human videos into practical robot demonstrations and train the meta-policy with adaptive loss based on the quality of the translated data. Our approach relies only on human videos and does not require robot demonstration, which facilitates data collection and is more in line with human imitation behavior. Experiments reveal that our method achieves the comparable performance to the baseline on fast learning a set of vision-based tasks through watching a single video demonstration.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|metaimitation_learning_by_watching_video_demonstrations", "pdf": "/pdf/c676abf58096dad39810f6901eb441d104797d02.pdf", "one-sentence_summary": "We present an approach of meta-imitation learning by watching video demonstrations from humans.", "supplementary_material": "/attachment/7b23ff088793f0676ba938e53d1a26e1aebaf1c9.zip", "_bibtex": "@inproceedings{\nli2022metaimitation,\ntitle={Meta-Imitation Learning by Watching Video Demonstrations},\nauthor={Jiayi Li and Tao Lu and Xiaoge Cao and Yinghao Cai and Shuo Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KTPuIsx4pmo}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "6ET9SzlgNX", "original": "dBJIHMz8GFVm", "number": 3096, "cdate": 1632875665429, "mdate": null, "ddate": null, "tcdate": 1632875665429, "tmdate": 1676330523070, "tddate": null, "forum": "6ET9SzlgNX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding Intrinsic Robustness Using Label Uncertainty", "authorids": ["~Xiao_Zhang2", "~David_Evans1"], "authors": ["Xiao Zhang", "David Evans"], "keywords": ["Concentration of Measure", "Intrinsic Adversarial Robustness", "Label Uncertainty"], "abstract": "A fundamental question in adversarial machine learning is whether a robust classifier exists for a given task. A line of research has made some progress towards this goal by studying the concentration of measure, but we argue standard concentration fails to fully characterize the intrinsic robustness of a classification problem since it ignores data labels which are essential to any classification task. Building on a novel definition of label uncertainty, we empirically demonstrate that error regions induced by state-of-the-art models tend to have much higher label uncertainty than randomly-selected subsets. This observation motivates us to adapt a concentration estimation algorithm to account for label uncertainty, resulting in more accurate intrinsic robustness measures for benchmark image classification problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|understanding_intrinsic_robustness_using_label_uncertainty", "pdf": "/pdf/7fac6994dd2b9e6641f9bacef039b34d4d5dd804.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nzhang2022understanding,\ntitle={Understanding Intrinsic Robustness Using Label Uncertainty},\nauthor={Xiao Zhang and David Evans},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6ET9SzlgNX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "_QLmakITKg", "original": "WzQTNb6BUIL", "number": 3094, "cdate": 1632875665292, "mdate": null, "ddate": null, "tcdate": 1632875665292, "tmdate": 1697934648307, "tddate": null, "forum": "_QLmakITKg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization", "authorids": ["~Junyuan_Hong1", "~Haotao_Wang1", "~Zhangyang_Wang1", "~Jiayu_Zhou1"], "authors": ["Junyuan Hong", "Haotao Wang", "Zhangyang Wang", "Jiayu Zhou"], "keywords": ["federated learning"], "abstract": "Federated learning (FL) provides a distributed learning framework for multiple participants to collaborate learning without sharing raw data. In many practical FL scenarios, participants have heterogeneous resources due to disparities in hardware and inference dynamics that require quickly loading models of different sizes and levels of robustness. The heterogeneity and dynamics together impose significant challenges to existing FL approaches and thus greatly limit FL's applicability. In this paper, we propose a novel Split-Mix FL strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness. Specifically, we achieve customization by learning a set of base sub-networks of different sizes and robustness levels, which are later aggregated on-demand according to inference requirements. This split-mix strategy achieves customization with high efficiency in communication, storage, and inference. Extensive experiments demonstrate that our method provides better in-situ customization than the existing heterogeneous-architecture FL methods. Codes and pre-trained models are available: https://github.com/illidanlab/SplitMix.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hong|efficient_splitmix_federated_learning_for_ondemand_and_insitu_customization", "pdf": "/pdf/98ee807a90553d41affe59b4484a0754e4bdb5d7.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.09747/code)", "_bibtex": "@inproceedings{\nhong2022efficient,\ntitle={Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization},\nauthor={Junyuan Hong and Haotao Wang and Zhangyang Wang and Jiayu Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_QLmakITKg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "RXQ-FPbQYVn", "original": "BmuK8bkohr", "number": 3083, "cdate": 1632875664558, "mdate": null, "ddate": null, "tcdate": 1632875664558, "tmdate": 1697934649445, "tddate": null, "forum": "RXQ-FPbQYVn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Anti-Concentrated Confidence Bonuses For Scalable Exploration", "authorids": ["~Jordan_T._Ash1", "~Cyril_Zhang1", "~Surbhi_Goel1", "~Akshay_Krishnamurthy1", "~Sham_M._Kakade1"], "authors": ["Jordan T. Ash", "Cyril Zhang", "Surbhi Goel", "Akshay Krishnamurthy", "Sham M. Kakade"], "keywords": ["deep reinforcement learning", "reinforcement learning", "bandits", "exploration"], "abstract": "Intrinsic rewards play a central role in handling the exploration-exploitation tradeoff when designing sequential decision-making algorithms, in both foundational theory and state-of-the-art deep reinforcement learning. The LinUCB algorithm, a centerpiece of the stochastic linear bandits literature, prescribes an elliptical bonus which addresses the challenge of leveraging shared information in large action spaces. This bonus scheme cannot be directly transferred to high-dimensional exploration problems, however, due to the computational cost of maintaining the inverse covariance matrix of action features. We introduce anti-concentrated confidence bounds for efficiently approximating the elliptical bonus, using an ensemble of regressors trained to predict random noise from policy network-derived features. Using this approximation, we obtain stochastic linear bandit algorithms which obtain $\\tilde O(d \\sqrt{T})$ regret bounds for $\\mathsf{poly}(d)$ fixed actions. We develop a practical variant that is competitive with contemporary intrinsic reward heuristics on Atari benchmarks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ash|anticoncentrated_confidence_bonuses_for_scalable_exploration", "pdf": "/pdf/d7951ff75473361f77560f2a0e4763704e2580cd.pdf", "supplementary_material": "/attachment/cc029b6b133f3bddce88b90804802c6b76f479f2.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.11202/code)", "_bibtex": "@inproceedings{\nash2022anticoncentrated,\ntitle={Anti-Concentrated Confidence Bonuses For Scalable Exploration},\nauthor={Jordan T. Ash and Cyril Zhang and Surbhi Goel and Akshay Krishnamurthy and Sham M. Kakade},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RXQ-FPbQYVn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "5-2mX9_U5i", "original": "fdfnw0SzVI", "number": 3082, "cdate": 1632875664491, "mdate": null, "ddate": null, "tcdate": 1632875664491, "tmdate": 1676330523723, "tddate": null, "forum": "5-2mX9_U5i", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sqrt(d) Dimension Dependence of Langevin Monte Carlo", "authorids": ["~Ruilin_Li1", "~Hongyuan_Zha1", "~Molei_Tao1"], "authors": ["Ruilin Li", "Hongyuan Zha", "Molei Tao"], "keywords": ["unadjusted Langevin algorithm / Langevin Monte Carlo", "non-asymptotic sampling error in Wasserstein-2 distance", "optimal dimension dependence", "mean square analysis"], "abstract": "This article considers the popular MCMC method of unadjusted Langevin Monte Carlo (LMC) and provides a non-asymptotic analysis of its sampling error in 2-Wasserstein distance. The proof is based on a refinement of mean-square analysis in Li et al. (2019), and this refined framework automates the analysis of a large class of sampling algorithms based on discretizations of contractive SDEs. Using this framework, we establish an $\\tilde{O}(\\sqrt{d}/\\epsilon)$ mixing time bound for LMC, without warm start, under the common log-smooth and log-strongly-convex conditions, plus a growth condition on the 3rd-order derivative of the potential of target measures. This bound improves the best previously known $\\tilde{O}(d/\\epsilon)$ result and is optimal (in terms of order) in both dimension $d$ and accuracy tolerance $\\epsilon$ for target measures satisfying the aforementioned assumptions. Our theoretical analysis is further validated by numerical experiments.", "one-sentence_summary": "The known dimension dependence of LMC is improved, under regularity assumptions, from d to sqrt(d), based on a refined mean square analysis framework.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|sqrtd_dimension_dependence_of_langevin_monte_carlo", "pdf": "/pdf/e0edbcfc68c6fe7fb5ce570f96df1a072918442e.pdf", "supplementary_material": "/attachment/521f7ed39d0e85dd0af2fa732a35179f57a82703.zip", "_bibtex": "@inproceedings{\nli2022sqrtd,\ntitle={Sqrt(d) Dimension Dependence of Langevin Monte Carlo},\nauthor={Ruilin Li and Hongyuan Zha and Molei Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5-2mX9_U5i}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "dZPgfwaTaXv", "original": "-VXHe6_23nh", "number": 3071, "cdate": 1632875663759, "mdate": null, "ddate": null, "tcdate": 1632875663759, "tmdate": 1697934649979, "tddate": null, "forum": "dZPgfwaTaXv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Relational Surrogate Loss Learning", "authorids": ["~Tao_Huang5", "~Zekang_Li1", "~Hua_Lu2", "~Yong_Shan1", "~Shusheng_Yang1", "~Yang_Feng4", "~Fei_Wang9", "~Shan_You3", "~Chang_Xu4"], "authors": ["Tao Huang", "Zekang Li", "Hua Lu", "Yong Shan", "Shusheng Yang", "Yang Feng", "Fei Wang", "Shan You", "Chang Xu"], "keywords": [], "abstract": "Evaluation metrics in machine learning are often hardly taken as loss functions, as they could be non-differentiable and non-decomposable, e.g., average precision and F1 score. This paper aims to address this problem by revisiting the surrogate loss learning, where a deep neural network is employed to approximate the evaluation metrics. Instead of pursuing an exact recovery of the evaluation metric through a deep neural network, we are reminded of the purpose of the existence of these evaluation metrics, which is to distinguish whether one model is better or worse than another. In this paper, we show that directly maintaining the relation of models between surrogate losses and metrics suffices, and propose a rank correlation-based optimization method to maximize this relation and learn surrogate losses. Compared to previous works, our method is much easier to optimize and enjoys significant efficiency and performance gains. Extensive experiments show that our method achieves improvements on various tasks including image classification and neural machine translation, and even outperforms state-of-the-art methods on human pose estimation and machine reading comprehension tasks. Code is available at: https://github.com/hunto/ReLoss.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|relational_surrogate_loss_learning", "pdf": "/pdf/da75d64e0685a0086938d2d63a0b5ef70e48324a.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2202.13197/code)", "_bibtex": "@inproceedings{\nhuang2022relational,\ntitle={Relational Surrogate Loss Learning},\nauthor={Tao Huang and Zekang Li and Hua Lu and Yong Shan and Shusheng Yang and Yang Feng and Fei Wang and Shan You and Chang Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dZPgfwaTaXv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "fy_XRVHqly", "original": "smgSMhDjGbxI", "number": 3068, "cdate": 1632875663562, "mdate": null, "ddate": null, "tcdate": 1632875663562, "tmdate": 1676330524278, "tddate": null, "forum": "fy_XRVHqly", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning", "authorids": ["~Sunghoon_Hong2", "~Deunsol_Yoon1", "~Kee-Eung_Kim2"], "authors": ["Sunghoon Hong", "Deunsol Yoon", "Kee-Eung Kim"], "keywords": ["Multitask Reinforcement Learning", "Modular Reinforcement Learning", "Transfer Learning", "Transformer", "Structural Embedding"], "abstract": "Modular Reinforcement Learning, where the agent is assumed to be morphologically structured as a graph, for example composed of limbs and joints, aims to learn a policy that is transferable to a structurally similar but different agent. Compared to traditional Multi-Task Reinforcement Learning, this promising approach allows us to cope with inhomogeneous tasks where the state and action space dimensions differ across tasks. Graph Neural Networks are a natural model for representing the pertinent policies, but a recent work has shown that their multi-hop message passing mechanism is not ideal for conveying important information to other modules and thus a transformer model without morphological information was proposed. In this work, we argue that the morphological information is still very useful and propose a transformer policy model that effectively encodes such information. Specifically, we encode the morphological information in terms of the traversal-based positional embedding and the graph-based relational embedding. We empirically show that the morphological information is crucial for modular reinforcement learning, substantially outperforming prior state-of-the-art methods on multi-task learning as well as transfer learning settings with different state and action space dimensions.", "one-sentence_summary": "We present a modular Multi-task Reinforcement Learning method for inhomogeneous control tasks incorporating structural embedding of morphology.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hong|structureaware_transformer_policy_for_inhomogeneous_multitask_reinforcement_learning", "pdf": "/pdf/111d5058b0200075159b27c0969addf7f1a2a871.pdf", "_bibtex": "@inproceedings{\nhong2022structureaware,\ntitle={Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning},\nauthor={Sunghoon Hong and Deunsol Yoon and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fy_XRVHqly}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "3HJOA-1hb0e", "original": "gsS8I57PupO", "number": 3067, "cdate": 1632875663496, "mdate": null, "ddate": null, "tcdate": 1632875663496, "tmdate": 1676330524420, "tddate": null, "forum": "3HJOA-1hb0e", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization", "authorids": ["~Sunwoo_Lee2", "~Jeongwoo_Park1", "~Dongsuk_Jeon1"], "authors": ["Sunwoo Lee", "Jeongwoo Park", "Dongsuk Jeon"], "keywords": ["low-precision training", "quantized training", "logarithmic weight", "data format optimization", "hysteresis quantization"], "abstract": "As the complexity and size of deep neural networks continue to increase, low-precision training has been extensively studied in the last few years to reduce hardware overhead. Training performance is largely affected by the numeric formats representing different values in low-precision training, but finding an optimal format typically requires numerous training runs, which is a very time-consuming process. In this paper, we propose a method to efficiently find an optimal format for activations and errors without actual training. We employ this method to determine an 8-bit format suitable for training various models. In addition, we propose hysteresis quantization to suppress undesired fluctuation in quantized weights during training. This scheme enables deeply quantized training using 4-bit weights, exhibiting only 0.2% degradation for ResNet-18 trained on ImageNet.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|toward_efficient_lowprecision_training_data_format_optimization_and_hysteresis_quantization", "pdf": "/pdf/d108ae611f0e33dfa11c0140644d61b0b0774170.pdf", "one-sentence_summary": "We propose a systematic data format optimization method and hysteresis quantization scheme to enable efficient low-precision training.", "supplementary_material": "/attachment/483c369adb5fb87c20db91b838816460ccc91150.zip", "data": "", "_bibtex": "@inproceedings{\nlee2022toward,\ntitle={Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization},\nauthor={Sunwoo Lee and Jeongwoo Park and Dongsuk Jeon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3HJOA-1hb0e}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 29}}, {"id": "upnDJ7itech", "original": "1VUQRqf8ZNxy", "number": 3063, "cdate": 1632875663234, "mdate": null, "ddate": null, "tcdate": 1632875663234, "tmdate": 1697934650810, "tddate": null, "forum": "upnDJ7itech", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Knowledge Infused Decoding", "authorids": ["~Ruibo_Liu1", "~Guoqing_Zheng1", "~Shashank_Gupta3", "~Radhika_Gaonkar1", "~Chongyang_Gao1", "~Soroush_Vosoughi1", "~Milad_Shokouhi1", "~Ahmed_Hassan_Awadallah1"], "authors": ["Ruibo Liu", "Guoqing Zheng", "Shashank Gupta", "Radhika Gaonkar", "Chongyang Gao", "Soroush Vosoughi", "Milad Shokouhi", "Ahmed Hassan Awadallah"], "keywords": ["natural language", "decoding", "reinforcement learning", "knowledge integration", "generation"], "abstract": "Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence. they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. Recent remedies to this problem focus on modifying either the pre-training or task fine-tuning objectives to incorporate knowledge, which normally require additional costly training or architecture modification of LMs for practical applications.\n\nWe present Knowledge Infused Decoding (KID)---a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. On six diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART) armed with KID outperform many task-optimized state-of-the-art models, and show particularly strong performance in few-shot scenarios over seven related knowledge-infusion techniques. Human evaluation confirms KID's ability to generate more relevant and factual language for the input context when compared with multiple baselines. Finally, KID also alleviates exposure bias and provides stable generation quality when generating longer sequences.", "one-sentence_summary": "We propose a new decoding algorithm for language model generation, to obtain better performance in knowledge-intensive tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|knowledge_infused_decoding", "pdf": "/pdf/a95e766c5c2193002a679156d0b1d1b582238b50.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2204.03084/code)", "_bibtex": "@inproceedings{\nliu2022knowledge,\ntitle={Knowledge Infused Decoding},\nauthor={Ruibo Liu and Guoqing Zheng and Shashank Gupta and Radhika Gaonkar and Chongyang Gao and Soroush Vosoughi and Milad Shokouhi and Ahmed Hassan Awadallah},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=upnDJ7itech}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "N1WI0vJLER", "original": "fEVZ5Y7S3LLg", "number": 3062, "cdate": 1632875663169, "mdate": null, "ddate": null, "tcdate": 1632875663169, "tmdate": 1676330524791, "tddate": null, "forum": "N1WI0vJLER", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences", "authorids": ["~Euhyun_Moon1", "~Eric_C_Cyr1"], "authors": ["Euhyun Moon", "Eric C Cyr"], "keywords": ["GRU", "MGRIT", "parallel-in-time", "distributed machine learning"], "abstract": "Parallelizing Gated Recurrent Unit (GRU) is a challenging task, as the training procedure of GRU is inherently sequential. Prior efforts to parallelize GRU have largely focused on conventional parallelization strategies such as data-parallel and model-parallel training algorithms. However, when the given sequences are very long, existing approaches are still inevitably performance limited in terms of both training time and model accuracy. In this paper, we present a novel parallel training scheme (called  parallel-in-time) for GRU based on a multigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into multiple shorter sub-sequences and trains the sub-sequences on different processors in parallel. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset, where each video is an image sequence, demonstrate that a new parallel training scheme of GRU achieves up to $6.5 \\times$ speedup over a serial approach. As efficiency of our new parallelization strategy is associated with the sequence length, our parallel GRU algorithm achieves significant performance improvement as the length of sequence increases. Further, the proposed approach can be applied simultaneously with batch and other forms of model parallelism.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "moon|parallel_training_of_gru_networks_with_a_multigrid_solver_for_long_sequences", "pdf": "/pdf/8571a323c3024175c5f0d27227e6c74f594b1290.pdf", "one-sentence_summary": "This paper presents a novel parallel-in-time training scheme for GRU networks based on a MGRIT solver.", "supplementary_material": "/attachment/99ce66901d3d33245bc46fa1b7b162007f5e8cfc.zip", "data": "", "_bibtex": "@inproceedings{\nmoon2022parallel,\ntitle={Parallel Training of {GRU} Networks with a Multi-Grid Solver for Long Sequences},\nauthor={Euhyun Moon and Eric C Cyr},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=N1WI0vJLER}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "73MEhZ0anV", "original": "kpmWYUQFOP", "number": 3054, "cdate": 1632875662640, "mdate": null, "ddate": null, "tcdate": 1632875662640, "tmdate": 1676330524982, "tddate": null, "forum": "73MEhZ0anV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "QUERY EFFICIENT DECISION BASED SPARSE ATTACKS AGAINST BLACK-BOX DEEP LEARNING MODELS", "authorids": ["~Viet_Vo1", "~Ehsan_M_Abbasnejad1", "damith.ranasinghe@adelaide.edu.au"], "authors": ["Viet Vo", "Ehsan M Abbasnejad", "Damith Ranasinghe"], "keywords": ["decision-based attacks", "sparse attacks", "evolution algorithms", "vision transformer", "convolutional neural network"], "abstract": "Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as Machine Learning as a Service (MLaaS), particularly $sparse~attacks$. The realization of sparse attacks in black-box settings demonstrates that machine learning models are more vulnerable than we believe. Because, these attacks aim to $minimize~the~number~of~perturbed~pixels$\u2014measured by $l_0$ norm\u2014required to mislead a model by $solely$ observing the decision ($the~predicted~label$) returned to a model query; the so-called $decision-based~setting$. But, such an attack leads to an NP-hard optimization problem. We develop an evolution-based algorithm\u2014$SparseEvo$\u2014for the problem and evaluate it against both convolutional deep neural networks and $vision~transformers$. Notably, vision transformers are yet to be investigated under a decision-based attack setting. SparseEvo requires significantly fewer queries than the state-of-the-art sparse attack $Pointwise$ for both untargeted and targeted attacks. The attack algorithm, although conceptually simple, is competitive with only a limited query budget against the state-of-the-art gradient-based $white-box$ attacks in standard computer vision tasks such as $ImageNet$. Importantly, the query efficient SparseEvo, along with decision-based attacks, in general, raises new questions regarding the safety of deployed systems and poses new directions to study and understand the robustness of machine learning models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "vo|query_efficient_decision_based_sparse_attacks_against_blackbox_deep_learning_models", "pdf": "/pdf/02ebabe963b750dd8f1c95cf41ba5e78dd63b8f9.pdf", "data": "", "_bibtex": "@inproceedings{\nvo2022query,\ntitle={{QUERY} {EFFICIENT} {DECISION} {BASED} {SPARSE} {ATTACKS} {AGAINST} {BLACK}-{BOX} {DEEP} {LEARNING} {MODELS}},\nauthor={Viet Vo and Ehsan M Abbasnejad and Damith Ranasinghe},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=73MEhZ0anV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "gJLEXy3ySpu", "original": "lDtb5v25Fnxt", "number": 3051, "cdate": 1632875662444, "mdate": null, "ddate": null, "tcdate": 1632875662444, "tmdate": 1676330525187, "tddate": null, "forum": "gJLEXy3ySpu", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations", "authorids": ["~Jinyuan_Jia2", "~Binghui_Wang2", "~Xiaoyu_Cao1", "~Hongbin_Liu2", "~Neil_Zhenqiang_Gong1"], "authors": ["Jinyuan Jia", "Binghui Wang", "Xiaoyu Cao", "Hongbin Liu", "Neil Zhenqiang Gong"], "keywords": [], "abstract": "Top-$k$ predictions are used in many real-world applications such as machine learning as a service, recommender systems, and web searches. $\\ell_0$-norm adversarial perturbation characterizes an attack that arbitrarily modifies some features of an input such that a classifier makes an incorrect prediction for the perturbed input. $\\ell_0$-norm adversarial perturbation is easy to interpret and can be implemented in the physical world. Therefore, certifying  robustness of top-$k$ predictions against $\\ell_0$-norm adversarial perturbation is important. However, existing studies either focused on certifying $\\ell_0$-norm robustness of top-$1$ predictions or  $\\ell_2$-norm robustness of top-$k$ predictions. In this work, we aim to bridge the gap. Our approach is based on randomized smoothing, which builds a provably robust classifier from an arbitrary classifier via randomizing an input. Our major theoretical contribution is an almost tight $\\ell_0$-norm certified robustness guarantee for top-$k$ predictions. We empirically evaluate our method on CIFAR10 and ImageNet. For instance, our method can build a classifier that achieves a certified top-3 accuracy of 69.2\\% on ImageNet when an attacker can arbitrarily perturb 5 pixels of a testing image. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jia|almost_tight_l0norm_certified_robustness_of_topk_predictions_against_adversarial_perturbations", "pdf": "/pdf/468bb7a69cab231837989615dedd6b3b33a6c683.pdf", "one-sentence_summary": "In this work, we derive the certified robustness against $\\ell_0$-norm adversarial perturbation for top-$k$ prediction.", "data": "", "code": "", "_bibtex": "@inproceedings{\njia2022almost,\ntitle={Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations},\nauthor={Jinyuan Jia and Binghui Wang and Xiaoyu Cao and Hongbin Liu and Neil Zhenqiang Gong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gJLEXy3ySpu}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "Vjki79-619-", "original": "Qh_4l9dAaER", "number": 3046, "cdate": 1632875662178, "mdate": null, "ddate": null, "tcdate": 1632875662178, "tmdate": 1676330525629, "tddate": null, "forum": "Vjki79-619-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks", "authorids": ["~Arthur_da_Cunha1", "~Emanuele_Natale1", "~Laurent_Viennot1"], "authors": ["Arthur da Cunha", "Emanuele Natale", "Laurent Viennot"], "keywords": ["lottery ticket hypothesis", "convolutional neural network", "network pruning", "random subset sum", "random neural network"], "abstract": "The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training, achieves accuracy comparable to that of the trained large network. These works left as an open problem to extend the result to convolutional neural networks (CNNs).\nIn this work we provide such generalization by showing that, with high probability, it is possible to approximate any CNN by pruning a random CNN whose size is larger by a logarithmic factor.", "one-sentence_summary": "We prove the lottery ticket hypothesis for convolutional neural networks", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cunha|proving_the_lottery_ticket_hypothesis_for_convolutional_neural_networks", "pdf": "/pdf/883b1c57d46f2d43344902ddf405db1696b615f9.pdf", "supplementary_material": "/attachment/1f32548502745eb0169a1544d59ad80fcae3ad3e.zip", "data": "", "_bibtex": "@inproceedings{\ncunha2022proving,\ntitle={Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks},\nauthor={Arthur da Cunha and Emanuele Natale and Laurent Viennot},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Vjki79-619-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "Vog_3GXsgmb", "original": "pUH4xoJWxg4", "number": 3042, "cdate": 1632875661906, "mdate": null, "ddate": null, "tcdate": 1632875661906, "tmdate": 1676330526001, "tddate": null, "forum": "Vog_3GXsgmb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Discovering Nonlinear PDEs from Scarce Data with Physics-encoded Learning", "authorids": ["~Chengping_Rao1", "~Pu_Ren1", "~Yang_Liu52", "~Hao_Sun4"], "authors": ["Chengping Rao", "Pu Ren", "Yang Liu", "Hao Sun"], "keywords": ["Data-driven equation discovery", "dynamical system modeling", "physics-encoded learning"], "abstract": "There have been growing interests in leveraging experimental measurements to discover the underlying partial differential equations (PDEs) that govern complex physical phenomena. Although past research attempts have achieved great success in data-driven PDE discovery, the robustness of the existing methods cannot be guaranteed when dealing with low-quality measurement data. To overcome this challenge, we propose a novel physics-encoded discrete learning framework for discovering spatiotemporal PDEs from scarce and noisy data. The general idea is to (1) firstly introduce a novel deep convolutional-recurrent networks, which can encode prior physics knowledge (e.g., known terms, assumed PDE structure, initial/boundary conditions, etc.) while remaining flexible on representation capability, to accurately reconstruct high-fidelity data, and (2) then perform sparse regression with the reconstructed data to identify the analytical form of the governing PDEs. We validate our proposed framework on three high-dimensional PDE systems. The effectiveness and superiority of the proposed method over baselines are demonstrated.", "one-sentence_summary": "This work seeks to solve the data-driven governing equation discovery problem with a novel physics-encoded learning framework.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rao|discovering_nonlinear_pdes_from_scarce_data_with_physicsencoded_learning", "pdf": "/pdf/82ec65c9323f68d0077918ee9391653298302272.pdf", "_bibtex": "@inproceedings{\nrao2022discovering,\ntitle={Discovering Nonlinear {PDE}s from Scarce Data with Physics-encoded Learning},\nauthor={Chengping Rao and Pu Ren and Yang Liu and Hao Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Vog_3GXsgmb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 42}}, {"id": "KJztlfGPdwW", "original": "Mv_miAfDjso", "number": 3038, "cdate": 1632875661628, "mdate": null, "ddate": null, "tcdate": 1632875661628, "tmdate": 1676330526186, "tddate": null, "forum": "KJztlfGPdwW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL", "authorids": ["~Rui_Yang8", "~Yiming_Lu1", "~Wenzhe_Li2", "~Hao_Sun3", "~Meng_Fang1", "~Yali_Du1", "~Xiu_Li1", "~Lei_Han1", "~Chongjie_Zhang1"], "authors": ["Rui Yang", "Yiming Lu", "Wenzhe Li", "Hao Sun", "Meng Fang", "Yali Du", "Xiu Li", "Lei Han", "Chongjie Zhang"], "keywords": ["Goal-conditioned reinforcement learning", "offline reinforcement learning", "goal-conditioned supervised learning"], "abstract": "Solving goal-conditioned tasks with sparse rewards using self-supervised learning is promising because of its simplicity and stability over current reinforcement learning (RL) algorithms. A recent work, called Goal-Conditioned Supervised Learning (GCSL), provides a new learning framework by iteratively relabeling and imitating self-generated experiences. In this paper, we revisit the theoretical property of GCSL --- optimizing a lower bound of the goal reaching objective, and extend GCSL as a novel offline goal-conditioned RL algorithm. The proposed method is named Weighted GCSL (WGCSL), in which we introduce an advanced compound weight consisting of three parts (1) discounted weight for goal relabeling, (2) goal-conditioned exponential advantage weight, and (3) best-advantage weight. Theoretically, WGCSL is proved to optimize an equivalent lower bound of the goal-conditioned RL objective and generates monotonically improved policies via an iterated scheme. The monotonic property holds for any behavior policies, and therefore WGCSL can be applied to both online and offline settings. To evaluate algorithms in the offline goal-conditioned RL setting, we provide a benchmark including a range of point and simulated robot domains. Experiments in the introduced benchmark demonstrate that WGCSL can consistently outperform GCSL and existing state-of-the-art offline methods in the fully offline goal-conditioned setting.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|rethinking_goalconditioned_supervised_learning_and_its_connection_to_offline_rl", "pdf": "/pdf/4480bd8b649d86ec422acd68ae606cd7f8fa1d6d.pdf", "one-sentence_summary": "We revisit GCSL's theoretical foundation and present a simple but effective algorithm for offline goal-conditioned RL via weighted supervised learning", "_bibtex": "@inproceedings{\nyang2022rethinking,\ntitle={Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline {RL}},\nauthor={Rui Yang and Yiming Lu and Wenzhe Li and Hao Sun and Meng Fang and Yali Du and Xiu Li and Lei Han and Chongjie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KJztlfGPdwW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 40}}, {"id": "P1QUVhOtEFP", "original": "N_XuAkHhPjbf", "number": 3034, "cdate": 1632875661430, "mdate": null, "ddate": null, "tcdate": 1632875661430, "tmdate": 1697934653905, "tddate": null, "forum": "P1QUVhOtEFP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Topologically Regularized Data Embeddings", "authorids": ["~Robin_Vandaele1", "~Bo_Kang1", "~Jefrey_Lijffijt1", "~Tijl_De_Bie1", "~Yvan_Saeys1"], "authors": ["Robin Vandaele", "Bo Kang", "Jefrey Lijffijt", "Tijl De Bie", "Yvan Saeys"], "keywords": ["Embedding", "Dimensionality Reduction", "Topological Data Analysis", "Persistent Homology", "Optimization", "Regularization"], "abstract": "Unsupervised feature learning often finds low-dimensional embeddings that capture the structure of complex data.  For tasks for which prior expert topological knowledge is available, incorporating this into the learned representation may lead to higher quality embeddings. For example, this may help one to embed the data into a given number of clusters, or to accommodate for noise that prevents one from deriving the distribution of the data over the model directly, which can then be learned more effectively. However, a general tool for integrating different prior topological knowledge into embeddings is lacking. Although differentiable topology layers have been recently developed that can (re)shape embeddings into prespecified topological models, they have two important limitations for representation learning, which we address in this paper. First, the currently suggested topological losses fail to represent simple models such as clusters and flares in a natural manner. Second, these losses neglect all original structural (such as neighborhood) information in the data that is useful for learning. We overcome these limitations by introducing a new set of topological losses, and proposing their usage as a way for topologically regularizing data embeddings to naturally represent a prespecified model. We include thorough experiments on synthetic and real data that highlight the usefulness and versatility of this approach, with applications ranging from modeling high-dimensional single-cell data, to graph embedding.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "vandaele|topologically_regularized_data_embeddings", "pdf": "/pdf/8b4e6211ed28114e194a05b0d878e41273cc19f5.pdf", "one-sentence_summary": "A method for incorporating expert prior topological knowledge into data embeddings.", "supplementary_material": "/attachment/a25b264664ba40dc2024eb6ce17f9cfd303ceeca.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.09193/code)", "_bibtex": "@inproceedings{\nvandaele2022topologically,\ntitle={Topologically Regularized Data Embeddings},\nauthor={Robin Vandaele and Bo Kang and Jefrey Lijffijt and Tijl De Bie and Yvan Saeys},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=P1QUVhOtEFP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "oh4TirnfSem", "original": "r6K6pV_OjQ3", "number": 3033, "cdate": 1632875661359, "mdate": null, "ddate": null, "tcdate": 1632875661359, "tmdate": 1676330526318, "tddate": null, "forum": "oh4TirnfSem", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PF-GNN: Differentiable particle filtering based approximation of universal graph representations", "authorids": ["~Mohammed_Haroon_Dupty1", "~Yanfei_Dong1", "~Wee_Sun_Lee1"], "authors": ["Mohammed Haroon Dupty", "Yanfei Dong", "Wee Sun Lee"], "keywords": ["Graph Neural Networks", "Graph representation learning", "Expressive GNN"], "abstract": "Message passing Graph Neural Networks (GNNs) are known to be limited in expressive power by the 1-WL color-refinement test for graph isomorphism. Other more expressive models either are computationally expensive or need preprocessing to extract structural features from the graph. In this work, we propose to make GNNs universal by guiding the learning process with exact isomorphism solver techniques which operate on the paradigm of $\\textit{Individualization and refinement}$ (IR), a method to artificially introduce asymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers generate a search-tree of colorings whose leaves uniquely identify the graph. However, the tree grows exponentially large and needs hand-crafted pruning techniques which are not desirable from a learning perspective. We take a probabilistic view and approximate the search tree of colorings ( i.e. embeddings) by sampling multiple paths from root to leaves of the search-tree. To learn more discriminative representations, we guide the sampling process with $\\textit{particle filter}$ updates, a principled approach for sequential state estimation. Our algorithm is end-to-end differentiable, can be applied with any GNN as backbone and learns richer graph representations with only linear increase in runtime. Experimental evaluation shows that our approach consistently outperforms leading GNN models on both synthetic benchmarks for isomorphism detection as well as real-world datasets.", "one-sentence_summary": "Increasing the expressive power of Graph Neural Networks by using techniques from exact isomorphism solvers with a particle filtering approach.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dupty|pfgnn_differentiable_particle_filtering_based_approximation_of_universal_graph_representations", "pdf": "/pdf/0118eb807320ffa7b7d1699e78d45f3a138d256c.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\ndupty2022pfgnn,\ntitle={{PF}-{GNN}: Differentiable particle filtering based approximation of universal graph representations},\nauthor={Mohammed Haroon Dupty and Yanfei Dong and Wee Sun Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oh4TirnfSem}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "AMpki9kp8Cn", "original": "wr6HJOiT9f", "number": 3031, "cdate": 1632875661224, "mdate": null, "ddate": null, "tcdate": 1632875661224, "tmdate": 1676330526465, "tddate": null, "forum": "AMpki9kp8Cn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Nonlinear ICA Using Volume-Preserving Transformations", "authorids": ["~Xiaojiang_Yang1", "~Yi_Wang24", "~Jiacheng_Sun1", "~Xing_Zhang6", "~Shifeng_Zhang5", "~Zhenguo_Li1", "~Junchi_Yan2"], "authors": ["Xiaojiang Yang", "Yi Wang", "Jiacheng Sun", "Xing Zhang", "Shifeng Zhang", "Zhenguo Li", "Junchi Yan"], "keywords": ["Independent Component Analysis", "Nonlinear ICA", "Identifiability"], "abstract": "Nonlinear ICA is a fundamental problem in machine learning, aiming to identify the underlying independent components (sources) from data which is assumed to be a nonlinear function (mixing function) of these sources. Recent works prove that if the sources have some particular structures (e.g. temporal structure), they are theoretically identifiable even if the mixing function is arbitrary. However, in many cases such restrictions on the sources are difficult to satisfy or even verify, hence it inhibits the applicability of the proposed methods. Different from these works, we propose a general framework for nonlinear ICA, in which the mixing function is assumed to be a volume-preserving transformation, and meanwhile the conditions on the sources can be much looser. We provide an insightful proof of the identifiability of the proposed framework. We implement the framework by volume-preserving Flow-based models, and verify our theory by experiments on artificial data and synthesized images. Moreover, results on real-world images indicate that our framework can disentangle interpretable features.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|nonlinear_ica_using_volumepreserving_transformations", "pdf": "/pdf/3a5f3218a8dcb5454a7f2124915513a251c47534.pdf", "one-sentence_summary": "We propose a general framework for nonlinear ICA, in which the mixing function is restricted to a volume-preserving transformation, and establish two novel identifiability theorems and provide insightful proofs.", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nyang2022nonlinear,\ntitle={Nonlinear {ICA} Using Volume-Preserving Transformations},\nauthor={Xiaojiang Yang and Yi Wang and Jiacheng Sun and Xing Zhang and Shifeng Zhang and Zhenguo Li and Junchi Yan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AMpki9kp8Cn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "18Ys0-PzyPI", "original": "NV6cedl3tYJ", "number": 3030, "cdate": 1632875661152, "mdate": null, "ddate": null, "tcdate": 1632875661152, "tmdate": 1676330526567, "tddate": null, "forum": "18Ys0-PzyPI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Online Ad Hoc Teamwork under Partial Observability", "authorids": ["~Pengjie_Gu1", "~Mengchen_Zhao1", "haojianye@huawei.com", "~Bo_An2"], "authors": ["Pengjie Gu", "Mengchen Zhao", "Jianye Hao", "Bo An"], "keywords": ["coordination", "reinforcement learning"], "abstract": "Autonomous agents often need to work together as a team to accomplish complex cooperative tasks. Due to privacy and other realistic constraints, agents might need to collaborate with previously unknown teammates on the fly. This problem is known as ad hoc teamwork, which remains a core research challenge. Prior works usually rely heavily on strong assumptions like full observability, fixed and predefined teammates' types. This paper relaxes these assumptions with a novel reinforcement learning framework called ODITS, which allows the autonomous agent to adapt to arbitrary teammates in an online fashion. Instead of limiting teammates into a finite set of predefined types, ODITS automatically learns latent variables of teammates' behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, we introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc teamwork tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gu|online_ad_hoc_teamwork_under_partial_observability", "pdf": "/pdf/69ebe98356c87ca06fb980028a882b8bf4ea9078.pdf", "_bibtex": "@inproceedings{\ngu2022online,\ntitle={Online Ad Hoc Teamwork under Partial Observability},\nauthor={Pengjie Gu and Mengchen Zhao and Jianye Hao and Bo An},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=18Ys0-PzyPI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "vwLLQ-HwqhZ", "original": "C2zEkldkkM2", "number": 3026, "cdate": 1632875660887, "mdate": null, "ddate": null, "tcdate": 1632875660887, "tmdate": 1697934655155, "tddate": null, "forum": "vwLLQ-HwqhZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Continual Normalization: Rethinking Batch Normalization for Online Continual Learning", "authorids": ["~Quang_Pham1", "~Chenghao_Liu1", "~Steven_HOI1"], "authors": ["Quang Pham", "Chenghao Liu", "Steven HOI"], "keywords": ["Continual Learning", "Batch Normalization"], "abstract": "Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and non-stationary nature of continual learning data, especially in the online setting, amplify the discrepancy between training and testing in BN and hinder the performance of older tasks. In this work, we study the cross-task normalization effect of BN in online continual learning where BN normalizes the testing data using moments biased towards the current task, resulting in higher catastrophic forgetting. This limitation motivates us to propose a simple yet effective method that we call Continual Normalization (CN) to facilitate training similar to BN while mitigating its negative effect. Extensive experiments on different continual learning algorithms and online scenarios show that CN is a direct replacement for BN and can provide substantial performance improvements. Our implementation will be made publicly available upon acceptance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pham|continual_normalization_rethinking_batch_normalization_for_online_continual_learning", "pdf": "/pdf/10665207380ab7045b3c071ec3558f28f179a53a.pdf", "one-sentence_summary": "A negative effect of BN in online continual learning and a simple strategy to alleviate it.", "supplementary_material": "/attachment/fe4928c18bdb976a8e37480f0491f9b68c5d10a5.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.16102/code)", "_bibtex": "@inproceedings{\npham2022continual,\ntitle={Continual Normalization: Rethinking Batch Normalization for Online Continual Learning},\nauthor={Quang Pham and Chenghao Liu and Steven HOI},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vwLLQ-HwqhZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "SHbhHHfePhP", "original": "HJ18-ZZk7TdK", "number": 3025, "cdate": 1632875660760, "mdate": null, "ddate": null, "tcdate": 1632875660760, "tmdate": 1697934655531, "tddate": null, "forum": "SHbhHHfePhP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Equivariant Graph Mechanics Networks with Constraints", "authorids": ["~Wenbing_Huang1", "~Jiaqi_Han2", "~Yu_Rong1", "~Tingyang_Xu1", "~Fuchun_Sun1", "~Junzhou_Huang2"], "authors": ["Wenbing Huang", "Jiaqi Han", "Yu Rong", "Tingyang Xu", "Fuchun Sun", "Junzhou Huang"], "keywords": [], "abstract": "Learning to reason about relations and dynamics over multiple interacting objects is a challenging topic in machine learning. The challenges mainly stem from that the interacting systems are exponentially-compositional, symmetrical, and commonly geometrically-constrained.\nCurrent methods, particularly the ones based on equivariant Graph Neural Networks (GNNs), have targeted on the first two challenges but remain immature for constrained systems. \nIn this paper, we propose Graph Mechanics Network (GMN) which is combinatorially efficient, equivariant and constraint-aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward kinematics. Moreover, to allow equivariant message passing in GMN, we have developed a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Theoretically, the proposed equivariant formulation is proved to be universally expressive under certain conditions. Extensive experiments  support the advantages of GMN compared to the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency on the simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|equivariant_graph_mechanics_networks_with_constraints", "pdf": "/pdf/d257472583625e833ddf5bdf3abbbee9da8421da.pdf", "supplementary_material": "/attachment/f5b19aca48ed36558730af2f074518bc232d9a9e.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.06442/code)", "_bibtex": "@inproceedings{\nhuang2022equivariant,\ntitle={Equivariant Graph Mechanics Networks with Constraints},\nauthor={Wenbing Huang and Jiaqi Han and Yu Rong and Tingyang Xu and Fuchun Sun and Junzhou Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SHbhHHfePhP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 36}}, {"id": "vfsRB5MImo9", "original": "waUEq-lLU9-", "number": 3023, "cdate": 1632875660627, "mdate": null, "ddate": null, "tcdate": 1632875660627, "tmdate": 1697934655825, "tddate": null, "forum": "vfsRB5MImo9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Continual Knowledge Learning of Language Models", "authorids": ["~Joel_Jang1", "~Seonghyeon_Ye1", "~Sohee_Yang1", "~Joongbo_Shin1", "~Janghoon_Han1", "~Gyeonghun_KIM1", "~Stanley_Jungkyu_Choi2", "~Minjoon_Seo1"], "authors": ["Joel Jang", "Seonghyeon Ye", "Sohee Yang", "Joongbo Shin", "Janghoon Han", "Gyeonghun KIM", "Stanley Jungkyu Choi", "Minjoon Seo"], "keywords": ["continual learning", "knowledge acquisition", "catastrophic forgetting", "large language models", "pretraining", "natural language processing"], "abstract": "Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.", "one-sentence_summary": "We propose a novel continual learning formulation named Continual Knowledge Learning which allows large language models to constantly obtain new and updated knowledge while mitigating forgetting of previous learned time-invariant knowledge.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jang|towards_continual_knowledge_learning_of_language_models", "pdf": "/pdf/873039fe2b300e8ad8b1dd3a34054bad1386f1a1.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.03215/code)", "_bibtex": "@inproceedings{\njang2022towards,\ntitle={Towards Continual Knowledge Learning of Language Models},\nauthor={Joel Jang and Seonghyeon Ye and Sohee Yang and Joongbo Shin and Janghoon Han and Gyeonghun KIM and Stanley Jungkyu Choi and Minjoon Seo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vfsRB5MImo9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "nf3A0WZsXS5", "original": "U-lmosr1Wnq", "number": 3016, "cdate": 1632875660156, "mdate": null, "ddate": null, "tcdate": 1632875660156, "tmdate": 1676330527594, "tddate": null, "forum": "nf3A0WZsXS5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns", "authorids": ["~Zhijian_Yang2", "junhao.wen89@gmail.com", "~Christos_Davatzikos1"], "authors": ["Zhijian Yang", "Junhao Wen", "Christos Davatzikos"], "keywords": ["Representation Learning", "Disease-related imaging patterns", "Alzheimer's disease", "MRI", "GAN"], "abstract": "A plethora of machine learning methods have been applied to imaging data, enabling the construction of clinically relevant imaging signatures of neurological and neuropsychiatric diseases. Oftentimes, such methods don't explicitly model the heterogeneity of disease effects, or approach it via nonlinear models that are not interpretable. Moreover, unsupervised methods may parse heterogeneity that is driven by nuisance confounding factors that affect brain structure or function, rather than heterogeneity relevant to a pathology of interest. On the other hand, semi-supervised clustering methods seek to derive a dichotomous subtype membership, ignoring the truth that disease heterogeneity spatially and temporally extends along a continuum.  To address the aforementioned limitations, herein, we propose a novel method, termed Surreal-GAN (Semi-SUpeRvised ReprEsentAtion Learning via GAN). Using cross-sectional imaging data, Surreal-GAN dissects underlying disease-related heterogeneity under the principle of semi-supervised clustering (cluster mappings from normal control to patient), proposes a continuously dimensional representation, and infers the disease severity of patients at individual level along each dimension. The model first learns a transformation function from normal control (CN) domain to the patient (PT) domain with latent variables controlling transformation directions. An inverse mapping function together with regularization on function continuity, pattern orthogonality and monotonicity was also imposed to make sure that the transformation function captures necessarily meaningful imaging patterns with clinical significance. We first validated the model through extensive semi-synthetic experiments, and then demonstrate its potential in capturing biologically plausible imaging patterns in Alzheimer's disease (AD).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|surrealgansemisupervised_representation_learning_via_gan_for_uncovering_heterogeneous_diseaserelated_imaging_patterns", "pdf": "/pdf/169e0c29b5ff15ab03db77fc148c513fa8a71257.pdf", "one-sentence_summary": "We proposed a novel method, Surreal-GAN, to derive low dimensional representation of disease-related patterns from neuroimaging data.", "_bibtex": "@inproceedings{\nyang2022surrealgansemisupervised,\ntitle={Surreal-{GAN}:Semi-Supervised Representation Learning via {GAN} for uncovering heterogeneous disease-related imaging patterns},\nauthor={Zhijian Yang and Junhao Wen and Christos Davatzikos},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nf3A0WZsXS5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "TfhfZLQ2EJO", "original": "GFCDt3BXya7", "number": 3012, "cdate": 1632875659956, "mdate": null, "ddate": null, "tcdate": 1632875659956, "tmdate": 1676330527688, "tddate": null, "forum": "TfhfZLQ2EJO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning", "authorids": ["~Jongjin_Park1", "~Younggyo_Seo1", "~Jinwoo_Shin1", "~Honglak_Lee2", "~Pieter_Abbeel2", "~Kimin_Lee1"], "authors": ["Jongjin Park", "Younggyo Seo", "Jinwoo Shin", "Honglak Lee", "Pieter Abbeel", "Kimin Lee"], "keywords": ["preference-based reinforcement learning", "human-in-the-loop reinforcement learning", "deep reinforcement learning", "semi-supervised learning"], "abstract": "Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor\u2019s preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-of-the-art preference-based method on a variety of locomotion and robotic manipulation tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "park|surf_semisupervised_reward_learning_with_data_augmentation_for_feedbackefficient_preferencebased_reinforcement_learning", "pdf": "/pdf/2fe39fd19cc40472a98221890fb4cfd5f924e6c7.pdf", "one-sentence_summary": "We present SURF, a semi-supervised reward learning algorithm with data augmentation for feedback-efficient preference-based RL.", "supplementary_material": "/attachment/ea8664f4948105bfdacb400acbef1e1e97f21a05.zip", "data": "", "_bibtex": "@inproceedings{\npark2022surf,\ntitle={{SURF}: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning},\nauthor={Jongjin Park and Younggyo Seo and Jinwoo Shin and Honglak Lee and Pieter Abbeel and Kimin Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TfhfZLQ2EJO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "ItkxLQU01lD", "original": "VdiM0FRtGHc", "number": 3011, "cdate": 1632875659887, "mdate": null, "ddate": null, "tcdate": 1632875659887, "tmdate": 1697934657318, "tddate": null, "forum": "ItkxLQU01lD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Convergent Graph Solvers", "authorids": ["~Junyoung_Park1", "~Jinhyun_Choo1", "~Jinkyoo_Park1"], "authors": ["Junyoung Park", "Jinhyun Choo", "Jinkyoo Park"], "keywords": ["Graph", "Graph Neural Network", "Fixed point", "Implicit model", "Implicit function theorem", "Convergent"], "abstract": "We propose the convergent graph solver (CGS), a deep learning method that learns iterative mappings to predict the properties of a graph system at its stationary state (fixed point) with guaranteed convergence. The forward propagation of CGS proceeds in three steps: (1) constructing the input-dependent linear contracting iterative maps, (2) computing the fixed points of the iterative maps, and (3) decoding the fixed points to estimate the properties. The contractivity of the constructed linear maps guarantees the existence and uniqueness of the fixed points following the Banach fixed point theorem. To train CGS efficiently, we also derive a tractable analytical expression for its gradient by leveraging the implicit function theorem. We evaluate the performance of CGS by applying it to various network-analytic and graph benchmark problems. The results indicate that CGS has competitive capabilities for predicting the stationary properties of graph systems,  irrespective of whether the target systems are linear or non-linear. CGS also shows high performance for graph classification problems where the existence or the meaning of a fixed point is hard to be clearly defined, which highlights the potential of CGS as a general graph neural network architecture.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "park|convergent_graph_solvers", "pdf": "/pdf/fb18728414ffbbdaa143c3e893d041eb10d254ce.pdf", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.01680/code)", "_bibtex": "@inproceedings{\npark2022convergent,\ntitle={Convergent Graph Solvers},\nauthor={Junyoung Park and Jinhyun Choo and Jinkyoo Park},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ItkxLQU01lD}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "_F9xpOrqyX9", "original": "5_pAEe4gnHf", "number": 3007, "cdate": 1632875659609, "mdate": null, "ddate": null, "tcdate": 1632875659609, "tmdate": 1676330527975, "tddate": null, "forum": "_F9xpOrqyX9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation ", "authorids": ["~Junhyun_Nam1", "~Jaehyung_Kim1", "~Jaeho_Lee3", "~Jinwoo_Shin1"], "authors": ["Junhyun Nam", "Jaehyung Kim", "Jaeho Lee", "Jinwoo Shin"], "keywords": ["worst-group loss minimization", "spurious correlation"], "abstract": "The paradigm of worst-group loss minimization has shown its promise in avoiding to learn spurious correlations, but requires costly additional supervision on spurious attributes. To resolve this, recent works focus on developing weaker forms of supervision---e.g., hyperparameters discovered with a small number of validation samples with spurious attribute annotation---but none of the methods retain comparable performance to methods using full supervision on the spurious attribute. In this paper, instead of searching for weaker supervisions, we ask: Given access to a fixed number of samples with spurious attribute annotations, what is the best achievable worst-group loss if we ''fully exploit'' them? To this end, we propose a pseudo-attribute-based algorithm, coined Spread Spurious Attribute (SSA), for improving the worst-group accuracy. In particular, we leverage samples both with and without spurious attribute annotations to train a model to predict the spurious attribute, then use the pseudo-attribute predicted by the trained model as supervision on the spurious attribute to train a new robust model having minimal worst-group loss. Our experiments on various benchmark datasets show that our algorithm consistently outperforms the baseline methods using the same number of validation samples with spurious attribute annotations. We also demonstrate that the proposed SSA can achieve comparable performances to methods using full (100%) spurious attribute supervision, by using a much smaller number of annotated samples---from 0.6% and up to 1.5%, depending on the dataset.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nam|spread_spurious_attribute_improving_worstgroup_accuracy_with_spurious_attribute_estimation", "pdf": "/pdf/1ef5907fce7a34e12970b51ba7aa96afb5e46997.pdf", "one-sentence_summary": "Using a small amount of attribute annotated samples for training can boost worst-group performance in the presence of spurious correlation.", "supplementary_material": "/attachment/6b693e71064507cd3a8cd859ba3b1899c1cb3cc6.zip", "data": "", "_bibtex": "@inproceedings{\nnam2022spread,\ntitle={Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation },\nauthor={Junhyun Nam and Jaehyung Kim and Jaeho Lee and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_F9xpOrqyX9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "06Wy2BtxXrz", "original": "jEprZgb5xkh", "number": 2998, "cdate": 1632875658994, "mdate": null, "ddate": null, "tcdate": 1632875658994, "tmdate": 1676330528559, "tddate": null, "forum": "06Wy2BtxXrz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs", "authorids": ["~Yaoxin_Wu2", "~Wen_Song1", "~Zhiguang_Cao1", "~Jie_Zhang9"], "authors": ["Yaoxin Wu", "Wen Song", "Zhiguang Cao", "Jie Zhang"], "keywords": ["Conditional Variational Autoencoder", "Stochastic Integer Programming", "Scenario Reduction"], "abstract": "Many practical combinatorial optimization problems under uncertainty can be modeled as stochastic integer programs (SIPs), which are extremely challenging to solve due to the high complexity. To solve two-stage SIPs efficiently, we propose a conditional variational autoencoder (CVAE) based method to learn scenario representation for a class of SIP instances. Specifically, we design a graph convolutional network based encoder to embed each scenario with the deterministic part of its instance (i.e. context) into a low-dimensional latent space, from which a decoder reconstructs the scenario from its latent representation conditioned on the context. Such a design effectively captures the dependencies of the scenarios on their corresponding instances. We apply the trained encoder to two tasks in typical SIP solving, i.e. scenario reduction and objective prediction. Experiments on two SIP problems show that the learned latent representation significantly boosts the solving performance to attain high-quality solutions in short computational time, and generalizes fairly well to problems of larger sizes or with more scenarios.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|learning_scenario_representation_for_solving_twostage_stochastic_integer_programs", "pdf": "/pdf/87cf7ed439965040c8013759c94576533321c4b5.pdf", "one-sentence_summary": "This paper provides a CVAE based method to learn scenario representations for solving stochastic integer programs.", "_bibtex": "@inproceedings{\nwu2022learning,\ntitle={Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs},\nauthor={Yaoxin Wu and Wen Song and Zhiguang Cao and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=06Wy2BtxXrz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "7grkzyj89A_", "original": "IQGVzTSPGw_", "number": 2996, "cdate": 1632875658859, "mdate": null, "ddate": null, "tcdate": 1632875658859, "tmdate": 1697934659204, "tddate": null, "forum": "7grkzyj89A_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalization Through the Lens of Leave-One-Out Error", "authorids": ["~Gregor_Bachmann1", "~Thomas_Hofmann1", "~Aurelien_Lucchi1"], "authors": ["Gregor Bachmann", "Thomas Hofmann", "Aurelien Lucchi"], "keywords": [], "abstract": "Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is very limited. Classical generalization bounds based on tools such as the VC dimension or Rademacher complexity, are so far unsuitable for deep models and it is doubtful that these techniques can yield tight bounds even in the most idealistic settings~\\citep{nagarajan2019uniform}. In this work, we instead revisit the concept of leave-one-out (LOO) error to measure the generalization ability of deep models in the so-called kernel regime. While popular in statistics, the LOO error has been largely overlooked in the context of deep learning. By building upon the recently established connection between neural networks and kernel learning, we leverage the closed-form expression for the leave-one-out error, giving us access to an efficient proxy for the test error. We show both theoretically and empirically that the leave-one-out error is capable of capturing various phenomena in generalization theory, such as double descent, random labels or transfer learning.\nOur work therefore demonstrates that the leave-one-out error provides a tractable way to estimate the generalization ability of deep neural networks in the kernel regime, opening the door to potential, new research directions in the field of generalization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bachmann|generalization_through_the_lens_of_leaveoneout_error", "pdf": "/pdf/16d55edf6606d7cd0494df4bb7f9a39233c8c550.pdf", "supplementary_material": "/attachment/55ab265d30781917ebea7e446995bd0d61e4ac9d.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.03443/code)", "_bibtex": "@inproceedings{\nbachmann2022generalization,\ntitle={Generalization Through the Lens of Leave-One-Out Error},\nauthor={Gregor Bachmann and Thomas Hofmann and Aurelien Lucchi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7grkzyj89A_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "VPjw9KPWRSK", "original": "tvg5ESx698E", "number": 2992, "cdate": 1632875658585, "mdate": null, "ddate": null, "tcdate": 1632875658585, "tmdate": 1676330528855, "tddate": null, "forum": "VPjw9KPWRSK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Self-Supervised Inference in State-Space Models", "authorids": ["~David_Ruhe1", "~Patrick_Forr\u00e91"], "authors": ["David Ruhe", "Patrick Forr\u00e9"], "keywords": ["self-supervision", "inference", "state-space model", "Kalman filter", "recurrent neural network"], "abstract": "We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. It comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ruhe|selfsupervised_inference_in_statespace_models", "pdf": "/pdf/0b43e4ea11108c37cd4f7f5548e1d635e571c000.pdf", "supplementary_material": "/attachment/bc99a5668a801964b3b7d57eb65d64e2594f690f.zip", "_bibtex": "@inproceedings{\nruhe2022selfsupervised,\ntitle={Self-Supervised Inference in State-Space Models},\nauthor={David Ruhe and Patrick Forr{\\'e}},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VPjw9KPWRSK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "SwIp410B6aQ", "original": "OOWsE-Mz-ro", "number": 2983, "cdate": 1632875657981, "mdate": null, "ddate": null, "tcdate": 1632875657981, "tmdate": 1676330529182, "tddate": null, "forum": "SwIp410B6aQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Role of Neural Collapse in Transfer Learning", "authorids": ["~Tomer_Galanti1", "~Andr\u00e1s_Gy\u00f6rgy2", "~Marcus_Hutter1"], "authors": ["Tomer Galanti", "Andr\u00e1s Gy\u00f6rgy", "Marcus Hutter"], "keywords": [], "abstract": "We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. In this paper, we provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. We demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and -- more importantly -- to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "galanti|on_the_role_of_neural_collapse_in_transfer_learning", "pdf": "/pdf/92ef43eae5d5f2d0950bab4169ae4d3e49bbe534.pdf", "_bibtex": "@inproceedings{\ngalanti2022on,\ntitle={On the Role of Neural Collapse in Transfer Learning},\nauthor={Tomer Galanti and Andr{\\'a}s Gy{\\\"o}rgy and Marcus Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SwIp410B6aQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "IpctgL7khPp", "original": "8cXuHMJlhT", "number": 2977, "cdate": 1632875657581, "mdate": null, "ddate": null, "tcdate": 1632875657581, "tmdate": 1676330529893, "tddate": null, "forum": "IpctgL7khPp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Information-theoretic Online Memory Selection for Continual Learning", "authorids": ["~Shengyang_Sun4", "~Daniele_Calandriello1", "~Huiyi_Hu1", "~Ang_Li1", "~Michalis_Titsias1"], "authors": ["Shengyang Sun", "Daniele Calandriello", "Huiyi Hu", "Ang Li", "Michalis Titsias"], "keywords": ["Task-free continual learning", "replay memory", "information theoretic", "reservoir sampling"], "abstract": "A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the \\textit{surprise} and the \\textit{learnability} criteria to pick informative points and to avoid outliers. We present a Bayesian model to compute the criteria efficiently by exploiting rank-one matrix structures. We demonstrate that these criteria encourage selecting informative points in a greedy algorithm for online memory selection. Furthermore, by identifying the importance of \\textit{the timing to update the memory}, we introduce a stochastic information-theoretic reservoir sampler (InfoRS), which conducts sampling among selective points with high information. Compared to reservoir sampling, InfoRS demonstrates improved robustness against data imbalance. Finally, empirical performances over continual learning benchmarks manifest its efficiency and efficacy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sun|informationtheoretic_online_memory_selection_for_continual_learning", "pdf": "/pdf/c23f5db3dfaa1687d370340809b6da378029c78e.pdf", "one-sentence_summary": "We present information-theoretic algorithms to tackle the online memory selection problem in task-free and data imbalanced continual learning.", "data": "", "_bibtex": "@inproceedings{\nsun2022informationtheoretic,\ntitle={Information-theoretic Online Memory Selection for Continual Learning},\nauthor={Shengyang Sun and Daniele Calandriello and Huiyi Hu and Ang Li and Michalis Titsias},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=IpctgL7khPp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "XHUxf5aRB3s", "original": "IrofOGydDew", "number": 2975, "cdate": 1632875657446, "mdate": null, "ddate": null, "tcdate": 1632875657446, "tmdate": 1676330530063, "tddate": null, "forum": "XHUxf5aRB3s", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Dealing with Non-Stationarity in MARL via Trust-Region Decomposition", "authorids": ["~Wenhao_Li2", "~Xiangfeng_Wang1", "~Bo_Jin1", "~Junjie_Sheng1", "~Hongyuan_Zha1"], "authors": ["Wenhao Li", "Xiangfeng Wang", "Bo Jin", "Junjie Sheng", "Hongyuan Zha"], "keywords": ["Nonstationarity", "Trust-Region Methods", "Multi-Agent Reinforcement Learning"], "abstract": "Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are inevitably inconsistent and disparate. In this paper, we introduce a novel notion, the $\\delta$-$stationarity$ measurement, to explicitly measure the non-stationarity of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. A straightforward but highly non-trivial way is to control the joint policies' divergence, which is difficult to estimate accurately by imposing the trust-region constraint on the joint policy. Although it has lower computational complexity to decompose the joint policy and impose trust-region constraints on the factorized policies, simple policy factorization like mean-field approximation will lead to more considerable policy divergence, which can be considered as the trust-region decomposition dilemma. We model the joint policy as a pairwise Markov random field and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust-region of the local policies adaptively in an end-to-end manner. MAMT can approximately constrain the consecutive joint policies' divergence to satisfy $\\delta$-stationarity and alleviate the non-stationarity problem. Our method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|dealing_with_nonstationarity_in_marl_via_trustregion_decomposition", "pdf": "/pdf/533ea53e5b32c81e14b06b4528f54a68836c63a0.pdf", "supplementary_material": "/attachment/d9c695cd93c7205a3899766695e25ab3c0607aa9.zip", "_bibtex": "@inproceedings{\nli2022dealing,\ntitle={Dealing with Non-Stationarity in {MARL} via Trust-Region Decomposition},\nauthor={Wenhao Li and Xiangfeng Wang and Bo Jin and Junjie Sheng and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XHUxf5aRB3s}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "kF9DZQQrU0w", "original": "Gujk6MUdIfB", "number": 2974, "cdate": 1632875657375, "mdate": null, "ddate": null, "tcdate": 1632875657375, "tmdate": 1697934661707, "tddate": null, "forum": "kF9DZQQrU0w", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks", "authorids": ["~Stephan_Sloth_Lorenzen1", "~Christian_Igel1", "~Mads_Nielsen2"], "authors": ["Stephan Sloth Lorenzen", "Christian Igel", "Mads Nielsen"], "keywords": ["information bottleneck", "quantization", "neural network"], "abstract": "The information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Notably, separate fitting and compression phases  during training have been reported. This led to some controversy including claims that the observations are not reproducible and strongly dependent on the type of activation function used as well as on the way the MI is estimated. Our study confirms that  different ways of binning  when computing the MI lead to qualitatively different results, either supporting or refusing IB conjectures.\nTo resolve the controversy, we study the IB principle in settings where MI is non-trivial and can be computed exactly. We monitor the dynamics of quantized neural networks, that is, we discretize the whole deep learning system so that no approximation is required when computing the MI. This allows us to quantify the information flow without measurement errors. \nIn this setting, we observed a fitting phase for all layers and a compression phase for the output layer in all experiments; the compression in the hidden layers was dependent on the type of activation function. Our study shows that the initial IB results were not artifacts of binning when computing the MI. However, the critical claim that the compression phase may not be observed for some networks also holds true.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lorenzen|information_bottleneck_exact_analysis_of_quantized_neural_networks", "pdf": "/pdf/b617eacd043db498a3da393c1fd34223f0f25f55.pdf", "one-sentence_summary": "We investigate the information bottleneck in quantized neural networks, allowing us to compute the exact mutual information and provide an analysis free from estimation artifacts.", "supplementary_material": "/attachment/bfe7acd343967f1901729b4096b52d95c46e1ca9.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.12912/code)", "_bibtex": "@inproceedings{\nlorenzen2022information,\ntitle={Information Bottleneck: Exact Analysis of (Quantized) Neural Networks},\nauthor={Stephan Sloth Lorenzen and Christian Igel and Mads Nielsen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kF9DZQQrU0w}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "XLxhEjKNbXj", "original": "1tC3s6RQ4VO", "number": 2965, "cdate": 1632875656758, "mdate": null, "ddate": null, "tcdate": 1632875656758, "tmdate": 1676330530415, "tddate": null, "forum": "XLxhEjKNbXj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GLASS: GNN with Labeling Tricks for Subgraph Representation Learning", "authorids": ["~Xiyuan_Wang1", "~Muhan_Zhang1"], "authors": ["Xiyuan Wang", "Muhan Zhang"], "keywords": [], "abstract": "Despite the remarkable achievements of Graph Neural Networks (GNNs) on graph representation learning, few works have tried to use them to predict properties of subgraphs in the whole graph. The existing state-of-the-art method SubGNN introduces an overly complicated subgraph-level GNN model which synthesizes three artificial channels each of which has two carefully designed subgraph-level message passing modules, yet only slightly outperforms a plain GNN which performs node-level message passing and then pools node embeddings within the subgraph. By analyzing SubGNN and plain GNNs, we find that the key for subgraph representation learning might be to distinguish nodes inside and outside the subgraph. With this insight, we propose an expressive and scalable labeling trick, namely max-zero-one, to enhance plain GNNs for subgraph tasks. The resulting model is called GLASS (GNN with LAbeling trickS for Subgraph). We theoretically characterize GLASS's expressive power. Compared with SubGNN, GLASS is more expressive, more scalable, and easier to implement. Experiments on eight benchmark datasets show that GLASS outperforms the strongest baseline by $14.8\\%$ on average. And ablation analysis shows that our max-zero-one labeling trick can boost the performance of a plain GNN by up to $105\\%$ in maximum, which illustrates the effectiveness of labeling trick on subgraph tasks. Furthermore, training a GLASS model only takes $37\\%$ time needed for a SubGNN on average. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|glass_gnn_with_labeling_tricks_for_subgraph_representation_learning", "pdf": "/pdf/68ad0d714b6345734d34cf7edafb258497151385.pdf", "supplementary_material": "/attachment/96c54aaacec89e6855925cba2d3bed81d7952a71.zip", "_bibtex": "@inproceedings{\nwang2022glass,\ntitle={{GLASS}: {GNN} with Labeling Tricks for Subgraph Representation Learning},\nauthor={Xiyuan Wang and Muhan Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XLxhEjKNbXj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "DnG75_KyHjX", "original": "mF4U9PidFrN", "number": 2964, "cdate": 1632875656692, "mdate": null, "ddate": null, "tcdate": 1632875656692, "tmdate": 1676330530493, "tddate": null, "forum": "DnG75_KyHjX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MoReL: Multi-omics Relational Learning", "authorids": ["~Arman_Hasanzadeh1", "~Ehsan_Hajiramezanali1", "~Nick_Duffield1", "~Xiaoning_Qian2"], "authors": ["Arman Hasanzadeh", "Ehsan Hajiramezanali", "Nick Duffield", "Xiaoning Qian"], "keywords": ["relational learning", "data integration", "multi-view learning", "Bayesian generative model"], "abstract": "Multi-omics data analysis has the potential to discover hidden molecular interactions, revealing potential regulatory and/or signal transduction pathways for cellular processes of interest when studying life and disease systems. One of critical challenges when dealing with real-world multi-omics data is that they may manifest heterogeneous structures and data quality as often existing data may be collected from different subjects under different conditions for each type of omics data. We propose a novel deep Bayesian generative model to efficiently infer a multi-partite graph encoding molecular interactions across such heterogeneous views, using a fused Gromov-Wasserstein (FGW) regularization between latent representations of corresponding views for integrative analysis. With such an optimal transport regularization in the deep Bayesian generative model, it not only allows incorporating view-specific side information, either with graph-structured or unstructured data in different views, but also increases the model flexibility with the distribution-based regularization. This allows efficient alignment of heterogeneous latent variable distributions to derive reliable interaction predictions compared to the existing point-based graph embedding methods. Our experiments on several real-world datasets demonstrate enhanced performance of MoReL in inferring meaningful interactions compared to existing baselines.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hasanzadeh|morel_multiomics_relational_learning", "pdf": "/pdf/081ce5efc31fe21daad00db739aa3979506ff0d3.pdf", "_bibtex": "@inproceedings{\nhasanzadeh2022morel,\ntitle={MoReL: Multi-omics Relational Learning},\nauthor={Arman Hasanzadeh and Ehsan Hajiramezanali and Nick Duffield and Xiaoning Qian},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DnG75_KyHjX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "BwPaPxwgyQb", "original": "gvhRV3fFq4E", "number": 2961, "cdate": 1632875656558, "mdate": null, "ddate": null, "tcdate": 1632875656558, "tmdate": 1676330530622, "tddate": null, "forum": "BwPaPxwgyQb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Provable Learning-based Algorithm For Sparse Recovery", "authorids": ["~Xinshi_Chen1", "~Haoran_Sun2", "~Le_Song1"], "authors": ["Xinshi Chen", "Haoran Sun", "Le Song"], "keywords": ["learning to learn", "sparse parameter estimation", "learning to optimize", "algorithm unrolling", "generalization bound"], "abstract": "Recovering sparse parameters from observational data is a fundamental problem in machine learning with wide applications. Many classic algorithms can solve this problem with theoretical guarantees, but their performances rely on choosing the correct hyperparameters. Besides, hand-designed algorithms do not fully exploit the particular problem distribution of interest. In this work, we propose a deep learning method for algorithm learning called PLISA (Provable Learning-based Iterative Sparse recovery Algorithm). PLISA is designed by unrolling a classic path-following algorithm for sparse recovery, with some components being more flexible and learnable. We theoretically show the improved recovery accuracy achievable by PLISA. Furthermore, we analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set. This paper contains novel theoretical contributions to the area of learning-based algorithms in the sense that (i) PLISA is generically applicable to a broad class of sparse estimation problems, (ii) generalization analysis has received less attention so far, and (iii) our analysis makes novel connections between the generalization ability and algorithmic properties such as stability and convergence of the unrolled algorithm, which leads to a tighter bound that can explain the empirical observations. The techniques could potentially be applied to analyze other learning-based algorithms in the literature.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|provable_learningbased_algorithm_for_sparse_recovery", "pdf": "/pdf/5b72c15b934bbc248f48b3fadda48f2ffbdfaece.pdf", "_bibtex": "@inproceedings{\nchen2022provable,\ntitle={Provable Learning-based Algorithm For Sparse Recovery},\nauthor={Xinshi Chen and Haoran Sun and Le Song},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BwPaPxwgyQb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "jJOjjiZHy3h", "original": "Z4NYjynYJ", "number": 2957, "cdate": 1632875656282, "mdate": null, "ddate": null, "tcdate": 1632875656282, "tmdate": 1697934663348, "tddate": null, "forum": "jJOjjiZHy3h", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Defending Against Image Corruptions Through Adversarial Augmentations", "authorids": ["~Dan_Andrei_Calian1", "~Florian_Stimberg1", "~Olivia_Wiles1", "~Sylvestre-Alvise_Rebuffi1", "~Andr\u00e1s_Gy\u00f6rgy2", "~Timothy_A_Mann1", "~Sven_Gowal2"], "authors": ["Dan Andrei Calian", "Florian Stimberg", "Olivia Wiles", "Sylvestre-Alvise Rebuffi", "Andr\u00e1s Gy\u00f6rgy", "Timothy A Mann", "Sven Gowal"], "keywords": ["robustness", "adversarial training", "image corruptions"], "abstract": "Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on Lp-norm bounded perturbations focuses on defenses against worst-case corruptions. In this work, we reconcile both approaches by proposing AdversarialAugment, a technique which optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. We theoretically motivate our method and give sufficient conditions for the consistency of its idealized version as well as that of DeepAugment. Our classifiers improve upon the state-of-the-art on common image corruption benchmarks conducted in expectation on CIFAR-10-C and improve worst-case performance against Lp-norm bounded perturbations on both CIFAR-10 and ImageNet.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "calian|defending_against_image_corruptions_through_adversarial_augmentations", "pdf": "/pdf/22ad1c6524e3463a96b8f63f8935a1ed975fbede.pdf", "one-sentence_summary": "Our theoretically-supported method finds adversarial examples by optimizing over the weights of pre-trained autoencoders, and yields classifiers with improved robustness to image corruptions.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2104.01086/code)", "_bibtex": "@inproceedings{\ncalian2022defending,\ntitle={Defending Against Image Corruptions Through Adversarial Augmentations},\nauthor={Dan Andrei Calian and Florian Stimberg and Olivia Wiles and Sylvestre-Alvise Rebuffi and Andr{\\'a}s Gy{\\\"o}rgy and Timothy A Mann and Sven Gowal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=jJOjjiZHy3h}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 34}}, {"id": "Zf4ZdI4OQPV", "original": "CeRzuQn7P_", "number": 2950, "cdate": 1632875655795, "mdate": null, "ddate": null, "tcdate": 1632875655795, "tmdate": 1697934663600, "tddate": null, "forum": "Zf4ZdI4OQPV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Attacking deep networks with surrogate-based adversarial black-box methods is easy", "authorids": ["~Nicholas_A._Lord1", "~Romain_Mueller1", "~Luca_Bertinetto1"], "authors": ["Nicholas A. Lord", "Romain Mueller", "Luca Bertinetto"], "keywords": ["adversarial attacks", "black-box attacks", "network robustness", "network analysis"], "abstract": "A recent line of work on black-box adversarial attacks has revived the use of transfer from surrogate models by integrating it into query-based search. However, we find that existing approaches of this type underperform their potential, and can be overly complicated besides. Here, we provide a short and simple algorithm which achieves state-of-the-art results through a search which uses the surrogate network's class-score gradients, with no need for other priors or heuristics. The guiding assumption of the algorithm is that the studied networks are in a fundamental sense learning similar functions, and that a transfer attack from one to the other should thus be fairly \"easy\". This assumption is validated by the extremely low query counts and failure rates achieved: e.g. an untargeted attack on a VGG-16 ImageNet network using a ResNet-152 as the surrogate yields a median query count of 6 at a success rate of 99.9%. Code is available at https://github.com/fiveai/GFCS.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lord|attacking_deep_networks_with_surrogatebased_adversarial_blackbox_methods_is_easy", "pdf": "/pdf/a443a40817b896b8fa11383453c012d785af6a17.pdf", "one-sentence_summary": "We present a simple and extremely effective score- and surrogate-based black-box adversarial attack which uses a specific gradient/Jacobian transfer strategy.", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.08725/code)", "_bibtex": "@inproceedings{\nlord2022attacking,\ntitle={Attacking deep networks with surrogate-based adversarial black-box methods is easy},\nauthor={Nicholas A. Lord and Romain Mueller and Luca Bertinetto},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Zf4ZdI4OQPV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "Lm8T39vLDTE", "original": "B-fPLaLS-b1", "number": 2941, "cdate": 1632875655177, "mdate": null, "ddate": null, "tcdate": 1632875655177, "tmdate": 1697934665137, "tddate": null, "forum": "Lm8T39vLDTE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Autoregressive Diffusion Models", "authorids": ["~Emiel_Hoogeboom1", "~Alexey_A._Gritsenko1", "~Jasmijn_Bastings1", "~Ben_Poole1", "~Rianne_van_den_Berg1", "~Tim_Salimans1"], "authors": ["Emiel Hoogeboom", "Alexey A. Gritsenko", "Jasmijn Bastings", "Ben Poole", "Rianne van den Berg", "Tim Salimans"], "keywords": ["diffusion", "autoregressive models", "lossless compression"], "abstract": "We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hoogeboom|autoregressive_diffusion_models", "pdf": "/pdf/1e4e9d350d86450e306f14f662b9cdf718fce184.pdf", "one-sentence_summary": "A new model class for discrete variables encompassing order agnostic autoregressive models and absorbing discrete diffusion.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02037/code)", "_bibtex": "@inproceedings{\nhoogeboom2022autoregressive,\ntitle={Autoregressive Diffusion Models},\nauthor={Emiel Hoogeboom and Alexey A. Gritsenko and Jasmijn Bastings and Ben Poole and Rianne van den Berg and Tim Salimans},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Lm8T39vLDTE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "H94a1_Pyr-6", "original": "37WPti8rO1R", "number": 2939, "cdate": 1632875655039, "mdate": null, "ddate": null, "tcdate": 1632875655039, "tmdate": 1697934665341, "tddate": null, "forum": "H94a1_Pyr-6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Auto-scaling Vision Transformers without Training", "authorids": ["~Wuyang_Chen1", "~Wei_Huang6", "~Xianzhi_Du4", "~Xiaodan_Song1", "~Zhangyang_Wang1", "~Denny_Zhou1"], "authors": ["Wuyang Chen", "Wei Huang", "Xianzhi Du", "Xiaodan Song", "Zhangyang Wang", "Denny Zhou"], "keywords": ["vision transformer", "neural architecture search", "training-free search", "efficient training"], "abstract": "This work targets automated designing and scaling of Vision Transformers (ViTs). The motivation comes from two pain spots: 1) the lack of efficient and principled methods for designing and scaling ViTs; 2) the tremendous computational cost of training ViT that is much heavier than its convolution counterpart. To tackle these issues, we propose As-ViT, an auto-scaling framework for ViTs without training, which automatically discovers and scales up ViTs in an efficient and principled manner. Specifically, we first design a \"seed\" ViT topology by leveraging a training-free search process. This extremely fast search is fulfilled by a comprehensive study of ViT's network complexity, yielding a strong Kendall-tau correlation with ground-truth accuracies. Second, starting from the \"seed\" topology, we automate the scaling rule for ViTs by growing widths/depths to different ViT layers. This results in a series of architectures with different numbers of parameters in a single run. Finally, based on the observation that ViTs can tolerate coarse tokenization in early training stages, we propose a progressive tokenization strategy to train ViTs faster and cheaper. As a unified framework, As-ViT achieves strong performance on classification (83.5% top1 on ImageNet-1k) and detection (52.7% mAP on COCO) without any manual crafting nor scaling of ViT architectures: the end-to-end model design and scaling process costs only 12 hours on one V100 GPU. Our code is available at https://github.com/VITA-Group/AsViT.", "one-sentence_summary": "We automate the design and scaling of vision transformers without any training, achieving state-of-the-art performance on ImageNet classification and COCO object detection.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|autoscaling_vision_transformers_without_training", "pdf": "/pdf/ef4c46fde37c64720cfb924254715a045c828420.pdf", "supplementary_material": "/attachment/dba5196ff5e54a10474c6fda22c90ef3376e36f6.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.11921/code)", "_bibtex": "@inproceedings{\nchen2022autoscaling,\ntitle={Auto-scaling Vision Transformers without Training},\nauthor={Wuyang Chen and Wei Huang and Xianzhi Du and Xiaodan Song and Zhangyang Wang and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=H94a1_Pyr-6}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "KPEFXR1HdIo", "original": "aH0rzlHsxp", "number": 2937, "cdate": 1632875654904, "mdate": null, "ddate": null, "tcdate": 1632875654904, "tmdate": 1697934665712, "tddate": null, "forum": "KPEFXR1HdIo", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fine-grained Differentiable Physics: A Yarn-level Model for Fabrics", "authorids": ["~Deshan_Gong1", "~Zhanxing_Zhu1", "~Andrew_J._Bulpitt1", "~He_Wang6"], "authors": ["Deshan Gong", "Zhanxing Zhu", "Andrew J. Bulpitt", "He Wang"], "keywords": [], "abstract": "Differentiable physics modeling combines physics models with gradient-based learning to provide model explicability and data efficiency. It has been used to learn dynamics, solve inverse problems and facilitate design, and is at its inception of impact. Current successes have concentrated on general physics models such as rigid bodies, deformable sheets, etc, assuming relatively simple structures and forces. Their granularity is intrinsically coarse and therefore incapable of modelling complex physical phenomena. Fine-grained models are still to be developed to incorporate sophisticated material structures and force interactions with gradient-based learning. Following this motivation, we propose a new differentiable fabrics model for composite materials such as cloths, where we dive into the granularity of yarns and model individual yarn physics and yarn-to-yarn interactions. To this end, we propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. These forces, albeit applied to cloths, are ubiquitous in various physical systems. Through comprehensive evaluation and comparison, we demonstrate our model's $\\textit{explicability}$ in learning meaningful physical parameters, $\\textit{versatility}$ in incorporating complex physical structures and heterogeneous materials, $\\textit{data-efficiency}$ in learning, and $\\textit{high-fidelity}$ in capturing subtle dynamics.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gong|finegrained_differentiable_physics_a_yarnlevel_model_for_fabrics", "pdf": "/pdf/6d1903facd585c8674990513d5c9f2112b98160d.pdf", "supplementary_material": "/attachment/521a2efe339cce59a7c62f9c9b81400a3b42a5fd.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.00504/code)", "_bibtex": "@inproceedings{\ngong2022finegrained,\ntitle={Fine-grained Differentiable Physics: A Yarn-level Model for Fabrics},\nauthor={Deshan Gong and Zhanxing Zhu and Andrew J. Bulpitt and He Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KPEFXR1HdIo}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "6y2KBh-0Fd9", "original": "NWJqU06BSGm", "number": 2934, "cdate": 1632875654708, "mdate": null, "ddate": null, "tcdate": 1632875654708, "tmdate": 1676330532094, "tddate": null, "forum": "6y2KBh-0Fd9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Revisiting flow generative models for Out-of-distribution detection", "authorids": ["~Dihong_Jiang1", "~Sun_Sun1", "~Yaoliang_Yu1"], "authors": ["Dihong Jiang", "Sun Sun", "Yaoliang Yu"], "keywords": ["flow models", "out-of-distribution detection", "random projection", "distribution comparison"], "abstract": "Deep generative models have been widely used in practical applications such as the detection of out-of-distribution (OOD) data. In this work,  we aim to re-examine the potential of generative flow models in OOD detection. We first propose a simple combination of univariate one-sample statistical test (e.g., Kolmogorov-Smirnov) and random projections in the latent space of flow models to perform OOD detection.  Then, we propose a two-sample version of our test to account for imperfect flow models. Quite distinctly, our method does not pose parametric assumptions on OOD data and is capable of exploiting any flow model. Experimentally, firstly we confirm the efficacy of our method against state-of-the-art baselines through extensive experiments on several image datasets; secondly we investigate the relationship between model accuracy (e.g., the generation quality) and the OOD detection performance, and found surprisingly that they are not always positively correlated; and thirdly we show that detection in the latent space of flow models generally outperforms detection in the sample space across various OOD datasets, hence highlighting the benefits of training a flow model.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|revisiting_flow_generative_models_for_outofdistribution_detection", "pdf": "/pdf/beca73c1366c5e014f7cb574e09aa9de9598f794.pdf", "data": "", "_bibtex": "@inproceedings{\njiang2022revisiting,\ntitle={Revisiting flow generative models for Out-of-distribution detection},\nauthor={Dihong Jiang and Sun Sun and Yaoliang Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6y2KBh-0Fd9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "Te5ytkqsnl", "original": "-KU6y_DVq8", "number": 2929, "cdate": 1632875654365, "mdate": null, "ddate": null, "tcdate": 1632875654365, "tmdate": 1697934666708, "tddate": null, "forum": "Te5ytkqsnl", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Missingness Bias in Model Debugging", "authorids": ["~Saachi_Jain1", "~Hadi_Salman1", "~Eric_Wong1", "~Pengchuan_Zhang1", "~Vibhav_Vineet5", "~Sai_Vemprala1", "~Aleksander_Madry1"], "authors": ["Saachi Jain", "Hadi Salman", "Eric Wong", "Pengchuan Zhang", "Vibhav Vineet", "Sai Vemprala", "Aleksander Madry"], "keywords": ["model debugging", "vision transformers", "missingness"], "abstract": "Missingness, or the absence of features from an input, is a concept fundamental to many model debugging tools. However, in computer vision, pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels, which may in turn introduce bias into the debugging process. We study such biases and, in particular, show how transformer-based architectures can enable a more natural implementation of missingness, which side-steps these issues and improves the reliability of model debugging in practice.\n", "one-sentence_summary": "We investigate how current missingness approximations for model debugging can impose undesirable biases on the model predictions and hinder our ability to debug models, and we show how transformer-based architectures can side-step these issues.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jain|missingness_bias_in_model_debugging", "pdf": "/pdf/c17968a4f7476f9b48d38625f1f4024268d299a9.pdf", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2204.08945/code)", "_bibtex": "@inproceedings{\njain2022missingness,\ntitle={Missingness Bias in Model Debugging},\nauthor={Saachi Jain and Hadi Salman and Eric Wong and Pengchuan Zhang and Vibhav Vineet and Sai Vemprala and Aleksander Madry},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Te5ytkqsnl}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "GQd7mXSPua", "original": "rpPFbK1weNF", "number": 2905, "cdate": 1632875652724, "mdate": null, "ddate": null, "tcdate": 1632875652724, "tmdate": 1676330533571, "tddate": null, "forum": "GQd7mXSPua", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Meta Learning Low Rank Covariance Factors for Energy Based Deterministic Uncertainty", "authorids": ["~Jeffrey_Ryan_Willette1", "~Hae_Beom_Lee1", "~Juho_Lee2", "~Sung_Ju_Hwang1"], "authors": ["Jeffrey Ryan Willette", "Hae Beom Lee", "Juho Lee", "Sung Ju Hwang"], "keywords": ["calibration", "meta-learning"], "abstract": "Numerous recent works utilize bi-Lipschitz regularization of neural network layers to preserve relative distances between data instances in the feature spaces of each layer. This distance sensitivity with respect to the data aids in tasks such as uncertainty calibration and out-of-distribution (OOD) detection. In previous works, features extracted with a distance sensitive model are used to construct feature covariance matrices which are used in deterministic uncertainty estimation or OOD detection. However, in cases where there is a distribution over tasks, these methods result in covariances which are sub-optimal, as they may not leverage all of the meta information which can be shared among tasks. With the use of an attentive set encoder, we propose to meta learn either diagonal or diagonal plus low-rank factors to efficiently construct task specific covariance matrices. Additionally, we propose an inference procedure which utilizes scaled energy to achieve a final predictive distribution which is well calibrated under a distributional dataset shift. ", "one-sentence_summary": "We propose a novel meta learning algorithm which learns low rank covariance factors, and utilizes an energy-based inference to achieve a calibrated prediction. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "willette|meta_learning_low_rank_covariance_factors_for_energy_based_deterministic_uncertainty", "pdf": "/pdf/62e7fc9d01f6386911b5a5f46df746fe07a8c819.pdf", "supplementary_material": "/attachment/e13025311124d129bb380faf1997dadc5b06f9c2.zip", "data": "", "_bibtex": "@inproceedings{\nwillette2022meta,\ntitle={Meta Learning Low Rank Covariance Factors for Energy Based Deterministic Uncertainty},\nauthor={Jeffrey Ryan Willette and Hae Beom Lee and Juho Lee and Sung Ju Hwang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=GQd7mXSPua}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "aD7uesX1GF_", "original": "MUQg7HeJfOg", "number": 2904, "cdate": 1632875652658, "mdate": null, "ddate": null, "tcdate": 1632875652658, "tmdate": 1697934668729, "tddate": null, "forum": "aD7uesX1GF_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Conditional Object-Centric Learning from Video", "authorids": ["~Thomas_Kipf2", "~Gamaleldin_Fathy_Elsayed1", "~Aravindh_Mahendran2", "~Austin_Stone1", "~Sara_Sabour1", "~Georg_Heigold1", "~Rico_Jonschkowski1", "~Alexey_Dosovitskiy1", "~Klaus_Greff1"], "authors": ["Thomas Kipf", "Gamaleldin Fathy Elsayed", "Aravindh Mahendran", "Austin Stone", "Sara Sabour", "Georg Heigold", "Rico Jonschkowski", "Alexey Dosovitskiy", "Klaus Greff"], "keywords": [], "abstract": "Object-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built. Recent work on simple 2D and 3D datasets has shown that models with object-centric inductive biases can learn to segment and represent meaningful objects from the statistical structure of the data alone without the need for any supervision. However, such fully-unsupervised methods still fail to scale to diverse realistic data, despite the use of increasingly complex inductive biases such as priors for the size of objects or the 3D geometry of the scene. In this paper, we instead take a weakly-supervised approach and focus on how 1) using the temporal dynamics of video data in the form of optical flow and 2) conditioning the model on simple object location cues can be used to enable segmenting and tracking objects in significantly more realistic synthetic data. We introduce a sequential extension to Slot Attention which we train to predict optical flow for realistic looking synthetic scenes and show that conditioning the initial state of this model on a small set of hints, such as center of mass of objects in the first frame, is sufficient to significantly improve instance segmentation. These benefits generalize beyond the training distribution to novel objects, novel backgrounds, and to longer video sequences. We also find that such initial-state-conditioning can be used during inference as a flexible interface to query the model for specific objects or parts of objects, which could pave the way for a range of weakly-supervised approaches and allow more effective interaction with trained models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kipf|conditional_objectcentric_learning_from_video", "pdf": "/pdf/1316f4949f9f4e100ae886bd9d15e275c1b8a39b.pdf", "supplementary_material": "/attachment/dc4c20bb657ae995416042d310bc4a52027c0095.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2111.12594/code)", "_bibtex": "@inproceedings{\nkipf2022conditional,\ntitle={Conditional Object-Centric Learning from Video},\nauthor={Thomas Kipf and Gamaleldin Fathy Elsayed and Aravindh Mahendran and Austin Stone and Sara Sabour and Georg Heigold and Rico Jonschkowski and Alexey Dosovitskiy and Klaus Greff},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aD7uesX1GF_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "f2OYVDyfIB", "original": "woGPbVl6gcP", "number": 2891, "cdate": 1632875651740, "mdate": null, "ddate": null, "tcdate": 1632875651740, "tmdate": 1676330534195, "tddate": null, "forum": "f2OYVDyfIB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Scale Efficiently: Insights from Pretraining and Finetuning Transformers", "authorids": ["~Yi_Tay1", "~Mostafa_Dehghani1", "~Jinfeng_Rao1", "~William_Fedus2", "~Samira_Abnar1", "~Hyung_Won_Chung1", "~Sharan_Narang1", "~Dani_Yogatama2", "~Ashish_Vaswani1", "~Donald_Metzler1"], "authors": ["Yi Tay", "Mostafa Dehghani", "Jinfeng Rao", "William Fedus", "Samira Abnar", "Hyung Won Chung", "Sharan Narang", "Dani Yogatama", "Ashish Vaswani", "Donald Metzler"], "keywords": ["transformers", "attention", "deep learning"], "abstract": "There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\\% fewer parameters and training 40\\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis.", "one-sentence_summary": "Scaling laws for upstream and downstream tasks ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tay|scale_efficiently_insights_from_pretraining_and_finetuning_transformers", "pdf": "/pdf/bd048936d58108f06fb49d610e86a9a5dcb9b281.pdf", "data": "", "_bibtex": "@inproceedings{\ntay2022scale,\ntitle={Scale Efficiently: Insights from Pretraining and Finetuning Transformers},\nauthor={Yi Tay and Mostafa Dehghani and Jinfeng Rao and William Fedus and Samira Abnar and Hyung Won Chung and Sharan Narang and Dani Yogatama and Ashish Vaswani and Donald Metzler},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=f2OYVDyfIB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "Ow1C7s3UcY", "original": "G6YzEM_31Fc", "number": 2889, "cdate": 1632875651592, "mdate": null, "ddate": null, "tcdate": 1632875651592, "tmdate": 1697934670465, "tddate": null, "forum": "Ow1C7s3UcY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Vitruvion: A Generative Model of Parametric CAD Sketches", "authorids": ["~Ari_Seff1", "~Wenda_Zhou1", "~Nick_Richardson1", "~Ryan_P_Adams1"], "authors": ["Ari Seff", "Wenda Zhou", "Nick Richardson", "Ryan P Adams"], "keywords": ["generative modeling", "CAD", "transformers", "design", "geometric constraints"], "abstract": "Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards. The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements. This relational specification can be viewed as the construction of a constraint program, allowing edits to coherently propagate to other parts of the design. Machine learning offers the intriguing possibility of accelerating the design process via generative modeling of these structures, enabling new tools such as autocompletion, constraint inference, and conditional synthesis. In this work, we present such an approach to generative modeling of parametric CAD sketches, which constitute the basic computational building blocks of modern mechanical design. Our model, trained on real-world designs from the SketchGraphs dataset, autoregressively synthesizes sketches as sequences of primitives, with initial coordinates, and constraints that reference back to the sampled primitives. As samples from the model match the constraint graph representation used in standard CAD software, they may be directly imported, solved, and edited according to downstream design tasks. In addition, we condition the model on various contexts, including partial sketches (primers) and images of hand-drawn sketches. Evaluation of the proposed approach demonstrates its ability to synthesize realistic CAD sketches and its potential to aid the mechanical design workflow.", "one-sentence_summary": "We build a generative model for parametric CAD sketches and use it to perform autocompletion and hand drawing conversion tasks relevant to design.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "seff|vitruvion_a_generative_model_of_parametric_cad_sketches", "pdf": "/pdf/38ea22fe4cd42ce014e63702972f486b3c0b3995.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.14124/code)", "_bibtex": "@inproceedings{\nseff2022vitruvion,\ntitle={Vitruvion: A Generative Model of Parametric {CAD} Sketches},\nauthor={Ari Seff and Wenda Zhou and Nick Richardson and Ryan P Adams},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Ow1C7s3UcY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "XJiajt89Omg", "original": "P5WM2xeg-o", "number": 2882, "cdate": 1632875651106, "mdate": null, "ddate": null, "tcdate": 1632875651106, "tmdate": 1697934671094, "tddate": null, "forum": "XJiajt89Omg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Space-Time Graph Neural Networks", "authorids": ["~Samar_Hadou1", "kanac@seas.upenn.edu", "~Alejandro_Ribeiro1"], "authors": ["Samar Hadou", "Charilaos I Kanatsoulis", "Alejandro Ribeiro"], "keywords": ["ST-GNNs", "GNNs", "stability", "graph-time perturbations"], "abstract": "We introduce space-time graph neural network (ST-GNN), a novel GNN architecture, tailored to jointly process the underlying space-time topology of time-varying network data. The cornerstone of our proposed architecture is the composition of time and graph convolutional filters followed by pointwise nonlinear activation functions. We introduce a generic definition of convolution operators that mimic the diffusion process of signals over its underlying support. On top of this definition, we propose space-time graph convolutions that are built upon a composition of time and graph shift operators.  We prove that ST-GNNs with multivariate integral Lipschitz filters are stable to small perturbations in the underlying graphs as well as small perturbations in the time domain caused by time warping. Our analysis shows that small variations in the network topology and time evolution of a system does not significantly affect the performance of ST-GNNs. Numerical experiments with decentralized control systems showcase the effectiveness and stability of the proposed ST-GNNs.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hadou|spacetime_graph_neural_networks", "pdf": "/pdf/45cb8a6a28cdfdbd34c4053a847215f8bfe894a0.pdf", "one-sentence_summary": "We introduce space-time graph neural network (ST-GNN) tailored to jointly process the underlying space-time topology of time-varying network data, and we show its stability to perturbations.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02880/code)", "_bibtex": "@inproceedings{\nhadou2022spacetime,\ntitle={Space-Time Graph Neural Networks},\nauthor={Samar Hadou and Charilaos I Kanatsoulis and Alejandro Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XJiajt89Omg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "bjy5Zb2fo2", "original": "Nz30I4AaBIw", "number": 2879, "cdate": 1632875650900, "mdate": null, "ddate": null, "tcdate": 1632875650900, "tmdate": 1676330534764, "tddate": null, "forum": "bjy5Zb2fo2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs", "authorids": ["~Jason_McEwen1", "christophergrwallis@gmail.com", "~Augustine_N._Mavor-Parker1"], "authors": ["Jason McEwen", "Christopher Wallis", "Augustine N. Mavor-Parker"], "keywords": [], "abstract": "Convolutional neural networks (CNNs) constructed natively on the sphere have been developed recently and shown to be highly effective for the analysis of spherical data.  While an efficient framework has been formulated, spherical CNNs are nevertheless highly computationally demanding; typically they cannot scale beyond spherical signals of thousands of pixels.  We develop scattering networks constructed natively on the sphere that provide a powerful representational space for spherical data.  Spherical scattering networks are computationally scalable and exhibit rotational equivariance, while their representational space is invariant to isometries and provides efficient and stable signal representations.  By integrating scattering networks as an additional type of layer in the generalized spherical CNN framework, we show how they can be leveraged to scale spherical CNNs to the high-resolution data typical of many practical applications, with spherical signals of many tens of megapixels and beyond.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mcewen|scattering_networks_on_the_sphere_for_scalable_and_rotationally_equivariant_spherical_cnns", "pdf": "/pdf/583873ca12cbac94a9df056b123925c4d9781ccd.pdf", "one-sentence_summary": "Scaling rotationally equivariant spherical CNNs to high-resolution data through spherical scattering networks", "_bibtex": "@inproceedings{\nmcewen2022scattering,\ntitle={Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical {CNN}s},\nauthor={Jason McEwen and Christopher Wallis and Augustine N. Mavor-Parker},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bjy5Zb2fo2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "xkjqJYqRJy", "original": "Ktim8iPbWv", "number": 2867, "cdate": 1632875650081, "mdate": null, "ddate": null, "tcdate": 1632875650081, "tmdate": 1697934672608, "tddate": null, "forum": "xkjqJYqRJy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bayesian Neural Network Priors Revisited", "authorids": ["~Vincent_Fortuin1", "~Adri\u00e0_Garriga-Alonso1", "~Sebastian_W._Ober1", "~Florian_Wenzel1", "~Gunnar_Ratsch1", "~Richard_E_Turner1", "~Mark_van_der_Wilk1", "~Laurence_Aitchison1"], "authors": ["Vincent Fortuin", "Adri\u00e0 Garriga-Alonso", "Sebastian W. Ober", "Florian Wenzel", "Gunnar Ratsch", "Richard E Turner", "Mark van der Wilk", "Laurence Aitchison"], "keywords": ["Bayesian deep learning", "Bayesian neural networks", "Priors"], "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets.", "one-sentence_summary": "Using BNN priors that are not isotropic Gaussians can improve performance and reduce the cold posterior effect.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fortuin|bayesian_neural_network_priors_revisited", "pdf": "/pdf/5c1c882b8e944c1b7c1eef378588e7c5b8144659.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2102.06571/code)", "_bibtex": "@inproceedings{\nfortuin2022bayesian,\ntitle={Bayesian Neural Network Priors Revisited},\nauthor={Vincent Fortuin and Adri{\\`a} Garriga-Alonso and Sebastian W. Ober and Florian Wenzel and Gunnar Ratsch and Richard E Turner and Mark van der Wilk and Laurence Aitchison},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xkjqJYqRJy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "6NePxZwfae", "original": "FWdw1X1OKy", "number": 2861, "cdate": 1632875649652, "mdate": null, "ddate": null, "tcdate": 1632875649652, "tmdate": 1676330535892, "tddate": null, "forum": "6NePxZwfae", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Goal-Directed Planning via Hindsight Experience Replay", "authorids": ["~Lorenzo_Moro1", "~Amarildo_Likmeta1", "~Enrico_Prati1", "~Marcello_Restelli1"], "authors": ["Lorenzo Moro", "Amarildo Likmeta", "Enrico Prati", "Marcello Restelli"], "keywords": ["Reinforcement Learning", "Goal-Directed Planning", "Monte Carlo Tree Search"], "abstract": "We consider the problem of goal-directed planning under a deterministic transition model. Monte Carlo Tree Search has shown remarkable performance in solving deterministic control problems. It has been extended from complex continuous domains through function approximators to bias the search of the planning tree in AlphaZero. Nonetheless, these algorithms still struggle with control problems with sparse rewards, such as goal-directed domains, where a positive reward is awarded only when reaching a goal state. In this work, we recast AlphaZero with Hindsight Experience Replay to tackle complex goal-directed planning tasks. We perform a thorough empirical evaluation in several simulated domains, including a novel application to a quantum compiling domain.", "one-sentence_summary": "This paper presents an extension of AlphaZero to tackle sparse reward goal-based tasks", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "moro|goaldirected_planning_via_hindsight_experience_replay", "pdf": "/pdf/97fe32650bb5e2dbad89bb4c2de9b120fc5caea0.pdf", "_bibtex": "@inproceedings{\nmoro2022goaldirected,\ntitle={Goal-Directed Planning via Hindsight Experience Replay},\nauthor={Lorenzo Moro and Amarildo Likmeta and Enrico Prati and Marcello Restelli},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6NePxZwfae}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "EMigfE6ZeS", "original": "muSEX3Z5kHW", "number": 2859, "cdate": 1632875649520, "mdate": null, "ddate": null, "tcdate": 1632875649520, "tmdate": 1676330536027, "tddate": null, "forum": "EMigfE6ZeS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hybrid Random Features", "authorids": ["~Krzysztof_Marcin_Choromanski1", "~Han_Lin1", "~Haoxian_Chen2", "~Arijit_Sehanobish1", "~Yuanzhe_Ma1", "~Deepali_Jain1", "~Jake_Varley1", "~Andy_Zeng1", "~Michael_S_Ryoo1", "~Valerii_Likhosherstov2", "~Dmitry_Kalashnikov1", "~Vikas_Sindhwani1", "~Adrian_Weller1"], "authors": ["Krzysztof Marcin Choromanski", "Han Lin", "Haoxian Chen", "Arijit Sehanobish", "Yuanzhe Ma", "Deepali Jain", "Jake Varley", "Andy Zeng", "Michael S Ryoo", "Valerii Likhosherstov", "Dmitry Kalashnikov", "Vikas Sindhwani", "Adrian Weller"], "keywords": ["random features", "softmax kernel", "attention mechanism", "compositional kernels"], "abstract": "We propose a new class of random feature methods for linearizing softmax and Gaussian kernels called hybrid random features (HRFs) that automatically adapt the quality of kernel estimation to provide most accurate approximation in the defined regions of interest. Special instantiations of HRFs lead to well-known methods such as trigonometric (Rahimi & Recht, 2007) or (recently introduced in the context of linear-attention Transformers) positive random features (Choromanski et al., 2021). By generalizing Bochner\u2019s Theorem for softmax/Gaussian kernels and leveraging random features for compositional kernels, the HRF-mechanism provides strong theoretical guarantees - unbiased approximation and strictly smaller worst-case relative errors than its counterparts.  We conduct exhaustive empirical evaluation of HRF ranging from pointwise kernel estimation experiments, through tests on data admitting clustering structure to benchmarking implicit-attention Transformers (also for downstream Robotics applications), demonstrating its quality in a wide spectrum of machine learning problems.", "one-sentence_summary": "We propose a new class of random feature methods for softmax and Gaussian kernel estimation that are adaptable to provide particularly accurate approximation in the desired regions of interest.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "choromanski|hybrid_random_features", "pdf": "/pdf/21d31a35dc86b24e3937c906608595058a93a24c.pdf", "supplementary_material": "/attachment/1e565f73725ab0fcc22d7315f40dd525194cf5f2.zip", "code": "", "_bibtex": "@inproceedings{\nchoromanski2022hybrid,\ntitle={Hybrid Random Features},\nauthor={Krzysztof Marcin Choromanski and Han Lin and Haoxian Chen and Arijit Sehanobish and Yuanzhe Ma and Deepali Jain and Jake Varley and Andy Zeng and Michael S Ryoo and Valerii Likhosherstov and Dmitry Kalashnikov and Vikas Sindhwani and Adrian Weller},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EMigfE6ZeS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 31}}, {"id": "figzpGMrdD", "original": "DyBSqAK4ZpN", "number": 2857, "cdate": 1632875649383, "mdate": null, "ddate": null, "tcdate": 1632875649383, "tmdate": 1676330536120, "tddate": null, "forum": "figzpGMrdD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pretrained Language Model in Continual Learning: A Comparative Study", "authorids": ["~Tongtong_Wu1", "~Massimo_Caccia1", "~Zhuang_Li1", "~Yuan-Fang_Li1", "gqi@seu.edu.cn", "~Gholamreza_Haffari1"], "authors": ["Tongtong Wu", "Massimo Caccia", "Zhuang Li", "Yuan-Fang Li", "Guilin Qi", "Gholamreza Haffari"], "keywords": ["Continual Learning", "Pre-trained Language Model"], "abstract": "Continual learning (CL) is a  setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many continual learning methods and PLMs, understanding and disentangling their interactions become essential for continued improvement of continual learning performance. In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. Our extensive experimental analyses reveal interesting performance differences across PLMs and across CL methods. Furthermore, our representativeness probing analyses dissect PLMs\u2019 performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches on each layer. Finally, our observations and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques.", "one-sentence_summary": "In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 veins of CL methods on 3 benchmarks in 2 typical incremental settings. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|pretrained_language_model_in_continual_learning_a_comparative_study", "pdf": "/pdf/eecf8ea887fe5b42c7b3b93b2fb938fed766f7e2.pdf", "supplementary_material": "/attachment/8c5d1ea68a211565fb9255957340c8c65a5781d1.zip", "data": "", "_bibtex": "@inproceedings{\nwu2022pretrained,\ntitle={Pretrained Language Model in Continual Learning: A Comparative Study},\nauthor={Tongtong Wu and Massimo Caccia and Zhuang Li and Yuan-Fang Li and Guilin Qi and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=figzpGMrdD}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "XVPqLyNxSyh", "original": "A2XVaHGWfnl", "number": 2856, "cdate": 1632875649309, "mdate": null, "ddate": null, "tcdate": 1632875649309, "tmdate": 1676330536310, "tddate": null, "forum": "XVPqLyNxSyh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Salient ImageNet: How to discover spurious features in Deep Learning?", "authorids": ["~Sahil_Singla1", "~Soheil_Feizi2"], "authors": ["Sahil Singla", "Soheil Feizi"], "keywords": ["interpretability", "failure explanation", "debugging", "robustness"], "abstract": "Deep neural networks can be unreliable in the real world especially when they heavily use {\\it spurious} features for their predictions. Focusing on image classifications, we define {\\it core features} as the set of visual features that are always a part of the object definition while {\\it spurious features} are the ones that are likely to {\\it co-occur} with the object but not a part of it (e.g., attribute ``fingers\" for class ``band aid\"). Traditional methods for discovering spurious features either require extensive human annotations (thus, not scalable), or are useful on specific models. In this work, we introduce a {\\it general} framework to discover a subset of spurious and core visual features used in inferences of a general model and localize them on a large number of images with minimal human supervision. Our methodology is based on this key idea: to identify spurious or core \\textit{visual features} used in model predictions, we identify spurious or core \\textit{neural features} (penultimate layer neurons of a robust model) via limited human supervision (e.g., using top 5 activating images per feature). We then show that these neural feature annotations {\\it generalize} extremely well to many more images {\\it without} any human supervision. We use the activation maps for these neural features as the soft masks to highlight spurious or core visual features. Using this methodology, we introduce the {\\it Salient Imagenet} dataset containing core and spurious masks for a large set of samples from Imagenet. Using this dataset, we show that several popular Imagenet models rely heavily on various spurious features in their predictions, indicating the standard accuracy alone is not sufficient to fully assess model' performance specially in safety-critical applications. Code is available at \\url{https://github.com/singlasahil14/salient_imagenet}.", "one-sentence_summary": "A scalable framework for discovering spurious features of deep neural networks", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "singla|salient_imagenet_how_to_discover_spurious_features_in_deep_learning", "pdf": "/pdf/078e6e26ef046a2af14430f17a7b149613824cc7.pdf", "supplementary_material": "/attachment/335d0a4d0307c714ec937fc7c941d3f6e1e926c2.zip", "data": "", "_bibtex": "@inproceedings{\nsingla2022salient,\ntitle={Salient ImageNet: How to discover spurious features in Deep Learning?},\nauthor={Sahil Singla and Soheil Feizi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XVPqLyNxSyh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "9wOQOgNe-w", "original": "d95aD7PUAbu", "number": 2855, "cdate": 1632875649241, "mdate": null, "ddate": null, "tcdate": 1632875649241, "tmdate": 1697934674660, "tddate": null, "forum": "9wOQOgNe-w", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Differentiable DAG Sampling", "authorids": ["~Bertrand_Charpentier2", "~Simon_Kibler1", "~Stephan_G\u00fcnnemann1"], "authors": ["Bertrand Charpentier", "Simon Kibler", "Stephan G\u00fcnnemann"], "keywords": ["DAG", "Differentiable", "Sampling", "Probabilistic model"], "abstract": "We propose a new differentiable probabilistic model over DAGs (DP-DAG). DP-DAG allows fast and differentiable DAG sampling suited to continuous optimization. To this end, DP-DAG samples a DAG by successively (1) sampling a linear ordering of the node and (2) sampling edges consistent with the sampled linear ordering. We further propose VI-DP-DAG, a new method for DAG learning from observational data which combines DP-DAG with variational inference. Hence,VI-DP-DAG approximates the posterior probability over DAG edges given the observed data. VI-DP-DAG is guaranteed to output a valid DAG at any time during training and does not require any complex augmented Lagrangian optimization scheme in contrast to existing differentiable DAG learning approaches. In our extensive experiments, we compare VI-DP-DAG to other differentiable DAG learning baselines on synthetic and real datasets. VI-DP-DAG significantly improves DAG structure and causal mechanism learning while training faster than competitors.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "charpentier|differentiable_dag_sampling", "pdf": "/pdf/b8fd21af0b7c600db973be1927034685eec3b104.pdf", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2203.08509/code)", "_bibtex": "@inproceedings{\ncharpentier2022differentiable,\ntitle={Differentiable {DAG} Sampling},\nauthor={Bertrand Charpentier and Simon Kibler and Stephan G{\\\"u}nnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9wOQOgNe-w}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "SS8F6tFX3-", "original": "Yu7mCk9FUgP", "number": 2851, "cdate": 1632875648969, "mdate": null, "ddate": null, "tcdate": 1632875648969, "tmdate": 1676330536681, "tddate": null, "forum": "SS8F6tFX3-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Evaluating Model-Based Planning and Planner Amortization for Continuous Control", "authorids": ["~Arunkumar_Byravan1", "~Leonard_Hasenclever1", "~Piotr_Trochim1", "~Mehdi_Mirza1", "~Alessandro_Davide_Ialongo1", "~Yuval_Tassa2", "~Jost_Tobias_Springenberg1", "~Abbas_Abdolmaleki3", "~Nicolas_Heess1", "~Josh_Merel1", "~Martin_Riedmiller1"], "authors": ["Arunkumar Byravan", "Leonard Hasenclever", "Piotr Trochim", "Mehdi Mirza", "Alessandro Davide Ialongo", "Yuval Tassa", "Jost Tobias Springenberg", "Abbas Abdolmaleki", "Nicolas Heess", "Josh Merel", "Martin Riedmiller"], "keywords": ["Model-based Reinforcement Learning", "Planning", "Robotics", "Model Predictive Control", "Learning"], "abstract": "There is a widespread intuition that model-based control methods should be able to surpass the data efficiency of model-free approaches. In this paper we attempt to evaluate this intuition on various challenging locomotion tasks. We take a hybrid approach, combining model predictive control (MPC) with a learned model and model-free policy learning; the learned policy serves as a proposal for MPC. We show that MPC with learned proposals and models (trained on the fly or transferred from related tasks) can significantly improve performance and data efficiency with respect to model-free methods. However, we find that well-tuned model-free agents are strong baselines even for high DoF control problems. Finally, we show that it is possible to distil a model-based planner into a policy that amortizes the planning computation without any loss of performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "byravan|evaluating_modelbased_planning_and_planner_amortization_for_continuous_control", "pdf": "/pdf/ae6d13b114d9f64756fe8146f4c5b0e55037ad9a.pdf", "one-sentence_summary": "We combine MPC with model-free RL and evaluate on continuous control tasks from scratch and in transfer settings; our results show that model-free RL is a strong baseline in single task settings and model-based methods shine in multi-goal tasks.", "supplementary_material": "/attachment/fcdd96007624b2d68dbadfdc2ea68e6d0b39c857.zip", "data": "", "_bibtex": "@inproceedings{\nbyravan2022evaluating,\ntitle={Evaluating Model-Based Planning and Planner Amortization for Continuous Control},\nauthor={Arunkumar Byravan and Leonard Hasenclever and Piotr Trochim and Mehdi Mirza and Alessandro Davide Ialongo and Yuval Tassa and Jost Tobias Springenberg and Abbas Abdolmaleki and Nicolas Heess and Josh Merel and Martin Riedmiller},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SS8F6tFX3-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "xKZ4K0lTj_", "original": "DWuuXVT02dF", "number": 2845, "cdate": 1632875648553, "mdate": null, "ddate": null, "tcdate": 1632875648553, "tmdate": 1697934675870, "tddate": null, "forum": "xKZ4K0lTj_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hierarchical Few-Shot Imitation with Skill Transition Models", "authorids": ["~Kourosh_Hakhamaneshi1", "~Ruihan_Zhao1", "~Albert_Zhan1", "~Pieter_Abbeel2", "~Michael_Laskin1"], "authors": ["Kourosh Hakhamaneshi", "Ruihan Zhao", "Albert Zhan", "Pieter Abbeel", "Michael Laskin"], "keywords": ["behavioral priors", "skill extraction", "imitation learning", "few-shot learning"], "abstract": "A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during behavioral prior training remains an outstanding challenge. To this end, we present Few-shot Imitation with Skill Transition Models (FIST), an algorithm that extracts skills from offline data and utilizes them to generalize to unseen tasks given a few downstream demonstrations. FIST learns an inverse skill dynamics model, a distance function, and utilizes a semi-parametric approach for imitation. We show that FIST is capable of generalizing to new tasks and substantially outperforms prior baselines in navigation experiments requiring traversing unseen parts of a large maze and 7-DoF robotic arm experiments requiring manipulating previously unseen objects in a kitchen.", "one-sentence_summary": "We introduce a new algorithm (FIST) which extracts skills from offline data and adapts them in few-shot to solve unseen complex long-horizon tasks by utilizing an inverse skill dynamics model and semi-parametric imitation. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hakhamaneshi|hierarchical_fewshot_imitation_with_skill_transition_models", "pdf": "/pdf/2b401efeb1537504e778bc1d6c08a29ac6cdf9f1.pdf", "supplementary_material": "/attachment/5c4744a72b39f3ec513ad38da22c69182f7bc1f2.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.08981/code)", "_bibtex": "@inproceedings{\nhakhamaneshi2022hierarchical,\ntitle={Hierarchical Few-Shot Imitation with Skill Transition Models},\nauthor={Kourosh Hakhamaneshi and Ruihan Zhao and Albert Zhan and Pieter Abbeel and Michael Laskin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xKZ4K0lTj_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "g2LCQwG7Of", "original": "ObW6PDH9WJS", "number": 2835, "cdate": 1632875647880, "mdate": null, "ddate": null, "tcdate": 1632875647880, "tmdate": 1676330537334, "tddate": null, "forum": "g2LCQwG7Of", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "End-to-End Learning of Probabilistic Hierarchies on Graphs", "authorids": ["~Daniel_Z\u00fcgner1", "~Bertrand_Charpentier2", "~Morgane_Ayle1", "~Sascha_Geringer1", "~Stephan_G\u00fcnnemann1"], "authors": ["Daniel Z\u00fcgner", "Bertrand Charpentier", "Morgane Ayle", "Sascha Geringer", "Stephan G\u00fcnnemann"], "keywords": ["hierarchical clustering", "graphs", "networks", "graph mining", "network mining", "graph custering"], "abstract": "We propose a novel probabilistic model over hierarchies on graphs obtained by continuous relaxation of tree-based hierarchies. We draw connections to Markov chain theory, enabling us to perform hierarchical clustering by efficient end-to-end optimization of relaxed versions of quality metrics such as Dasgupta cost or Tree-Sampling Divergence (TSD). \nWe show that our model learns rich, high-quality hierarchies present in 11 real world graphs, including a large  graph with 2.3M nodes. Our model consistently outperforms recent as well as strong traditional baselines such as average linkage. \nOur model also obtains strong results on link prediction despite not being trained on this task, highlighting the quality of the hierarchies discovered by our model.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "z\u00fcgner|endtoend_learning_of_probabilistic_hierarchies_on_graphs", "pdf": "/pdf/fe59d617689e48f429a561c55779a24886d3ecf4.pdf", "one-sentence_summary": "End-to-end gradient-based hierarchical clustering on graphs by exploiting Markov chain theory leads to state-of-the-art results.", "supplementary_material": "/attachment/d9198f18d4990899db54ba0f02ba3f931374569c.zip", "data": "", "_bibtex": "@inproceedings{\nz{\\\"u}gner2022endtoend,\ntitle={End-to-End Learning of Probabilistic Hierarchies on Graphs},\nauthor={Daniel Z{\\\"u}gner and Bertrand Charpentier and Morgane Ayle and Sascha Geringer and Stephan G{\\\"u}nnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=g2LCQwG7Of}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "-w2oomO6qgc", "original": "JdVdHo4ENb6", "number": 2831, "cdate": 1632875647668, "mdate": null, "ddate": null, "tcdate": 1632875647668, "tmdate": 1676330537469, "tddate": null, "forum": "-w2oomO6qgc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GeneDisco: A Benchmark for Experimental Design in Drug Discovery", "authorids": ["~Arash_Mehrjou1", "~Ashkan_Soleymani1", "~Andrew_Jesson1", "~Pascal_Notin1", "~Yarin_Gal1", "~Stefan_Bauer1", "~Patrick_Schwab1"], "authors": ["Arash Mehrjou", "Ashkan Soleymani", "Andrew Jesson", "Pascal Notin", "Yarin Gal", "Stefan Bauer", "Patrick Schwab"], "keywords": ["batch active learning", "drug discovery", "benchmark"], "abstract": "In vitro cellular experimentation with genetic interventions, using for example CRISPR technologies, is an essential step in early-stage drug discovery and target validation that serves to assess initial hypotheses about causal associations between biological mechanisms and disease pathologies. With billions of potential hypotheses to test, the experimental design space for in vitro genetic experiments is extremely vast, and the available experimental capacity - even at the largest research institutions in the world - pales in relation to the size of this biological hypothesis space. Machine learning methods, such as active and reinforcement learning, could aid in optimally exploring the vast biological space by integrating prior knowledge from various information sources as well as extrapolating to yet unexplored areas of the experimental design space based on available data. However, there exist no standardised benchmarks and data sets for this challenging task and little research has been conducted in this area to date. Here, we introduce GeneDisco, a benchmark suite for evaluating active learning algorithms for experimental design in drug discovery. GeneDisco contains a curated set of multiple publicly available experimental data sets as well as open-source implementations of state-of-the-art active learning policies for experimental design and exploration.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mehrjou|genedisco_a_benchmark_for_experimental_design_in_drug_discovery", "pdf": "/pdf/46263f4b010fc363d4351c09cb675b69418c8680.pdf", "one-sentence_summary": "A comprehensive benchmark for batch active learning in drug discovery. ", "supplementary_material": "/attachment/fb40e81982ddd0cf54a44b00407c812f769a14db.zip", "_bibtex": "@inproceedings{\nmehrjou2022genedisco,\ntitle={GeneDisco: A Benchmark for Experimental Design in Drug Discovery},\nauthor={Arash Mehrjou and Ashkan Soleymani and Andrew Jesson and Pascal Notin and Yarin Gal and Stefan Bauer and Patrick Schwab},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-w2oomO6qgc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "MXEl7i-iru", "original": "tMan4G_EyjB", "number": 2829, "cdate": 1632875647602, "mdate": null, "ddate": null, "tcdate": 1632875647602, "tmdate": 1676330537570, "tddate": null, "forum": "MXEl7i-iru", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GraphENS: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification", "authorids": ["~Joonhyung_Park1", "~Jaeyun_Song2", "~Eunho_Yang1"], "authors": ["Joonhyung Park", "Jaeyun Song", "Eunho Yang"], "keywords": ["Deep learning", "Node classification", "Class imbalance", "Data Augmentation"], "abstract": "In many real-world node classification scenarios, nodes are highly class-imbalanced, where graph neural networks (GNNs) can be readily biased to major class instances. Albeit existing class imbalance approaches in other domains can alleviate this issue to some extent, they do not consider the impact of message passing between nodes. In this paper, we hypothesize that overfitting to the neighbor sets of minor class due to message passing is a major challenge for class-imbalanced node classification. To tackle this issue, we propose GraphENS, a novel augmentation method that synthesizes the whole ego network for minor class (minor node and its one-hop neighbors) by combining two different ego networks based on their similarity. Additionally, we introduce a saliency-based node mixing method to exploit the abundant class-generic attributes of other nodes while blocking the injection of class-specific features. Our approach consistently outperforms the baselines over multiple node classification benchmark datasets and architectures.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "park|graphens_neighboraware_ego_network_synthesis_for_classimbalanced_node_classification", "pdf": "/pdf/2005a08039a74bf579bee253f6edc87920ecb3af.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\npark2022graphens,\ntitle={Graph{ENS}: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification},\nauthor={Joonhyung Park and Jaeyun Song and Eunho Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MXEl7i-iru}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "hcQHRHKfN_", "original": "U1mcJ1F6nZp", "number": 2821, "cdate": 1632875647190, "mdate": null, "ddate": null, "tcdate": 1632875647190, "tmdate": 1676330537952, "tddate": null, "forum": "hcQHRHKfN_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization", "authorids": ["~Zihan_Zhou1", "~Wei_Fu1", "~Bingliang_Zhang1", "~Yi_Wu1"], "authors": ["Zihan Zhou", "Wei Fu", "Bingliang Zhang", "Yi Wu"], "keywords": ["diverse behavior", "deep reinforcement learning", "multi-agent reinforcement learning"], "abstract": "We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones. To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process. When a sampled trajectory is sufficiently distinct, RSPO performs standard policy optimization with extrinsic rewards. For trajectories with high likelihood under existing policies, RSPO utilizes an intrinsic diversity reward to promote exploration. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent navigation tasks and MuJoCo control to multi-agent stag-hunt games and the StarCraft II Multi-Agent Challenge.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhou|continuously_discovering_novel_strategies_via_rewardswitching_policy_optimization", "pdf": "/pdf/a123c43ffc76a63f5bbe2eccdf4bba5a4233b9ed.pdf", "one-sentence_summary": "We propose Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones.", "supplementary_material": "/attachment/346c31fe592463574dc5a3cb22c9ee707492775a.zip", "data": "", "_bibtex": "@inproceedings{\nzhou2022continuously,\ntitle={Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization},\nauthor={Zihan Zhou and Wei Fu and Bingliang Zhang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hcQHRHKfN_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 33}}, {"id": "wwDg3bbYBIq", "original": "_H6NOi7iJsZ", "number": 2814, "cdate": 1632875646704, "mdate": null, "ddate": null, "tcdate": 1632875646704, "tmdate": 1676330538522, "tddate": null, "forum": "wwDg3bbYBIq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Remember Patterns: Pattern Matching Memory Networks for Traffic Forecasting", "authorids": ["~Hyunwook_Lee1", "skyjin@unist.ac.kr", "hyeshinchu@unist.ac.kr", "limhongkyu1219@unist.ac.kr", "~Sungahn_Ko1"], "authors": ["Hyunwook Lee", "Seungmin Jin", "Hyeshin Chu", "Hongkyu Lim", "Sungahn Ko"], "keywords": ["Traffic Forecasting", "Deep Learning"], "abstract": "Traffic forecasting is a challenging problem due to complex road networks and sudden speed changes caused by various events on roads. Several models have been proposed to solve this challenging problem, with a focus on learning the spatio-temporal dependencies of roads. In this work, we propose a new perspective for converting the forecasting problem into a pattern-matching task, assuming that large traffic data can be represented by a set of patterns. To evaluate the validity of this new perspective, we design a novel traffic forecasting model called Pattern-Matching Memory Networks (PM-MemNet), which learns to match input data to representative patterns with a key-value memory structure. We first extract and cluster representative traffic patterns that serve as keys in the memory. Then, by matching the extracted keys and inputs, PM-MemNet acquires the necessary information on existing traffic patterns from the memory and uses it for forecasting. To model the spatio-temporal correlation of traffic, we proposed a novel memory architecture, GCMem, which integrates attention and graph convolution. The experimental results indicate that PM-MemNet is more accurate than state-of-the-art models, such as Graph WaveNet, with higher responsiveness. We also present a qualitative analysis describing how PM-MemNet works and achieves higher accuracy when road speed changes rapidly.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|learning_to_remember_patterns_pattern_matching_memory_networks_for_traffic_forecasting", "pdf": "/pdf/3d2d0f8c044b58b34cdb6d2a91fcf86fc3792cae.pdf", "one-sentence_summary": "We presents a novel graph convolutional memory network, PM-MemNet, for the traffic forecasting.", "_bibtex": "@inproceedings{\nlee2022learning,\ntitle={Learning to Remember Patterns: Pattern Matching Memory Networks for Traffic Forecasting},\nauthor={Hyunwook Lee and Seungmin Jin and Hyeshin Chu and Hongkyu Lim and Sungahn Ko},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wwDg3bbYBIq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "VTNjxbFRKly", "original": "LkmpUnHxJdC", "number": 2810, "cdate": 1632875646431, "mdate": null, "ddate": null, "tcdate": 1632875646431, "tmdate": 1676330538759, "tddate": null, "forum": "VTNjxbFRKly", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Why Propagate Alone? Parallel Use of Labels and Features on Graphs", "authorids": ["~Yangkun_Wang1", "~Jiarui_Jin1", "~Weinan_Zhang1", "~Yang_Yongyi1", "~Jiuhai_Chen1", "~Quan_Gan1", "~Yong_Yu1", "~Zheng_Zhang1", "~Zengfeng_Huang1", "~David_Wipf1"], "authors": ["Yangkun Wang", "Jiarui Jin", "Weinan Zhang", "Yang Yongyi", "Jiuhai Chen", "Quan Gan", "Yong Yu", "Zheng Zhang", "Zengfeng Huang", "David Wipf"], "keywords": [], "abstract": "One of the challenges of graph-based semi-supervised learning over ordinary supervised learning for classification tasks lies in label utilization.  The direct use of ground-truth labels in graphs for training purposes can result in a parametric model learning trivial degenerate solutions (e.g., an identity mapping from input to output).  In addressing this issue, a label trick has recently been proposed in the literature and applied to a wide range of graph neural network (GNN) architectures, achieving state-of-the-art results on various datasets.  The essential idea is to randomly split the observed labels on the graph and use a fraction of them as input to the model (along with original node features), and predict the remaining fraction.  Despite its success in enabling GNNs to propagate features and labels simultaneously, this approach has never been analyzed from a theoretical perspective, nor fully explored across certain natural use cases.  In this paper, we demonstrate that under suitable settings, this stochastic trick can be reduced to a more interpretable deterministic form, allowing us to better explain its behavior, including an emergent regularization effect, and motivate broader application scenarios.  Our experimental results corroborate these analyses while also demonstrating improved node classification performance applying the label trick in new domains.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|why_propagate_alone_parallel_use_of_labels_and_features_on_graphs", "pdf": "/pdf/b179d9c177f43181658e714875f794ec799afbb8.pdf", "one-sentence_summary": "We analyze a powerful label trick from a theoretical perspective and reduce it to an interpretable form that inspires broader application scenarios.", "supplementary_material": "/attachment/8c1d7aa49d4f0aec97df66f4789ca9a817d13a19.zip", "_bibtex": "@inproceedings{\nwang2022why,\ntitle={Why Propagate Alone? Parallel Use of Labels and Features on Graphs},\nauthor={Yangkun Wang and Jiarui Jin and Weinan Zhang and Yang Yongyi and Jiuhai Chen and Quan Gan and Yong Yu and Zheng Zhang and Zengfeng Huang and David Wipf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VTNjxbFRKly}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 35}}, {"id": "5i7lJLuhTm", "original": "iOzC7fHytxj", "number": 2809, "cdate": 1632875646361, "mdate": null, "ddate": null, "tcdate": 1632875646361, "tmdate": 1676330538797, "tddate": null, "forum": "5i7lJLuhTm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning by Directional Gradient Descent", "authorids": ["~David_Silver1", "~Anirudh_Goyal1", "~Ivo_Danihelka1", "~Matteo_Hessel1", "~Hado_van_Hasselt1"], "authors": ["David Silver", "Anirudh Goyal", "Ivo Danihelka", "Matteo Hessel", "Hado van Hasselt"], "keywords": ["credit assignment", "directional derivative", "recurrent networks"], "abstract": "How should state be constructed from a sequence of observations, so as to best achieve some objective? Most deep learning methods update the parameters of the state representation by gradient descent. However, no prior method for computing the gradient is fully satisfactory, for example consuming too much memory, introducing too much variance, or adding too much bias. In this work, we propose a new learning algorithm that addresses these limitations. The basic idea is to update the parameters of the representation by using the directional derivative along a candidate direction, a quantity that may be computed online with the same computational cost as the representation itself. We consider several different choices of candidate direction, including random selection and approximations to the true gradient, and investigate their performance on several synthetic tasks.  \n", "one-sentence_summary": "Computing directional derivative of a recurrent function along a candidate direction, and using it to create a valid descent direction.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "silver|learning_by_directional_gradient_descent", "pdf": "/pdf/a9213856f2d7ed9866d90862df188f6d5456262b.pdf", "_bibtex": "@inproceedings{\nsilver2022learning,\ntitle={Learning by Directional Gradient Descent},\nauthor={David Silver and Anirudh Goyal and Ivo Danihelka and Matteo Hessel and Hado van Hasselt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5i7lJLuhTm}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "PtSAD3caaA2", "original": "Tn10twAEhs", "number": 2804, "cdate": 1632875646017, "mdate": null, "ddate": null, "tcdate": 1632875646017, "tmdate": 1697934679973, "tddate": null, "forum": "PtSAD3caaA2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems", "authorids": ["~Benjamin_Eysenbach1", "~Sergey_Levine1"], "authors": ["Benjamin Eysenbach", "Sergey Levine"], "keywords": ["reinforcement learning", "robustness", "maximum entropy"], "abstract": "Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that maximum entropy (MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be used to learn policies that are robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL is a simple robust RL method with appealing formal guarantees.", "one-sentence_summary": "Maximum Entropy RL (provably) solves some robust RL problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "eysenbach|maximum_entropy_rl_provably_solves_some_robust_rl_problems", "pdf": "/pdf/3fb82fdb60ea0802001d004d0b6951f5226c3ff0.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2103.06257/code)", "_bibtex": "@inproceedings{\neysenbach2022maximum,\ntitle={Maximum Entropy {RL} (Provably) Solves Some Robust {RL} Problems},\nauthor={Benjamin Eysenbach and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PtSAD3caaA2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "XhF2VOMRHS", "original": "Gs0PflkV6Q", "number": 2801, "cdate": 1632875645816, "mdate": null, "ddate": null, "tcdate": 1632875645816, "tmdate": 1676330539519, "tddate": null, "forum": "XhF2VOMRHS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training", "authorids": ["~Yifei_Wang1", "~Yisen_Wang1", "yjs@math.pku.edu.cn", "~Zhouchen_Lin1"], "authors": ["Yifei Wang", "Yisen Wang", "Jiansheng Yang", "Zhouchen Lin"], "keywords": ["Generative Models", "Energy-based Models", "Sampling", "Adversarial Training"], "abstract": "Adversarial Training (AT) is known as an effective approach to enhance the robustness of deep neural networks. Recently researchers notice that robust models with AT have good generative ability and can synthesize realistic images, while the reason behind it is yet under-explored. In this paper, we demystify this phenomenon by developing a unified probabilistic framework, called Contrastive Energy-based Models (CEM). On the one hand, we provide the first probabilistic characterization of AT through a unified understanding of robustness and generative ability. On the other hand, our unified framework can be extended to the unsupervised scenario, which interprets unsupervised contrastive learning as an important sampling of CEM. Based on these, we propose a principled method to develop adversarial learning and sampling methods. Experiments show that the sampling methods derived from our framework improve the sample quality in both supervised and unsupervised learning. Notably, our unsupervised adversarial sampling method achieves an Inception score of 9.61 on CIFAR-10, which is superior to previous energy-based models and comparable to state-of-the-art generative models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|a_unified_contrastive_energybased_model_for_understanding_the_generative_ability_of_adversarial_training", "pdf": "/pdf/d2b73abba174f5c4a4f7a8236fccc58a742e0bc5.pdf", "_bibtex": "@inproceedings{\nwang2022a,\ntitle={A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training},\nauthor={Yifei Wang and Yisen Wang and Jiansheng Yang and Zhouchen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XhF2VOMRHS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "e95i1IHcWj", "original": "amSODqtmseB", "number": 2799, "cdate": 1632875645678, "mdate": null, "ddate": null, "tcdate": 1632875645678, "tmdate": 1697934680801, "tddate": null, "forum": "e95i1IHcWj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks", "authorids": ["~Haorui_Wang1", "~Haoteng_Yin1", "~Muhan_Zhang1", "~Pan_Li2"], "authors": ["Haorui Wang", "Haoteng Yin", "Muhan Zhang", "Pan Li"], "keywords": ["Graph Neural Network", "Spectral Graph Theory", "System Stability"], "abstract": "Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task-based on sets of nodes such as link/motif prediction and so on.  Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence, inaccurate prediction, or high complexity. In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. GNNs with PE often get criticized because they are not generalizable to unseen graphs (inductive) or stable.  Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original node features and rotation equivariance w.r.t. the positional features simultaneously. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability. Code is available at https://github.com/Graph-COM/PEG.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|equivariant_and_stable_positional_encoding_for_more_powerful_graph_neural_networks", "pdf": "/pdf/cf28a7040f308283c69b833cd6901be5151a8cbc.pdf", "supplementary_material": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.00199/code)", "_bibtex": "@inproceedings{\nwang2022equivariant,\ntitle={Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks},\nauthor={Haorui Wang and Haoteng Yin and Muhan Zhang and Pan Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=e95i1IHcWj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "Mng8CQ9eBW", "original": "xfIt-fwZq9z", "number": 2792, "cdate": 1632875645270, "mdate": null, "ddate": null, "tcdate": 1632875645270, "tmdate": 1676330539954, "tddate": null, "forum": "Mng8CQ9eBW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models", "authorids": ["~Kangjie_Chen1", "~Yuxian_Meng1", "~Xiaofei_Sun1", "~Shangwei_Guo1", "~Tianwei_Zhang1", "~Jiwei_Li1", "~Chun_Fan1"], "authors": ["Kangjie Chen", "Yuxian Meng", "Xiaofei Sun", "Shangwei Guo", "Tianwei Zhang", "Jiwei Li", "Chun Fan"], "keywords": [], "abstract": "Pre-trained Natural Language Processing (NLP) models, which can be adapted to a variety of downstream language tasks via fine-tuning, highly accelerate the learning progress of NLP models. However, NLP models have been shown to be vulnerable to backdoor attacks. Previous NLP backdoor attacks mainly focus on one specific task. This limitation makes existing solutions less applicable to different NLP models which have been widely used in various tasks.\nIn this work, we propose BadPre, the first backdoor attack against various downstream models built based on pre-trained NLP models. BadPre can launch trojan attacks against different language tasks with the same trigger.\nThe key insight of our approach is that downstream models can inherit the security characteristics from the pre-trained models. Specifically, we leverage data posing to the pre-trained NLP models and then inference the downstream models with sentences embedded triggers. Furthermore, to fool backdoor detectors, we design a novel adversarial attack method to generate a more robust trigger.\nExperimental results indicate that our approach can effectively attack a wide range of downstream NLP tasks and exhibit significant robustness against backdoor detectors.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|badpre_taskagnostic_backdoor_attacks_to_pretrained_nlp_foundation_models", "pdf": "/pdf/4ac63a977d9d85abecd09a2214e195931e835ab5.pdf", "data": "", "_bibtex": "@inproceedings{\nchen2022badpre,\ntitle={BadPre: Task-agnostic Backdoor Attacks to Pre-trained {NLP} Foundation Models},\nauthor={Kangjie Chen and Yuxian Meng and Xiaofei Sun and Shangwei Guo and Tianwei Zhang and Jiwei Li and Chun Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Mng8CQ9eBW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "AV8FPoMTTa", "original": "R1Mq4lf0I5x", "number": 2790, "cdate": 1632875645138, "mdate": null, "ddate": null, "tcdate": 1632875645138, "tmdate": 1676330539969, "tddate": null, "forum": "AV8FPoMTTa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Shallow and Deep Networks are Near-Optimal Approximators of Korobov Functions", "authorids": ["~Moise_Blanchard1", "~Mohammed_Amine_Bennouna1"], "authors": ["Moise Blanchard", "Mohammed Amine Bennouna"], "keywords": [], "abstract": "In this paper, we analyze the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives --- Korobov functions. We prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. Our bounds hold for general activation functions, including ReLU. We further prove that these bounds nearly match the minimal number of parameters any continuous function approximator needs to approximate Korobov functions, showing that neural networks are near-optimal function approximators.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "blanchard|shallow_and_deep_networks_are_nearoptimal_approximators_of_korobov_functions", "pdf": "/pdf/b207b8552118608b3abd1048731747b6575620fc.pdf", "one-sentence_summary": "We analyze the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives", "_bibtex": "@inproceedings{\nblanchard2022shallow,\ntitle={Shallow and Deep Networks are Near-Optimal Approximators of Korobov Functions},\nauthor={Moise Blanchard and Mohammed Amine Bennouna},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AV8FPoMTTa}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "Ucx3DQbC9GH", "original": "YfPYNg9kfPU", "number": 2777, "cdate": 1632875644260, "mdate": null, "ddate": null, "tcdate": 1632875644260, "tmdate": 1676330540499, "tddate": null, "forum": "Ucx3DQbC9GH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "What Makes Better Augmentation Strategies? Augment Difficult but Not too Different", "authorids": ["~Jaehyung_Kim1", "~Dongyeop_Kang2", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaehyung Kim", "Dongyeop Kang", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["NLP", "data augmentation", "learning augmentation policy", "text classification"], "abstract": "The practice of data augmentation has been extensively used to boost the performance of deep neural networks for various NLP tasks. It is more effective when only a limited number of labeled samples is available, e.g., low-data or class-imbalanced regimes. Most current augmentation techniques rely on parameter tuning or inherent randomness; hence, their effectiveness largely varies on the tasks. To efficiently find the best augmentation strategy for each task, learning data augmentation policy is a promising solution, but the question of what makes a good augmentation in NLP tasks and how to design the reward function for learning a good policy remains under-explored. To answer this, we hypothesize that good data augmentation should construct more diverse and challenging samples for providing informative training signals, while avoiding the risk of losing the semantics of original samples. Therefore, we design a novel reward function for updating the augmentation policy to construct difficult but not too different samples (DND). Particularly, we jointly optimize a data augmentation policy while training the model, to construct the augmented samples with low confidence but a high semantic similarity with original ones. In addition, we introduce a sample re-weighting scheme to focus on difficult augmented samples after the original ones are learned confidently for more effective learning from the augmented ones. Our learning-based augmentation outperforms the recent state-of-the-art augmentation schemes on various text classification tasks and GLUE benchmark by successfully discovering the effective augmentations for each task. Remarkably, our method is more effective on the challenging low-data and class-imbalanced regimes, and the learned augmentation policy is well-transferable to the different tasks and models. ", "one-sentence_summary": "Effective learning-based augmentation in NLP tasks by constructing difficult but not too different samples ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kim|what_makes_better_augmentation_strategies_augment_difficult_but_not_too_different", "pdf": "/pdf/133b80ca3acfcd64abacb3126997fc8a6c4b95ea.pdf", "supplementary_material": "/attachment/4121bfd41ea185360c1b2ebf964a64b1ad1f816f.zip", "data": "", "_bibtex": "@inproceedings{\nkim2022what,\ntitle={What Makes Better Augmentation Strategies? Augment Difficult but Not too Different},\nauthor={Jaehyung Kim and Dongyeop Kang and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Ucx3DQbC9GH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "Harn4_EZBw", "original": "88rq4CBnPxr", "number": 2774, "cdate": 1632875644054, "mdate": null, "ddate": null, "tcdate": 1632875644054, "tmdate": 1676330540869, "tddate": null, "forum": "Harn4_EZBw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generative Pseudo-Inverse Memory", "authorids": ["~Kha_Pham2", "~Hung_Le1", "nmman@hcmus.edu.vn", "~Truyen_Tran1", "bao.ho@jvn.edu.vn", "~Svetha_Venkatesh1"], "authors": ["Kha Pham", "Hung Le", "Man Ngo", "Truyen Tran", "Bao Ho", "Svetha Venkatesh"], "keywords": [], "abstract": "We propose Generative Pseudo-Inverse Memory (GPM), a class of deep generative memory models that are fast to write in and read out. Memory operations are recast as seeking robust solutions of linear systems, which naturally lead to the use of matrix pseudo-inverses. The pseudo-inverses are iteratively approximated, with practical computation complexity of almost $O(1)$. We prove theoretically and verify empirically that our model can retrieve exactly what have been written to the memory under mild conditions. A key capability of GPM is iterative reading, during which the attractor dynamics towards fixed points are enabled, allowing the model to iteratively improve sample quality in denoising and generating. More impressively, GPM can store a large amount of data while maintaining key abilities of accurate retrieving of stored patterns, denoising of corrupted data and generating novel samples. Empirically we demonstrate the efficiency and versatility of GPM on a comprehensive suite of experiments involving binarized MNIST, binarized Omniglot, FashionMNIST, CIFAR10 & CIFAR100 and CelebA.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pham|generative_pseudoinverse_memory", "pdf": "/pdf/1ff8e1fcd5f0f7899be75a462bcde482ae98fbfd.pdf", "data": "", "_bibtex": "@inproceedings{\npham2022generative,\ntitle={Generative Pseudo-Inverse Memory},\nauthor={Kha Pham and Hung Le and Man Ngo and Truyen Tran and Bao Ho and Svetha Venkatesh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Harn4_EZBw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "RQ428ZptQfU", "original": "WVgOL6-g4_Y", "number": 2773, "cdate": 1632875643988, "mdate": null, "ddate": null, "tcdate": 1632875643988, "tmdate": 1697934684060, "tddate": null, "forum": "RQ428ZptQfU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Deep Variational Approach to Clustering Survival Data", "authorids": ["~Laura_Manduchi2", "~Ri\u010dards_Marcinkevi\u010ds1", "michelacarlotta.massi@polimi.it", "thomas.weikert@usb.ch", "alexander.sauter@usb.ch", "verena.gotta@ukbb.ch", "timothy.mueller@uzh.ch", "flavio.vasella@uzh.ch", "marian.neidert@kssg.ch", "marc.pfister@ukbb.ch", "~Bram_Stieltjes2", "~Julia_E_Vogt1"], "authors": ["Laura Manduchi", "Ri\u010dards Marcinkevi\u010ds", "Michela C. Massi", "Thomas Weikert", "Alexander Sauter", "Verena Gotta", "Timothy M\u00fcller", "Flavio Vasella", "Marian C. Neidert", "Marc Pfister", "Bram Stieltjes", "Julia E Vogt"], "keywords": ["survival analysis", "clustering", "healthcare", "variational autoencoders", "deep generative models"], "abstract": "In this work, we study the problem of clustering survival data \u2014 a challenging and so far under-explored task. We introduce a novel semi-supervised probabilistic approach to cluster survival data by leveraging recent advances in stochastic gradient variational inference. In contrast to previous work, our proposed method employs a deep generative model to uncover the underlying distribution of both the explanatory variables and censored survival times. We compare our model to the related work on clustering and mixture models for survival data in comprehensive experiments on a wide range of synthetic, semi-synthetic, and real-world datasets, including medical imaging data. Our method performs better at identifying clusters and is competitive at predicting survival times. Relying on novel generative assumptions, the proposed model offers a holistic perspective on clustering survival data and holds a promise of discovering subpopulations whose survival is regulated by different generative mechanisms.", "one-sentence_summary": "We introduce a novel semi-supervised probabilistic approach to cluster survival data", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "manduchi|a_deep_variational_approach_to_clustering_survival_data", "pdf": "/pdf/4e7d71ca800835719ffc2ca92d89f481f7c33f71.pdf", "supplementary_material": "/attachment/5a33c3e98cf2441e07016f9fdd6734b26e2a5539.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2106.05763/code)", "_bibtex": "@inproceedings{\nmanduchi2022a,\ntitle={A Deep Variational Approach to Clustering Survival Data},\nauthor={Laura Manduchi and Ri{\\v{c}}ards Marcinkevi{\\v{c}}s and Michela C. Massi and Thomas Weikert and Alexander Sauter and Verena Gotta and Timothy M{\\\"u}ller and Flavio Vasella and Marian C. Neidert and Marc Pfister and Bram Stieltjes and Julia E Vogt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RQ428ZptQfU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "qaxhBG1UUaS", "original": "YwazlblfM5", "number": 2769, "cdate": 1632875643715, "mdate": null, "ddate": null, "tcdate": 1632875643715, "tmdate": 1676330541174, "tddate": null, "forum": "qaxhBG1UUaS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems", "authorids": ["~Youngsoo_Jang2", "~Jongmin_Lee1", "~Kee-Eung_Kim2"], "authors": ["Youngsoo Jang", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["task-oriented dialogue", "pre-trained language model", "offline reinforcement learning"], "abstract": "Training a task-oriented dialogue agent can be naturally formulated as offline reinforcement learning (RL) problem, where the agent aims to learn a conversational strategy to achieve user goals, only from a dialogue corpus. It is very challenging in terms of RL since the natural language action space is astronomical, while feasible (syntactically and semantically correct) actions are very sparse. Thus, standard RL methods easily fail and generate responses diverging from human language, even when fine-tuning a powerful pre-trained language model. In this paper, we introduce GPT-Critic, an offline RL method for task-oriented dialogue. GPT-Critic is built upon GPT-2, fine-tuning the language model through behavior cloning of the critic-guided self-generated sentences. GPT-Critic is essentially free from the issue of diverging from human language since it learns from the sentences sampled from the pre-trained language model. In the experiments, we demonstrate that our algorithm outperforms the state-of-the-art in the task-oriented dialogue benchmarks including MultiWOZ 2.0 and ConvLab.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jang|gptcritic_offline_reinforcement_learning_for_endtoend_taskoriented_dialogue_systems", "pdf": "/pdf/c1d0299a479595ff9da923e14398de97f2382394.pdf", "_bibtex": "@inproceedings{\njang2022gptcritic,\ntitle={{GPT}-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems},\nauthor={Youngsoo Jang and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qaxhBG1UUaS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "JtBRnrlOEFN", "original": "ZOMubro0Dfz", "number": 2763, "cdate": 1632875643305, "mdate": null, "ddate": null, "tcdate": 1632875643305, "tmdate": 1676330541414, "tddate": null, "forum": "JtBRnrlOEFN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization", "authorids": ["~Yi_Tay1", "~Vinh_Q._Tran1", "~Sebastian_Ruder2", "~Jai_Gupta1", "~Hyung_Won_Chung1", "~Dara_Bahri1", "~Zhen_Qin5", "~Simon_Baumgartner1", "~Cong_Yu1", "~Donald_Metzler1"], "authors": ["Yi Tay", "Vinh Q. Tran", "Sebastian Ruder", "Jai Gupta", "Hyung Won Chung", "Dara Bahri", "Zhen Qin", "Simon Baumgartner", "Cong Yu", "Donald Metzler"], "keywords": ["transformers", "NLP", "language"], "abstract": "State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce Charformer, a deep Transformer model that integrates GBST and operates on the character level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that Charformer outperforms a series of competitive character-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, Charformer is fast, improving the speed of vanilla character-level Transformers by up to  while maintaining quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.", "one-sentence_summary": "Fast Token-Free Models", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tay|charformer_fast_character_transformers_via_gradientbased_subword_tokenization", "pdf": "/pdf/73e9030ce98662e38514127e7c9b665b4386ed44.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\ntay2022charformer,\ntitle={Charformer: Fast Character Transformers via Gradient-based Subword Tokenization},\nauthor={Yi Tay and Vinh Q. Tran and Sebastian Ruder and Jai Gupta and Hyung Won Chung and Dara Bahri and Zhen Qin and Simon Baumgartner and Cong Yu and Donald Metzler},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JtBRnrlOEFN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "mQxt8l7JL04", "original": "q39lF6HHFH9", "number": 2757, "cdate": 1632875642891, "mdate": null, "ddate": null, "tcdate": 1632875642891, "tmdate": 1676330541716, "tddate": null, "forum": "mQxt8l7JL04", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Regularized Autoencoders for Isometric Representation Learning", "authorids": ["~Yonghyeon_LEE1", "~Sangwoong_Yoon1", "~MinJun_Son1", "~Frank_C._Park1"], "authors": ["Yonghyeon LEE", "Sangwoong Yoon", "MinJun Son", "Frank C. Park"], "keywords": ["Autoencoders", "Manifold Learning", "Regularization", "Geometry", "Distortion"], "abstract": "The recent success of autoencoders for representation learning can be traced in large part to the addition of a regularization term.\nSuch regularized autoencoders ``constrain\" the representation so as to prevent overfitting to the data while producing a parsimonious generative model. A regularized autoencoder should in principle learn not only the data manifold, but also a set of geometry-preserving coordinates for the latent representation space; by geometry-preserving we mean that the latent space representation should attempt to preserve actual distances and angles on the data manifold. In this paper we first formulate a hierarchy for geometry-preserving mappings (isometry, conformal mapping of degree $k$, area-preserving mappings). We then show that a conformal regularization term of degree zero -- i.e., one that attempts to preserve angles and relative distances, instead of angles and exact distances -- produces data representations that are superior to other existing methods. Applying our algorithm to an unsupervised information retrieval task for CelebA data with 40 annotations, we achieve 79\\% precision at five retrieved images, an improvement of more than 10\\% compared to recent related work. Code is available at https://github.com/Gabe-YHLee/IRVAE-public.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|regularized_autoencoders_for_isometric_representation_learning", "pdf": "/pdf/a3c7c1d2d5e61bde51506a5b40b5a02f66d0d8e3.pdf", "one-sentence_summary": "Regularized Autoencoders that simultaneously learn data manifold and a set of latent space coordinates that preserves the geometry of the learned manifold.", "data": "", "_bibtex": "@inproceedings{\nlee2022regularized,\ntitle={Regularized Autoencoders for Isometric Representation Learning},\nauthor={Yonghyeon LEE and Sangwoong Yoon and MinJun Son and Frank C. Park},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mQxt8l7JL04}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "dTqOcTUOQO", "original": "ywoGTdK93CL", "number": 2756, "cdate": 1632875642818, "mdate": null, "ddate": null, "tcdate": 1632875642818, "tmdate": 1697934685516, "tddate": null, "forum": "dTqOcTUOQO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Knowledge Removal in Sampling-based Bayesian Inference", "authorids": ["~Shaopeng_Fu1", "~Fengxiang_He1", "~Dacheng_Tao1"], "authors": ["Shaopeng Fu", "Fengxiang He", "Dacheng Tao"], "keywords": ["Bayesian inference", "Markov chain Monte Carlo", "machine unlearning"], "abstract": "The right to be forgotten has been legislated in many countries, but its enforcement in the AI industry would cause unbearable costs. When single data deletion requests come, companies may need to delete the whole models learned with massive resources. Existing works propose methods to remove knowledge learned from data for explicitly parameterized models, which however are not appliable to the sampling-based Bayesian inference, {\\it i.e.}, Markov chain Monte Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we propose the first machine unlearning algorithm for MCMC. We first convert the MCMC unlearning problem into an explicit optimization problem. Based on this problem conversion, an {\\it MCMC influence function} is designed to provably characterize the learned knowledge from data, which then delivers the MCMC unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not compromise the generalizability of the MCMC models. Experiments on Gaussian mixture models and Bayesian neural networks confirm the effectiveness of the proposed algorithm. The code is available at \\url{https://github.com/fshp971/mcmc-unlearning}.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fu|knowledge_removal_in_samplingbased_bayesian_inference", "pdf": "/pdf/a42ad90a502167268f1ba4c67f57150bf59ccbc9.pdf", "one-sentence_summary": "This paper proposes the first machine unlearning algorithm for MCMC.", "supplementary_material": "/attachment/331250a7e3bdf071d5d4598ff8f4ab72e4c4e6e9.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.12964/code)", "_bibtex": "@inproceedings{\nfu2022knowledge,\ntitle={Knowledge Removal in Sampling-based Bayesian Inference},\nauthor={Shaopeng Fu and Fengxiang He and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dTqOcTUOQO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "vEZyTBRPP6o", "original": "ymdByyz3JJ", "number": 2750, "cdate": 1632875642413, "mdate": null, "ddate": null, "tcdate": 1632875642413, "tmdate": 1697934685789, "tddate": null, "forum": "vEZyTBRPP6o", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Actor-critic is implicitly biased towards high entropy optimal policies", "authorids": ["~Yuzheng_Hu1", "~Ziwei_Ji1", "~Matus_Telgarsky1"], "authors": ["Yuzheng Hu", "Ziwei Ji", "Matus Telgarsky"], "keywords": ["implicit bias", "reinforcement learning", "actor-critic", "policy gradient", "mixing time", "convergence rate", "mirror ascent."], "abstract": "We show that the simplest actor-critic method \u2014 a linear softmax policy updated with TD through interaction with a linear MDP, but featuring no explicit regularization or exploration \u2014 does not merely find an optimal policy, but moreover prefers high entropy optimal policies. To demonstrate the strength of this bias, the algorithm not only has no regularization, no projections, and no exploration like $\\epsilon$-greedy, but is moreover trained on a single trajectory with no resets. The key consequence of the high entropy bias is that uniform mixing assumptions on the MDP, which exist in some form in all prior work, can be dropped: the implicit regularization of the high entropy bias is enough to ensure that all chains mix and an optimal policy is reached with high probability. As auxiliary contributions, this work decouples concerns between the actor and critic by writing the actor update as an explicit mirror descent, provides tools to uniformly bound mixing times within KL balls of policy space, and provides a projection-free TD analysis with its own implicit bias which can be run from an unmixed starting distribution.\n", "one-sentence_summary": "We show that actor-critic, without any explicit exploration or regularization, can obtain an $\\epsilon$-optimal high entropy policy in $\\text{poly}(1/\\epsilon)$ samples via a single trajectory without the usual uniform mixing assumptions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hu|actorcritic_is_implicitly_biased_towards_high_entropy_optimal_policies", "pdf": "/pdf/e3148627ceb1e142df2619915a089a8c90153ef0.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.11280/code)", "_bibtex": "@inproceedings{\nhu2022actorcritic,\ntitle={Actor-critic is implicitly biased towards high entropy optimal policies},\nauthor={Yuzheng Hu and Ziwei Ji and Matus Telgarsky},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vEZyTBRPP6o}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "mfwdY3U_9ea", "original": "q4gHNKxt2xs", "number": 2749, "cdate": 1632875642345, "mdate": null, "ddate": null, "tcdate": 1632875642345, "tmdate": 1676330542031, "tddate": null, "forum": "mfwdY3U_9ea", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Igeood: An Information Geometry Approach to Out-of-Distribution Detection", "authorids": ["~Eduardo_Dadalto_Camara_Gomes1", "~Florence_Alberge1", "~Pierre_Duhamel1", "~Pablo_Piantanida2"], "authors": ["Eduardo Dadalto Camara Gomes", "Florence Alberge", "Pierre Duhamel", "Pablo Piantanida"], "keywords": ["out-of-distribution detection", "anomaly detection", "deep learning"], "abstract": "Reliable out-of-distribution (OOD) detection is fundamental to implementing safer modern machine learning (ML)  systems. In this paper, we introduce Igeood, an effective method for detecting OOD samples. Igeood applies to any pre-trained neural network, works under various degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD samples. By building on the geodesic (Fisher-Rao) distance between the underlying data distributions, our discriminator can combine confidence scores from the logits outputs and the learned features of a deep neural network. Empirically, we show that Igeood outperforms competing state-of-the-art methods on a variety of network architectures and datasets.", "one-sentence_summary": "We propose a flexible and effective out-of-distribution detection method by building on the Fisher-Rao distance between probability distributions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gomes|igeood_an_information_geometry_approach_to_outofdistribution_detection", "pdf": "/pdf/63528e722b436ffe0f150c1de5ef3b5ad5c47352.pdf", "supplementary_material": "/attachment/c051c492e82c715c1205d85e6cb456deb71c00ca.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\ngomes2022igeood,\ntitle={Igeood: An Information Geometry Approach to Out-of-Distribution Detection},\nauthor={Eduardo Dadalto Camara Gomes and Florence Alberge and Pierre Duhamel and Pablo Piantanida},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mfwdY3U_9ea}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "N0uJGWDw21d", "original": "DiO_xZBG1nf", "number": 2742, "cdate": 1632875641931, "mdate": null, "ddate": null, "tcdate": 1632875641931, "tmdate": 1676330542320, "tddate": null, "forum": "N0uJGWDw21d", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bag of Instances Aggregation Boosts Self-supervised Distillation", "authorids": ["~Haohang_Xu1", "~Jiemin_Fang1", "~XIAOPENG_ZHANG7", "~Lingxi_Xie1", "~Xinggang_Wang1", "~Wenrui_Dai1", "~Hongkai_Xiong1", "~Qi_Tian3"], "authors": ["Haohang Xu", "Jiemin Fang", "XIAOPENG ZHANG", "Lingxi Xie", "Xinggang Wang", "Wenrui Dai", "Hongkai Xiong", "Qi Tian"], "keywords": ["Self-supervised learning", "knowledge distillation", "instance bagging"], "abstract": "Recent advances in self-supervised learning have experienced remarkable progress, especially for contrastive learning based methods, which regard each image as well as its augmentations as an individual class and try to distinguish them from all other images. However, due to the large quantity of exemplars, this kind of pretext task intrinsically suffers from slow convergence and is hard for optimization. This is especially true for small-scale models, in which we find the performance drops dramatically comparing with its supervised counterpart. In this paper, we propose a simple but effective distillation strategy for unsupervised learning. The highlight is that the relationship among similar samples counts and can be seamlessly transferred to the student to boost the performance. Our method, termed as BINGO, which is short for Bag of InstaNces aGgregatiOn, targets at transferring the relationship learned by the teacher to the student. Here bag of instances indicates a set of similar samples constructed by the teacher and are grouped within a bag, and the goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. Notably, BINGO achieves new state-of-the-art performance on small-scale models, i.e., 65.5% and 68.9% top-1 accuracies with linear evaluation on ImageNet, using ResNet-18 and ResNet-34 as the backbones respectively, surpassing baselines (52.5% and 57.4% top-1 accuracies) by a significant margin. The code is available at https://github.com/haohang96/bingo.", "one-sentence_summary": "This paper proposes a new self-supervised distillation method which aggregates related instances bagged by the teacher and shows stronger performance than previous relation-agnostic methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|bag_of_instances_aggregation_boosts_selfsupervised_distillation", "pdf": "/pdf/43c928d51f32bf6433e4fb0642c8ef6ee1a4af36.pdf", "data": "", "_bibtex": "@inproceedings{\nxu2022bag,\ntitle={Bag of Instances Aggregation Boosts Self-supervised Distillation},\nauthor={Haohang Xu and Jiemin Fang and XIAOPENG ZHANG and Lingxi Xie and Xinggang Wang and Wenrui Dai and Hongkai Xiong and Qi Tian},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=N0uJGWDw21d}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "6tmjoym9LR6", "original": "-GE48U4QnG9", "number": 2740, "cdate": 1632875641793, "mdate": null, "ddate": null, "tcdate": 1632875641793, "tmdate": 1676330542520, "tddate": null, "forum": "6tmjoym9LR6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Stability Regularization for Discrete Representation Learning", "authorids": ["~Adeel_Pervez1", "~Efstratios_Gavves1"], "authors": ["Adeel Pervez", "Efstratios Gavves"], "keywords": [], "abstract": "We present a method for training neural network models with discrete stochastic variables.\nThe core of the method is \\emph{stability regularization}, which is a regularization procedure based on the idea of noise stability developed in Gaussian isoperimetric theory in the analysis of Gaussian functions.\nStability regularization is method to make the output of continuous functions of Gaussian random variables close to discrete, that is binary or categorical, without the need for significant manual tuning.\nThe method allows control over the extent to which a Gaussian function's output is close to discrete, thus allowing for continued flow of gradient.\nThe method can be used standalone or in combination with existing continuous relaxation methods.\nWe validate the method in a broad range of experiments using discrete variables including neural relational inference, generative modeling, clustering and conditional computing.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pervez|stability_regularization_for_discrete_representation_learning", "pdf": "/pdf/599ab5c6c8513865531af017802b3e9a891d86b0.pdf", "_bibtex": "@inproceedings{\npervez2022stability,\ntitle={Stability Regularization for Discrete Representation Learning},\nauthor={Adeel Pervez and Efstratios Gavves},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6tmjoym9LR6}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "aBVxf5NaaRt", "original": "IKFDnOXyg8z", "number": 2739, "cdate": 1632875641723, "mdate": null, "ddate": null, "tcdate": 1632875641723, "tmdate": 1697934686876, "tddate": null, "forum": "aBVxf5NaaRt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Unrolling PALM for Sparse Semi-Blind Source Separation", "authorids": ["~Mohammad_Fahes2", "~Christophe_Kervazo1", "jerome.bobin@cea.fr", "~Florence_Tupin2"], "authors": ["Mohammad Fahes", "Christophe Kervazo", "J\u00e9r\u00f4me Bobin", "Florence Tupin"], "keywords": ["Algorithm Unrolling/Unfolding", "Blind Source Separation", "Sparse Representations", "Multi-Convex Optimization", "Hyper-parameter Choice"], "abstract": "Sparse  Blind  Source  Separation  (BSS)  has  become  a  well  established  tool  for a  wide  range  of  applications  \u2013  for  instance,  in  astrophysics  and  remote  sensing.  Classical sparse BSS methods, such as the Proximal Alternating Linearized Minimization (PALM) algorithm, nevertheless often suffer from a difficult hyper-parameter choice, which undermines their results.  To bypass this pitfall, we propose in this work to build on the thriving field of algorithm unfolding/unrolling. Unrolling PALM enables to leverage the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both PALM hyper-parameters and variables.  In contrast to most existing unrolled algorithms, which assume a fixed known dictionary during the training and testing phases, this article further emphasizes on the ability to deal with variable mixing matrices (a.k.a.  dictionaries).  The proposed Learned PALM (LPALM) algorithm thus enables to perform semi-blind source separation, which is key to increase the generalization of the learnt model in real-world applications. We illustrate the relevance of LPALM in astrophysical multispectral imaging: the algorithm not only needs up to $10^4\u221210^5$ times less iterations than PALM, but also improves the separation quality, while avoiding the cumbersome hyper-parameter and initialization choice of PALM. We further show that LPALM outperforms other unrolled source separation methods in the semi-blind setting.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fahes|unrolling_palm_for_sparse_semiblind_source_separation", "pdf": "/pdf/a94df1bb2104d5b98e23e4936bda23189f807583.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.05694/code)", "_bibtex": "@inproceedings{\nfahes2022unrolling,\ntitle={Unrolling {PALM} for Sparse Semi-Blind Source Separation},\nauthor={Mohammad Fahes and Christophe Kervazo and J{\\'e}r{\\^o}me Bobin and Florence Tupin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aBVxf5NaaRt}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "fQTlgI2qZqE", "original": "KU0cArjyF44", "number": 2736, "cdate": 1632875641586, "mdate": null, "ddate": null, "tcdate": 1632875641586, "tmdate": 1676330542746, "tddate": null, "forum": "fQTlgI2qZqE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fast Generic Interaction Detection for Model Interpretability and Compression", "authorids": ["~Tianjian_Zhang1", "~Feng_Yin1", "~Zhi-Quan_Luo1"], "authors": ["Tianjian Zhang", "Feng Yin", "Zhi-Quan Luo"], "keywords": [], "abstract": "The ability of discovering feature interactions in a black-box model is vital to explainable deep learning. We propose a principled, global interaction detection method by casting our target as a multi-arm bandits problem and solving it swiftly with the UCB algorithm. This adaptive method is free of ad-hoc assumptions and among the cutting-edge methods with outstanding detection accuracy and stability. Based on the detection outcome, a lightweight and interpretable deep learning model (called ParaACE) is further built using the alternating conditional expectation (ACE) method. Our proposed ParaACE improves the prediction performance by 26 % and reduces the model size by 100+ times as compared to its Teacher model over various datasets. Furthermore, we show the great potential of our method for scientific discovery through interpreting various real datasets in the economics and smart medicine sectors. The code is available at https://github.com/zhangtj1996/ParaACE. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|fast_generic_interaction_detection_for_model_interpretability_and_compression", "pdf": "/pdf/f1581535b01f857847507915f4721abcb1e41529.pdf", "supplementary_material": "/attachment/13b6c5f5d5d4a02b2b8445bbf706ffb338b8840c.zip", "_bibtex": "@inproceedings{\nzhang2022fast,\ntitle={Fast Generic Interaction Detection for Model Interpretability and Compression},\nauthor={Tianjian Zhang and Feng Yin and Zhi-Quan Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fQTlgI2qZqE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "cGDAkQo1C0p", "original": "zBW1BjerD0S", "number": 2724, "cdate": 1632875640758, "mdate": null, "ddate": null, "tcdate": 1632875640758, "tmdate": 1676330543126, "tddate": null, "forum": "cGDAkQo1C0p", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift", "authorids": ["~Taesung_Kim1", "~Jinhee_Kim1", "~Yunwon_Tae1", "~Cheonbok_Park1", "~Jang-Ho_Choi1", "~Jaegul_Choo1"], "authors": ["Taesung Kim", "Jinhee Kim", "Yunwon Tae", "Cheonbok Park", "Jang-Ho Choi", "Jaegul Choo"], "keywords": ["Time-series forecasting", "Normalization", "Distribution shift"], "abstract": "Statistical properties such as mean and variance often change over time in time series, i.e., time-series data suffer from a distribution shift problem. This change in temporal distribution is one of the main challenges that prevent accurate time-series forecasting. To address this issue, we propose a simple yet effective normalization method called reversible instance normalization (RevIN), a generally-applicable normalization-and-denormalization method with learnable affine transformation. The proposed method is symmetrically structured to remove and restore the statistical information of a time-series instance, leading to significant performance improvements in time-series forecasting, as shown in Fig. 1. We demonstrate the effectiveness of RevIN via extensive quantitative and qualitative analyses on various real-world datasets, addressing the distribution shift problem.", "one-sentence_summary": "We propose a simple yet effective normalization method, reversible instance normalization (RevIN), which solves the time-series forecasting task against the distribution shift problem.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kim|reversible_instance_normalization_for_accurate_timeseries_forecasting_against_distribution_shift", "pdf": "/pdf/1d6c993e5092c7d7d1690b7adb1c2ae08d71f9dc.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nkim2022reversible,\ntitle={Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift},\nauthor={Taesung Kim and Jinhee Kim and Yunwon Tae and Cheonbok Park and Jang-Ho Choi and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cGDAkQo1C0p}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "8uz0EWPQIMu", "original": "a3dJMt5pzzO", "number": 2711, "cdate": 1632875639941, "mdate": null, "ddate": null, "tcdate": 1632875639941, "tmdate": 1697934689311, "tddate": null, "forum": "8uz0EWPQIMu", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Pitfalls of Analyzing Individual Neurons in Language Models", "authorids": ["~Omer_Antverg1", "~Yonatan_Belinkov1"], "authors": ["Omer Antverg", "Yonatan Belinkov"], "keywords": ["NLP", "interpretability", "multilingual", "individual neurons"], "abstract": "While many studies have shown that linguistic information is encoded in hidden word representations, few have studied individual neurons, to show how and in which neurons it is encoded.\nAmong these, the common approach is to use an external probe to rank neurons according to their relevance to some linguistic attribute, and to evaluate the obtained ranking using the same probe that produced it.\nWe show two pitfalls in this methodology:\n    1. It confounds distinct factors: probe quality and ranking quality.\n    We separate them and draw conclusions on each.\n    2. It focuses on encoded information, rather than information that is used by the model.\n    We show that these are not the same.\nWe compare two recent ranking methods and a simple one we introduce, and evaluate them with regard to both of these aspects.", "pdf": "/pdf/48b5f2da77098455f23b9b9c4bd9b7a6cd9712e8.pdf", "one-sentence_summary": "We analyze and compare methods to rank neurons in hidden representations according to their relevance to morphologic attributes, and show some of their weaknesses.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "antverg|on_the_pitfalls_of_analyzing_individual_neurons_in_language_models", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.07483/code)", "_bibtex": "@inproceedings{\nantverg2022on,\ntitle={On the Pitfalls of Analyzing Individual Neurons in Language Models},\nauthor={Omer Antverg and Yonatan Belinkov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8uz0EWPQIMu}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "4rLw09TgRw9", "original": "irLrR-_OmVA", "number": 2707, "cdate": 1632875639669, "mdate": null, "ddate": null, "tcdate": 1632875639669, "tmdate": 1697934689513, "tddate": null, "forum": "4rLw09TgRw9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Query Embedding on Hyper-Relational Knowledge Graphs", "authorids": ["~Dimitrios_Alivanistos1", "~Max_Berrendorf1", "~Michael_Cochez2", "~Mikhail_Galkin1"], "authors": ["Dimitrios Alivanistos", "Max Berrendorf", "Michael Cochez", "Mikhail Galkin"], "keywords": ["Query embedding", "Approximate Query Answering", "Graph Neural Network", "Hyper-relational Graph", "Knowledge Graph"], "abstract": "Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs). It subsumes both one-hop link prediction as well as other more complex types of logical queries. Existing algorithms operate only on classical, triple-based graphs, whereas modern KGs often employ a hyper-relational modeling paradigm. In this paradigm, typed edges may have several key-value pairs known as qualifiers that provide fine-grained context for facts. In queries, this context modifies the meaning of relations, and usually reduces the answer set. Hyper-relational queries are often observed in real-world KG applications, and existing approaches for approximate query answering cannot make use of qualifier pairs. In this work, we bridge this gap and extend the multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new type of complex queries. Building upon recent advancements in Graph Neural Networks and query embedding techniques, we study how to embed and answer hyper-relational conjunctive queries. Besides that, we propose a method to answer such queries and demonstrate in our experiments that qualifiers improve query answering on a diverse set of query patterns.", "one-sentence_summary": "We investigate how to extend the multi-hop reasoning problem to hyper-relational queries on knowledge graphs and consider methods for solving it.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "alivanistos|query_embedding_on_hyperrelational_knowledge_graphs", "pdf": "/pdf/d5bd37f34db1f59820dc265c98290e913e627460.pdf", "supplementary_material": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.08166/code)", "_bibtex": "@inproceedings{\nalivanistos2022query,\ntitle={Query Embedding on Hyper-Relational Knowledge Graphs},\nauthor={Dimitrios Alivanistos and Max Berrendorf and Michael Cochez and Mikhail Galkin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=4rLw09TgRw9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "m8bypnj7Yl5", "original": "67VnbeGKx6z", "number": 2706, "cdate": 1632875639601, "mdate": null, "ddate": null, "tcdate": 1632875639601, "tmdate": 1697934689990, "tddate": null, "forum": "m8bypnj7Yl5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Solvers for Fast and Accurate Numerical Optimal Control", "authorids": ["~Federico_Berto1", "~Stefano_Massaroli1", "~Michael_Poli1", "~Jinkyoo_Park1"], "authors": ["Federico Berto", "Stefano Massaroli", "Michael Poli", "Jinkyoo Park"], "keywords": ["Deep Learning", "Numerical Methods", "Optimal Control"], "abstract": "Synthesizing optimal controllers for dynamical systems often involves solving optimization problems with hard real-time constraints. These constraints determine the class of numerical methods that can be applied: computationally expensive but accurate numerical routines are replaced by fast and inaccurate methods, trading inference time for solution accuracy. This paper provides techniques to improve the quality of optimized control policies given a fixed computational budget. We achieve the above via a hypersolvers approach, which hybridizes a differential equation solver and a neural network. The performance is evaluated in direct and receding-horizon optimal control tasks in both low and high dimensions, where the proposed approach shows consistent Pareto improvements in solution accuracy and control performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "berto|neural_solvers_for_fast_and_accurate_numerical_optimal_control", "pdf": "/pdf/c4f0fbf5ef323ba4d9446eec32b783a9366d37cf.pdf", "one-sentence_summary": "We propose a hypersolvers approach for numerical optimal control which shows consistent Pareto improvements in solution accuracy and control performance.", "supplementary_material": "/attachment/43f9be371d40c4433861b0220f08aa0ba007b3ab.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.08072/code)", "_bibtex": "@inproceedings{\nberto2022neural,\ntitle={Neural Solvers for Fast and Accurate Numerical Optimal Control},\nauthor={Federico Berto and Stefano Massaroli and Michael Poli and Jinkyoo Park},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=m8bypnj7Yl5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "Ix_mh42xq5w", "original": "OUz60A0ywrJ", "number": 2700, "cdate": 1632875639189, "mdate": null, "ddate": null, "tcdate": 1632875639189, "tmdate": 1697934690922, "tddate": null, "forum": "Ix_mh42xq5w", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series", "authorids": ["~Paul_Jeha1", "~Michael_Bohlke-Schneider1", "~Pedro_Mercado2", "~Shubham_Kapoor1", "~Rajbir_Singh_Nirwan1", "~Valentin_Flunkert2", "~Jan_Gasthaus2", "~Tim_Januschowski2"], "authors": ["Paul Jeha", "Michael Bohlke-Schneider", "Pedro Mercado", "Shubham Kapoor", "Rajbir Singh Nirwan", "Valentin Flunkert", "Jan Gasthaus", "Tim Januschowski"], "keywords": ["Synthetic Time Series", "GAN", "Generative Modeling", "Time Series", "Forecasting"], "abstract": "Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in several downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet-Inception Distance-like score for time series, Context-FID, assessing the quality of synthetic time series samples. We find that Context-FID is indicative for downstream performance. Therefore, Context-FID could be a useful tool to develop time series GAN models. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jeha|psagan_progressive_self_attention_gans_for_synthetic_time_series", "pdf": "/pdf/22703570f03d2a1280efaf132cfd17e250dd590c.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2108.00981/code)", "_bibtex": "@inproceedings{\njeha2022psagan,\ntitle={{PSA}-{GAN}: Progressive Self Attention {GAN}s for Synthetic Time Series},\nauthor={Paul Jeha and Michael Bohlke-Schneider and Pedro Mercado and Shubham Kapoor and Rajbir Singh Nirwan and Valentin Flunkert and Jan Gasthaus and Tim Januschowski},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Ix_mh42xq5w}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "2t7CkQXNpuq", "original": "6TmmoUPtphU", "number": 2699, "cdate": 1632875639119, "mdate": null, "ddate": null, "tcdate": 1632875639119, "tmdate": 1676330544699, "tddate": null, "forum": "2t7CkQXNpuq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind", "authorids": ["~Yuanfei_Wang1", "~fangwei_zhong1", "~Jing_Xu2", "~Yizhou_Wang1"], "authors": ["Yuanfei Wang", "fangwei zhong", "Jing Xu", "Yizhou Wang"], "keywords": ["Theory of Mind", "Target-oriented Multi-Agent Cooperation", "Multi-agent Communication"], "abstract": "Being able to predict the mental states of others is a key factor to effective social interaction. It is also crucial for distributed multi-agent systems, where agents are required to communicate and cooperate. In this paper, we introduce such an important social-cognitive skill, i.e. Theory of Mind (ToM), to build socially intelligent agents who are able to communicate and cooperate effectively to accomplish challenging tasks. With ToM, each agent is capable of inferring the mental states and intentions of others according to its (local) observation. Based on the inferred states, the agents decide \"when'' and with \"whom'' to share their intentions. With the information observed, inferred, and received, the agents decide their sub-goals and reach a consensus among the team. In the end, the low-level executors independently take primitive actions to accomplish the sub-goals. We demonstrate the idea in two typical target-oriented multi-agent tasks: cooperative navigation and multi-sensor target coverage. The experiments show that the proposed model not only outperforms the state-of-the-art methods on reward and communication efficiency, but also shows good generalization across different scales of the environment.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|tom2c_targetoriented_multiagent_communication_and_cooperation_with_theory_of_mind", "pdf": "/pdf/a18a759fefcf7c83cb3cd488541d2df743059f5b.pdf", "supplementary_material": "/attachment/7d57aba638cf68329bdd4dd937db8435cef956d5.zip", "_bibtex": "@inproceedings{\nwang2022tomc,\ntitle={ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind},\nauthor={Yuanfei Wang and fangwei zhong and Jing Xu and Yizhou Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=2t7CkQXNpuq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "Iog0djAdbHj", "original": "3e23EOwuh03", "number": 2689, "cdate": 1632875638432, "mdate": null, "ddate": null, "tcdate": 1632875638432, "tmdate": 1697934692141, "tddate": null, "forum": "Iog0djAdbHj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Better Supervisory Signals by Observing Learning Paths", "authorids": ["~Yi_Ren6", "~Shangmin_Guo1", "~Danica_J._Sutherland1"], "authors": ["Yi Ren", "Shangmin Guo", "Danica J. Sutherland"], "keywords": ["Classification", "Supervision", "Knowledge Distillation"], "abstract": "Better-supervised models might have better performance. In this paper, we first clarify what makes for good supervision for a classification problem, and then explain two existing label refining methods, label smoothing and knowledge distillation, in terms of our proposed criterion. To further answer why and how better supervision emerges, we observe the learning path, i.e., the trajectory of the model's predictions during training, for each training sample. We find that the model can spontaneously refine \"bad\" labels through a \"zig-zag\" learning path, which occurs on both toy and real datasets. Observing the learning path not only provides a new perspective for understanding knowledge distillation, overfitting, and learning dynamics, but also reveals that the supervisory signal of a teacher network can be very unstable near the best points in training on real tasks. Inspired by this, we propose a new knowledge distillation scheme, Filter-KD, which improves downstream classification performance in various settings.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ren|better_supervisory_signals_by_observing_learning_paths", "pdf": "/pdf/e1c609447b4fe82d58e7ebc489aaed7002dd8e7e.pdf", "one-sentence_summary": "A study of how and why teachers in knowledge distillation end up with better supervisory signals than the original labels.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.02485/code)", "_bibtex": "@inproceedings{\nren2022better,\ntitle={Better Supervisory Signals by Observing Learning Paths},\nauthor={Yi Ren and Shangmin Guo and Danica J. Sutherland},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Iog0djAdbHj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "izj68lUcBpt", "original": "n2VC3XEC-An", "number": 2680, "cdate": 1632875637788, "mdate": null, "ddate": null, "tcdate": 1632875637788, "tmdate": 1697934693580, "tddate": null, "forum": "izj68lUcBpt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "TAda! Temporally-Adaptive Convolutions for Video Understanding", "authorids": ["~Ziyuan_Huang1", "~Shiwei_Zhang2", "~Liang_Pan2", "~Zhiwu_Qing1", "~Mingqian_Tang1", "~Ziwei_Liu1", "~Marcelo_H_Ang_Jr1"], "authors": ["Ziyuan Huang", "Shiwei Zhang", "Liang Pan", "Zhiwu Qing", "Mingqian Tang", "Ziwei Liu", "Marcelo H Ang Jr"], "keywords": ["Video understanding", "Action classification", "Dynamic networks"], "abstract": "Spatial convolutions are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modelling complex temporal dynamics in videos. Specifically, TAdaConv empowers the spatial convolutions with temporal modelling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to previous temporal modelling operations, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, the kernel calibration brings an increased model capacity. We construct TAda2D and TAdaConvNeXt networks by replacing the 2D convolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks. We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin.", "one-sentence_summary": "A stand-alone temporal modelling module or a plug-in enhancement of the 1D/2D/3D convolutions used in video models for better and more efficient temporal modelling.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|tada_temporallyadaptive_convolutions_for_video_understanding", "pdf": "/pdf/e6be2c061809752df033e546c9b00585b1c261f3.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.06178/code)", "_bibtex": "@inproceedings{\nhuang2022tada,\ntitle={{TA}da! Temporally-Adaptive Convolutions for Video Understanding},\nauthor={Ziyuan Huang and Shiwei Zhang and Liang Pan and Zhiwu Qing and Mingqian Tang and Ziwei Liu and Marcelo H Ang Jr},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=izj68lUcBpt}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "4Muj-t_4o4", "original": "OApZPqcsXtr", "number": 2661, "cdate": 1632875636511, "mdate": null, "ddate": null, "tcdate": 1632875636511, "tmdate": 1676330546208, "tddate": null, "forum": "4Muj-t_4o4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning a subspace of policies for online adaptation in Reinforcement Learning", "authorids": ["~Jean-Baptiste_Gaya1", "~Laure_Soulier1", "~Ludovic_Denoyer1"], "authors": ["Jean-Baptiste Gaya", "Laure Soulier", "Ludovic Denoyer"], "keywords": ["Deep Reinforcement Learning", "Online adaptation"], "abstract": "Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications, these environments may differ. For instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. It can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions). There is  a need to develop RL methods that generalize well to variations of the training conditions. In this article, we consider the simplest yet hard to tackle generalization setting where the test environment is unknown at train time, forcing the agent to adapt to the system's new dynamics. This online adaptation process can be computationally expensive (e.g., fine-tuning) and cannot rely on meta-RL techniques since there is just a single train environment. To do so, we propose an approach where we learn a subspace of policies within the parameter space. This subspace contains an infinite number of policies that are trained to solve the training environment while having different parameter values. As a consequence, two policies in that subspace process information differently and exhibit different behaviors when facing variations of the train environment. Our experiments carried out over a large variety of benchmarks compare our approach with baselines, including diversity-based methods. In comparison, our approach is simple to tune, does not need any extra component  (e.g., discriminator) and learns policies able to gather a high reward on unseen environments.", "one-sentence_summary": "We propose an approach to learn a subspace of policies that are robust to different variations of the train environment. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gaya|learning_a_subspace_of_policies_for_online_adaptation_in_reinforcement_learning", "pdf": "/pdf/2845a7c71512ebc0d2961233b0ec523e86dd65a8.pdf", "data": "", "_bibtex": "@inproceedings{\ngaya2022learning,\ntitle={Learning a subspace of policies for online adaptation in Reinforcement Learning},\nauthor={Jean-Baptiste Gaya and Laure Soulier and Ludovic Denoyer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=4Muj-t_4o4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "2sDQwC_hmnM", "original": "ZPViNem-kwf", "number": 2658, "cdate": 1632875636300, "mdate": null, "ddate": null, "tcdate": 1632875636300, "tmdate": 1676330546581, "tddate": null, "forum": "2sDQwC_hmnM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ZeroFL: Efficient On-Device Training for  Federated Learning with Local Sparsity", "authorids": ["~Xinchi_Qiu1", "~Javier_Fernandez-Marques1", "pedropgusmao@gmail.com", "~Yan_Gao4", "~Titouan_Parcollet1", "~Nicholas_Donald_Lane1"], "authors": ["Xinchi Qiu", "Javier Fernandez-Marques", "Pedro PB Gusmao", "Yan Gao", "Titouan Parcollet", "Nicholas Donald Lane"], "keywords": ["Federated Learning", "sparse training"], "abstract": "When the available hardware cannot meet the memory and compute requirements to efficiently train high performing machine learning models, a compromise in either the training quality or the model complexity is needed. In Federated Learning (FL), nodes are orders of magnitude more constrained than traditional server-grade hardware and are often battery powered, severely limiting the sophistication of models that can be trained under this paradigm. While most research has focused on designing better aggregation strategies to improve convergence rates and in alleviating the communication costs of FL, fewer efforts have been devoted to accelerating on-device training. Such stage, which repeats hundreds of times (i.e. every round) and can involve thousands of devices, accounts for the majority of the time required to train federated models and, the totality of the energy consumption at the client side. In this work, we present the first study on the unique aspects that arise when introducing sparsity at training time in FL workloads. We then propose ZeroFL, a framework that relies on highly sparse operations to accelerate on-device training. Models trained with ZeroFL and 95% sparsity achieve up to 2.3% higher accuracy compared to competitive baselines obtained from adapting a state-of-the-art sparse training framework to the FL setting.", "one-sentence_summary": "This work studies the challenges of accelerating on-device training in Federated Learning workloads by using highly sparse operations, with our framework we achieve lower accuracy degradation in addition to reducing the uplink communication costs. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "qiu|zerofl_efficient_ondevice_training_for_federated_learning_with_local_sparsity", "pdf": "/pdf/51f094e72c1306f6546a372f9bb52b32060ebcf4.pdf", "data": "", "_bibtex": "@inproceedings{\nqiu2022zerofl,\ntitle={Zero{FL}: Efficient On-Device Training for  Federated Learning with Local Sparsity},\nauthor={Xinchi Qiu and Javier Fernandez-Marques and Pedro PB Gusmao and Yan Gao and Titouan Parcollet and Nicholas Donald Lane},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=2sDQwC_hmnM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "Oxeka7Z7Hor", "original": "ZdVOu3UZHWO", "number": 2649, "cdate": 1632875635851, "mdate": null, "ddate": null, "tcdate": 1632875635851, "tmdate": 1697934695016, "tddate": null, "forum": "Oxeka7Z7Hor", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Gaussian Mixture Convolution Networks", "authorids": ["~Adam_Celarek1", "~Pedro_Hermosilla1", "~Bernhard_Kerbl1", "~Timo_Ropinski2", "~Michael_Wimmer1"], "authors": ["Adam Celarek", "Pedro Hermosilla", "Bernhard Kerbl", "Timo Ropinski", "Michael Wimmer"], "keywords": ["deep learning architecture", "gaussian convolution", "gaussian mixture", "3d"], "abstract": "This paper proposes a novel method for deep learning based on the analytical convolution of multidimensional Gaussian mixtures.\nIn contrast to tensors, these do not suffer from the curse of dimensionality and allow for a compact representation, as data is only stored where details exist.\nConvolution kernels and data are Gaussian mixtures with unconstrained weights, positions, and covariance matrices.\nSimilar to discrete convolutional networks, each convolution step produces several feature channels, represented by independent Gaussian mixtures.\nSince traditional transfer functions like ReLUs do not produce Gaussian mixtures, we propose using a fitting of these functions instead.\nThis fitting step also acts as a pooling layer if the number of Gaussian components is reduced appropriately.\nWe demonstrate that networks based on this architecture reach competitive accuracy on Gaussian mixtures fitted to the MNIST and ModelNet data sets.", "one-sentence_summary": "Deep learning based on the analytical convolution of multi-dimensional Gaussian mixtures", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "celarek|gaussian_mixture_convolution_networks", "pdf": "/pdf/19208a1745f081633db29c386c4625dd681c9411.pdf", "supplementary_material": "/attachment/c1cc3c8dfeda01da7b0daf0413f5b96d77686e51.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.09153/code)", "_bibtex": "@inproceedings{\ncelarek2022gaussian,\ntitle={Gaussian Mixture Convolution Networks},\nauthor={Adam Celarek and Pedro Hermosilla and Bernhard Kerbl and Timo Ropinski and Michael Wimmer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Oxeka7Z7Hor}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "bwq6O4Cwdl", "original": "W4q8PS8pgmL", "number": 2648, "cdate": 1632875635780, "mdate": null, "ddate": null, "tcdate": 1632875635780, "tmdate": 1676330546829, "tddate": null, "forum": "bwq6O4Cwdl", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning", "authorids": ["~Chaoning_Zhang1", "~Kang_Zhang6", "~Chenshuang_Zhang2", "~Trung_X._Pham1", "~Chang_D._Yoo1", "~In_So_Kweon2"], "authors": ["Chaoning Zhang", "Kang Zhang", "Chenshuang Zhang", "Trung X. Pham", "Chang D. Yoo", "In So Kweon"], "keywords": ["SimSiam", "Negative samples", "SSL", "Collapse", "Covariance"], "abstract": "To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely used but often requires a large number of negative samples. Without negative samples yet achieving competitive performance, a recent work~\\citep{chen2021exploring} has attracted significant attention for providing a minimalist simple Siamese (SimSiam) method to avoid collapse. However, the reason for how it avoids collapse without negative samples remains not fully clear and our investigation starts by revisiting the explanatory claims in the original SimSiam. After refuting their claims, we introduce vector decomposition for analyzing the collapse based on the gradient analysis of the $l_2$-normalized representation vector. This yields a unified perspective on how negative samples and SimSiam alleviate collapse. Such a unified perspective comes timely for understanding the recent progress in SSL. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|how_does_simsiam_avoid_collapse_without_negative_samples_a_unified_understanding_with_selfsupervised_contrastive_learning", "pdf": "/pdf/83121a24e1afcc8d9cc5b1489f38d0c54a411bf7.pdf", "_bibtex": "@inproceedings{\nzhang2022how,\ntitle={How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning},\nauthor={Chaoning Zhang and Kang Zhang and Chenshuang Zhang and Trung X. Pham and Chang D. Yoo and In So Kweon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bwq6O4Cwdl}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "kAa9eDS0RdO", "original": "YdEpL-sfzfy", "number": 2647, "cdate": 1632875635707, "mdate": null, "ddate": null, "tcdate": 1632875635707, "tmdate": 1676330546924, "tddate": null, "forum": "kAa9eDS0RdO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Attention-based Interpretability with Concept Transformers", "authorids": ["~Mattia_Rigotti1", "~Christoph_Miksovic1", "~Ioana_Giurgiu2", "~Thomas_Gschwind1", "psc@zurich.ibm.com"], "authors": ["Mattia Rigotti", "Christoph Miksovic", "Ioana Giurgiu", "Thomas Gschwind", "Paolo Scotton"], "keywords": ["attention", "transformer", "concepts", "interpretability"], "abstract": "Attention is a mechanism that has been instrumental in driving remarkable performance gains of deep neural network models in a host of visual, NLP and multimodal tasks.\nOne additional notable aspect of attention is that it conveniently exposes the ``reasoning'' behind each particular output generated by the model.\nSpecifically, attention scores over input regions or intermediate features have been interpreted as a measure of the contribution of the attended element to the model inference.\nWhile the debate in regard to the interpretability of attention is still not settled, researchers have pointed out the existence of architectures and scenarios that afford a meaningful interpretation of the attention mechanism.\n\nHere we propose the generalization of attention from low-level input features to high-level concepts as a mechanism to ensure the interpretability of attention scores within a given application domain.\nIn particular, we design the ConceptTransformer, a deep learning module that exposes explanations of the output of a model in which it is embedded in terms of attention over user-defined high-level concepts.\nSuch explanations are \\emph{plausible} (i.e.\\ convincing to the human user) and \\emph{faithful} (i.e.\\ truly reflective of the reasoning process of the model).\nPlausibility of such explanations is obtained by construction by training the attention heads to conform with known relations between inputs, concepts and outputs dictated by domain knowledge.\nFaithfulness is achieved by design by enforcing a linear relation between the transformer value vectors that represent the concepts and their contribution to the classification log-probabilities.\n\nWe validate our ConceptTransformer module on established explainability benchmarks and show how it can be used to infuse domain knowledge into classifiers to improve accuracy, and conversely to extract concept-based explanations of classification outputs. Code to reproduce our results is available at: \\url{https://github.com/ibm/concept_transformer}.", "one-sentence_summary": "We the Concept Transformer an architecture that generalization attention from low-level input features to high-level concepts as a mechanism to ensure the interpretability of attention scores within a given application domain.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rigotti|attentionbased_interpretability_with_concept_transformers", "pdf": "/pdf/d910731148e5b8279d1974d45e83aada94c35e55.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nrigotti2022attentionbased,\ntitle={Attention-based Interpretability with Concept Transformers},\nauthor={Mattia Rigotti and Christoph Miksovic and Ioana Giurgiu and Thomas Gschwind and Paolo Scotton},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kAa9eDS0RdO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "PTRo58zPt3P", "original": "S-1dplMAVz", "number": 2645, "cdate": 1632875635555, "mdate": null, "ddate": null, "tcdate": 1632875635555, "tmdate": 1676330547013, "tddate": null, "forum": "PTRo58zPt3P", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Inductive Relation Prediction Using Analogy Subgraph Embeddings", "authorids": ["~Jiarui_Jin1", "~Yangkun_Wang1", "~Kounianhua_Du1", "~Weinan_Zhang1", "~Zheng_Zhang1", "~David_Wipf1", "~Yong_Yu1", "~Quan_Gan1"], "authors": ["Jiarui Jin", "Yangkun Wang", "Kounianhua Du", "Weinan Zhang", "Zheng Zhang", "David Wipf", "Yong Yu", "Quan Gan"], "keywords": ["Link Prediction", "Relation Modelling", "Heterogeneous Graphs", "Knowledge Graphs"], "abstract": "Prevailing methods for relation prediction in heterogeneous graphs aim at learning latent representations (i.e., embeddings) of observed nodes and relations, and thus are limited to the transductive setting where the relation types must be known during training.  Here,  we propose ANalogy  SubGraphEmbeddingLearning (GraphANGEL), a novel relation prediction framework that predicts relations5between each node pair based on the subgraphs containing the pair, as well as other  (analogy)  subgraphs with the same graph patterns.   Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relations and leads to more explainable predictive models. Moreover, our method also removes the limited neighborhood constraint of graph neural networks. Our model consistently outperforms existing models on heterogeneous graph based recommendation as well as knowledge graph completion.  We also empirically demonstrate our model\u2019s capability in generalizing to new relations while producing explainable heat maps of attention scores across the discovered logic.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jin|inductive_relation_prediction_using_analogy_subgraph_embeddings", "pdf": "/pdf/7ef097299fb4b886196be6fa5e836e944a2a770a.pdf", "one-sentence_summary": "In this paper, we propose GraphANGEL, a novel relation prediction framework that predicts (new) relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraphs containing the considered relation.", "data": "", "_bibtex": "@inproceedings{\njin2022inductive,\ntitle={Inductive Relation Prediction Using Analogy Subgraph Embeddings},\nauthor={Jiarui Jin and Yangkun Wang and Kounianhua Du and Weinan Zhang and Zheng Zhang and David Wipf and Yong Yu and Quan Gan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PTRo58zPt3P}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "CmsfC7u054S", "original": "QphkhGyJ_Rw", "number": 2638, "cdate": 1632875635097, "mdate": null, "ddate": null, "tcdate": 1632875635097, "tmdate": 1676330547256, "tddate": null, "forum": "CmsfC7u054S", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Reinforcement Learning in Presence of Discrete Markovian Context Evolution ", "authorids": ["~Hang_Ren2", "~Aivar_Sootla1", "~Taher_Jafferjee1", "~Junxiao_Shen1", "~Jun_Wang2", "~Haitham_Bou_Ammar1"], "authors": ["Hang Ren", "Aivar Sootla", "Taher Jafferjee", "Junxiao Shen", "Jun Wang", "Haitham Bou Ammar"], "keywords": ["context-dependent Reinforcement Learning", "model-based reinforcement learning", "hierarchical Dirichlet process"], "abstract": "We consider a context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. We argue that this challenging case is often met in applications and we tackle it using a Bayesian model-based approach and variational inference. We adapt a sticky Hierarchical Dirichlet Process (HDP) prior for model learning, which is arguably best-suited for infinite Markov chain modeling. We then derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. We argue that the combination of these two components allows inferring the number of contexts from data thus dealing with the context cardinality assumption. We then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, we demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that our approach succeeds where state-of-the-art methods of other frameworks fail and elaborate on the reasons for such failures.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ren|reinforcement_learning_in_presence_of_discrete_markovian_context_evolution", "pdf": "/pdf/53bda3136fe85f6fe3bed9a024f40991920a62fc.pdf", "_bibtex": "@inproceedings{\nren2022reinforcement,\ntitle={Reinforcement Learning in Presence of Discrete Markovian Context Evolution },\nauthor={Hang Ren and Aivar Sootla and Taher Jafferjee and Junxiao Shen and Jun Wang and Haitham Bou Ammar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CmsfC7u054S}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "t98k9ePQQpn", "original": "tlXueOx9b8n", "number": 2637, "cdate": 1632875635017, "mdate": null, "ddate": null, "tcdate": 1632875635017, "tmdate": 1676330547298, "tddate": null, "forum": "t98k9ePQQpn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix", "authorids": ["~Hanyu_Peng1", "~Mingming_Sun1", "~Ping_Li3"], "authors": ["Hanyu Peng", "Mingming Sun", "Ping Li"], "keywords": ["Long-tailed recognition", "imbalanced classification", "optimal transport"], "abstract": "It is attracting attention to the long-tailed recognition problem, a burning issue that has become very popular recently. Distinctive from conventional recognition is that it posits that the allocation of the training set is supremely distorted. Predictably, it will pose challenges to the generalisation behaviour of the model. Approaches to these challenges revolve into two groups: firstly, training-aware methods, with the aim of enhancing the generalisability of the model by exploiting its potential in the training period; and secondly, post-hoc correction, liberally coupled with training-aware methods, which is intended to refine the predictions to the extent possible in the post-processing stage, offering the advantages of simplicity and effectiveness. This paper introduces an alternative direction to do the post-hoc correction, which goes beyond the statistical methods. Mathematically, we approach this issue from the perspective of optimal transport (OT), yet, choosing the exact cost matrix when applying OT is challenging and requires expert knowledge of various tasks. To overcome this limitation, we propose to employ linear mapping to learn the cost matrix without necessary configurations adaptively. Testing our methods in practice, along with high efficiency and excellent performance, our method surpasses all previous methods and has the best performance to date.", "one-sentence_summary": "A new paradigm for post-hoc correction based on optimal transport to cope with long-tailed recognition.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "peng|optimal_transport_for_longtailed_recognition_with_learnable_cost_matrix", "pdf": "/pdf/d835dec4f40d7fbc936204b98e3ddea446bc757c.pdf", "data": "", "_bibtex": "@inproceedings{\npeng2022optimal,\ntitle={Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix},\nauthor={Hanyu Peng and Mingming Sun and Ping Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=t98k9ePQQpn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "_BNiN4IjC5", "original": "GET1qN-67zv", "number": 2630, "cdate": 1632875634480, "mdate": null, "ddate": null, "tcdate": 1632875634480, "tmdate": 1676330547726, "tddate": null, "forum": "_BNiN4IjC5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior", "authorids": ["~Sang-gil_Lee1", "~Heeseung_Kim1", "~Chaehun_Shin1", "~Xu_Tan1", "~Chang_Liu10", "~Qi_Meng1", "~Tao_Qin1", "~Wei_Chen1", "~Sungroh_Yoon1", "~Tie-Yan_Liu1"], "authors": ["Sang-gil Lee", "Heeseung Kim", "Chaehun Shin", "Xu Tan", "Chang Liu", "Qi Meng", "Tao Qin", "Wei Chen", "Sungroh Yoon", "Tie-Yan Liu"], "keywords": ["diffusion-based model", "generative model", "speech synthesis"], "abstract": "Denoising diffusion probabilistic models have been recently proposed to generate high-quality samples by estimating the gradient of the data density. The framework assumes the prior noise as a standard Gaussian distribution, whereas the corresponding data distribution may be more complicated than the standard Gaussian distribution, which potentially introduces inefficiency in denoising the prior noise into the data sample because of the discrepancy between the data and the prior. In this paper, we propose PriorGrad to improve the efficiency of the conditional diffusion model (for example, a vocoder using a mel-spectrogram as the condition) by applying an adaptive prior derived from the data statistics based on the conditional information. We formulate the training and sampling procedures of PriorGrad and demonstrate the advantages of an adaptive prior through a theoretical analysis. Focusing on the audio domain, we consider the recently proposed diffusion-based audio generative models based on both the spectral and time domains and show that PriorGrad achieves faster convergence and superior performance, leading to an improved perceptual quality and tolerance to a smaller network capacity, and thereby demonstrating the efficiency of a data-dependent adaptive prior.", "one-sentence_summary": "We improve the efficiency of diffusion-based conditional generative models for audio by using data-dependent non-standard Gaussian as a prior.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|priorgrad_improving_conditional_denoising_diffusion_models_with_datadependent_adaptive_prior", "pdf": "/pdf/0e602657a19a26315b3e07434feb9b84217302e6.pdf", "data": "", "_bibtex": "@inproceedings{\nlee2022priorgrad,\ntitle={PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior},\nauthor={Sang-gil Lee and Heeseung Kim and Chaehun Shin and Xu Tan and Chang Liu and Qi Meng and Tao Qin and Wei Chen and Sungroh Yoon and Tie-Yan Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_BNiN4IjC5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "pz1euXohm4H", "original": "0hwN3DwBSC", "number": 2629, "cdate": 1632875634382, "mdate": null, "ddate": null, "tcdate": 1632875634382, "tmdate": 1676330547867, "tddate": null, "forum": "pz1euXohm4H", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Target-Side Input Augmentation for Sequence to Sequence Generation", "authorids": ["~Shufang_Xie1", "~Ang_Lv1", "~Yingce_Xia1", "~Lijun_Wu1", "~Tao_Qin1", "~Tie-Yan_Liu1", "~Rui_Yan2"], "authors": ["Shufang Xie", "Ang Lv", "Yingce Xia", "Lijun Wu", "Tao Qin", "Tie-Yan Liu", "Rui Yan"], "keywords": ["Sequence Gerneration", "Data Augmentation"], "abstract": "Autoregressive sequence generation, a prevalent task in machine learning and natural language processing, generates every target token conditioned on both a source input and previously generated target tokens. Previous data augmentation methods, which have been shown to be effective for the task, mainly enhance source inputs (e.g., injecting noise into the source sequence by random swapping or masking, back translation, etc.) while overlooking the target-side augmentation. In this work, we propose a target-side augmentation method for sequence generation. In training, we use the decoder output probability distributions as soft indicators, which are multiplied with target token embeddings, to build pseudo tokens. These soft pseudo tokens are then used as target tokens to enhance the training. We conduct comprehensive experiments on various sequence generation tasks, including dialog generation, machine translation, and abstractive summarization. Without using any extra labeled data or introducing additional model parameters, our method significantly outperforms strong baselines. The code is available at https://github.com/TARGET-SIDE-DATA-AUG/TSDASG.", "pdf": "/pdf/422cd72cb7110055373e0c085cdbbdc9b40d0811.pdf", "one-sentence_summary": "We study the data augmentation for target-side conditional input of autoregressive sequence generation and propose a new method to build soft synthetic data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xie|targetside_input_augmentation_for_sequence_to_sequence_generation", "code": "", "data": "", "_bibtex": "@inproceedings{\nxie2022targetside,\ntitle={Target-Side Input Augmentation for Sequence to Sequence Generation},\nauthor={Shufang Xie and Ang Lv and Yingce Xia and Lijun Wu and Tao Qin and Tie-Yan Liu and Rui Yan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pz1euXohm4H}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "nBU_u6DLvoK", "original": "_9ABPsAnjHP", "number": 2620, "cdate": 1632875633766, "mdate": null, "ddate": null, "tcdate": 1632875633766, "tmdate": 1676330548327, "tddate": null, "forum": "nBU_u6DLvoK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning", "authorids": ["~Kunchang_Li1", "~Yali_Wang1", "~Gao_Peng1", "~Guanglu_Song2", "~Yu_Liu2", "~Hongsheng_Li3", "~Yu_Qiao1"], "authors": ["Kunchang Li", "Yali Wang", "Gao Peng", "Guanglu Song", "Yu Liu", "Hongsheng Li", "Yu Qiao"], "keywords": ["Spatial-Temporal Representation Learning", "3D Convolution", "Transformer"], "abstract": "It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local context to suppress local redundancy from a small 3D neighborhood, it lacks the capability to capture global dependency because of the limited receptive field. Alternatively, vision transformers can effectively capture long-range dependency by self-attention mechanism, while having the limitation on reducing local redundancy with blind similarity comparison among all the tokens in each layer. Based on these observations, we propose a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatiotemporal self-attention in a concise transformer format, and achieves a preferable balance between computation and accuracy. Different from traditional transformers, our relation aggregator can tackle both spatiotemporal redundancy and dependency, by learning local and global token affinity respectively in shallow and deep layers. We conduct extensive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&V2. With only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1 accuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than other state-of-the-art methods. For Something-Something V1 and V2, our UniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1 accuracy respectively. Code is available at https://github.com/Sense-X/UniFormer.", "one-sentence_summary": "We introduce a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatial-temporal self-attention in a concise transformer format, and achieves new state-of-the-art performances on Something-Something.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|uniformer_unified_transformer_for_efficient_spatialtemporal_representation_learning", "pdf": "/pdf/4e646685f068cfb87b0f47952a01638be74eaacc.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nli2022uniformer,\ntitle={UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning},\nauthor={Kunchang Li and Yali Wang and Gao Peng and Guanglu Song and Yu Liu and Hongsheng Li and Yu Qiao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nBU_u6DLvoK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "DYypjaRdph2", "original": "PrWkV2iu-x", "number": 2615, "cdate": 1632875633491, "mdate": null, "ddate": null, "tcdate": 1632875633491, "tmdate": 1676330548577, "tddate": null, "forum": "DYypjaRdph2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies", "authorids": ["~Alex_Chan2", "~Alicia_Curth1", "~Mihaela_van_der_Schaar2"], "authors": ["Alex Chan", "Alicia Curth", "Mihaela van der Schaar"], "keywords": ["Decision Modelling", "Imitation Learning", "Inverse Online Learning"], "abstract": "Human decision making is well known to be imperfect and the ability to analyse such processes individually is crucial when attempting to aid or improve a decision-maker's ability to perform a task, e.g. to alert them to potential biases or oversights on their part. To do so, it is necessary to develop interpretable representations of how agents make decisions and how this process changes over time as the agent learns online in reaction to the accrued experience. To then understand the decision-making processes underlying a set of observed trajectories, we cast the policy inference problem as the inverse to this online learning problem. By interpreting actions within a potential outcomes framework, we introduce a meaningful mapping based on agents choosing an action they believe to have the greatest treatment effect. We introduce a practical algorithm for retrospectively estimating such perceived effects, alongside the process through which agents update them, using a novel architecture built upon an expressive family of deep state-space models. Through application to the analysis of UNOS organ donation acceptance decisions, we demonstrate that our approach can bring valuable insights into the factors that govern decision processes and how they change over time. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chan|inverse_online_learning_understanding_nonstationary_and_reactionary_policies", "pdf": "/pdf/dc7c6c1afc5a4d35d2f621c9684cf86f140adf79.pdf", "_bibtex": "@inproceedings{\nchan2022inverse,\ntitle={Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies},\nauthor={Alex Chan and Alicia Curth and Mihaela van der Schaar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DYypjaRdph2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "6YVIk0sAkF_", "original": "Lve85AqdIk7", "number": 2614, "cdate": 1632875633426, "mdate": null, "ddate": null, "tcdate": 1632875633426, "tmdate": 1676330548667, "tddate": null, "forum": "6YVIk0sAkF_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multi-Mode Deep Matrix and Tensor Factorization", "authorids": ["~Jicong_Fan2"], "authors": ["Jicong Fan"], "keywords": [], "abstract": "Recently, deep linear and nonlinear matrix factorizations gain increasing attention in the area of machine learning. Existing deep nonlinear matrix factorization methods can only exploit partial nonlinearity of the data and are not effective in handling matrices of which the number of rows is comparable to the number of columns. On the other hand, there is still a gap between deep learning and tensor decomposition. This paper presents a framework of multi-mode deep matrix and tensor factorizations to explore and exploit the full nonlinearity of the data in matrices and tensors. We use the factorization methods to solve matrix and tensor completion problems and prove that our methods have tighter generalization error bounds than conventional matrix and tensor factorization methods. The experiments on synthetic data and real datasets showed that the proposed methods have much higher recovery accuracy than many baselines.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fan|multimode_deep_matrix_and_tensor_factorization", "pdf": "/pdf/79b1b1ca98990989bacbd3c32a685e6ce2323668.pdf", "_bibtex": "@inproceedings{\nfan2022multimode,\ntitle={Multi-Mode Deep Matrix and Tensor Factorization},\nauthor={Jicong Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6YVIk0sAkF_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "fCG75wd39ze", "original": "ZSdf24To-uG", "number": 2613, "cdate": 1632875633354, "mdate": null, "ddate": null, "tcdate": 1632875633354, "tmdate": 1676330548773, "tddate": null, "forum": "fCG75wd39ze", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations", "authorids": ["~JAEHOON_LEE5", "~Jinsung_Jeon1", "~Sheo_yon_Jhin1", "~Jihyeon_Hyeong1", "~Jayoung_Kim1", "~Minju_Jo1", "~Kook_Seungji1", "~Noseong_Park1"], "authors": ["JAEHOON LEE", "Jinsung Jeon", "Sheo yon Jhin", "Jihyeon Hyeong", "Jayoung Kim", "Minju Jo", "Kook Seungji", "Noseong Park"], "keywords": [], "abstract": "The problem of processing very long time-series data (e.g., a length of more than 10,000) is a long-standing research problem in machine learning. Recently, one breakthrough, called neural rough differential equations (NRDEs), has been proposed and has shown that it is able to process such data. Their main concept is to use the log-signature transform, which is known to be more efficient than the Fourier transform for irregular long time-series, to convert a very long time-series sample into a relatively shorter series of feature vectors. However, the log-signature transform causes non-trivial spatial overheads. To this end, we present the method of LOweR-Dimensional embedding of log-signature (LORD), where we define an NRDE-based autoencoder to implant the higher-depth log-signature knowledge into the lower-depth log-signature. We show that the encoder successfully combines the higher-depth and the lower-depth log-signature knowledge, which greatly stabilizes the training process and increases the model accuracy. In our experiments with benchmark datasets, the improvement ratio by our method is up to 75\\% in terms of various classification and forecasting evaluation metrics.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|lord_lowerdimensional_embedding_of_logsignature_in_neural_rough_differential_equations", "pdf": "/pdf/178b571a9442283c345d20ba2bdda24dab7e0aea.pdf", "one-sentence_summary": "We reduce the complexity of processing higher depth log-signatures in NRDE.", "supplementary_material": "/attachment/2766ee250764ecb2befd7d8a98a36a95aef018cd.zip", "_bibtex": "@inproceedings{\nlee2022lord,\ntitle={{LORD}: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations},\nauthor={JAEHOON LEE and Jinsung Jeon and Sheo yon Jhin and Jihyeon Hyeong and Jayoung Kim and Minju Jo and Kook Seungji and Noseong Park},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fCG75wd39ze}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "bsycpMi00R1", "original": "WMgSh8cvEGx", "number": 2608, "cdate": 1632875633010, "mdate": null, "ddate": null, "tcdate": 1632875633010, "tmdate": 1676330549064, "tddate": null, "forum": "bsycpMi00R1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalized Natural Gradient Flows in Hidden Convex-Concave Games and GANs", "authorids": ["~Andjela_Mladenovic1", "~Iosif_Sakos1", "~Gauthier_Gidel1", "~Georgios_Piliouras1"], "authors": ["Andjela Mladenovic", "Iosif Sakos", "Gauthier Gidel", "Georgios Piliouras"], "keywords": [], "abstract": "Game-theoretic formulations in machine learning have recently risen in prominence, whereby entire modeling paradigms are best captured as zero-sum games. Despite their popularity, however, their dynamics are still poorly understood. This lack of theory is often substantiated with painful empirical observations of volatile training dynamics and even divergence. Such results highlight the need to develop an appropriate theory with convergence guarantees that are powerful enough to inform practice. This paper studies the generalized Gradient Descent-Ascent (GDA) flow in a large class of non-convex non-concave Zero-Sum games dubbed Hidden Convex-Concave games, a class of games that includes GANs. We focus on two specific geometries: a novel geometry induced by the hidden convex-concave structure that we call the hidden mapping geometry and the Fisher information geometry. For the hidden mapping geometry, we prove global convergence under mild assumptions. In the case of Fisher information geometry, we provide a complete picture of the dynamics in an interesting special setting of team competition via invariant function analysis.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mladenovic|generalized_natural_gradient_flows_in_hidden_convexconcave_games_and_gans", "pdf": "/pdf/0630aee415ca1b8fb2ede23f4f1a8a70876fb603.pdf", "supplementary_material": "/attachment/78797d649101f0173bb8b2e22fbd6908eb573b3d.zip", "_bibtex": "@inproceedings{\nmladenovic2022generalized,\ntitle={Generalized Natural Gradient Flows in Hidden Convex-Concave Games and {GAN}s},\nauthor={Andjela Mladenovic and Iosif Sakos and Gauthier Gidel and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bsycpMi00R1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "sPIFuucA3F", "original": "O_-JnS1w3g", "number": 2602, "cdate": 1632875632592, "mdate": null, "ddate": null, "tcdate": 1632875632592, "tmdate": 1676330549294, "tddate": null, "forum": "sPIFuucA3F", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Offline Neural Contextual Bandits: Pessimism, Optimization and Generalization", "authorids": ["~Thanh_Nguyen-Tang1", "~Sunil_Gupta2", "~A._Tuan_Nguyen1", "~Svetha_Venkatesh1"], "authors": ["Thanh Nguyen-Tang", "Sunil Gupta", "A. Tuan Nguyen", "Svetha Venkatesh"], "keywords": ["offline policy learning", "offline contextual bandits", "neural network function approximation"], "abstract": "Offline policy learning (OPL) leverages existing data collected a priori for policy optimization without any active exploration. Despite the prevalence and recent interest in this problem, its theoretical and algorithmic foundations in function approximation settings remain under-developed. In this paper, we consider this problem on the axes of distributional shift, optimization, and generalization in offline contextual bandits with neural networks. In particular, we propose a provably efficient offline contextual bandit with neural network function approximation that does not require any functional assumption on the reward. We show that our method provably generalizes over unseen contexts under a milder condition for distributional shift than the existing OPL works. Notably, unlike any other OPL method, our method learns from the offline data in an online manner using stochastic gradient descent, allowing us to leverage the benefits of online learning into an offline setting. Moreover, we show that our method is more computationally efficient and has a better dependence on the effective dimension of the neural network than an online counterpart. Finally, we demonstrate the empirical effectiveness of our method in a range of synthetic and real-world OPL problems.", "one-sentence_summary": "Provably efficient offline contextual bandits with neural network function approximation ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nguyentang|offline_neural_contextual_bandits_pessimism_optimization_and_generalization", "pdf": "/pdf/adc89029ee15c473bef28493efafef96544ac523.pdf", "_bibtex": "@inproceedings{\nnguyen-tang2022offline,\ntitle={Offline Neural Contextual Bandits: Pessimism, Optimization and Generalization},\nauthor={Thanh Nguyen-Tang and Sunil Gupta and A. Tuan Nguyen and Svetha Venkatesh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=sPIFuucA3F}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "QDdJhACYrlX", "original": "nHQCOegfqEY", "number": 2601, "cdate": 1632875632522, "mdate": null, "ddate": null, "tcdate": 1632875632522, "tmdate": 1676330549386, "tddate": null, "forum": "QDdJhACYrlX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling", "authorids": ["~Thomas_Gilles1", "~Stefano_Sabatini1", "~Dzmitry_Tsishkou1", "~Bogdan_Stanciulescu1", "~Fabien_Moutarde1"], "authors": ["Thomas Gilles", "Stefano Sabatini", "Dzmitry Tsishkou", "Bogdan Stanciulescu", "Fabien Moutarde"], "keywords": ["Trajectory prediction", "Multi-agent", "Motion forecasting", "Motion estimation", "Autonomous driving"], "abstract": "In this paper, we propose THOMAS, a joint multi-agent trajectory prediction framework allowing for an efficient and consistent prediction of multi-agent multi-modal trajectories. We present a unified model architecture for simultaneous agent future heatmap estimation, in which we leverage hierarchical and sparse image generation for fast and memory-efficient inference. We propose a learnable trajectory recombination model that takes as input a set of predicted trajectories for each agent and outputs its consistent reordered recombination. This recombination module is able to realign the initially independent modalities so that they do no collide and are coherent with each other.  We report our results on the Interaction multi-agent prediction challenge and rank $1^{st}$ on the online test leaderboard.", "pdf": "/pdf/a8ce9facf1e0dfc642c02f9849f5b7910589efad.pdf", "one-sentence_summary": "We propose a solution for multi-agent coherent multimodal trajectory prediction by learning a recombination of each agent predicted modalities.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gilles|thomas_trajectory_heatmap_output_with_learned_multiagent_sampling", "_bibtex": "@inproceedings{\ngilles2022thomas,\ntitle={{THOMAS}: Trajectory Heatmap Output with learned Multi-Agent Sampling},\nauthor={Thomas Gilles and Stefano Sabatini and Dzmitry Tsishkou and Bogdan Stanciulescu and Fabien Moutarde},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=QDdJhACYrlX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "rHMaBYbkkRJ", "original": "dWK8bbpzGc", "number": 2595, "cdate": 1632875632114, "mdate": null, "ddate": null, "tcdate": 1632875632114, "tmdate": 1676330549865, "tddate": null, "forum": "rHMaBYbkkRJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CLEVA-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability", "authorids": ["~Martin_Mundt1", "~Steven_Lang2", "~Quentin_Delfosse1", "~Kristian_Kersting1"], "authors": ["Martin Mundt", "Steven Lang", "Quentin Delfosse", "Kristian Kersting"], "keywords": ["continual learning", "lifelong learning", "machine learning evaluation"], "abstract": "What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation.  The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and evaluation protocols being too narrow, resulting in several attempts at constructing guidelines in favor of specific desiderata or arguing against the validity of prevalent assumptions.  In this work, we depart from this mindset and argue that the goal of a precise formulation of desiderata is an ill-posed one, as diverse applications may always warrant distinct scenarios. Instead, we introduce the Continual Learning EValuation Assessment Compass: the CLEVA-Compass. The compass provides the visual means to both identify how approaches are practically reported and how works can simultaneously be contextualized in the broader literature landscape.  In addition to promoting compact specification in the spirit of recent replication trends, it thus provides an intuitive chart to understand the priorities of individual systems, where they resemble each other, and what elements are missing towards a fair comparison.  ", "one-sentence_summary": "We introduce the Continual Learning EValuation Assessment Compass, which provides the visual means to both identify how approaches are practically reported and how they can simultaneously be contextualized in the broader literature landscape.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mundt|clevacompass_a_continual_learning_evaluation_assessment_compass_to_promote_research_transparency_and_comparability", "pdf": "/pdf/966f7548b61575e6823e6bf65299692e5dc4bc71.pdf", "supplementary_material": "/attachment/1f0fc05a83dd0837ce91e091e9707aa8a66f70d9.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nmundt2022clevacompass,\ntitle={{CLEVA}-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability},\nauthor={Martin Mundt and Steven Lang and Quentin Delfosse and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rHMaBYbkkRJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "aisKPsMM3fg", "original": "w63h-PWx0HO", "number": 2588, "cdate": 1632875631641, "mdate": null, "ddate": null, "tcdate": 1632875631641, "tmdate": 1676330550088, "tddate": null, "forum": "aisKPsMM3fg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Stochastic Dual Dynamic Programming", "authorids": ["~Hanjun_Dai1", "~Yuan_Xue5", "~Zia_Syed1", "~Dale_Schuurmans1", "~Bo_Dai1"], "authors": ["Hanjun Dai", "Yuan Xue", "Zia Syed", "Dale Schuurmans", "Bo Dai"], "keywords": ["data-driven algorithm design", "learning to optimize", "multi-stage stochastic optimization", "primal-dual dynamic programming"], "abstract": "Stochastic dual dynamic programming (SDDP) is a state-of-the-art method for solving multi-stage stochastic optimization, widely used for modeling real-world process optimization tasks. Unfortunately, SDDP has a worst-case complexity that scales exponentially in the number of decision variables, which severely limits applicability to only low dimensional problems. To overcome this limitation, we extend SDDP by introducing a trainable neural model that learns to map problem instances to a piece-wise linear value function within intrinsic low-dimension space, which is architected specifically to interact with a base SDDP solver, so that can accelerate optimization performance on new instances. The proposed Neural Stochastic Dual Dynamic Programming ($$\\nu$$-SDDP) continually self-improves by solving successive problems. An empirical investigation demonstrates that $$\\nu$$-SDDP can significantly reduce problem solving cost without sacrificing solution quality over competitors such as SDDP and reinforcement learning algorithms, across a range of synthetic and real-world process optimization problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dai|neural_stochastic_dual_dynamic_programming", "pdf": "/pdf/ae3ff3d1303130f7aae694a6d73bb6bef6e9970e.pdf", "one-sentence_summary": "We proposed neural-SDDP pushing the frontier of multi-stage stochastic optimization solver towards practical problem size.", "supplementary_material": "/attachment/6b962962b0cd1d1b2d322ce2469fe12b573e4587.zip", "_bibtex": "@inproceedings{\ndai2022neural,\ntitle={Neural Stochastic Dual Dynamic Programming},\nauthor={Hanjun Dai and Yuan Xue and Zia Syed and Dale Schuurmans and Bo Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aisKPsMM3fg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "BrPdX1bDZkQ", "original": "W2G0-04ocLI", "number": 2582, "cdate": 1632875631225, "mdate": null, "ddate": null, "tcdate": 1632875631225, "tmdate": 1676330550388, "tddate": null, "forum": "BrPdX1bDZkQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations", "authorids": ["~Geon-Hyeong_Kim2", "~Seokin_Seo1", "~Jongmin_Lee1", "~Wonseok_Jeon1", "~HyeongJoo_Hwang1", "~Hongseok_Yang2", "~Kee-Eung_Kim2"], "authors": ["Geon-Hyeong Kim", "Seokin Seo", "Jongmin Lee", "Wonseok Jeon", "HyeongJoo Hwang", "Hongseok Yang", "Kee-Eung Kim"], "keywords": ["imitation learning", "offline imitation learning", "imperfect demonstration", "non-expert demonstration"], "abstract": "We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is to deal with the narrow support of the data distribution exhibited by the expert demonstrations that cover only a small fraction of the state and the action spaces. As a result, offline IL algorithms that rely only on expert demonstrations are very unstable since the situation easily deviates from those in the expert demonstrations. In this paper, we assume additional demonstration data of unknown degrees of optimality, which we call imperfect demonstrations. Under this setting, we propose DemoDICE, which effectively utilizes imperfect demonstrations by matching the stationary distribution of a policy with experts' distribution while penalizing its deviation from the overall demonstrations. Compared with the recent IL algorithms that adopt adversarial minimax training objectives, we substantially stabilize overall learning process by reducing minimax optimization to a direct convex optimization in a principled manner. Using extensive tasks, we show that DemoDICE achieves promising results in the offline IL from expert and imperfect demonstrations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kim|demodice_offline_imitation_learning_with_supplementary_imperfect_demonstrations", "pdf": "/pdf/e5325598f049024b1d7f5b5d86157b8d521d2547.pdf", "data": "", "_bibtex": "@inproceedings{\nkim2022demodice,\ntitle={Demo{DICE}: Offline Imitation Learning with Supplementary Imperfect Demonstrations},\nauthor={Geon-Hyeong Kim and Seokin Seo and Jongmin Lee and Wonseok Jeon and HyeongJoo Hwang and Hongseok Yang and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BrPdX1bDZkQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "ZTsoE8G3GG", "original": "5gG1XbXzq9U", "number": 2571, "cdate": 1632875630480, "mdate": null, "ddate": null, "tcdate": 1632875630480, "tmdate": 1697934702655, "tddate": null, "forum": "ZTsoE8G3GG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Extend Molecular Scaffolds with Structural Motifs", "authorids": ["~Krzysztof_Maziarz1", "~Henry_Richard_Jackson-Flux1", "~Pashmina_Cameron1", "~Finton_Sirockin1", "~Nadine_Schneider1", "~Nikolaus_Stiefl1", "~Marwin_Segler2", "~Marc_Brockschmidt1"], "authors": ["Krzysztof Maziarz", "Henry Richard Jackson-Flux", "Pashmina Cameron", "Finton Sirockin", "Nadine Schneider", "Nikolaus Stiefl", "Marwin Segler", "Marc Brockschmidt"], "keywords": ["molecules", "graph neural networks", "scaffold", "generative model"], "abstract": "Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. Here, we propose MoLeR, a graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. Our experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. Furthermore, we show the influence of a number of seemingly minor design choices on the overall performance.", "one-sentence_summary": "We propose a new fragment-based generative model of molecules that can be constrained to include an arbitrary subgraph (scaffold).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "maziarz|learning_to_extend_molecular_scaffolds_with_structural_motifs", "pdf": "/pdf/329f388ad7404ff01bdbb88c90c981af357646e0.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2103.03864/code)", "_bibtex": "@inproceedings{\nmaziarz2022learning,\ntitle={Learning to Extend Molecular Scaffolds with Structural Motifs},\nauthor={Krzysztof Maziarz and Henry Richard Jackson-Flux and Pashmina Cameron and Finton Sirockin and Nadine Schneider and Nikolaus Stiefl and Marwin Segler and Marc Brockschmidt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZTsoE8G3GG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "p98WJxUC3Ca", "original": "JzVIhPsiERd", "number": 2561, "cdate": 1632875629857, "mdate": null, "ddate": null, "tcdate": 1632875629857, "tmdate": 1697934703800, "tddate": null, "forum": "p98WJxUC3Ca", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Discrepancy-Based Active Learning for Domain Adaptation", "authorids": ["~Antoine_de_Mathelin2", "~Fran\u00e7ois_Deheeger1", "~Mathilde_MOUGEOT1", "~Nicolas_Vayatis1"], "authors": ["Antoine de Mathelin", "Fran\u00e7ois Deheeger", "Mathilde MOUGEOT", "Nicolas Vayatis"], "keywords": ["active learning", "domain adaptation", "discrepancy", "kmedoids", "single batch", "covariate shift"], "abstract": "The goal of the paper is to design active learning strategies which lead to domain adaptation under an assumption of Lipschitz functions. Building on previous work by Mansour et al. (2009) we adapt the concept of discrepancy distance between source and target distributions to restrict the maximization over the hypothesis class to a localized class of functions which are performing accurate labeling on the source domain. We derive generalization error bounds for such active learning strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition. A practical K-medoids algorithm that can address the case of large data set is inferred from the theoretical bounds. Our numerical experiments show that the proposed algorithm is competitive against other state-of-the-art active learning techniques in the context of domain adaptation, in particular on large data sets of around one hundred thousand images.", "one-sentence_summary": "This paper presents an active learning for domain adaptation method based on a localized discrepancy between source and target distributions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mathelin|discrepancybased_active_learning_for_domain_adaptation", "pdf": "/pdf/c414fd73c946f1f7e7d5cd305b3daff711d9c75b.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2103.03757/code)", "_bibtex": "@inproceedings{\nmathelin2022discrepancybased,\ntitle={Discrepancy-Based Active Learning for Domain Adaptation},\nauthor={Antoine De mathelin and Fran{\\c{c}}ois Deheeger and Mathilde MOUGEOT and Nicolas Vayatis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=p98WJxUC3Ca}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "vDwBW49HmO", "original": "0lD-XhoAnZd", "number": 2560, "cdate": 1632875629792, "mdate": null, "ddate": null, "tcdate": 1632875629792, "tmdate": 1697934703814, "tddate": null, "forum": "vDwBW49HmO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Gradient Matching for Domain Generalization", "authorids": ["~Yuge_Shi1", "~Jeffrey_Seely1", "~Philip_Torr1", "~Siddharth_N1", "~Awni_Hannun1", "~Nicolas_Usunier1", "~Gabriel_Synnaeve1"], "authors": ["Yuge Shi", "Jeffrey Seely", "Philip Torr", "Siddharth N", "Awni Hannun", "Nicolas Usunier", "Gabriel Synnaeve"], "keywords": ["Domain generalization", "multi-source domain adaptation"], "abstract": "Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an _inter-domain gradient matching_ objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive --- it requires computation of second-order derivatives \u2013-- we derive a simpler first-order algorithm named Fish that approximates its optimization. We perform experiments on the Wilds benchmark, which captures distribution shift in the real world, as well as the DomainBed benchmark that focuses more on synthetic-to-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks.", "one-sentence_summary": "We propose to learn features that are invariant across domains by maximizing the gradient inner product between domains.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shi|gradient_matching_for_domain_generalization", "pdf": "/pdf/8a8aa9b1acdc5b55622687f272cb96ad87fa97b8.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2104.09937/code)", "_bibtex": "@inproceedings{\nshi2022gradient,\ntitle={Gradient Matching for Domain Generalization},\nauthor={Yuge Shi and Jeffrey Seely and Philip Torr and Siddharth N and Awni Hannun and Nicolas Usunier and Gabriel Synnaeve},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vDwBW49HmO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "d5SCUJ5t1k", "original": "pQrGTgNQuVE", "number": 2530, "cdate": 1632875627849, "mdate": null, "ddate": null, "tcdate": 1632875627849, "tmdate": 1697934706955, "tddate": null, "forum": "d5SCUJ5t1k", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Objects in Semantic Topology", "authorids": ["~Shuo_Yang5", "~Peize_Sun1", "~Yi_Jiang2", "~Xiaobo_Xia1", "~Ruiheng_Zhang1", "~Zehuan_Yuan1", "~Changhu_Wang3", "~Ping_Luo2", "~Min_Xu5"], "authors": ["Shuo Yang", "Peize Sun", "Yi Jiang", "Xiaobo Xia", "Ruiheng Zhang", "Zehuan Yuan", "Changhu Wang", "Ping Luo", "Min Xu"], "keywords": [], "abstract": "A more realistic object detection paradigm, Open-World Object Detection, has arised increasing research interests in the community recently. A qualified open-world object detector can not only identify objects of known categories, but also discover unknown objects, and incrementally learn to categorize them when their annotations progressively arrive. Previous works rely on independent modules to recognize unknown categories and perform incremental learning, respectively. In this paper, we provide a unified perspective: Semantic Topology. During the life-long learning of an open-world object detector, all object instances from the same category are assigned to their corresponding pre-defined node in the semantic topology, including the `unknown' category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that semantic topology, either randomly-generated or derived from a well-trained language model, could outperform the current state-of-the-art open-world object detectors by a large margin, e.g., the absolute open-set error (the number of unknown instances that are wrongly labeled as known) is reduced from 7832 to 2546, exhibiting the inherent superiority of semantic topology on open-world object detection.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|objects_in_semantic_topology", "pdf": "/pdf/ff70332fa4b027995f092ed696137154488aa5fc.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02687/code)", "_bibtex": "@inproceedings{\nyang2022objects,\ntitle={Objects in Semantic Topology},\nauthor={Shuo Yang and Peize Sun and Yi Jiang and Xiaobo Xia and Ruiheng Zhang and Zehuan Yuan and Changhu Wang and Ping Luo and Min Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=d5SCUJ5t1k}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "ds8yZOUsea", "original": "zoL3oV6MStY", "number": 2523, "cdate": 1632875627427, "mdate": null, "ddate": null, "tcdate": 1632875627427, "tmdate": 1676330553493, "tddate": null, "forum": "ds8yZOUsea", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios", "authorids": ["~Vaisakh_Shaj1", "~Dieter_B\u00fcchler1", "rohitsonker96@gmail.com", "~Philipp_Becker1", "~Gerhard_Neumann2"], "authors": ["Vaisakh Shaj", "Dieter B\u00fcchler", "Rohit Sonker", "Philipp Becker", "Gerhard Neumann"], "keywords": ["State Space Models", "Changing Dynamics", "Recurrent Neural Networks", "Multi Task Learning"], "abstract": "Recurrent State-space models (RSSMs) are highly expressive models for learning patterns in time series data and for system identification. However, these models are often based on the assumption that the dynamics are fixed and unchanging, which is rarely the case in real-world scenarios. Many control applications often exhibit tasks with similar, but not identical dynamics, that can be modelled as having a common latent structure. We introduce the Hidden Parameter Recurrent State Space Models (HiP-RSSMs), a framework that parametrizes a family of related state-space models with a low-dimensional set of latent factors. We present a simple and effective way of performing learning and inference over this Gaussian graphical model that avoids approximations like variational inference. We show that HiP-RSSMs outperforms RSSMs and competing multi-task models on several challenging robotic benchmarks both on real systems and simulations.", "one-sentence_summary": "A new formalism for extending Recurrent State Space Models (RSSMs) to changing dynamics scenarios.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shaj|hidden_parameter_recurrent_state_space_models_for_changing_dynamics_scenarios", "pdf": "/pdf/677a627df12a4c559d9876846d7c116f34b2f4cd.pdf", "_bibtex": "@inproceedings{\nshaj2022hidden,\ntitle={Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios},\nauthor={Vaisakh Shaj and Dieter B{\\\"u}chler and Rohit Sonker and Philipp Becker and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ds8yZOUsea}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "ar92oEosBIg", "original": "sVX_OTik1r6", "number": 2522, "cdate": 1632875627360, "mdate": null, "ddate": null, "tcdate": 1632875627360, "tmdate": 1676330553598, "tddate": null, "forum": "ar92oEosBIg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph Neural Network Guided Local Search for the Traveling Salesperson Problem", "authorids": ["~Benjamin_Hudson1", "~Qingbiao_Li1", "~Matthew_Malencia1", "~Amanda_Prorok1"], "authors": ["Benjamin Hudson", "Qingbiao Li", "Matthew Malencia", "Amanda Prorok"], "keywords": ["Traveling Salesman Problem", "Graph Neural Network", "Metaheuristic", "Guided Local Search", "Hybrid"], "abstract": "Solutions to the Traveling Salesperson Problem (TSP) have practical applications to processes in transportation, logistics, and automation, yet must be computed with minimal delay to satisfy the real-time nature of the underlying tasks. However, solving large TSP instances quickly without sacrificing solution quality remains challenging for current approximate algorithms. To close this gap, we present a hybrid data-driven approach for solving the TSP based on Graph Neural Networks (GNNs) and Guided Local Search (GLS). Our model predicts the regret of including each edge of the problem graph in the solution; GLS uses these predictions in conjunction with the original problem graph to find solutions. Our experiments demonstrate that this approach converges to optimal solutions at a faster rate than three recent learning based approaches for the TSP. Notably, we reduce the mean optimality gap on the 100-node problem set from 1.534% to 0.705%, a 2x improvement. When generalizing from 20-node instances to the 100-node problem set, we reduce the optimality gap from 18.845% to 2.622%, a 7x improvement.", "one-sentence_summary": "We present a hybrid data-driven approach for solving the TSP based on Graph Neural Networks (GNNs) and Guided Local Search (GLS), which outperforms state-of-the-art learning-based approaches and non-learning GLS algorithms.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hudson|graph_neural_network_guided_local_search_for_the_traveling_salesperson_problem", "pdf": "/pdf/353e2dea7badc5c7bf552499ab129def8f532705.pdf", "_bibtex": "@inproceedings{\nhudson2022graph,\ntitle={Graph Neural Network Guided Local Search for the Traveling Salesperson Problem},\nauthor={Benjamin Hudson and Qingbiao Li and Matthew Malencia and Amanda Prorok},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ar92oEosBIg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "aPOpXlnV1T", "original": "N31lNY7rvF3", "number": 2513, "cdate": 1632875626733, "mdate": null, "ddate": null, "tcdate": 1632875626733, "tmdate": 1697934708607, "tddate": null, "forum": "aPOpXlnV1T", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks", "authorids": ["~Maximilian_Seitzer1", "~Arash_Tavakoli1", "~Dimitrije_Antic1", "~Georg_Martius1"], "authors": ["Maximilian Seitzer", "Arash Tavakoli", "Dimitrije Antic", "Georg Martius"], "keywords": ["Uncertainty Estimation", "Probabilistic Neural Networks", "Aleatoric Uncertainty", "Heteroscedastic Uncertainty", "Analysis"], "abstract": "Capturing aleatoric uncertainty is a critical part of many machine learning systems. In deep learning, a common approach to this end is to train a neural network to estimate the parameters of a heteroscedastic Gaussian distribution by maximizing the logarithm of the likelihood function under the observed data. In this work, we examine this approach and identify potential hazards associated with the use of log-likelihood in conjunction with gradient-based optimizers. First, we present a synthetic example illustrating how this approach can lead to very poor but stable parameter estimates. Second, we identify the culprit to be the log-likelihood loss, along with certain conditions that exacerbate the issue. Third, we present an alternative formulation, termed $\\beta$-NLL, in which each data point's contribution to the loss is weighted by the $\\beta$-exponentiated variance estimate. We show that using an appropriate $\\beta$ largely mitigates the issue in our illustrative example. Fourth, we evaluate this approach on a range of domains and tasks and show that it achieves considerable improvements and performs more robustly concerning hyperparameters, both in predictive RMSE and log-likelihood criteria.", "one-sentence_summary": "We analyse problems with the training objective of probabilistic neural networks and propose a fix in the form of a new loss function.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "seitzer|on_the_pitfalls_of_heteroscedastic_uncertainty_estimation_with_probabilistic_neural_networks", "pdf": "/pdf/542fc7335389cbc9933fb0ec11722efc30b958e8.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.09168/code)", "_bibtex": "@inproceedings{\nseitzer2022on,\ntitle={On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks},\nauthor={Maximilian Seitzer and Arash Tavakoli and Dimitrije Antic and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aPOpXlnV1T}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "SlxSY2UZQT", "original": "U7rH2ToknLF", "number": 2504, "cdate": 1632875626167, "mdate": null, "ddate": null, "tcdate": 1632875626167, "tmdate": 1697934709575, "tddate": null, "forum": "SlxSY2UZQT", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Label-Efficient Semantic Segmentation with Diffusion Models", "authorids": ["~Dmitry_Baranchuk2", "~Andrey_Voynov1", "~Ivan_Rubachev1", "~Valentin_Khrulkov1", "~Artem_Babenko1"], "authors": ["Dmitry Baranchuk", "Andrey Voynov", "Ivan Rubachev", "Valentin Khrulkov", "Artem Babenko"], "keywords": [], "abstract": "Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "baranchuk|labelefficient_semantic_segmentation_with_diffusion_models", "pdf": "/pdf/7f702e218df7a81da790cff07136c4f77297f473.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2112.03126/code)", "_bibtex": "@inproceedings{\nbaranchuk2022labelefficient,\ntitle={Label-Efficient Semantic Segmentation with Diffusion Models},\nauthor={Dmitry Baranchuk and Andrey Voynov and Ivan Rubachev and Valentin Khrulkov and Artem Babenko},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SlxSY2UZQT}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "uPv9Y3gmAI5", "original": "5KzlSFB244t", "number": 2488, "cdate": 1632875625262, "mdate": null, "ddate": null, "tcdate": 1632875625262, "tmdate": 1697934710826, "tddate": null, "forum": "uPv9Y3gmAI5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Language model compression with weighted low-rank factorization", "authorids": ["~Yen-Chang_Hsu1", "~Ting_Hua1", "chang.sun@northeastern.edu", "~Qian_Lou1", "~Yilin_Shen1", "~Hongxia_Jin1"], "authors": ["Yen-Chang Hsu", "Ting Hua", "Sungen Chang", "Qian Lou", "Yilin Shen", "Hongxia Jin"], "keywords": ["model compression", "low-rank approximation", "transformer", "language model"], "abstract": "Factorizing a large matrix into small matrices is a popular strategy for model compression. Singular value decomposition (SVD) plays a vital role in this compression strategy, approximating a learned matrix with fewer parameters. However, SVD minimizes the squared error toward reconstructing the original matrix without gauging the importance of the parameters, potentially giving a larger reconstruction error for those who affect the task accuracy more. In other words, the optimization objective of SVD is not aligned with the trained model's task accuracy. We analyze this previously unexplored problem, make observations, and address it by introducing Fisher information to weigh the importance of parameters affecting the model prediction. This idea leads to our method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance. We perform analysis with the transformer-based language models, showing our weighted SVD largely alleviates the mismatched optimization objectives and can maintain model performance with a higher compression rate. Our method can directly compress a task-specific model while achieving better performance than other compact model strategies requiring expensive model pre-training. Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9% to 30% parameters with an insignificant impact on task accuracy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hsu|language_model_compression_with_weighted_lowrank_factorization", "pdf": "/pdf/a5edead703a518eda031d7e25734d372b8287883.pdf", "one-sentence_summary": "Fisher-weighted SVD for language model compression", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2207.00112/code)", "_bibtex": "@inproceedings{\nhsu2022language,\ntitle={Language model compression with weighted low-rank factorization},\nauthor={Yen-Chang Hsu and Ting Hua and Sungen Chang and Qian Lou and Yilin Shen and Hongxia Jin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=uPv9Y3gmAI5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "QuObT9BTWo", "original": "B0pJBi9PERD", "number": 2479, "cdate": 1632875624632, "mdate": null, "ddate": null, "tcdate": 1632875624632, "tmdate": 1676330555587, "tddate": null, "forum": "QuObT9BTWo", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization", "authorids": ["~Xi_Lin2", "~Zhiyuan_Yang2", "~Qingfu_Zhang1"], "authors": ["Xi Lin", "Zhiyuan Yang", "Qingfu Zhang"], "keywords": ["Multiobjective Combinatorial Optimization", "Combinatorial Optimization", "Neural Combinatorial Optimization", "Multiobjective Optimization"], "abstract": "Multiobjective combinatorial optimization (MOCO) problems can be found in many real-world applications. However, exactly solving these problems would be very challenging, particularly when they are NP-hard. Many handcrafted heuristic methods have been proposed to tackle different MOCO problems over the past decades. In this work, we generalize the idea of neural combinatorial optimization, and develop a learning-based approach to approximate the whole Pareto set for a given MOCO problem without further search procedure. We propose a single preference-conditioned model to directly generate approximate Pareto solutions for any trade-off preference, and design an efficient multiobjective reinforcement learning algorithm to train this model. Our proposed method can be treated as a learning-based extension for the widely-used decomposition-based multiobjective evolutionary algorithm (MOEA/D). It uses a single model to accommodate all the possible preferences, whereas other methods use a finite number of solutions to approximate the Pareto set. Experimental results show that our proposed method significantly outperforms some other methods on the multiobjective traveling salesman problem, multiobjective vehicle routing problem, and multiobjective knapsack problem in terms of solution quality, speed, and model efficiency.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lin|pareto_set_learning_for_neural_multiobjective_combinatorial_optimization", "pdf": "/pdf/975b8b82804eaa05309f02856d161ef85810c9ca.pdf", "one-sentence_summary": "We propose a learning-based method to approximate the whole Pareto set for multi-objective combinatorial optimization problems with a single model.", "_bibtex": "@inproceedings{\nlin2022pareto,\ntitle={Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization},\nauthor={Xi Lin and Zhiyuan Yang and Qingfu Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=QuObT9BTWo}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "8la28hZOwug", "original": "Oa6-SHH_ZjO", "number": 2478, "cdate": 1632875624561, "mdate": null, "ddate": null, "tcdate": 1632875624561, "tmdate": 1676330555640, "tddate": null, "forum": "8la28hZOwug", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Prototypical Contrastive Predictive Coding", "authorids": ["~Kyungmin_Lee1"], "authors": ["Kyungmin Lee"], "keywords": ["Knowledge distillation", "contrastive learning", "self-supervised learning"], "abstract": "Transferring representational knowledge of a model to another is a wide-ranging topic in machine learning. Those applications include the distillation of a large supervised or self-supervised teacher model to a smaller student model or self-supervised learning via self-distillation. Knowledge distillation is an original method to solve these problems, which minimizes a cross-entropy loss between the prototypical probabilistic outputs of teacher and student networks. On the other hand, contrastive learning has shown its competency in transferring representations as they allow students to capture the information of teacher representations. In this paper, we amalgamate the advantages of knowledge distillation and contrastive learning by modeling the critic of a contrastive objective by the prototypical probabilistic discrepancy between two features. We refer to it as prototypical contrastive predictive coding and present efficient implementation using the proposed objective for three distillation tasks: supervised model compression, self-supervised model compression, and self-supervised learning via self-distillation. Through extensive experiments, we validate the effectiveness of our method and show that our method achieves state-of-the-art performance in supervised / self-supervised model compression. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|prototypical_contrastive_predictive_coding", "pdf": "/pdf/9d170cd1c9aae6c853bd762d8238dfed410f721c.pdf", "one-sentence_summary": "We propose prototypical contrastive predictive coding for efficient distillation of representational knowledge of one network into other network.", "data": "", "_bibtex": "@inproceedings{\nlee2022prototypical,\ntitle={Prototypical Contrastive Predictive Coding},\nauthor={Kyungmin Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8la28hZOwug}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "cZAi1yWpiXQ", "original": "3O6q_0Qf7s", "number": 2476, "cdate": 1632875624417, "mdate": null, "ddate": null, "tcdate": 1632875624417, "tmdate": 1697934711996, "tddate": null, "forum": "cZAi1yWpiXQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Adversarial Robustness Through the Lens of Causality", "authorids": ["~Yonggang_Zhang1", "~Mingming_Gong1", "~Tongliang_Liu1", "~Gang_Niu1", "~Xinmei_Tian1", "~Bo_Han1", "~Bernhard_Sch\u00f6lkopf1", "~Kun_Zhang1"], "authors": ["Yonggang Zhang", "Mingming Gong", "Tongliang Liu", "Gang Niu", "Xinmei Tian", "Bo Han", "Bernhard Sch\u00f6lkopf", "Kun Zhang"], "keywords": ["Adversarial examples", "Causality"], "abstract": "The adversarial vulnerability of deep neural networks has attracted signi\ufb01cant attention in machine learning. As causal reasoning has an instinct for modeling distribution change, it is essential to incorporate causality into analyzing this specific type of distribution change induced by adversarial attacks. However, causal formulations of the intuition of adversarial attacks and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of adversarial attacks. From the causal perspective, we study the distinction between the natural and adversarial distribution and conclude that the origin of adversarial vulnerability is the focus of models on spurious correlations. Inspired by the causal understanding, we propose the \\emph{Causal}-inspired \\emph{Adv}ersarial distribution alignment method, CausalAdv, to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method. Our work is the first attempt towards using causality to understand and mitigate the adversarial vulnerability.", "one-sentence_summary": "The first attempt towards using causality to understand and mitigate adversarial vulnerability.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|adversarial_robustness_through_the_lens_of_causality", "pdf": "/pdf/409af234b081e8d93ddd0a2b3e2d79d3f3a24b19.pdf", "supplementary_material": "/attachment/e3a862284b6d989e49146e3a75afc2615170ea64.zip", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.06196/code)", "_bibtex": "@inproceedings{\nzhang2022adversarial,\ntitle={Adversarial Robustness Through the Lens of Causality},\nauthor={Yonggang Zhang and Mingming Gong and Tongliang Liu and Gang Niu and Xinmei Tian and Bo Han and Bernhard Sch{\\\"o}lkopf and Kun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cZAi1yWpiXQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "9NVd-DMtThY", "original": "pJCEB6goUsZ", "number": 2460, "cdate": 1632875623429, "mdate": null, "ddate": null, "tcdate": 1632875623429, "tmdate": 1697934712747, "tddate": null, "forum": "9NVd-DMtThY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Distributionally Robust Fair Principal Components via Geodesic Descents", "authorids": ["~Hieu_Vu1", "~Toan_Tran1", "~Man-Chung_Yue1", "~Viet_Anh_Nguyen2"], "authors": ["Hieu Vu", "Toan Tran", "Man-Chung Yue", "Viet Anh Nguyen"], "keywords": ["fair principal component analysis", "distributionally robust optimization", "manifold optimization"], "abstract": "Principal component analysis is a simple yet useful dimensionality reduction technique in modern machine learning pipelines. In consequential domains such as college admission, healthcare and credit approval, it is imperative to take into account emerging criteria such as the fairness and the robustness of the learned projection. In this paper, we propose a distributionally robust optimization problem for principal component analysis which internalizes a fairness criterion in the objective function. The learned projection thus balances the trade-off between the total reconstruction error and the reconstruction error gap between subgroups, taken in the min-max sense over all distributions in a moment-based ambiguity set. The resulting optimization problem over the Stiefel manifold can be efficiently solved by a Riemannian subgradient descent algorithm with a sub-linear convergence rate. Our experimental results on real-world datasets show the merits of our proposed method over state-of-the-art baselines. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "vu|distributionally_robust_fair_principal_components_via_geodesic_descents", "pdf": "/pdf/a8fe9d7b929beb8c14e69b5eaf7902ec099f3aa4.pdf", "supplementary_material": "/attachment/5acab76e829bf9742c11d96850e8882322444cbe.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.03071/code)", "_bibtex": "@inproceedings{\nvu2022distributionally,\ntitle={Distributionally Robust Fair Principal Components via Geodesic Descents},\nauthor={Hieu Vu and Toan Tran and Man-Chung Yue and Viet Anh Nguyen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9NVd-DMtThY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "wkMG8cdvh7-", "original": "dQRYLYDCWYq", "number": 2436, "cdate": 1632875621726, "mdate": null, "ddate": null, "tcdate": 1632875621726, "tmdate": 1697934714957, "tddate": null, "forum": "wkMG8cdvh7-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding and Improving Graph Injection Attack by Promoting Unnoticeability", "authorids": ["~Yongqiang_Chen1", "~Han_Yang1", "~Yonggang_Zhang1", "~MA_KAILI1", "~Tongliang_Liu1", "~Bo_Han1", "~James_Cheng2"], "authors": ["Yongqiang Chen", "Han Yang", "Yonggang Zhang", "MA KAILI", "Tongliang Liu", "Bo Han", "James Cheng"], "keywords": ["Graph Neural Networks", "Adversarial Attacks", "Node Classification"], "abstract": "Recently Graph Injection Attack (GIA) emerges as a practical attack scenario on Graph Neural Networks (GNNs), where the adversary can merely inject few malicious nodes instead of modifying existing nodes or edges, i.e., Graph Modification Attack (GMA). Although GIA has achieved promising results, little is known about why it is successful and whether there is any pitfall behind the success. To understand the power of GIA, we compare it with GMA and find that GIA can be provably more harmful than GMA due to its relatively high flexibility. However, the high flexibility will also lead to great damage to the homophily distribution of the original graph, i.e., similarity among neighbors. Consequently, the threats of GIA can be easily alleviated or even prevented by homophily-based defenses designed to recover the original homophily. To mitigate the issue, we introduce a novel constraint \u2013 homophily unnoticeability that enforces GIA to preserve the homophily, and propose Harmonious Adversarial Objective (HAO) to instantiate it. Extensive experiments verify that GIA with HAO can break homophily-based defenses and outperform previous GIA attacks by a significant margin. We believe our methods can serve for a more reliable evaluation of the robustness of GNNs.", "one-sentence_summary": "We find GIA can be provably more harmful than GMA while at the price of bringing more damage to the homophily distribution, which makes it can easily defendable, hence we propose a novel adversarial objective to mitigate the issue. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|understanding_and_improving_graph_injection_attack_by_promoting_unnoticeability", "pdf": "/pdf/fe3533162beb8ac7e98d14852e9e6ec3ba4f5fd7.pdf", "supplementary_material": "/attachment/3b3e4c1335f5458952b6fc4f996c9af64ff1cf0e.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.08057/code)", "_bibtex": "@inproceedings{\nchen2022understanding,\ntitle={Understanding and Improving Graph Injection Attack by Promoting Unnoticeability},\nauthor={Yongqiang Chen and Han Yang and Yonggang Zhang and MA KAILI and Tongliang Liu and Bo Han and James Cheng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wkMG8cdvh7-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "swiyAeGzFhQ", "original": "8Z_azOAehR", "number": 2424, "cdate": 1632875620934, "mdate": null, "ddate": null, "tcdate": 1632875620934, "tmdate": 1697934716270, "tddate": null, "forum": "swiyAeGzFhQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Guide and to be Guided in the Architect-Builder Problem", "authorids": ["~Paul_Barde1", "~Tristan_Karch1", "~Derek_Nowrouzezahrai1", "~Cl\u00e9ment_Moulin-Frier2", "~Christopher_Pal1", "~Pierre-Yves_Oudeyer1"], "authors": ["Paul Barde", "Tristan Karch", "Derek Nowrouzezahrai", "Cl\u00e9ment Moulin-Frier", "Christopher Pal", "Pierre-Yves Oudeyer"], "keywords": ["Social Learning", "Interactive Learning", "Teacher-Student Learning", "Computational Experimental Semiotics", "Socially Supervised Learning"], "abstract": "We are interested in interactive agents that learn to coordinate, namely, a $builder$ -- which performs actions but ignores the goal of the task, i.e. has no access to rewards -- and an $architect$ which guides the builder towards the goal of the task. \nWe define and explore a formal setting where artificial agents are equipped with mechanisms that allow them to simultaneously learn a task while at the same time evolving a shared communication protocol.  \nIdeally, such learning should only rely on high-level communication priors and be able to handle a large variety of tasks and meanings while deriving communication protocols that can be reused across tasks.\nThe field of Experimental Semiotics has shown the extent of human proficiency at learning from a priori unknown instructions meanings. Therefore, we take inspiration from it and present the Architect-Builder Problem (ABP): an asymmetrical setting in which an architect must learn to guide a builder towards constructing a specific structure. The architect knows the target structure but cannot act in the environment and can only send arbitrary messages to the builder. The builder on the other hand can act in the environment, but receives no rewards nor has any knowledge about the task, and must learn to solve it relying only on the messages sent by the architect. Crucially, the meaning of messages is initially not defined nor shared between the agents but must be negotiated throughout learning.\nUnder these constraints, we propose Architect-Builder Iterated Guiding (ABIG), a solution to the Architect-Builder Problem where the architect leverages a learned model of the builder to guide it while the builder uses self-imitation learning to reinforce its guided behavior. To palliate to the non-stationarity induced by the two agents concurrently learning, ABIG structures the sequence of interactions between the agents into interaction frames. We analyze the key learning mechanisms of ABIG and test it in a 2-dimensional instantiation of the ABP where tasks involve grasping cubes, placing them at a given location, or building various shapes. In this environment, ABIG results in a low-level, high-frequency, guiding communication protocol that not only enables an architect-builder pair to solve the task at hand, but that can also generalize to unseen tasks. ", "one-sentence_summary": "We formulate -- and propose a solution to -- the Architect-Builder Problem, a new asymmetrical Interactive Learning setting where one agent must guide the other agent towards the goal at hand. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "barde|learning_to_guide_and_to_be_guided_in_the_architectbuilder_problem", "pdf": "/pdf/87250d43c6be74fc8c9ea00693ffaa2364df1b2f.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.07342/code)", "_bibtex": "@inproceedings{\nbarde2022learning,\ntitle={Learning to Guide and to be Guided in the Architect-Builder Problem},\nauthor={Paul Barde and Tristan Karch and Derek Nowrouzezahrai and Cl{\\'e}ment Moulin-Frier and Christopher Pal and Pierre-Yves Oudeyer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=swiyAeGzFhQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "iPHLcmtietq", "original": "M-XgKVp1xba", "number": 2421, "cdate": 1632875620714, "mdate": null, "ddate": null, "tcdate": 1632875620714, "tmdate": 1697934716769, "tddate": null, "forum": "iPHLcmtietq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Phase Collapse in Neural Networks", "authorids": ["~Florentin_Guth1", "~John_Zarka1", "~St\u00e9phane_Mallat1"], "authors": ["Florentin Guth", "John Zarka", "St\u00e9phane Mallat"], "keywords": ["phase collapse", "neural collapse", "concentration", "classification", "imagenet", "deep networks", "complex networks", "sparsity in deep networks"], "abstract": "Deep convolutional classifiers linearly separate image classes and improve accuracy as depth increases. They progressively reduce the spatial dimension whereas the number of channels grows with depth. Spatial variability is therefore transformed into variability along channels. A fundamental challenge is to understand the role of non-linearities together with convolutional filters in this transformation. ReLUs with biases are often interpreted as thresholding operators that improve discrimination through sparsity. This paper demonstrates that it is a different mechanism called \\emph{phase collapse} which eliminates spatial variability while linearly separating classes. We show that collapsing the phases of complex wavelet coefficients is sufficient to reach the classification accuracy of ResNets of similar depths. However, replacing the phase collapses with thresholding operators that enforce sparsity considerably degrades the performance. We explain these numerical results by showing that the iteration of phase collapses progressively improves separation of classes, as opposed to thresholding non-linearities.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guth|phase_collapse_in_neural_networks", "pdf": "/pdf/e7edb0ba8bb5814255b8fcf9d0c3100a71b6718d.pdf", "one-sentence_summary": "The classification accuracy of CNNs mostly relies on the mechanism of phase collapses to eliminate spatial variability and linearly separate class means.", "supplementary_material": "/attachment/1b7e418c72c48ae40cb2086fe8c0d683b5a117e2.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.05283/code)", "_bibtex": "@inproceedings{\nguth2022phase,\ntitle={Phase Collapse in Neural Networks},\nauthor={Florentin Guth and John Zarka and St{\\'e}phane Mallat},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iPHLcmtietq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "TBpg4PnXhYH", "original": "Z8WiJfK-LFi", "number": 2417, "cdate": 1632875620432, "mdate": null, "ddate": null, "tcdate": 1632875620432, "tmdate": 1676330558415, "tddate": null, "forum": "TBpg4PnXhYH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training", "authorids": ["~Wenyong_Huang1", "zhangzhenhe1@huawei.com", "yeung.yu.ting@huawei.com", "~Xin_Jiang1", "~Qun_Liu1"], "authors": ["Wenyong Huang", "Zhenhe Zhang", "Yu Ting Yeung", "Xin Jiang", "Qun Liu"], "keywords": ["Speech Representation Learning", "Speech Pre-training", "Speech Recognition", "Self-supervised Representation Learning"], "abstract": "We introduce a new approach for speech pre-training named SPIRAL which works by learning denoising representation of perturbed data in a teacher-student framework. \nSpecifically, given a speech utterance, we first feed the utterance to a teacher network to obtain corresponding representation. Then the same utterance is perturbed and fed to a student network. The student network is trained to output representation resembling that of the teacher. At the same time, the teacher network is updated as moving average of student's weights over training steps. In order to prevent representation collapse, we apply an in-utterance contrastive loss as pre-training objective and impose position randomization on the input to the teacher. SPIRAL achieves competitive or better results compared to state-of-the-art speech pre-training method wav2vec 2.0, with significant reduction of training cost (80% for BASE model, 65% for LARGE model). \nFurthermore, we address the problem of noise-robustness that is critical to real-world speech applications. We propose multi-condition pre-training by perturbing the student's input with various types of additive noise. We demonstrate that multi-condition pre-trained SPIRAL models are more robust to noisy speech (9.0% - 13.3% relative word error rate reduction on real noisy test data), compared to applying multi-condition training solely in the fine-tuning stage. Source code is available at https://github.com/huawei-noah/Speech-Backbones/tree/main/SPIRAL.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|spiral_selfsupervised_perturbationinvariant_representation_learning_for_speech_pretraining", "pdf": "/pdf/237e7f0b9a5b83acde4d28436da1c2d60b89393c.pdf", "_bibtex": "@inproceedings{\nhuang2022spiral,\ntitle={{SPIRAL}: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training},\nauthor={Wenyong Huang and Zhenhe Zhang and Yu Ting Yeung and Xin Jiang and Qun Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TBpg4PnXhYH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "J_PHjw4gvXJ", "original": "j0ZKc6bTQp", "number": 2415, "cdate": 1632875620286, "mdate": null, "ddate": null, "tcdate": 1632875620286, "tmdate": 1676330558463, "tddate": null, "forum": "J_PHjw4gvXJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Improving the Accuracy of Learning Example Weights for Imbalance Classification", "authorids": ["~Yuqi_Liu1", "bincao@zjut.edu.cn", "fanjing@zjut.edu.cn"], "authors": ["Yuqi Liu", "Bin Cao", "Jing Fan"], "keywords": ["Imbalance classification", "Meta learning", "Data weighting."], "abstract": "To solve the imbalance classification, methods of weighting examples have been proposed. Recent work has studied to assign adaptive weights to training examples through learning mechanisms, that is, the weights, similar to classification models, are regarded as parameters that need to be learned. However, the algorithms in recent work use local information to approximately optimize the weights, which may lead to inaccurate learning of the weights. In this work, we first propose a novel mechanism of learning with a constraint, which can accurately train the weights and model. Then, we propose a combined method of our learning mechanism and the work by Hu et al., which can promote each other to perform better. Our proposed method can be applied to any type of deep network model. Experiments show that compared with the state-of-the-art algorithms, our method has significant improvement in varieties of settings, including text and image classification over different imbalance ratios, binary and multi-class classification.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|improving_the_accuracy_of_learning_example_weights_for_imbalance_classification", "pdf": "/pdf/43e63bb40f4809118c1924577a2cac09588d2c23.pdf", "data": "", "_bibtex": "@inproceedings{\nliu2022improving,\ntitle={Improving the Accuracy of Learning Example Weights for Imbalance Classification},\nauthor={Yuqi Liu and Bin Cao and Jing Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=J_PHjw4gvXJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "Czsdv-S4-w9", "original": "PGnnWT4H0_w", "number": 2409, "cdate": 1632875619852, "mdate": null, "ddate": null, "tcdate": 1632875619852, "tmdate": 1697934718100, "tddate": null, "forum": "Czsdv-S4-w9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks", "authorids": ["~Sihyun_Yu2", "~Jihoon_Tack1", "~Sangwoo_Mo1", "~Hyunsu_Kim1", "~Junho_Kim3", "~Jung-Woo_Ha1", "~Jinwoo_Shin1"], "authors": ["Sihyun Yu", "Jihoon Tack", "Sangwoo Mo", "Hyunsu Kim", "Junho Kim", "Jung-Woo Ha", "Jinwoo Shin"], "keywords": ["video generation", "implicit neural representations", "generative adversarial networks"], "abstract": "In the deep learning era, long video generation of high-quality still remains challenging due to the spatio-temporal complexity and continuity of videos. Existing prior works have attempted to model video distribution by representing videos as 3D grids of RGB values, which impedes the scale of generated videos and neglects continuous dynamics. In this paper, we found that the recent emerging paradigm of implicit neural representations (INRs) that encodes a continuous signal into a parameterized neural network effectively mitigates the issue. By utilizing INRs of video, we propose dynamics-aware implicit generative adversarial network (DIGAN), a novel generative adversarial network for video generation. Specifically, we introduce (a) an INR-based video generator that improves the motion dynamics by manipulating the space and time coordinates differently and (b) a motion discriminator that efficiently identifies the unnatural motions without observing the entire long frame sequences. We demonstrate the superiority of DIGAN under various datasets, along with multiple intriguing properties, e.g., long video synthesis, video extrapolation, and non-autoregressive video generation. For example, DIGAN improves the previous state-of-the-art FVD score on UCF-101 by 30.7% and can be trained on 128 frame videos of 128x128 resolution, 80 frames longer than the 48 frames of the previous state-of-the-art method.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|generating_videos_with_dynamicsaware_implicit_generative_adversarial_networks", "pdf": "/pdf/e4d5da34d0754b0239cec0a03c6473b915bfd9a8.pdf", "one-sentence_summary": "We make video generation scalable leveraging implicit neural representations.", "supplementary_material": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.10571/code)", "_bibtex": "@inproceedings{\nyu2022generating,\ntitle={Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks},\nauthor={Sihyun Yu and Jihoon Tack and Sangwoo Mo and Hyunsu Kim and Junho Kim and Jung-Woo Ha and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Czsdv-S4-w9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "0cgU-BZp2ky", "original": "r_XmD4XTPAB", "number": 2406, "cdate": 1632875619639, "mdate": null, "ddate": null, "tcdate": 1632875619639, "tmdate": 1697934718104, "tddate": null, "forum": "0cgU-BZp2ky", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization", "authorids": ["~Quanyi_Li1", "~Zhenghao_Peng1", "~Bolei_Zhou5"], "authors": ["Quanyi Li", "Zhenghao Peng", "Bolei Zhou"], "keywords": ["Human in the Loop", "Safe Reinforcement Learning", "Autonomous Driving"], "abstract": "Human intervention is an effective way to inject human knowledge into the training loop of reinforcement learning, which can bring fast learning and ensured training safety. Given the very limited budget of human intervention, it remains challenging to design when and how human expert interacts with the learning agent in the training. In this work, we develop a novel human-in-the-loop learning method called Human-AI Copilot Optimization (HACO).To allow the agent's sufficient exploration in the risky environments while ensuring the training safety, the human expert can take over the control and demonstrate how to avoid probably dangerous situations or trivial behaviors. The proposed HACO then effectively utilizes the data both from the trial-and-error exploration and human's partial demonstration to train a high-performing agent. HACO extracts proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values meanwhile reduce the human interventions. The experiments show that HACO achieves a substantially high sample efficiency in the safe driving benchmark. HACO can train agents to drive in unseen traffic scenarios with a handful of human intervention budget and achieve high safety and generalizability, outperforming both reinforcement learning and imitation learning baselines with a large margin. Code and demo video are included in the supplementary materials.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|efficient_learning_of_safe_driving_policy_via_humanai_copilot_optimization", "pdf": "/pdf/c0b165aabfc0cf4dea07b0341e17033a3bc5722b.pdf", "supplementary_material": "/attachment/38010520ff1a8ed332ecadbf1abc4201ea99de9a.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.10341/code)", "_bibtex": "@inproceedings{\nli2022efficient,\ntitle={Efficient Learning of Safe Driving Policy via Human-{AI} Copilot Optimization},\nauthor={Quanyi Li and Zhenghao Peng and Bolei Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0cgU-BZp2ky}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "OjPmfr9GkVv", "original": "uTATJyr3EBj", "number": 2402, "cdate": 1632875619356, "mdate": null, "ddate": null, "tcdate": 1632875619356, "tmdate": 1697934718780, "tddate": null, "forum": "OjPmfr9GkVv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Enhancing Cross-lingual Transfer by Manifold Mixup", "authorids": ["~Huiyun_Yang1", "~Huadong_Chen1", "~Hao_Zhou5", "~Lei_Li11"], "authors": ["Huiyun Yang", "Huadong Chen", "Hao Zhou", "Lei Li"], "keywords": ["cross-lingual transfer", "cross-lingual understanding", "manifold mixup"], "abstract": "Based on large-scale pre-trained multilingual representations, recent cross-lingual transfer methods have achieved impressive transfer performances. However, the performance of target languages still lags far behind the source language. In this paper, our analyses indicate such a performance gap is strongly associated with the cross-lingual representation discrepancy. To achieve better cross-lingual transfer performance, we propose the cross-lingual manifold mixup (X-Mixup) method, which adaptively calibrates the representation discrepancy and gives a compromised representation for target languages. Experiments on the XTREME benchmark show X-Mixup achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and significantly reduces the cross-lingual representation discrepancy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|enhancing_crosslingual_transfer_by_manifold_mixup", "pdf": "/pdf/dbe81cd4937fe7d696c1a2beb6c1a81c871a7a56.pdf", "one-sentence_summary": "We propose the cross-lingual manifold mixup method to improve the cross-lingual transfer.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2205.04182/code)", "_bibtex": "@inproceedings{\nyang2022enhancing,\ntitle={Enhancing Cross-lingual Transfer by Manifold Mixup},\nauthor={Huiyun Yang and Huadong Chen and Hao Zhou and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OjPmfr9GkVv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "74x5BXs4bWD", "original": "1xFqLfwn_a1", "number": 2401, "cdate": 1632875619283, "mdate": null, "ddate": null, "tcdate": 1632875619283, "tmdate": 1676330559479, "tddate": null, "forum": "74x5BXs4bWD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning", "authorids": ["~Yutong_Wang2", "~Ke_Xue1", "~Chao_Qian1"], "authors": ["Yutong Wang", "Ke Xue", "Chao Qian"], "keywords": ["Reinforcement learning", "Quality-Diversity", "Evolutionary algorithms"], "abstract": "Reinforcement Learning (RL) has achieved significant successes, which aims to obtain a single policy maximizing the expected cumulative rewards for a given task. However, in many real-world scenarios, e.g., navigating in complex environments and controlling robots, one may need to find a set of policies having both high rewards and diverse behaviors, which can bring better exploration and robust few-shot adaptation. Recently, some methods have been developed by using evolutionary techniques, including iterative reproduction and selection of policies. However, due to the inefficient selection mechanisms, these methods cannot fully guarantee both high quality and diversity. In this paper, we propose EDO-CS, a new Evolutionary Diversity Optimization algorithm with Clustering-based Selection. In each iteration, the policies are divided into several clusters based on their behaviors, and a high-quality policy is selected from each cluster for reproduction. EDO-CS also adaptively balances the importance between quality and diversity in the reproduction process. Experiments on various (i.e., deceptive and multi-modal) continuous control tasks, show the superior performance of EDO-CS over previous methods, i.e., EDO-CS can achieve a set of policies with both high quality and diversity efficiently while previous methods cannot.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|evolutionary_diversity_optimization_with_clusteringbased_selection_for_reinforcement_learning", "pdf": "/pdf/b887e71bcdd86242a6fbbc501be12552141c01ed.pdf", "one-sentence_summary": "We propose EDO-CS, a new Evolutionary Diversity Optimization algorithm with Clustering-based Selection that can achieve a set of policies with both high quality and diversity efficiently.", "data": "", "_bibtex": "@inproceedings{\nwang2022evolutionary,\ntitle={Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning},\nauthor={Yutong Wang and Ke Xue and Chao Qian},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=74x5BXs4bWD}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "_Wzj0J2xs2D", "original": "XuuN7qaRduE", "number": 2400, "cdate": 1632875619210, "mdate": null, "ddate": null, "tcdate": 1632875619210, "tmdate": 1676330559531, "tddate": null, "forum": "_Wzj0J2xs2D", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CURVATURE-GUIDED DYNAMIC SCALE NETWORKS FOR MULTI-VIEW  STEREO", "authorids": ["~Khang_Truong_Giang1", "dramanet30@naver.com", "~Sungho_Jo1"], "authors": ["Khang Truong Giang", "Soohwan Song", "Sungho Jo"], "keywords": ["multi-view stereo", "3D reconstruction", "dynamic scale"], "abstract": "Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most recent studies tried to improve the performance of matching cost volume in MVS by introducing a skilled design to cost formulation or cost regularization. In this paper, we focus on learning robust feature extraction to enhance the performance of matching costs, without need of heavy computation in the other steps. In particular, we present a dynamic scale feature extraction network, namely, CDSFNet. It is composed of multiple novel convolution layers, each of which can select a proper patch scale for each pixel guided by the normal curvature of image surface. As a result, CDFSNet can estimate the optimal patch scales to learn discriminative features for accurate matching computation between reference and source images. By combining the robust extracted features with an appropriate cost formulation strategy, our final MVS architecture can estimate depth maps more precisely. Extensive experiments showed that the proposed method outperforms other state-of-the-art methods on complex outdoor scenes. It significantly improves the completeness of reconstructed models. Moreover, the method can process the high resolution with faster run-time and lower memory compared to the other MVS methods. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "giang|curvatureguided_dynamic_scale_networks_for_multiview_stereo", "pdf": "/pdf/d9996d1650b1f7ea346a668f1a2daf658ec29136.pdf", "one-sentence_summary": "This paper proposes a dynamic scale feature network to address the matching ambiguity problem in Multi-view stereo (MVS) and then designs an efficient MVS network to predict the depth maps.", "supplementary_material": "/attachment/99881fcea29ea2d22dd91e54ccf305d6e9f88d07.zip", "_bibtex": "@inproceedings{\ngiang2022curvatureguided,\ntitle={{CURVATURE}-{GUIDED} {DYNAMIC} {SCALE} {NETWORKS} {FOR} {MULTI}-{VIEW}  {STEREO}},\nauthor={Khang Truong Giang and Soohwan Song and Sungho Jo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_Wzj0J2xs2D}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "KLaDXLAzzFT", "original": "7aB777q68k", "number": 2399, "cdate": 1632875619136, "mdate": null, "ddate": null, "tcdate": 1632875619136, "tmdate": 1676330559616, "tddate": null, "forum": "KLaDXLAzzFT", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism", "authorids": ["~Ming_Yin4", "~Yaqi_Duan1", "~Mengdi_Wang1", "~Yu-Xiang_Wang1"], "authors": ["Ming Yin", "Yaqi Duan", "Mengdi Wang", "Yu-Xiang Wang"], "keywords": ["reinforcement learning theory", "markov decision process theory"], "abstract": "Offline reinforcement learning, which seeks to utilize offline/historical data to optimize sequential decision-making strategies, has gained surging prominence in recent studies. Due to the advantage that appropriate function approximators can help mitigate the sample complexity burden in modern reinforcement learning problems, existing endeavors usually enforce powerful function representation models (e.g. neural networks) to learn the optimal policies. However, a precise understanding of the statistical limits with function representations, remains elusive, even when such a representation is linear.\n\n\nTowards this goal, we study the statistical limits of offline reinforcement learning with linear model representations. To derive the tight offline learning bound, we design the variance-aware pessimistic value iteration (VAPVI), which adopts the conditional variance information of the value function for time-inhomogeneous episodic linear Markov decision processes (MDPs). VAPVI leverages estimated variances of the value functions to reweight the Bellman residuals in the least-square pessimistic value iteration and provides improved offline learning bounds over the best-known existing results (whereas the Bellman residuals are equally weighted by design). More importantly, our learning bounds are expressed in terms of system quantities, which provide natural instance-dependent characterizations that previous results are short of. We hope our results draw a clearer picture of what offline learning should look like when linear representations are provided.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yin|nearoptimal_offline_reinforcement_learning_with_linear_representation_leveraging_variance_information_with_pessimism", "pdf": "/pdf/4fcfeb93b8187d6d055e04332a5c8b1c37b10970.pdf", "supplementary_material": "/attachment/01bd083fc4ff2f46da240e36f09cefc0fcc97abf.zip", "_bibtex": "@inproceedings{\nyin2022nearoptimal,\ntitle={Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism},\nauthor={Ming Yin and Yaqi Duan and Mengdi Wang and Yu-Xiang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KLaDXLAzzFT}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "RftryyYyjiG", "original": "WiiExsLCfbq", "number": 2394, "cdate": 1632875618863, "mdate": null, "ddate": null, "tcdate": 1632875618863, "tmdate": 1697934719764, "tddate": null, "forum": "RftryyYyjiG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Exploring extreme parameter compression for pre-trained language models", "authorids": ["~Benyou_Wang2", "~Yuxin_Ren1", "~Lifeng_Shang1", "~Xin_Jiang1", "~Qun_Liu1"], "authors": ["Benyou Wang", "Yuxin Ren", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "keywords": ["pre-trained language models", "tensor decomposition", "compression", "BERT"], "abstract": "Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language processing. This raises many concerns from various perspectives, e.g.,  financial costs and carbon emissions. \nCompressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has attracted much attention. In this work, we aim to explore larger compression ratios for PLMs, among which tensor decomposition is a potential but under-investigated one. By comparing existing decomposition methods, Tucker decomposition is found to be parameter-efficient for compression.  Two decomposition and reconstruction protocols are further proposed to improve the effectiveness and efficiency of Tucker decomposition in parameter compression.\nOur compressed BERT with ${1}/{7}$ parameters in Transformer layers performs on-par with,  sometimes slightly better than the original BERT in GLUE benchmark. A tiny version achieves  96.7\\%  performance of  BERT-base with $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding the embedding layer) and  \\textbf{$2.7 \\times$} faster on inference. To show that the proposed method is orthogonal to existing compression methods like knowledge distillation, we also explore the benefit of the proposed method on a distilled BERT. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|exploring_extreme_parameter_compression_for_pretrained_language_models", "pdf": "/pdf/dab5dd8e405bdd89ffafedef9f081622e45d0c61.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2205.10036/code)", "_bibtex": "@inproceedings{\nwang2022exploring,\ntitle={Exploring extreme parameter compression for pre-trained language models},\nauthor={Benyou Wang and Yuxin Ren and Lifeng Shang and Xin Jiang and Qun Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RftryyYyjiG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "Sq0-tgDyHe4", "original": "JnFeNaEK09", "number": 2393, "cdate": 1632875618794, "mdate": null, "ddate": null, "tcdate": 1632875618794, "tmdate": 1697934719767, "tddate": null, "forum": "Sq0-tgDyHe4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Local Feature Swapping for Generalization in Reinforcement Learning", "authorids": ["~David_Bertoin1", "~Emmanuel_Rachelson1"], "authors": ["David Bertoin", "Emmanuel Rachelson"], "keywords": ["Reinforcement learning", "Generalization", "Regularization"], "abstract": "Over the past few years, the acceleration of computing resources and research in Deep Learning has led to significant practical successes in a range of tasks, including in particular in computer vision. Building on these advances, reinforcement learning has also seen a leap forward with the emergence of agents capable of making decisions directly from visual observations. Despite these successes, the over-parametrization of neural architectures leads to memorization of the data used during training and thus to a lack of generalization.\nReinforcement learning agents based on visual inputs also suffer from this phenomenon by erroneously correlating rewards with unrelated visual features such as background elements. To alleviate this problem, we introduce a new regularization layer consisting of channel-consistent local permutations (CLOP) of the feature maps. The proposed permutations induce robustness to spatial correlations and help prevent overfitting behaviors in RL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained with the CLOP layer exhibit robustness to visual changes and better generalization properties than agents trained using other state-of-the-art regularization techniques.", "one-sentence_summary": "We propose a simple yet effective layer increasing the generalization abilities of reinforcement learning agents", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bertoin|local_feature_swapping_for_generalization_in_reinforcement_learning", "pdf": "/pdf/679f3a9bf9ae7a8121b4cb0bb53f30887f029b89.pdf", "supplementary_material": "/attachment/51ef5aad80459ecd0f060db33b8336fea6db6442.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2204.06355/code)", "_bibtex": "@inproceedings{\nbertoin2022local,\ntitle={Local Feature Swapping for Generalization in Reinforcement Learning},\nauthor={David Bertoin and Emmanuel Rachelson},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Sq0-tgDyHe4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "lL3lnMbR4WU", "original": "1OZi40HoxKa", "number": 2383, "cdate": 1632875618167, "mdate": null, "ddate": null, "tcdate": 1632875618167, "tmdate": 1676330560005, "tddate": null, "forum": "lL3lnMbR4WU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation", "authorids": ["~Xiuye_Gu1", "~Tsung-Yi_Lin4", "~Weicheng_Kuo1", "~Yin_Cui1"], "authors": ["Xiuye Gu", "Tsung-Yi Lin", "Weicheng Kuo", "Yin Cui"], "keywords": ["Open-vocabulary recognition", "Object detection", "Knowledge distillation"], "abstract": "We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data.  It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask APr with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 APr. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP50 on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.", "one-sentence_summary": "We propose using knowledge distillation to train an object detector that can detect objects with arbitrary text inputs, outperforming its supervised counterparts on rare categories.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gu|openvocabulary_object_detection_via_vision_and_language_knowledge_distillation", "pdf": "/pdf/25cfe8bb2fa27d8a1c86a575dbf3b997754148be.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\ngu2022openvocabulary,\ntitle={Open-vocabulary Object Detection via Vision and Language Knowledge Distillation},\nauthor={Xiuye Gu and Tsung-Yi Lin and Weicheng Kuo and Yin Cui},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lL3lnMbR4WU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "EBn0uInJZWh", "original": "tUyx_DRYCWn", "number": 2360, "cdate": 1632875616772, "mdate": null, "ddate": null, "tcdate": 1632875616772, "tmdate": 1676330561320, "tddate": null, "forum": "EBn0uInJZWh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Model-Based Offline Meta-Reinforcement Learning with Regularization", "authorids": ["~Sen_Lin1", "jwan20@asu.edu", "~Tengyu_Xu1", "~Yingbin_Liang1", "~Junshan_Zhang1"], "authors": ["Sen Lin", "Jialin Wan", "Tengyu Xu", "Yingbin Liang", "Junshan Zhang"], "keywords": ["offline reinforcement learning", "model-based reinforcement learning", "behavior policy", "Meta-reinforcement learning"], "abstract": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL  could be outperformed  by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated  between \"exploring\" the out-of-distribution state-actions by following the meta-policy and \"exploiting\" the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we propose model-based offline $\\text{\\bf Me}$ta-RL with $\\text{\\bf r}$egularized $\\text{\\bf P}$olicy $\\text{\\bf O}$ptimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block  of MerPO, using both conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Our experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.", "one-sentence_summary": "This paper proposes a novel offline Meta-RL algorithm with regularization, which has provable performance improvement and outperforms the existing baselines empirically.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lin|modelbased_offline_metareinforcement_learning_with_regularization", "pdf": "/pdf/af10847f3163c50846528554895671f12dd3f6bd.pdf", "supplementary_material": "/attachment/efdfca464a58c1372bad2071a5e5ace74ee46795.zip", "_bibtex": "@inproceedings{\nlin2022modelbased,\ntitle={Model-Based Offline Meta-Reinforcement Learning with Regularization},\nauthor={Sen Lin and Jialin Wan and Tengyu Xu and Yingbin Liang and Junshan Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EBn0uInJZWh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "YVPBh4k78iZ", "original": "6DxDy1cQE4e", "number": 2359, "cdate": 1632875616702, "mdate": null, "ddate": null, "tcdate": 1632875616702, "tmdate": 1697934723170, "tddate": null, "forum": "YVPBh4k78iZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Scale Mixtures of Neural Network Gaussian Processes", "authorids": ["~Hyungi_Lee1", "~Eunggu_Yun1", "~Hongseok_Yang2", "~Juho_Lee2"], "authors": ["Hyungi Lee", "Eunggu Yun", "Hongseok Yang", "Juho Lee"], "keywords": ["Neural Network Gaussian Processes", "Infinitely-wide Neural Networks", "Scale Mixtures of Gaussians", "Heavy-tailed Stochastic Processes"], "abstract": "Recent works have revealed that infinitely-wide feed-forward or recurrent neural networks of any architecture correspond to Gaussian processes referred to as NNGP. While these works have extended the class of neural networks converging to Gaussian processes significantly, however, there has been little focus on broadening the class of stochastic processes that such neural networks converge to. In this work, inspired by the scale mixture of Gaussian random variables, we propose the scale mixture of NNGP for which we introduce a prior distribution on the scale of the last-layer parameters. We show that simply introducing a scale prior on the last-layer parameters can turn infinitely-wide neural networks of any architecture into a richer class of stochastic processes. With certain scale priors, we obtain heavy-tailed stochastic processes, and in the case of inverse gamma priors, we recover Student\u2019s $t$ processes. We further analyze the distributions of the neural networks initialized with our prior setting and trained with gradient descents and obtain similar results as for NNGP. We present a practical posterior-inference algorithm for the scale mixture of NNGP and empirically demonstrate its usefulness on regression and classification tasks. In particular, we show that in both tasks, the heavy-tailed stochastic processes obtained from our framework are robust to out-of-distribution data.", "one-sentence_summary": "Infinitely-wide neural networks can be equivalent to scale mixtures of Gaussian processes.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|scale_mixtures_of_neural_network_gaussian_processes", "pdf": "/pdf/2b9ab3a3ccae899d74d3b0dcba493cd95bedad20.pdf", "supplementary_material": "/attachment/8eb4aee476f11ba80d33da315e70f386c2ad16c4.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.01408/code)", "_bibtex": "@inproceedings{\nlee2022scale,\ntitle={Scale Mixtures of Neural Network Gaussian Processes},\nauthor={Hyungi Lee and Eunggu Yun and Hongseok Yang and Juho Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YVPBh4k78iZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "YX0lrvdPQc", "original": "mIqPYq_rT4B", "number": 2354, "cdate": 1632875616347, "mdate": null, "ddate": null, "tcdate": 1632875616347, "tmdate": 1676330561763, "tddate": null, "forum": "YX0lrvdPQc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs", "authorids": ["~Ido_Nachum1", "jan.hazla@epfl.ch", "~Michael_Gastpar1", "~Anatoly_Khina1"], "authors": ["Ido Nachum", "Jan Hazla", "Michael Gastpar", "Anatoly Khina"], "keywords": ["convolutional neural networks", "Johnson-Lindenstrauss lemma", "initialization", "isometry", "theory."], "abstract": "How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson-Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two input contracts according to a known mapping. The question for non-linear convolutional neural networks (CNNs) becomes much more intricate. To answer this question, we introduce a geometric framework. For linear CNNs, we show that the Johnson--Lindenstrauss lemma continues to hold, namely, that the angle between two inputs is preserved. For CNNs with ReLU activation, on the other hand, the behavior is richer: The angle between the outputs contracts, where the level of contraction depends on the nature of the inputs. In particular, after one layer, the geometry of natural images is essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the same contracting behavior as FNNs with ReLU activation. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nachum|a_johnsonlindenstrauss_framework_for_randomly_initialized_cnns", "pdf": "/pdf/e5dd1cf8cfc1ee79c13925ef1a7839e92785b7ab.pdf", "one-sentence_summary": "We study how the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network.", "_bibtex": "@inproceedings{\nnachum2022a,\ntitle={A Johnson-Lindenstrauss Framework for Randomly Initialized {CNN}s},\nauthor={Ido Nachum and Jan Hazla and Michael Gastpar and Anatoly Khina},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YX0lrvdPQc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "Vr_BTpw3wz", "original": "rBY7SRAt2FM", "number": 2351, "cdate": 1632875616207, "mdate": null, "ddate": null, "tcdate": 1632875616207, "tmdate": 1676330561810, "tddate": null, "forum": "Vr_BTpw3wz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hindsight: Posterior-guided training of retrievers for improved open-ended generation", "authorids": ["~Ashwin_Paranjape1", "~Omar_Khattab1", "~Christopher_Potts1", "~Matei_Zaharia1", "~Christopher_D_Manning1"], "authors": ["Ashwin Paranjape", "Omar Khattab", "Christopher Potts", "Matei Zaharia", "Christopher D Manning"], "keywords": ["retrieval", "generation", "retrieval-augmented generation", "open-ended generation", "informative conversations", "free-form QA", "posterior distribution", "ELBo"], "abstract": "Many text generation systems benefit from retrieving passages from a textual knowledge corpus (e.g., Wikipedia) and using them to generate the output. For open-ended generation tasks, like generating informative utterances in conversations, many varied passages $z$ are relevant to the context $x$ but few are relevant to the observed next utterance $y$ (label). For such tasks, existing methods (that jointly train the retriever and generator) underperform: during training the top-k context-relevant retrieved passages might not contain the label-relevant passage and the generator may hence not learn a preference to ground its generated output in them. We propose using an additional guide-retriever that also conditions on the observed label $y$ and \u201cin hindsight\u201d retrieves label-relevant passages during training. We maximize the evidence lower bound (ELBo) to jointly train the guide-retriever $Q(z|x,y)$ with the standard retriever $P_\\eta(z|x)$ and the generator $P_\\theta(y|x,z)$ and find that ELBo has better inductive biases than prior work. For informative conversations from the Wizard of Wikipedia dataset, with our posterior-guided training, the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator\u2019s responses are more grounded in the retrieved passage (19% relative improvement) and the end-to-end system produces better overall output (6.4% relative improvement). ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "paranjape|hindsight_posteriorguided_training_of_retrievers_for_improved_openended_generation", "pdf": "/pdf/0801402a22ce82661bf20317c66aeb13527df311.pdf", "one-sentence_summary": "We use a posterior-guide retriever to train a retrieval-augmented generation that performs well on open-ended one-to-many generation tasks.", "_bibtex": "@inproceedings{\nparanjape2022hindsight,\ntitle={Hindsight: Posterior-guided training of retrievers for improved open-ended generation},\nauthor={Ashwin Paranjape and Omar Khattab and Christopher Potts and Matei Zaharia and Christopher D Manning},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Vr_BTpw3wz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "k9bx1EfHI_-", "original": "e3JZMproSz9", "number": 2350, "cdate": 1632875616138, "mdate": null, "ddate": null, "tcdate": 1632875616138, "tmdate": 1676330561901, "tddate": null, "forum": "k9bx1EfHI_-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis", "authorids": ["~Siyi_Tang1", "~Jared_Dunnmon1", "~Khaled_Kamal_Saab1", "~Xuan_Zhang6", "~Qianying_Huang1", "~Florian_Dubost1", "~Daniel_Rubin1", "~Christopher_Lee-Messer1"], "authors": ["Siyi Tang", "Jared Dunnmon", "Khaled Kamal Saab", "Xuan Zhang", "Qianying Huang", "Florian Dubost", "Daniel Rubin", "Christopher Lee-Messer"], "keywords": ["Graph neural network", "Self-supervision", "Interpretability", "Visualization", "Neuroscience", "Electroencephalography", "Seizure", "Epilepsy", "Time Series"], "abstract": "Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model\u2019s ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset (5,499 EEGs), we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types (e.g. 47 points increase in combined tonic seizure accuracy over baselines). Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tang|selfsupervised_graph_neural_networks_for_improved_electroencephalographic_seizure_analysis", "pdf": "/pdf/17a7d200331982e9e2906bc6831d4cdc744a6f5c.pdf", "one-sentence_summary": "Self-supervised graph neural networks for seizure detection and classification from EEG.", "supplementary_material": "/attachment/a4cc6007cf7eeee223e5983202e49316eecb9ab4.zip", "_bibtex": "@inproceedings{\ntang2022selfsupervised,\ntitle={Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis},\nauthor={Siyi Tang and Jared Dunnmon and Khaled Kamal Saab and Xuan Zhang and Qianying Huang and Florian Dubost and Daniel Rubin and Christopher Lee-Messer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=k9bx1EfHI_-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "cw-EmNq5zfD", "original": "FyJdvT1VSUP", "number": 2346, "cdate": 1632875615849, "mdate": null, "ddate": null, "tcdate": 1632875615849, "tmdate": 1676330562120, "tddate": null, "forum": "cw-EmNq5zfD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Group-based Interleaved Pipeline Parallelism for Large-scale DNN Training", "authorids": ["~PengCheng_Yang4", "~Xiaoming_Zhang2", "~Wenpeng_Zhang1", "~Ming_Yang9", "~Hong_Wei3"], "authors": ["PengCheng Yang", "Xiaoming Zhang", "Wenpeng Zhang", "Ming Yang", "Hong Wei"], "keywords": ["Model parallelism", "Pipeline parallelism", "Distributed training"], "abstract": "The recent trend of using large-scale deep neural networks (DNN) to boost performance has propelled the development of the parallel pipelining technique for efficient DNN training, which has resulted in the development of several prominent pipelines such as GPipe, PipeDream, and PipeDream-2BW. However, the current leading pipeline PipeDream-2BW still suffers from two major drawbacks, i.e., the excessive memory redundancy and the delayed weight updates across all stages. In this work, we propose a novel pipeline named WPipe, which achieves better memory efficiency and fresher weight updates. WPipe uses a novel pipelining scheme that divides model partitions into two groups. It moves the forward pass of the next period of weight updates to the front of the backward pass of the current period of weight updates in the first group, retains the order in the second group, and updates each group alternatively. This scheme can eliminate half of the delayed gradients and memory redundancy compared to PipeDream-2BW. The experiments, which train large BERT language models, show that compared to PipeDream-2BW, WPipe achieves $1.4\\times$ acceleration and reduces the memory footprint by 36%, without nearly sacrificing any final model accuracy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|groupbased_interleaved_pipeline_parallelism_for_largescale_dnn_training", "pdf": "/pdf/2322466e5de76b982eaeca16cda0dc1dfd2f5563.pdf", "supplementary_material": "/attachment/6f606daa0d230a8689305e7361c07f60fb3c95a1.zip", "data": "", "_bibtex": "@inproceedings{\nyang2022groupbased,\ntitle={Group-based Interleaved Pipeline Parallelism for Large-scale {DNN} Training},\nauthor={PengCheng Yang and Xiaoming Zhang and Wenpeng Zhang and Ming Yang and Hong Wei},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cw-EmNq5zfD}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "nc0ETaieux", "original": "2S0ITC44m6D", "number": 2339, "cdate": 1632875615352, "mdate": null, "ddate": null, "tcdate": 1632875615352, "tmdate": 1676330562301, "tddate": null, "forum": "nc0ETaieux", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Minimax Optimality (Probably) Doesn't Imply Distribution Learning for GANs", "authorids": ["~Sitan_Chen1", "~Jerry_Li1", "~Yuanzhi_Li1", "~Raghu_Meka1"], "authors": ["Sitan Chen", "Jerry Li", "Yuanzhi Li", "Raghu Meka"], "keywords": ["theory of GANs", "distribution learning", "pseudorandom generators", "cryptography"], "abstract": "Arguably the most fundamental question in the theory of generative adversarial networks (GANs) is to understand when GANs can actually learn the underlying distribution. Theoretical and empirical evidence (see e.g. Arora-Risteski-Zhang '18) suggest local optimality of the empirical training objective is insufficient, yet it does not rule out the possibility that achieving a true population minimax optimal solution might imply distribution learning. In this paper, we show that standard cryptographic assumptions imply that this stronger condition is still insufficient. Namely, we show that if local pseudorandom generators (PRGs) exist, then for a large family of natural target distributions, there are ReLU network generators of constant depth and poly size which take Gaussian random seeds so that (i) the output is far in Wasserstein distance from the target distribution, but (ii) no polynomially large Lipschitz discriminator ReLU network can detect this. This implies that even achieving a population minimax optimal solution to the Wasserstein GAN objective is likely insufficient for distribution learning. Our techniques reveal a deep connection between GANs and PRGs, which we believe will lead to further insights into the computational landscape of GANs.", "one-sentence_summary": "Under the standard crypto assumption that local pseudorandom generators exist, we show that even a global optimizer for the population WGAN objective need not be close to the true distribution.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|minimax_optimality_probably_doesnt_imply_distribution_learning_for_gans", "pdf": "/pdf/6ace3e50695ef1af76dc61bfcfb736da932857ce.pdf", "supplementary_material": "/attachment/7036053b48423a94822be67ff3be2b1fcce7369a.zip", "_bibtex": "@inproceedings{\nchen2022minimax,\ntitle={Minimax Optimality (Probably) Doesn't Imply Distribution Learning for {GAN}s},\nauthor={Sitan Chen and Jerry Li and Yuanzhi Li and Raghu Meka},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nc0ETaieux}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "RCZqv9NXlZ", "original": "ROdaqu2svt5", "number": 2331, "cdate": 1632875614928, "mdate": null, "ddate": null, "tcdate": 1632875614928, "tmdate": 1697934726122, "tddate": null, "forum": "RCZqv9NXlZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Offline Reinforcement Learning with Value-based Episodic Memory", "authorids": ["~Xiaoteng_Ma1", "~Yiqin_Yang1", "~Hao_Hu3", "~Jun_Yang6", "~Chongjie_Zhang1", "~Qianchuan_Zhao1", "~Bin_Liang5", "~Qihan_Liu1"], "authors": ["Xiaoteng Ma", "Yiqin Yang", "Hao Hu", "Jun Yang", "Chongjie Zhang", "Qianchuan Zhao", "Bin Liang", "Qihan Liu"], "keywords": ["Reinforcement Learning", "Offline Learning", "Episodic Memory Control"], "abstract": "Offline reinforcement learning (RL) shows promise of applying RL to real-world problems by effectively utilizing previously collected data. Most existing offline RL algorithms use regularization or constraints to suppress extrapolation error for actions outside the dataset. In this paper, we adopt a different framework, which learns the V-function instead of the Q-function to naturally keep the learning procedure within the support of an offline dataset. To enable effective generalization while maintaining proper conservatism in offline learning, we propose Expectile V-Learning (EVL), which smoothly interpolates between the optimal value learning and behavior cloning. Further, we introduce implicit planning along offline trajectories to enhance learned V-values and accelerate convergence. Together, we present a new offline method called Value-based Episodic Memory (VEM). We provide theoretical analysis for the convergence properties of our proposed VEM method, and empirical results in the D4RL benchmark show that our method achieves superior performance in most tasks, particularly in sparse-reward tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ma|offline_reinforcement_learning_with_valuebased_episodic_memory", "pdf": "/pdf/02371afec918d5a82c74580cda7e3efbf18411d5.pdf", "one-sentence_summary": "We propose a new offline RL method which uses expectile value learning and memory-based planning.", "supplementary_material": "/attachment/1ff950cdd5225dd590f7f6d96f3e626049fc9cbb.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.09796/code)", "_bibtex": "@inproceedings{\nma2022offline,\ntitle={Offline Reinforcement Learning with Value-based Episodic Memory},\nauthor={Xiaoteng Ma and Yiqin Yang and Hao Hu and Jun Yang and Chongjie Zhang and Qianchuan Zhao and Bin Liang and Qihan Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RCZqv9NXlZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 38}}, {"id": "C54V-xTWfi", "original": "cJEKB1ZfXe6", "number": 2328, "cdate": 1632875614785, "mdate": null, "ddate": null, "tcdate": 1632875614785, "tmdate": 1697934726121, "tddate": null, "forum": "C54V-xTWfi", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MonoDistill: Learning Spatial Features for Monocular 3D Object Detection", "authorids": ["~Zhiyu_Chong1", "~Xinzhu_Ma1", "~Hong_Zhang8", "~Yuxin_Yue2", "~Haojie_Li2", "~Zhihui_Wang4", "~Wanli_Ouyang1"], "authors": ["Zhiyu Chong", "Xinzhu Ma", "Hong Zhang", "Yuxin Yue", "Haojie Li", "Zhihui Wang", "Wanli Ouyang"], "keywords": ["3D object detection", "monocular images"], "abstract": "3D object detection is a fundamental and challenging task for 3D scene understanding, and the monocular-based methods can serve as an economical alternative to the stereo-based or LiDAR-based methods. However, accurately locating objects in the 3D space from a single image is extremely difficult due to the lack of spatial cues. To mitigate this issue, we propose a simple and effective scheme to introduce the spatial information from LiDAR signals to the monocular 3D detectors, without introducing any extra cost in the inference phase. In particular, we first project the LiDAR signals into the image plane and align them with the RGB images. After that, we use the resulting data to train a 3D detector (LiDAR Net) using the same architecture as the baseline model. Finally, this LiDAR Net can serve as the teacher to transfer the learned knowledge to the baseline model. Experimental results show that the proposed method can significantly boost the performance of the baseline model and ranks the $1^{st}$ place among all monocular-based methods on the KITTI benchmark. Besides, extensive ablation studies are conducted, which further prove the effectiveness of each part of our designs and illustrate what the baseline model has learned from the LiDAR Net.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chong|monodistill_learning_spatial_features_for_monocular_3d_object_detection", "pdf": "/pdf/48f3e34df8a32755defa6cc07e7521b77cb01afc.pdf", "one-sentence_summary": "We propose the MonoDistill, which introduces spatial cues to the monocular 3D detector based on the knowledge distillation mechanism.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.10830/code)", "_bibtex": "@inproceedings{\nchong2022monodistill,\ntitle={MonoDistill: Learning Spatial Features for Monocular 3D Object Detection},\nauthor={Zhiyu Chong and Xinzhu Ma and Hong Zhang and Yuxin Yue and Haojie Li and Zhihui Wang and Wanli Ouyang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=C54V-xTWfi}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "vkaMaq95_rX", "original": "982czc-tDl4", "number": 2316, "cdate": 1632875614017, "mdate": null, "ddate": null, "tcdate": 1632875614017, "tmdate": 1676330563561, "tddate": null, "forum": "vkaMaq95_rX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression", "authorids": ["~Zirui_Liu1", "~Kaixiong_Zhou1", "~Fan_Yang27", "~Li_Li11", "~Rui_Chen4", "~Xia_Hu4"], "authors": ["Zirui Liu", "Kaixiong Zhou", "Fan Yang", "Li Li", "Rui Chen", "Xia Hu"], "keywords": ["graph neural networks", "scalable GNN training", "quantization", "random projection"], "abstract": "Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory.\nIn parallel, unlike what has been developed for other types of neural networks, training with compressed activation maps is less explored for GNNs. This extension is notoriously difficult to implement due to the miss of necessary tools in common graph learning packages. To unleash the potential of this direction, we provide {  an} optimized GPU implementation which supports training GNNs with compressed activations. Based on the implementation, we propose a memory-efficient framework called ``EXACT'', which for the first time demonstrate the potential and evaluate the feasibility of training GNNs with compressed activations. We systematically analyze the trade-off among the memory saving, time overhead, and accuracy drop. In practice, EXACT can reduce the memory footprint of activations by up to $32\\times$ with $0.2$-$0.5\\%$ accuracy drop and $10$-$25\\%$ time overhead across different models and datasets. We implement EXACT as an extension for Pytorch Geometric and Pytorch. In practice, for Pytorch Geometric, EXACT can trim down the hardware requirement of training a three-layer full-batch GraphSAGE on \\textit{ogbn-products} from a 48GB GPU to a 12GB GPU.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|exact_scalable_graph_neural_networks_training_via_extreme_activation_compression", "pdf": "/pdf/c4401e62dd352ef175de3558fd2ccb66fd2107e0.pdf", "supplementary_material": "/attachment/a8524b9b7ac5abbb8b31acf8cfa85ec2eed5365d.zip", "data": "", "_bibtex": "@inproceedings{\nliu2022exact,\ntitle={{EXACT}: Scalable Graph Neural Networks Training via Extreme Activation Compression},\nauthor={Zirui Liu and Kaixiong Zhou and Fan Yang and Li Li and Rui Chen and Xia Hu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vkaMaq95_rX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "MP904TiHqJ-", "original": "rwy0Ur50RFP", "number": 2298, "cdate": 1632875612821, "mdate": null, "ddate": null, "tcdate": 1632875612821, "tmdate": 1676330564237, "tddate": null, "forum": "MP904TiHqJ-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Provably convergent quasistatic dynamics for mean-field two-player zero-sum games", "authorids": ["~Chao_Ma8", "~Lexing_Ying1"], "authors": ["Chao Ma", "Lexing Ying"], "keywords": ["quasistatic", "minimax optimization", "mixed Nash equilibrium", "mean-field formulation"], "abstract": "In this paper, we study the problem of finding mixed Nash equilibrium for mean-field two-player zero-sum games. Solving this problem requires optimizing over two probability distributions. We consider a quasistatic Wasserstein gradient flow dynamics in which one probability distribution follows the Wasserstein gradient flow, while the other one is always at the equilibrium. Theoretical analysis are conducted on this dynamics, showing its convergence to the mixed Nash equilibrium under mild conditions. Inspired by the continuous dynamics of probability distributions, we derive a quasistatic Langevin gradient descent method with inner-outer iterations, and test the method on different problems, including training mixture of GANs. ", "one-sentence_summary": "We propose a quasistatic Wasserstein flow for finding mixed Nash equilibriums, and prove its convergence.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ma|provably_convergent_quasistatic_dynamics_for_meanfield_twoplayer_zerosum_games", "pdf": "/pdf/acb4ac9a6784cfd88f2b6afa489db7cb2af8de79.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nma2022provably,\ntitle={Provably convergent quasistatic dynamics for mean-field two-player zero-sum games},\nauthor={Chao Ma and Lexing Ying},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MP904TiHqJ-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "0RqDp8FCW5Z", "original": "pRTtt1Qpw3p", "number": 2297, "cdate": 1632875612752, "mdate": null, "ddate": null, "tcdate": 1632875612752, "tmdate": 1676330564322, "tddate": null, "forum": "0RqDp8FCW5Z", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "W-CTC: a Connectionist Temporal Classification Loss with Wild Cards", "authorids": ["~Xingyu_Cai1", "~Jiahong_Yuan1", "~Yuchen_Bian1", "~Guangxu_Xun1", "~Jiaji_Huang1", "~Kenneth_Church1"], "authors": ["Xingyu Cai", "Jiahong Yuan", "Yuchen Bian", "Guangxu Xun", "Jiaji Huang", "Kenneth Church"], "keywords": ["CTC", "wild cards", "dynamic programing", "partial alignment"], "abstract": "Connectionist Temporal Classification (CTC) loss is commonly used in sequence learning applications. For example, in Automatic Speech Recognition (ASR) task, the training data consists of pairs of audio (input sequence) and text (output label),without temporal alignment information. Standard CTC computes a loss by aggregating over all possible alignment paths, that map the entire sequence to the entire label (full alignment). However, in practice, there are often cases where the label is incomplete. Specifically, we solve the partial alignment problem where the label only matches a middle part of the sequence. This paper proposes the wild-card CTC (W-CTC) to address this issue, by padding wild-cards at both ends of the labels. Consequently, the proposed W-CTC improves the standard CTC via aggregating  over even more alignment paths. Evaluations on a number of tasks in speech and vision domains, show that the proposed W-CTC consistently outperforms the standard CTC by a large margin when label is incomplete. The effectiveness of the proposed method is further confirmed in an ablation study.", "one-sentence_summary": "This paper proposes wild-card CTC to solve the problem that the label only matches middle part of the sequence.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cai|wctc_a_connectionist_temporal_classification_loss_with_wild_cards", "pdf": "/pdf/037209609dda60fc4dd420af54cfcbfa8a63388e.pdf", "supplementary_material": "/attachment/741e32015e59043afbe4bf9a9f3aaac23463ab63.zip", "code": "", "_bibtex": "@inproceedings{\ncai2022wctc,\ntitle={W-{CTC}: a Connectionist Temporal Classification Loss with Wild Cards},\nauthor={Xingyu Cai and Jiahong Yuan and Yuchen Bian and Guangxu Xun and Jiaji Huang and Kenneth Church},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0RqDp8FCW5Z}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "Q83vFlie_Pr", "original": "vfog0tFe2hC", "number": 2289, "cdate": 1632875612318, "mdate": null, "ddate": null, "tcdate": 1632875612318, "tmdate": 1676330564551, "tddate": null, "forum": "Q83vFlie_Pr", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bandit Learning with Joint Effect of Incentivized Sampling, Delayed Sampling Feedback, and Self-Reinforcing User Preferences", "authorids": ["~Tianchen_Zhou1", "~Jia_Liu1", "~Chaosheng_Dong1", "yisun@amazon.com"], "authors": ["Tianchen Zhou", "Jia Liu", "Chaosheng Dong", "Yi Sun"], "keywords": [], "abstract": "In this paper, we consider a new multi-armed bandit (MAB) framework motivated by three common complications in online recommender systems in practice: (i) the platform (learning agent) cannot sample an intended product directly and has to incentivize customers to select this product (e.g., promotions and coupons); (ii) customer feedbacks are often received later than their selection times; and (iii) customer preferences among products are influenced and reinforced by historical feedbacks. From the platform's perspective, the goal of the MAB framework is to maximize total reward without incurring excessive incentive costs. A major challenge of this MAB framework is that the loss of information caused by feedback delay complicates both user preference evolution and arm incentivizing decisions, both of which are already highly non-trivial even by themselves. Toward this end, we first propose a policy called ``UCB-Filtering-with-Delayed-Feedback'' (UCB-FDF) policy for this new MAB framework. In our analysis, we consider delayed feedbacks that can have either arm-independent or arm-dependent distributions. In both cases, we allow unbounded support for the random delays, i.e., the random delay can be infinite. We show that the delay impacts in both cases can still be upper bounded by an additive penalty on both the regret and total incentive costs. This further implies that logarithmic regret and incentive cost growth rates are achievable under this new MAB framework. Experimental results corroborate our theoretical analysis on both regret and incentive costs.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhou|bandit_learning_with_joint_effect_of_incentivized_sampling_delayed_sampling_feedback_and_selfreinforcing_user_preferences", "pdf": "/pdf/d654e6eeca4a1faa806d427c0b60d31e7648ad5f.pdf", "supplementary_material": "/attachment/269adcd3db2846a2315e5c6daa7a63dbc05dfa62.zip", "_bibtex": "@inproceedings{\nzhou2022bandit,\ntitle={Bandit Learning with Joint Effect of Incentivized Sampling, Delayed Sampling Feedback, and Self-Reinforcing User Preferences},\nauthor={Tianchen Zhou and Jia Liu and Chaosheng Dong and Yi Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Q83vFlie_Pr}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "rWXfFogxRJN", "original": "lvdKTEjpUpi", "number": 2273, "cdate": 1632875611210, "mdate": null, "ddate": null, "tcdate": 1632875611210, "tmdate": 1676330565405, "tddate": null, "forum": "rWXfFogxRJN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies", "authorids": ["~Tsz-Him_Cheung1", "~Dit-Yan_Yeung2"], "authors": ["Tsz-Him Cheung", "Dit-Yan Yeung"], "keywords": ["Data Augmentation", "Automated Data Augmentation"], "abstract": "Data augmentation is an effective way to improve the generalization capability of modern deep learning models. However, the underlying augmentation methods mostly rely on handcrafted operations. Moreover, an augmentation policy useful to one dataset may not transfer well to other datasets. Therefore, Automated Data Augmentation (AutoDA) methods, like \\textit{AutoAugment} and \\textit{Population-based Augmentation}, have been proposed recently to automate the process of searching for optimal augmentation policies. However, the augmentation policies found are not adaptive to the dataset used, hindering the effectiveness of these AutoDA methods. In this paper, we propose a novel AutoDA method called \\texttt{AdaAug} to efficiently learn adaptive augmentation policies in a class-dependent and potentially instance-dependent manner. Our experiments show that the adaptive augmentation policies learned by our method transfer well to unseen datasets such as the Oxford Flowers, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars datasets when compared with other AutoDA baselines. In addition, our method also achieves state-of-the-art performance on the CIFAR-10, CIFAR-100, and SVHN datasets.", "one-sentence_summary": "We propose a novel Automated Data Augmentation method called \\texttt{AdaAug} to efficiently learn adaptive augmentation policies in a class-dependent and potentially instance-dependent manner.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cheung|adaaug_learning_class_and_instanceadaptive_data_augmentation_policies", "pdf": "/pdf/178ea7fe9306b26e1d623abc89a015f272a95bab.pdf", "supplementary_material": "/attachment/d21ee9d1692e0095d4a647a463a633089b02ff82.zip", "data": "", "_bibtex": "@inproceedings{\ncheung2022adaaug,\ntitle={AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies},\nauthor={Tsz-Him Cheung and Dit-Yan Yeung},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rWXfFogxRJN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "SaKO6z6Hl0c", "original": "ENNbYxaiP_e", "number": 2272, "cdate": 1632875611138, "mdate": null, "ddate": null, "tcdate": 1632875611138, "tmdate": 1697934730721, "tddate": null, "forum": "SaKO6z6Hl0c", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Semantic Segmentation by Distilling Feature Correspondences", "authorids": ["~Mark_Hamilton1", "~Zhoutong_Zhang1", "~Bharath_Hariharan3", "~Noah_Snavely1", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Zhoutong Zhang", "Bharath Hariharan", "Noah Snavely", "William T. Freeman"], "keywords": ["Unsupervised Semantic Segmentation", "Unsupervised Learning", "Deep Features", "Contrastive Learning", "Visual Transformers", "Cocostuff", "Cityscapes", "Semantic Segmentation"], "abstract": "Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO ($\\textbf{S}$elf-supervised $\\textbf{T}$ransformer with $\\textbf{E}$nergy-based $\\textbf{G}$raph $\\textbf{O}$ptimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their association pattern. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff ($\\textbf{+14 mIoU}$) and Cityscapes ($\\textbf{+9 mIoU}$) semantic segmentation challenges.  ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hamilton|unsupervised_semantic_segmentation_by_distilling_feature_correspondences", "pdf": "/pdf/585b6a94cde1c9886c51fbaa17688846d5729b69.pdf", "one-sentence_summary": "We use the correlations between self-supervised visual features to perform unsupervised semantic segmentation.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2203.08414/code)", "_bibtex": "@inproceedings{\nhamilton2022unsupervised,\ntitle={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},\nauthor={Mark Hamilton and Zhoutong Zhang and Bharath Hariharan and Noah Snavely and William T. Freeman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SaKO6z6Hl0c}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "TqNsv1TuCX9", "original": "IeGTw9_lB9d", "number": 2269, "cdate": 1632875610927, "mdate": null, "ddate": null, "tcdate": 1632875610927, "tmdate": 1676330565950, "tddate": null, "forum": "TqNsv1TuCX9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning", "authorids": ["~Mark_Hamilton1", "~Scott_Lundberg1", "~Stephanie_Fu1", "~Lei_Zhang23", "~William_T._Freeman1"], "authors": ["Mark Hamilton", "Scott Lundberg", "Stephanie Fu", "Lei Zhang", "William T. Freeman"], "keywords": ["Model Interpretability", "Shapley Values", "Search Engines", "Information Retrieval", "Visual Search", "Similarity Learning", "Metric Learning", "Black-box explanations"], "abstract": "Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate \"fairness\" and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained opaque-box models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures. ", "one-sentence_summary": "We show that cooperative game theory provides an axiomatic characterization of model interpretability for visual search, retrieval, and similarity learning architectures", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hamilton|axiomatic_explanations_for_visual_search_retrieval_and_similarity_learning", "pdf": "/pdf/75f834484ec638f9880a1bd687ced6d577076921.pdf", "_bibtex": "@inproceedings{\nhamilton2022axiomatic,\ntitle={Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning},\nauthor={Mark Hamilton and Scott Lundberg and Stephanie Fu and Lei Zhang and William T. Freeman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TqNsv1TuCX9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "kcwyXtt7yDJ", "original": "JjpQiXy8bl9", "number": 2266, "cdate": 1632875610717, "mdate": null, "ddate": null, "tcdate": 1632875610717, "tmdate": 1697934731856, "tddate": null, "forum": "kcwyXtt7yDJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph-Relational Domain Adaptation", "authorids": ["~Zihao_Xu2", "~Hao_He1", "~Guang-He_Lee1", "~Bernie_Wang1", "~Hao_Wang3"], "authors": ["Zihao Xu", "Hao He", "Guang-He Lee", "Bernie Wang", "Hao Wang"], "keywords": ["Graphs", "Network Topology", "Transfer Learning", "Domain Adaptation", "Adversarial Learning"], "abstract": "Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encoding-conditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented by graphs, and improves upon existing domain adaptation methods on both synthetic and real-world datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|graphrelational_domain_adaptation", "pdf": "/pdf/2b48e401e5c5e2e1cba961a5ef890a969861c810.pdf", "supplementary_material": "/attachment/fc5015d5bf007b274b411fd3ff306a96395d676f.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.03628/code)", "_bibtex": "@inproceedings{\nxu2022graphrelational,\ntitle={Graph-Relational Domain Adaptation},\nauthor={Zihao Xu and Hao He and Guang-He Lee and Bernie Wang and Hao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kcwyXtt7yDJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "LdEhiMG9WLO", "original": "Qv1zIUiroSr", "number": 2257, "cdate": 1632875610087, "mdate": null, "ddate": null, "tcdate": 1632875610087, "tmdate": 1676330566596, "tddate": null, "forum": "LdEhiMG9WLO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions", "authorids": ["~Shaochen_Zhong1", "zhanggq1994@mail.nankai.edu.cn", "nxh239@case.edu", "sxx214@case.edu"], "authors": ["Shaochen Zhong", "Guanqun Zhang", "Ningjia Huang", "Shuai Xu"], "keywords": ["structured pruning", "efficient computing", "parallel computing", "grouped convolution", "lottery ticket", "weights shifting"], "abstract": "Structured pruning methods which are capable of delivering a densely pruned network are among the most popular techniques in the realm of neural network pruning, where most methods prune the original network at a filter or layer level. Although such methods may provide immediate compression and acceleration benefits, we argue that the blanket removal of an entire filter or layer may result in undesired accuracy loss. In this paper, we revisit the idea of kernel pruning (to only prune one or several $k \\times k$ kernels out of a 3D-filter), a heavily overlooked approach under the context of structured pruning. This is because kernel pruning will naturally introduce sparsity to filters within the same convolutional layer \u2014 thus, making the remaining network no longer dense. We address this problem by proposing a versatile grouped pruning framework where we first cluster filters from each convolutional layer into equal-sized groups, prune the grouped kernels we deem unimportant from each filter group, then permute the remaining filters to form a densely grouped convolutional architecture (which also enables the parallel computing capability) for fine-tuning. Specifically, we consult empirical findings from a series of literature regarding $\\textit{Lottery Ticket Hypothesis}$ to determine the optimal clustering scheme per layer, and develop a simple yet cost-efficient greedy approximation algorithm to determine which group kernels to keep within each filter group. Extensive experiments also demonstrate our method often outperforms comparable SOTA methods with lesser data augmentation needed, smaller fine-tuning budget required, and sometimes even much simpler procedure executed (e.g., one-shot v. iterative). Please refer to our GitHub repository (https://github.com/choH/lottery_regulated_grouped_kernel_pruning) for code.", "one-sentence_summary": "A simple yet effective structured pruning framework based on kernel pruning, weights shifting, and grouped convolutions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhong|revisit_kernel_pruning_with_lottery_regulated_grouped_convolutions", "pdf": "/pdf/5b6a1f9771a20353393d13f0eaa86b05e16c80b2.pdf", "_bibtex": "@inproceedings{\nzhong2022revisit,\ntitle={Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions},\nauthor={Shaochen Zhong and Guanqun Zhang and Ningjia Huang and Shuai Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LdEhiMG9WLO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 45}}, {"id": "LedObtLmCjS", "original": "RlG5QDEBzvk", "number": 2256, "cdate": 1632875610016, "mdate": null, "ddate": null, "tcdate": 1632875610016, "tmdate": 1676330566659, "tddate": null, "forum": "LedObtLmCjS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bi-linear Value Networks for Multi-goal Reinforcement Learning", "authorids": ["~Zhang-Wei_Hong1", "~Ge_Yang1", "~Pulkit_Agrawal1"], "authors": ["Zhang-Wei Hong", "Ge Yang", "Pulkit Agrawal"], "keywords": ["Multi-goal reinforcement learning", "universal value function approximator"], "abstract": "Universal value functions are a core component of off-policy multi-goal reinforcement learning. \nThe de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks which lack inductive biases to produce complex interactions between the state s and the goal g. In this work, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, \u03d5(s, g), captures the global relationship between the current state and the goal.\nWe show that our bilinear decomposition scheme improves sample efficiency over the original monolithic value approximators, and transfer better to unseen goals. We demonstrate significant learning speed-up over a variety of tasks on a simulated robot arm, and the challenging task of dexterous manipulation with a Shadow hand.", "pdf": "/pdf/278077713254a379481bd2b7e25393ca3e4758b6.pdf", "one-sentence_summary": "We propose a bilinear value function for multi-goal reinforcement learning and show superior sample efficiency and generalizability.", "supplementary_material": "/attachment/9ee8526dc3095e4a45a82ea941176515fab86e1c.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hong|bilinear_value_networks_for_multigoal_reinforcement_learning", "_bibtex": "@inproceedings{\nyang2022bilinear,\ntitle={Bi-linear Value Networks for Multi-goal Reinforcement Learning},\nauthor={Ge Yang and Zhang-Wei Hong and Pulkit Agrawal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LedObtLmCjS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "BK-4qbGgIE3", "original": "BIMwYwlx6oI", "number": 2252, "cdate": 1632875609735, "mdate": null, "ddate": null, "tcdate": 1632875609735, "tmdate": 1676330566929, "tddate": null, "forum": "BK-4qbGgIE3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "No One Representation to Rule Them All: Overlapping Features of Training Methods", "authorids": ["~Raphael_Gontijo-Lopes1", "~Yann_Dauphin1", "~Ekin_Dogus_Cubuk1"], "authors": ["Raphael Gontijo-Lopes", "Yann Dauphin", "Ekin Dogus Cubuk"], "keywords": ["Representation Learning", "Understanding Deep Learning", "Deep Phenomena", "Diversity", "Novelty", "Features", "Training Methodologies", "Contrastive Learning"], "abstract": "Despite being able to capture a range of features of the data, high accuracy models trained with supervision tend to make similar predictions. This seemingly implies that high-performing models share similar biases regardless of training methodology, which would limit ensembling benefits and render low-accuracy models as having little practical use. Against this backdrop, recent work has developed quite different training techniques, such as large-scale contrastive learning, yielding competitively high accuracy on generalization and robustness benchmarks. This motivates us to revisit the assumption that models necessarily learn similar functions. We conduct a large-scale empirical study of models across hyper-parameters, architectures, frameworks, and datasets. We find that model pairs that diverge more in training methodology display categorically different generalization behavior, producing increasingly uncorrelated errors. We show these models specialize in subdomains of the data, leading to higher ensemble performance: with just 2 models (each with ImageNet accuracy \\~76.5\\%), we can create ensembles with 83.4\\% (+7\\% boost). Surprisingly, we find that even significantly low-accuracy models can be used to improve high-accuracy models. Finally, we show diverging training methodology yield representations that capture overlapping (but not supersetting) feature sets which, when combined, lead to increased downstream performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gontijolopes|no_one_representation_to_rule_them_all_overlapping_features_of_training_methods", "pdf": "/pdf/87ad344628d5996211382d2b966667df428d01f8.pdf", "one-sentence_summary": "We study the effect of training methodology on prediction diversity and show that diverging training setups produce diverse features, uncorrelated errors, and more efficient ensembles.", "_bibtex": "@inproceedings{\ngontijo-lopes2022no,\ntitle={No One Representation to Rule Them All: Overlapping Features of Training Methods},\nauthor={Raphael Gontijo-Lopes and Yann Dauphin and Ekin Dogus Cubuk},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BK-4qbGgIE3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "IfNu7Dr-3fQ", "original": "Kimg3ew2cRj", "number": 2245, "cdate": 1632875609256, "mdate": null, "ddate": null, "tcdate": 1632875609256, "tmdate": 1676330567754, "tddate": null, "forum": "IfNu7Dr-3fQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalized Kernel Thinning", "authorids": ["~Raaz_Dwivedi1", "~Lester_Mackey1"], "authors": ["Raaz Dwivedi", "Lester Mackey"], "keywords": ["coresets", "maximum mean discrepancy", "Markov chain Monte Carlo", "reproducing kernel Hilbert space", "thinning", "compression"], "abstract": "The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a probability distribution more effectively than independent sampling by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth square-root kernel. Here we provide four improvements.  First, we show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RKHS. Second, we show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square-root kernel.  Third, we prove that KT with a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matern, that do not have square-roots. Fourth, we establish that KT applied to a sum of the target and power kernels (a procedure we call KT+) simultaneously inherits the improved MMD guarantees of power KT and the tighter individual function guarantees of target KT.  In our experiments with target KT and KT+, we witness significant improvements in integration error even in 100 dimensions and when compressing challenging differential equation posteriors.", "pdf": "/pdf/65b13d5e7474f351e4b858aeaad4bb355151f0d6.pdf", "one-sentence_summary": "By generalizing kernel thinning, we develop new tools for compressing distributions more effectively than i.i.d. sampling and establish new dimension-free, $O(\\sqrt{\\log n/n})$ improvements over the usual $n^{-1/4}$ Monte Carlo integration error.", "supplementary_material": "/attachment/48b7fb8508b4a8f05d32b6b88dd9b2f62a6855fc.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dwivedi|generalized_kernel_thinning", "code": "", "_bibtex": "@inproceedings{\ndwivedi2022generalized,\ntitle={Generalized Kernel Thinning},\nauthor={Raaz Dwivedi and Lester Mackey},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=IfNu7Dr-3fQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "zf_Ll3HZWgy", "original": "uzIwPAbQyb", "number": 2244, "cdate": 1632875609189, "mdate": null, "ddate": null, "tcdate": 1632875609189, "tmdate": 1697934734912, "tddate": null, "forum": "zf_Ll3HZWgy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How Much Can CLIP Benefit Vision-and-Language Tasks?", "authorids": ["~Sheng_Shen2", "~Liunian_Harold_Li1", "~Hao_Tan1", "~Mohit_Bansal2", "~Anna_Rohrbach1", "~Kai-Wei_Chang1", "~Zhewei_Yao1", "~Kurt_Keutzer3"], "authors": ["Sheng Shen", "Liunian Harold Li", "Hao Tan", "Mohit Bansal", "Anna Rohrbach", "Kai-Wei Chang", "Zhewei Yao", "Kurt Keutzer"], "keywords": [], "abstract": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual encoders, using a relatively small set of manually-annotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image Pre-training), trained on a massive amount of image-caption pairs, has shown a strong zero-shot capability on various vision tasks. To further study the advantage brought by CLIP, we propose to use CLIP as the visual encoder in various V&L models in two typical scenarios: 1) plugging CLIP into task-specific fine-tuning; 2) combining CLIP with V&L pre-training and transferring to downstream tasks. We show that CLIP significantly outperforms widely-used visual encoders trained with in-domain annotated data, such as BottomUp-TopDown. We achieve competitive or better results on diverse V&L tasks, while establishing new state-of-the-art results on Visual Question Answering, Visual Entailment, and V&L Navigation tasks.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shen|how_much_can_clip_benefit_visionandlanguage_tasks", "pdf": "/pdf/f0691be3c885b77bb697dff205313230d0be1163.pdf", "supplementary_material": "/attachment/6be978f711b9dc6cc4892422e52cf8626d5a7ef7.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 8 code implementations](https://www.catalyzex.com/paper/arxiv:2107.06383/code)", "_bibtex": "@inproceedings{\nshen2022how,\ntitle={How Much Can {CLIP} Benefit Vision-and-Language Tasks?},\nauthor={Sheng Shen and Liunian Harold Li and Hao Tan and Mohit Bansal and Anna Rohrbach and Kai-Wei Chang and Zhewei Yao and Kurt Keutzer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zf_Ll3HZWgy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "3tbDrs77LJ5", "original": "hFQOA9hNXzL", "number": 2242, "cdate": 1632875609055, "mdate": null, "ddate": null, "tcdate": 1632875609055, "tmdate": 1676330567758, "tddate": null, "forum": "3tbDrs77LJ5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect", "authorids": ["~Yuqing_Wang3", "~Minshuo_Chen1", "~Tuo_Zhao1", "~Molei_Tao1"], "authors": ["Yuqing Wang", "Minshuo Chen", "Tuo Zhao", "Molei Tao"], "keywords": ["large learning rate", "gradient descent", "matrix factorization", "implicit regularization", "convergence", "balancing", "alignment"], "abstract": "Recent empirical advances show that training deep models with large learning rate often improves generalization performance. However, theoretical justifications on the benefits of large learning rate are highly limited, due to challenges in analysis. In this paper, we consider using Gradient Descent (GD) with a large learning rate on a homogeneous matrix factorization problem, i.e., $\\min_{X, Y} \\|A - XY^\\top\\|_{\\sf F}^2$. We prove a convergence theory for constant large learning rates well beyond $2/L$, where $L$ is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously establish an implicit bias of GD induced by such a large learning rate, termed `balancing', meaning that magnitudes of $X$ and $Y$ at the limit of GD iterations will be close even if their initialization is significantly unbalanced. Numerical experiments are provided to support our theory.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|large_learning_rate_tames_homogeneity_convergence_and_balancing_effect", "pdf": "/pdf/1e0a0df7af8a11a430889c2866c6de87982e1227.pdf", "one-sentence_summary": "Large learning rate well beyond 2/L provably induces an implicit regularization effect of balancing in gradient descent for matrix factorization.", "supplementary_material": "/attachment/7ba3e0b83326fa16426f54f6138a7355f584d7ce.zip", "_bibtex": "@inproceedings{\nwang2022large,\ntitle={Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect},\nauthor={Yuqing Wang and Minshuo Chen and Tuo Zhao and Molei Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3tbDrs77LJ5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "l5aSHXi8jG5", "original": "dTBX4EvlEF", "number": 2241, "cdate": 1632875608988, "mdate": null, "ddate": null, "tcdate": 1632875608988, "tmdate": 1676330567842, "tddate": null, "forum": "l5aSHXi8jG5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Demystifying Limited Adversarial Transferability in Automatic Speech Recognition Systems", "authorids": ["~Hadi_Abdullah1", "~Aditya_Karlekar1", "~Vincent_Bindschaedler1", "~Patrick_Traynor1"], "authors": ["Hadi Abdullah", "Aditya Karlekar", "Vincent Bindschaedler", "Patrick Traynor"], "keywords": ["optimization attacks", "transferability", "adversarial machine learning"], "abstract": "The targeted transferability of adversarial samples enables attackers to exploit black-box models in the real-world. The most popular method to produce these adversarial samples is optimization attacks, which have been shown to achieve a high level of transferability in some domains. However, recent research has demonstrated that these attack samples fail to transfer when applied to Automatic Speech Recognition Systems (ASRs). In this paper, we investigate factors preventing this transferability via exhaustive experimentation. To do so, we perform an ablation study on each stage of the ASR pipeline. We discover and quantify six factors (i.e., input type, MFCC, RNN, output type, and vocabulary and sequence sizes) that impact the targeted transferability of optimization attacks against ASRs. Future research can leverage our findings to build ASRs that are more robust to other transferable attack types (e.g., signal processing attacks), or to modify architectures in other domains to reduce their exposure to targeted transferability of optimization attacks.", "one-sentence_summary": "Uncover factors that limit transferability of the popular optimization attacks in the automatic speech recognition systems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "abdullah|demystifying_limited_adversarial_transferability_in_automatic_speech_recognition_systems", "pdf": "/pdf/1a5b08e254c26f966237f4cd25b8f69482718015.pdf", "data": "", "_bibtex": "@inproceedings{\nabdullah2022demystifying,\ntitle={Demystifying Limited Adversarial Transferability in Automatic Speech Recognition Systems},\nauthor={Hadi Abdullah and Aditya Karlekar and Vincent Bindschaedler and Patrick Traynor},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=l5aSHXi8jG5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "kSwqMH0zn1F", "original": "LYLaAvzPgtT", "number": 2231, "cdate": 1632875608302, "mdate": null, "ddate": null, "tcdate": 1632875608302, "tmdate": 1697934736103, "tddate": null, "forum": "kSwqMH0zn1F", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication", "authorids": ["~Cheng_Wan2", "~Youjie_Li1", "~Cameron_R._Wolfe1", "~Anastasios_Kyrillidis2", "~Nam_Sung_Kim3", "~Yingyan_Lin1"], "authors": ["Cheng Wan", "Youjie Li", "Cameron R. Wolfe", "Anastasios Kyrillidis", "Nam Sung Kim", "Yingyan Lin"], "keywords": ["Graph Neural Networks", "Graph Convolutional Networks", "Distributed Training", "Asynchronous Training", "Full-Graph Training", "Large-Graph Training", "Stale Features"], "abstract": "Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data, and training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and feature gradients among partitions for every GCN layer during each training iteration, limiting the achievable training efficiency and model scalability. To this end, we propose PipeGCN, a simple yet effective scheme that hides the communication overhead by pipelining inter-partition communication with intra-partition computation. It is non-trivial to pipeline for efficient GCN training, as communicated node features/gradients will become stale and thus can harm the convergence, negating the pipeline benefit. Notably, little is known regarding the convergence rate of GCN training with both stale features and stale feature gradients. This work not only provides a theoretical convergence analysis but also finds the convergence rate of PipeGCN to be close to that of the vanilla distributed GCN training without any staleness. Furthermore, we develop a smoothing method to further improve PipeGCN's convergence. Extensive experiments show that PipeGCN can largely boost the training throughput (1.7\u00d7~28.5\u00d7) while achieving the same accuracy as its vanilla counterpart and existing full-graph training methods. The code is available at https://github.com/RICE-EIC/PipeGCN.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wan|pipegcn_efficient_fullgraph_training_of_graph_convolutional_networks_with_pipelined_feature_communication", "pdf": "/pdf/ba8be163ee26a6f4bd01d0d635bc721c022fd88a.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.10428/code)", "_bibtex": "@inproceedings{\nwan2022pipegcn,\ntitle={Pipe{GCN}: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication},\nauthor={Cheng Wan and Youjie Li and Cameron R. Wolfe and Anastasios Kyrillidis and Nam Sung Kim and Yingyan Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kSwqMH0zn1F}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "7inCJ3MhXt3", "original": "edW3ktiSErB", "number": 2225, "cdate": 1632875607943, "mdate": null, "ddate": null, "tcdate": 1632875607943, "tmdate": 1676330568267, "tddate": null, "forum": "7inCJ3MhXt3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Neural Contextual Bandits through Perturbed Rewards", "authorids": ["~Yiling_Jia1", "~Weitong_ZHANG1", "~Dongruo_Zhou1", "~Quanquan_Gu1", "~Hongning_Wang1"], "authors": ["Yiling Jia", "Weitong ZHANG", "Dongruo Zhou", "Quanquan Gu", "Hongning Wang"], "keywords": ["contextual bandit", "neural bandit"], "abstract": "Thanks to the power of representation learning, neural contextual bandit algorithms demonstrate remarkable performance improvement against their classical counterparts. But because their exploration has to be performed in the entire neural network parameter space to obtain nearly optimal regret, the resulting computational cost is prohibitively high.  \nWe propose to perturb the rewards when updating the neural network to eliminate the need of explicit exploration and the corresponding computational overhead. We prove that a $\\tilde{O}(\\tilde{d}\\sqrt{T})$ regret upper bound is still achievable under standard regularity conditions, where $T$ is the number of rounds of interactions and $\\tilde{d}$ is the effective dimension of a neural tangent kernel matrix. \nExtensive comparisons with several benchmark contextual bandit algorithms, including two recent neural contextual bandit models, demonstrate the effectiveness and computational efficiency of our proposed neural bandit algorithm.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jia|learning_neural_contextual_bandits_through_perturbed_rewards", "pdf": "/pdf/03d7a2872a95102e45f49ccf6a1bd08bab2a19d8.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\njia2022learning,\ntitle={Learning Neural Contextual Bandits through Perturbed Rewards},\nauthor={Yiling Jia and Weitong ZHANG and Dongruo Zhou and Quanquan Gu and Hongning Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7inCJ3MhXt3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "MeeQkFYVbzW", "original": "AsLyn9c5s0g", "number": 2217, "cdate": 1632875607348, "mdate": null, "ddate": null, "tcdate": 1632875607348, "tmdate": 1697934737438, "tddate": null, "forum": "MeeQkFYVbzW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Adversarial Unlearning of Backdoors via Implicit Hypergradient", "authorids": ["~Yi_Zeng3", "~Si_Chen5", "~Won_Park1", "~Zhuoqing_Mao1", "~Ming_Jin2", "~Ruoxi_Jia1"], "authors": ["Yi Zeng", "Si Chen", "Won Park", "Zhuoqing Mao", "Ming Jin", "Ruoxi Jia"], "keywords": ["backdoor defense", "backdoor removal", "backdoor", "minimax", "implicit hypergradient"], "abstract": "We propose a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. We propose the Implicit Backdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which breaks down the minimax into separate inner and outer problems, our algorithm utilizes the implicit hypergradient to account for the interdependence between inner and outer optimization. We theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data. In our evaluation, we compare I-BAU with six state-of-art backdoor defenses on eleven backdoor attacks over two datasets and various attack settings, including the common setting where the attacker targets one class as well as important but underexplored settings where multiple classes are targeted. I-BAU's performance is comparable to and most often significantly better than the best baseline. Particularly, its performance is more robust to the variation on triggers, attack settings, poison ratio, and clean data size. Moreover, I-BAU requires less computation to take effect; particularly, it is more than $13\\times$ faster than the most efficient baseline in the single-target attack setting. Furthermore, it can remain effective in the extreme case where the defender can only access 100 clean samples---a setting where all the baselines fail to produce acceptable results.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zeng|adversarial_unlearning_of_backdoors_via_implicit_hypergradient", "pdf": "/pdf/6aeb6e81c9d0eadbb4cfbefb6caac0f155d561ea.pdf", "one-sentence_summary": "A minimax formulation of backdoor removal and an implicit gradient-based solver surpasses the state-of-art methods' best results in higher efficacy, efficiency, robustness to variations in triggers, settings, poison ratio, and clean data size.", "supplementary_material": "/attachment/a03a5fae63542d6c13f448faa64a8adfc618ac91.zip", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.03735/code)", "_bibtex": "@inproceedings{\nzeng2022adversarial,\ntitle={Adversarial Unlearning of Backdoors via Implicit Hypergradient},\nauthor={Yi Zeng and Si Chen and Won Park and Zhuoqing Mao and Ming Jin and Ruoxi Jia},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MeeQkFYVbzW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 41}}, {"id": "hjd-kcpDpf2", "original": "DC5AQFrUKEm", "number": 2216, "cdate": 1632875607273, "mdate": null, "ddate": null, "tcdate": 1632875607273, "tmdate": 1676330568963, "tddate": null, "forum": "hjd-kcpDpf2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Maximizing Ensemble Diversity in Deep Reinforcement Learning", "authorids": ["~Hassam_Sheikh1", "~Mariano_Phielipp2", "~Ladislau_Boloni1"], "authors": ["Hassam Sheikh", "Mariano Phielipp", "Ladislau Boloni"], "keywords": ["Ensemble Based Reinforcement Learning", "Ensemble Diversity"], "abstract": "Modern deep reinforcement learning (DRL) has been successful in solving a range of challenging sequential decision-making problems. Most of these algorithms use an ensemble of neural networks as their backbone structure and benefit from the diversity among the neural networks to achieve optimal results. Unfortunately, the members of the ensemble can converge to the same point either the parametric space or representation space during the training phase, therefore, losing all the leverage of an ensemble. In this paper, we describe Maximize Ensemble Diversity in Reinforcement Learning (MED-RL), a set of regularization methods inspired from the economics and consensus optimization to improve diversity in the ensemble-based deep reinforcement learning methods by encouraging inequality between the networks during training. We integrated MED-RL in five of the most common ensemble-based deep RL algorithms for both continuous and discrete control tasks and evaluated on six Mujoco environments and six Atari games. Our results show that MED-RL augmented algorithms outperform their un-regularized counterparts significantly and in some cases achieved more than 300$\\%$ in performance gains.", "one-sentence_summary": "Maximizing diversity in neural network improves performance ensemble based reinforcement learning", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sheikh|maximizing_ensemble_diversity_in_deep_reinforcement_learning", "pdf": "/pdf/01f7a1ad9dd4d2d9285af9a2c926b0cc1a282f4f.pdf", "_bibtex": "@inproceedings{\nsheikh2022maximizing,\ntitle={Maximizing Ensemble Diversity in Deep Reinforcement Learning},\nauthor={Hassam Sheikh and Mariano Phielipp and Ladislau Boloni},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hjd-kcpDpf2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "wTTjnvGphYj", "original": "TA4gduh7Cc", "number": 2215, "cdate": 1632875607198, "mdate": null, "ddate": null, "tcdate": 1632875607198, "tmdate": 1697934737715, "tddate": null, "forum": "wTTjnvGphYj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph Neural Networks with Learnable Structural and Positional Representations", "authorids": ["~Vijay_Prakash_Dwivedi1", "~Anh_Tuan_Luu2", "~Thomas_Laurent1", "~Yoshua_Bengio1", "~Xavier_Bresson6"], "authors": ["Vijay Prakash Dwivedi", "Anh Tuan Luu", "Thomas Laurent", "Yoshua Bengio", "Xavier Bresson"], "keywords": ["graph neural networks", "graph representation learning", "transformers", "positional encoding"], "abstract": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call \\texttt{LSPE} (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from $1.79\\%$ up to $64.14\\%$ when considering learnable PE for both GNN classes.", "one-sentence_summary": "We propose a novel GNN architecture (LSPE) which decouples structural and positional representations to make easy for the network to learn the two essential properties.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dwivedi|graph_neural_networks_with_learnable_structural_and_positional_representations", "pdf": "/pdf/d2f6438ccb5d7ec7570953e93a19f994a5894c93.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.07875/code)", "_bibtex": "@inproceedings{\ndwivedi2022graph,\ntitle={Graph Neural Networks with Learnable Structural and Positional Representations},\nauthor={Vijay Prakash Dwivedi and Anh Tuan Luu and Thomas Laurent and Yoshua Bengio and Xavier Bresson},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wTTjnvGphYj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "085y6YPaYjP", "original": "b7eTpsgrEti", "number": 2188, "cdate": 1632875605100, "mdate": null, "ddate": null, "tcdate": 1632875605100, "tmdate": 1676330570252, "tddate": null, "forum": "085y6YPaYjP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Zero-Shot Self-Supervised Learning for MRI Reconstruction", "authorids": ["~Burhaneddin_Yaman1", "~Seyed_Amir_Hossein_Hosseini1", "~Mehmet_Akcakaya1"], "authors": ["Burhaneddin Yaman", "Seyed Amir Hossein Hosseini", "Mehmet Akcakaya"], "keywords": ["Zero-shot learning", "Self-supervised learning", "MRI Reconstruction", "Transfer learning", "Physics-guided deep learning"], "abstract": "Deep learning (DL) has emerged as a powerful tool for accelerated MRI reconstruction, but often necessitates a database of fully-sampled measurements for training. Recent self-supervised and unsupervised learning approaches enable training without fully-sampled data. However, a database of undersampled measurements may not be available in many scenarios, especially for scans involving contrast or translational acquisitions in development. Moreover, recent studies show that database-trained models may not generalize well when the unseen measurements differ in terms of sampling pattern, acceleration rate, SNR, image contrast, and anatomy. Such challenges necessitate a new methodology to enable subject-specific DL MRI reconstruction without external training datasets, since it is clinically imperative to provide high-quality reconstructions that can be used to identify lesions/disease for $\\textit{every individual}$. In this work, we propose a zero-shot self-supervised learning approach to perform subject-specific accelerated DL MRI reconstruction to tackle these issues. The proposed approach partitions the available measurements from a single scan into three disjoint sets. Two of these sets are used to enforce data consistency and define loss during training for self-supervision, while the last set serves to self-validate, establishing an early stopping criterion. In the presence of models pre-trained on a database with different image characteristics, we show that the proposed approach can be combined with transfer learning for faster convergence time and reduced computational complexity.", "one-sentence_summary": " Zero-shot self-supervised learning to perform robust subject-specific MRI reconstruction", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yaman|zeroshot_selfsupervised_learning_for_mri_reconstruction", "pdf": "/pdf/72bc6e074441b3be4d8499c16c02fbae92fad23f.pdf", "_bibtex": "@inproceedings{\nyaman2022zeroshot,\ntitle={Zero-Shot Self-Supervised Learning for {MRI} Reconstruction},\nauthor={Burhaneddin Yaman and Seyed Amir Hossein Hosseini and Mehmet Akcakaya},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=085y6YPaYjP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "mwdfai8NBrJ", "original": "nmx66jb0wvs", "number": 2179, "cdate": 1632875604277, "mdate": null, "ddate": null, "tcdate": 1632875604277, "tmdate": 1697934741609, "tddate": null, "forum": "mwdfai8NBrJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Policy Smoothing for Provably Robust Reinforcement Learning", "authorids": ["~Aounon_Kumar1", "~Alexander_Levine2", "~Soheil_Feizi2"], "authors": ["Aounon Kumar", "Alexander Levine", "Soheil Feizi"], "keywords": ["Reinforcement Learning", "Provable Adversarial Robustness", "Randomized Smoothing"], "abstract": "The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on $\\textit{static}$ supervised learning tasks such as image classification. However, DNNs have been used extensively in real-world $\\textit{adaptive}$ tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in provable robustness in RL seek to certify the behaviour of the victim policy at every time-step against a non-adaptive adversary using methods developed for the static setting. But in the real world, an RL adversary can infer the defense strategy used by the victim agent by observing the states, actions, etc. from previous time-steps and adapt itself to produce stronger attacks in future steps (e.g., by focusing more on states critical to the agent's performance). We present an efficient procedure, designed specifically to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step. Focusing on randomized smoothing based defenses, our main theoretical contribution is to prove an $\\textit{adaptive version}$ of the Neyman-Pearson Lemma -- a key lemma for smoothing-based certificates -- where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Building on this result, we propose $\\textit{policy smoothing}$ where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Our robustness certificates guarantee that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack. We show that our certificates are $\\textit{tight}$ by constructing a worst-case scenario that achieves the bounds derived in our analysis. Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice.\n", "one-sentence_summary": "A provable adversarial robustness technique for reinforcement learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kumar|policy_smoothing_for_provably_robust_reinforcement_learning", "pdf": "/pdf/b1ed375c6d8559126ca3c590cf47feff1ae81aeb.pdf", "supplementary_material": "/attachment/3b9b7584c1f052d05681c01e9c2d48b380c17c27.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2106.11420/code)", "_bibtex": "@inproceedings{\nkumar2022policy,\ntitle={Policy Smoothing for Provably Robust Reinforcement Learning},\nauthor={Aounon Kumar and Alexander Levine and Soheil Feizi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mwdfai8NBrJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "gICys3ITSmj", "original": "K9SAitwk3Hk", "number": 2177, "cdate": 1632875604120, "mdate": null, "ddate": null, "tcdate": 1632875604120, "tmdate": 1676330570739, "tddate": null, "forum": "gICys3ITSmj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Close Relationship Between Contrastive Learning and Meta-Learning", "authorids": ["~Renkun_Ni1", "~Manli_Shu1", "~Hossein_Souri1", "~Micah_Goldblum1", "~Tom_Goldstein1"], "authors": ["Renkun Ni", "Manli Shu", "Hossein Souri", "Micah Goldblum", "Tom Goldstein"], "keywords": ["meta-learning", "contrastive learning", "self-supervised learning"], "abstract": "Contrastive learning has recently taken off as a paradigm for learning from unlabeled data. In this paper, we discuss the close relationship between contrastive learning and meta-learning under a certain task distribution. We complement this observation by showing that established meta-learning methods, such as Prototypical Networks, achieve comparable performance to SimCLR when paired with this task distribution. This relationship can be leveraged by taking established techniques from meta-learning, such as task-based data augmentation, and showing that they benefit contrastive learning as well. These tricks also benefit state-of-the-art self-supervised learners without using negative pairs such as BYOL, which achieves 94.6\\% accuracy on CIFAR-10 using a self-supervised ResNet-18 feature extractor trained with our meta-learning tricks.  We conclude that existing advances designed for contrastive learning or meta-learning can be exploited to benefit the other, and it is better for contrastive learning researchers to take lessons from the meta-learning literature (and vice-versa) than to reinvent the wheel. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ni|the_close_relationship_between_contrastive_learning_and_metalearning", "pdf": "/pdf/50e04918f102be63b77173d7b13e7c3a95d4d4b7.pdf", "one-sentence_summary": "We discuss the close relationship between contrastive learning and meta-learning, and we propose a meta-learning framework for self-supervised learning (SSL) along with meta-specific methods to improve contrastive learning performance for SSL.", "supplementary_material": "/attachment/9609c67c9ec522719fc98618be3f0f55b313a84b.zip", "data": "", "_bibtex": "@inproceedings{\nni2022the,\ntitle={The Close Relationship Between Contrastive Learning and Meta-Learning},\nauthor={Renkun Ni and Manli Shu and Hossein Souri and Micah Goldblum and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gICys3ITSmj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "rS9-7AuPKWK", "original": "njTcFdNl6fN", "number": 2171, "cdate": 1632875603447, "mdate": null, "ddate": null, "tcdate": 1632875603447, "tmdate": 1676330571313, "tddate": null, "forum": "rS9-7AuPKWK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Understanding Generalization via Decomposing Excess Risk Dynamics", "authorids": ["~Jiaye_Teng2", "~Jianhao_Ma1", "~Yang_Yuan4"], "authors": ["Jiaye Teng", "Jianhao Ma", "Yang Yuan"], "keywords": ["generalization", "excess risk", "stability", "dynamics"], "abstract": "Generalization is one of the fundamental issues in machine learning. However, traditional techniques like uniform convergence may be unable to explain generalization under overparameterization \\citep{nagarajan2019uniform}. As alternative approaches, techniques based on stability analyze the training dynamics and derive algorithm-dependent generalization bounds. Unfortunately, the stability-based bounds are still far from explaining the surprising generalization in deep learning since neural networks usually suffer from unsatisfactory stability. This paper proposes a novel decomposition framework to improve the stability-based bounds via a more fine-grained analysis of the signal and noise, inspired by the observation that neural networks converge relatively slowly when fitting noise (which indicates better stability). Concretely, we decompose the excess risk dynamics and apply the stability-based bound only on the noise component. The decomposition framework performs well in both linear regimes (overparameterized linear regression) and non-linear regimes (diagonal matrix recovery). Experiments on neural networks verify the utility of the decomposition framework.", "one-sentence_summary": "This paper proposes a decomposition framework to improve the stability-based bound. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "teng|towards_understanding_generalization_via_decomposing_excess_risk_dynamics", "pdf": "/pdf/fb22a53f794740f074cfc57fbfeb13cb402a0dc1.pdf", "_bibtex": "@inproceedings{\nteng2022towards,\ntitle={Towards Understanding Generalization via Decomposing Excess Risk Dynamics},\nauthor={Jiaye Teng and Jianhao Ma and Yang Yuan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rS9-7AuPKWK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "ATUh28lnSuW", "original": "RDNvbm8yDq2", "number": 2169, "cdate": 1632875603283, "mdate": null, "ddate": null, "tcdate": 1632875603283, "tmdate": 1697934743225, "tddate": null, "forum": "ATUh28lnSuW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction", "authorids": ["~Mingyue_Tang1", "~Pan_Li2", "~Carl_Yang1"], "authors": ["Mingyue Tang", "Pan Li", "Carl Yang"], "keywords": ["graph representation learning", "unsupervised learning", "autoencoder", "wasserstein distance"], "abstract": "Graph neural networks (GNNs) have drawn significant research attention recently, mostly under the setting of semi-supervised learning. When task-agnostic representations are preferred or supervision is simply unavailable, the auto-encoder framework comes in handy with a natural graph reconstruction objective for unsupervised GNN training. However, existing graph auto-encoders are designed to reconstruct the direct links, so GNNs trained in this way are only optimized towards proximity-oriented graph mining tasks, and will fall short when the topological structures matter. In this work, we revisit the graph encoding process of GNNs which essentially learns to encode the neighborhood information of each node into an embedding vector, and propose a novel graph decoder to reconstruct the entire neighborhood information regarding both proximity and structure via Neighborhood Wasserstein Reconstruction (NWR). Specifically, from the GNN embedding of each node, NWR jointly predicts its node degree and neighbor feature distribution, where the distribution prediction adopts an optimal-transport loss based on the Wasserstein distance. Extensive experiments on both synthetic and real-world network datasets show that the unsupervised node representations learned with NWR have much more advantageous in structure-oriented graph mining tasks, while also achieving competitive performance in proximity-oriented ones.", "one-sentence_summary": "We study unsupervised graph representation learning and propose a novel decoder based on neighborhood reconstruction with Wasserstein distance to facilitate the GNN encoding of entire neighborhood information beyond direct links.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tang|graph_autoencoder_via_neighborhood_wasserstein_reconstruction", "pdf": "/pdf/f6c2facccd48113154042dcd9e300784da586675.pdf", "supplementary_material": "/attachment/84a6ffeded2cf63f97ae0f08c294e37de61a18c3.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.09025/code)", "_bibtex": "@inproceedings{\ntang2022graph,\ntitle={Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction},\nauthor={Mingyue Tang and Pan Li and Carl Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ATUh28lnSuW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "nRj0NcmSuxb", "original": "5mh5XrYXuQo", "number": 2158, "cdate": 1632875602038, "mdate": null, "ddate": null, "tcdate": 1632875602038, "tmdate": 1676330571809, "tddate": null, "forum": "nRj0NcmSuxb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FairCal: Fairness Calibration for Face Verification", "authorids": ["~Tiago_Salvador1", "~Stephanie_Cairns2", "~Vikram_Voleti1", "~Noah_Marshall1", "~Adam_M_Oberman1"], "authors": ["Tiago Salvador", "Stephanie Cairns", "Vikram Voleti", "Noah Marshall", "Adam M Oberman"], "keywords": ["face verification", "bias", "fairness", "clustering", "calibration"], "abstract": "Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face. As a result, these models can disproportionately and negatively impact minority groups, particularly when used by law enforcement. The majority of bias reduction methods have several drawbacks: they use an end-to-end retraining approach, may not be feasible due to privacy issues, and often reduce accuracy. An alternative approach is post-processing methods that build fairer decision classifiers using the features of pre-trained models, thus avoiding the cost of retraining. However, they still have drawbacks: they reduce accuracy (AGENDA, FTC), or require retuning for different false positive rates (FSN). In this work, we introduce the Fairness Calibration (FairCal) method, a post-training approach that simultaneously: (i) increases model accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated probabilities, (iii) significantly reduces the gap in the false positive rates, (iv) does not require knowledge of the sensitive attribute, and (v) does not require retraining, training an additional model or retuning. We apply it to the task of Face Verification, and obtain state-of-the-art results with all the above advantages.", "one-sentence_summary": "We calibrate face verification models for fairness, without use of the sensitive attribute, without the need for retraining. This leads to SOTA accuracy, fairness calibration, and equal FPRs across subgroups.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "salvador|faircal_fairness_calibration_for_face_verification", "pdf": "/pdf/ddb357dbb2226fb417398315a7b416c8f611f59b.pdf", "supplementary_material": "/attachment/fe191ae42c33df9cd472f8bbf4d08425aa9d684f.zip", "data": "", "code": "", "_bibtex": "@inproceedings{\nsalvador2022faircal,\ntitle={FairCal: Fairness Calibration for Face Verification},\nauthor={Tiago Salvador and Stephanie Cairns and Vikram Voleti and Noah Marshall and Adam M Oberman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nRj0NcmSuxb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "k7-s5HSSPE5", "original": "LVBjBjCxUs", "number": 2156, "cdate": 1632875601892, "mdate": null, "ddate": null, "tcdate": 1632875601892, "tmdate": 1676330571853, "tddate": null, "forum": "k7-s5HSSPE5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations", "authorids": ["~Ruicheng_Xian1", "~Heng_Ji3", "~Han_Zhao1"], "authors": ["Ruicheng Xian", "Heng Ji", "Han Zhao"], "keywords": ["cross-lingual transfer", "unsupervised cross-lingual learning", "multilingual neural language model", "domain adaptation"], "abstract": "Recent advances in neural modeling have produced deep multilingual language models capable of extracting cross-lingual knowledge from non-parallel texts and enabling zero-shot downstream transfer. While their success is often attributed to shared representations, quantitative analyses are limited. Towards a better understanding, through empirical analyses, we show that the invariance of feature representations across languages\u2014an effect of shared representations\u2014strongly correlates with transfer performance. We also observe that distributional shifts in class priors between source and target language task data negatively affect performance, a largely overlooked issue that could cause negative transfer with existing unsupervised approaches. Based on these findings, we propose and evaluate a method for unsupervised transfer, called importance-weighted domain alignment (IWDA), that performs representation alignment with prior shift estimation and correction using unlabeled target language task data. Experiments demonstrate its superiority under large prior shifts, and show further performance gains when combined with existing semi-supervised learning techniques.", "one-sentence_summary": "We propose an importance-weighted domain adaptation method for unsupervised cross-lingual learning that is effective at correcting class prior shifts, a distributional property that negatively affects transfer performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xian|crosslingual_transfer_with_classweighted_languageinvariant_representations", "pdf": "/pdf/c75daaf7c5ca8ed7ab01e92c4cc16d55f5d6aff5.pdf", "_bibtex": "@inproceedings{\nxian2022crosslingual,\ntitle={Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations},\nauthor={Ruicheng Xian and Heng Ji and Han Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=k7-s5HSSPE5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "PgNEYaIc81Q", "original": "aG6KFJ1qBhG", "number": 2155, "cdate": 1632875601721, "mdate": null, "ddate": null, "tcdate": 1632875601721, "tmdate": 1697934743852, "tddate": null, "forum": "PgNEYaIc81Q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ComPhy: Compositional Physical Reasoning of Objects and Events from Videos", "authorids": ["~Zhenfang_Chen1", "~Kexin_Yi1", "~Yunzhu_Li1", "~Mingyu_Ding1", "~Antonio_Torralba1", "~Joshua_B._Tenenbaum1", "~Chuang_Gan1"], "authors": ["Zhenfang Chen", "Kexin Yi", "Yunzhu Li", "Mingyu Ding", "Antonio Torralba", "Joshua B. Tenenbaum", "Chuang Gan"], "keywords": ["Compositional", "Intutive Physics", "Video Reasoning", "Neural-Symbolic"], "abstract": "Objects' motions in nature are governed by complex interactions and their properties. While some properties, such as shape and material, can be identified via the object's visual appearances, others like mass and electric charge are not directly visible. The compositionality between the visible and hidden properties poses unique challenges for AI models to reason from the physical world, whereas humans can effortlessly infer them with limited observations. Existing studies on video reasoning mainly focus on visually observable elements such as object appearance, movement, and contact interaction. In this paper, we take an initial step to highlight the importance of inferring the hidden physical properties not directly observable from visual appearances, by introducing the Compositional Physical Reasoning (ComPhy) dataset. For a given set of objects, ComPhy includes few videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions posted on one of the videos. Evaluation results of several state-of-the-art video reasoning models on ComPhy show unsatisfactory performance as they fail to capture these hidden properties. We further propose an oracle neural-symbolic framework named Compositional Physics Learner (CPL), combining visual perception, physical property learning, dynamic prediction, and symbolic execution into a unified framework. CPL can effectively identify objects' physical properties from their interactions and predict their dynamics to answer questions. ", "one-sentence_summary": "We introduce a new dataset for Compositional Physical Reasoning", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|comphy_compositional_physical_reasoning_of_objects_and_events_from_videos", "pdf": "/pdf/2609709ef581b49b8b74d7d8cfd86d47897465ef.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2205.01089/code)", "_bibtex": "@inproceedings{\nchen2022comphy,\ntitle={ComPhy: Compositional Physical Reasoning of Objects and Events from Videos},\nauthor={Zhenfang Chen and Kexin Yi and Yunzhu Li and Mingyu Ding and Antonio Torralba and Joshua B. Tenenbaum and Chuang Gan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PgNEYaIc81Q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "ecH2FKaARUp", "original": "bzv-9kbWmmA", "number": 2154, "cdate": 1632875601639, "mdate": null, "ddate": null, "tcdate": 1632875601639, "tmdate": 1676330572030, "tddate": null, "forum": "ecH2FKaARUp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "An Information Fusion Approach to Learning with Instance-Dependent Label Noise", "authorids": ["~Zhimeng_Jiang1", "~Kaixiong_Zhou1", "~Zirui_Liu1", "~Li_Li11", "~Rui_Chen4", "~Soo-Hyun_Choi1", "~Xia_Hu4"], "authors": ["Zhimeng Jiang", "Kaixiong Zhou", "Zirui Liu", "Li Li", "Rui Chen", "Soo-Hyun Choi", "Xia Hu"], "keywords": ["Instance-dependent label noise", "posterior transition matrix", "statiscally consistent classifier"], "abstract": "Instance-dependent label noise (IDN) widely exists in real-world datasets and usually misleads the training of deep neural networks. Noise transition matrix (NTM) (i.e., the probability that clean labels flip into noisy labels) is used to characterize the label noise and can be adopted to bridge the gap between clean and noisy underlying data distributions. However, most instances are long-tail, i.e., the number of occurrences of each instance is usually limited, which leads to the gap between the underlying distribution and the empirical distribution. Therefore, the genuine problem caused by IDN is \\emph{empirical}, instead of underlying, \\emph{data distribution mismatch} during training. To directly tackle the empirical distribution mismatch problem, we propose \\emph{posterior transition matrix} (PTM) to posteriorly model label noise given limited observed noisy labels, which achieves \\emph{statistically consistent classifiers}. Note that even if an instance is corrupted by the same NTM, the intrinsic randomness incurs different noisy labels, and thus requires different correction methods. Motivated by this observation, we propose an \\textbf{I}nformation \\textbf{F}usion (IF) approach to fine-tune the NTM based on the estimated PTM. Specifically, we adopt the noisy labels and model predicted probabilities to estimate the PTM and then correct the NTM in \\emph{forward propagation}. Empirical evaluations on synthetic and real-world datasets demonstrate that our method is superior to the state-of-the-art approaches, and achieves more stable training for instance-dependent label noise. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|an_information_fusion_approach_to_learning_with_instancedependent_label_noise", "pdf": "/pdf/a975e6d9572bb58e97d4302f7bd2b003d925154b.pdf", "one-sentence_summary": "This work is the first time to realize and bridge the gap between clean and noisy empirical distribution mismatch.", "_bibtex": "@inproceedings{\njiang2022an,\ntitle={An Information Fusion Approach to Learning with Instance-Dependent Label Noise},\nauthor={Zhimeng Jiang and Kaixiong Zhou and Zirui Liu and Li Li and Rui Chen and Soo-Hyun Choi and Xia Hu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ecH2FKaARUp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 29}}, {"id": "rFJWoYoxrDB", "original": "8QqOWdQTfTn", "number": 2149, "cdate": 1632875601109, "mdate": null, "ddate": null, "tcdate": 1632875601109, "tmdate": 1697934744500, "tddate": null, "forum": "rFJWoYoxrDB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Redundancy and Diversity in Cell-based Neural Architecture Search", "authorids": ["~Xingchen_Wan1", "~Binxin_Ru1", "~Pedro_M_Esperan\u00e7a1", "~Zhenguo_Li1"], "authors": ["Xingchen Wan", "Binxin Ru", "Pedro M Esperan\u00e7a", "Zhenguo Li"], "keywords": ["NAS", "machine learning architectures", "AutoML"], "abstract": "Searching for the architecture cells is a dominant paradigm in NAS. However, little attention has been devoted to the analysis of the cell-based search spaces even though it is highly important for the continual development of NAS. \nIn this work, we conduct an empirical post-hoc analysis of architectures from the popular cell-based search spaces and find that the existing search spaces contain a high degree of redundancy: the architecture performance is less sensitive to changes at large parts of the cells, and universally adopted design rules, like the explicit search for a reduction cell, significantly increase the complexities but have very limited impact on the performance.\nAcross architectures found by a diverse set of search strategies, we consistently find that the parts of the cells that do matter for architecture performance often follow similar and simple patterns. By constraining cells to include these patterns, randomly sampled architectures can match or even outperform the state of the art.\nThese findings cast doubts into our ability to discover truly novel architectures in the existing cell-based search spaces and, inspire our suggestions for improvement to guide future NAS research.\nCode is available at https://github.com/xingchenwan/cell-based-NAS-analysis.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wan|on_redundancy_and_diversity_in_cellbased_neural_architecture_search", "pdf": "/pdf/9f2dd3fa246906ee1dfabf21cda76b01d386c017.pdf", "one-sentence_summary": "We analyse and explore the redundancies and diversity of popular cell-based search spaces in NAS.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.08887/code)", "_bibtex": "@inproceedings{\nwan2022on,\ntitle={On Redundancy and Diversity in Cell-based Neural Architecture Search},\nauthor={Xingchen Wan and Binxin Ru and Pedro M Esperan{\\c{c}}a and Zhenguo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rFJWoYoxrDB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 30}}, {"id": "U0k7XNTiFEq", "original": "DzjTcv7xFSM", "number": 2146, "cdate": 1632875600725, "mdate": null, "ddate": null, "tcdate": 1632875600725, "tmdate": 1676330572445, "tddate": null, "forum": "U0k7XNTiFEq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers", "authorids": ["~Guodong_Zhang1", "~Aleksandar_Botev1", "~James_Martens1"], "authors": ["Guodong Zhang", "Aleksandar Botev", "James Martens"], "keywords": ["Neural Network Training", "Kernel Approximation for Neural Networks", "Neural Network Initialization", "Generalization"], "abstract": "Training very deep neural networks is still an extremely challenging task. The common solution is to use shortcut connections and normalization layers, which are both crucial ingredients in the popular ResNet architecture. However, there is strong evidence to suggest that ResNets behave more like ensembles of shallower networks than truly deep ones. Recently, it was shown that deep vanilla networks (i.e.~networks without normalization layers or shortcut connections) can be trained as fast as ResNets by applying certain transformations to their activation functions. However, this method (called Deep Kernel Shaping) isn't fully compatible with ReLUs, and produces networks that overfit significantly more than ResNets on ImageNet. In this work, we rectify this situation by developing a new type of transformation that is fully compatible with a variant of ReLUs -- Leaky ReLUs. We show in experiments that our method, which introduces negligible extra computational cost, achieves validation accuracies with deep vanilla networks that are competitive with ResNets (of the same width/depth), and significantly higher than those obtained with the Edge of Chaos (EOC) method. And unlike with EOC, the validation accuracies we obtain do not get worse with depth.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|deep_learning_without_shortcuts_shaping_the_kernel_with_tailored_rectifiers", "pdf": "/pdf/e96e7c563d49e5a3e11edbc1d1106d452cd8f97b.pdf", "supplementary_material": "/attachment/ebf7676418732eb9b378910fa6bde6cc64c5ce61.zip", "data": "", "_bibtex": "@inproceedings{\nzhang2022deep,\ntitle={Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers},\nauthor={Guodong Zhang and Aleksandar Botev and James Martens},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=U0k7XNTiFEq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "y_op4lLLaWL", "original": "XgfVFwwkWOT", "number": 2143, "cdate": 1632875600291, "mdate": null, "ddate": null, "tcdate": 1632875600291, "tmdate": 1676330572627, "tddate": null, "forum": "y_op4lLLaWL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias", "authorids": ["~Frederic_Koehler1", "~Viraj_Mehta1", "~Chenghui_Zhou1", "~Andrej_Risteski2"], "authors": ["Frederic Koehler", "Viraj Mehta", "Chenghui Zhou", "Andrej Risteski"], "keywords": ["variational autoencoders", "encoder", "optima", "stability", "low-dimensional manifold"], "abstract": "Variational Autoencoders (VAEs) are one of the most commonly used generative models, particularly for image data. A prominent difficulty in training VAEs is data that is supported on a lower dimensional manifold. Recent work by Dai and Wipf (2020) proposes a two-stage training algorithm for VAEs, based on a conjecture that in standard VAE training the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold. They gave partial support for this conjecture by showing that some optima of the VAE loss do satisfy this property, but did not analyze the training dynamics.  In this paper, we show that for linear encoders/decoders, the conjecture is true\u2014that is the VAE training does recover a generator with support equal to the ground truth manifold\u2014and does so due to an implicit bias of gradient descent rather than merely the VAE loss itself. In the nonlinear case, we show that VAE training frequently learns a higher-dimensional manifold which is a superset of the ground truth manifold.", "one-sentence_summary": "We analyze the landscape and implicit optimization bias of VAEs in the presence of low-dimensional data and the implications thereof. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "koehler|variational_autoencoders_in_the_presence_of_lowdimensional_data_landscape_and_implicit_bias", "pdf": "/pdf/5fc178a777c9341b35f8427741eb86911e4b256a.pdf", "_bibtex": "@inproceedings{\nkoehler2022variational,\ntitle={Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias},\nauthor={Frederic Koehler and Viraj Mehta and Chenghui Zhou and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=y_op4lLLaWL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "cuvga_CiVND", "original": "SIMIzGGm2Mi", "number": 2140, "cdate": 1632875599987, "mdate": null, "ddate": null, "tcdate": 1632875599987, "tmdate": 1676330572840, "tddate": null, "forum": "cuvga_CiVND", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models", "authorids": ["~Chen_Liang3", "~Haoming_Jiang1", "~Simiao_Zuo1", "~Pengcheng_He2", "~Xiaodong_Liu1", "~Jianfeng_Gao1", "~Weizhu_Chen1", "~Tuo_Zhao1"], "authors": ["Chen Liang", "Haoming Jiang", "Simiao Zuo", "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen", "Tuo Zhao"], "keywords": ["Training Large Transformer Models", "Reducing Model Redundancy", "Parameter Sensitivity", "Adaptive Learning Rate Method", "Model Generalization", "Model Pruning"], "abstract": "Recent research has shown the existence of significant redundancy in large Transformer models. One can prune the redundant parameters without significantly sacrificing the generalization performance. However, we question whether the redundant parameters could have contributed more if they were properly trained. To answer this question, we propose a novel training strategy that encourages all parameters to be trained sufficiently. Specifically, we adaptively adjust the learning rate for each parameter according to its sensitivity, a robust gradient-based measure reflecting this parameter's contribution to the model performance. A parameter with low sensitivity is redundant, and we improve its fitting by increasing its learning rate. In contrast, a parameter with high sensitivity is well-trained, and we regularize it by decreasing its learning rate to prevent further overfitting. We conduct extensive experiments on natural language understanding, neural machine translation, and image classification to demonstrate the effectiveness of the proposed schedule. Analysis shows that the proposed schedule indeed reduces the redundancy and improves generalization performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liang|no_parameters_left_behind_sensitivity_guided_adaptive_learning_rate_for_training_large_transformer_models", "pdf": "/pdf/f29d145db699800c70bb362bb205f16575e30db7.pdf", "one-sentence_summary": "We propose a novel adaptive learning rate training strategy for large Transformer models that encourages all parameters to be trained sufficiently.", "supplementary_material": "/attachment/9f900a4dd519a5510d2149478bbee840169587c4.zip", "_bibtex": "@inproceedings{\nliang2022no,\ntitle={No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models},\nauthor={Chen Liang and Haoming Jiang and Simiao Zuo and Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen and Tuo Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cuvga_CiVND}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "aBsCjcPu_tE", "original": "RrWl2xXKQFv", "number": 2137, "cdate": 1632875599523, "mdate": null, "ddate": null, "tcdate": 1632875599523, "tmdate": 1676330573024, "tddate": null, "forum": "aBsCjcPu_tE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations", "authorids": ["~Chenlin_Meng1", "~Yutong_He1", "~Yang_Song1", "~Jiaming_Song1", "~Jiajun_Wu1", "~Jun-Yan_Zhu1", "~Stefano_Ermon1"], "authors": ["Chenlin Meng", "Yutong He", "Yang Song", "Jiaming Song", "Jiajun Wu", "Jun-Yan Zhu", "Stefano Ermon"], "keywords": [], "abstract": "Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user inputs (e.g., hand-drawn colored strokes) and realism of the synthesized images. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide in a form of manipulating RGB pixels, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "meng|sdedit_guided_image_synthesis_and_editing_with_stochastic_differential_equations", "pdf": "/pdf/e3d44fdd105753b51dcb91a908082d6317713ae9.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nmeng2022sdedit,\ntitle={{SDE}dit: Guided Image Synthesis and Editing with Stochastic Differential Equations},\nauthor={Chenlin Meng and Yutong He and Yang Song and Jiaming Song and Jiajun Wu and Jun-Yan Zhu and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aBsCjcPu_tE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "xNOVfCCvDpM", "original": "C4jTGI27NUn", "number": 2129, "cdate": 1632875598550, "mdate": null, "ddate": null, "tcdate": 1632875598550, "tmdate": 1676330573268, "tddate": null, "forum": "xNOVfCCvDpM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation", "authorids": ["~Julius_Adebayo1", "~Michael_Muelly1", "~Harold_Abelson1", "~Been_Kim1"], "authors": ["Julius Adebayo", "Michael Muelly", "Harold Abelson", "Been Kim"], "keywords": ["explanations", "feature attributions", "spurious correlation", "interpretability", "training point ranking"], "abstract": "We investigate whether three types of post hoc model explanations\u2013feature attribution, concept activation, and training point ranking\u2013are effective for detecting a model\u2019s reliance on spurious signals in the training data. Specifically, we consider the scenario where the spurious signal to be detected is unknown, at test-time, to the user of the explanation method. We design an empirical methodology that uses semi-synthetic datasets along with pre-specified spurious artifacts to obtain models that verifiably rely on these spurious training signals. We then provide a suite of metrics that assess an explanation method\u2019s reliability for spurious signal detection under various conditions. We find that the post hoc explanation methods tested are ineffective when the spurious artifact is unknown at test-time especially for non-visible artifacts like a background blur. Further, we find that feature attribution methods are susceptible to erroneously indicating dependence on spurious signals even when the model being explained does not rely on spurious artifacts. This finding casts doubt on the utility of these approaches, in the hands of a practitioner, for detecting a model\u2019s reliance on spurious signals.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "adebayo|post_hoc_explanations_may_be_ineffective_for_detecting_unknown_spurious_correlation", "pdf": "/pdf/25d1d4e7a5f415eccf5a40f584ff83b2d54db779.pdf", "one-sentence_summary": "Post hoc explanation methods struggle to detect that deep nets are reliant on spurious training signals.", "supplementary_material": "/attachment/ff0c2e3f25a18eba79cd77b4f76ad0931a71c342.zip", "_bibtex": "@inproceedings{\nadebayo2022post,\ntitle={Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation},\nauthor={Julius Adebayo and Michael Muelly and Harold Abelson and Been Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xNOVfCCvDpM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 6}}, {"id": "_jMtny3sMKU", "original": "ejmkcMsuIXP", "number": 2125, "cdate": 1632875598043, "mdate": null, "ddate": null, "tcdate": 1632875598043, "tmdate": 1697934746835, "tddate": null, "forum": "_jMtny3sMKU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalizing Few-Shot NAS with Gradient Matching", "authorids": ["~Shoukang_Hu1", "~Ruochen_Wang2", "~Lanqing_HONG1", "~Zhenguo_Li1", "~Cho-Jui_Hsieh1", "~Jiashi_Feng1"], "authors": ["Shoukang Hu", "Ruochen Wang", "Lanqing HONG", "Zhenguo Li", "Cho-Jui Hsieh", "Jiashi Feng"], "keywords": [], "abstract": "Efficient performance estimation of architectures drawn from large search spaces is essential to Neural Architecture Search. One-Shot methods tackle this challenge by training one supernet to approximate the performance of every architecture in the search space via weight-sharing, thereby drastically reducing the search cost. However, due to coupled optimization between child architectures caused by weight-sharing, One-Shot supernet's performance estimation could be inaccurate, leading to degraded search outcomes. To address this issue, Few-Shot NAS reduces the level of weight-sharing by splitting the One-Shot supernet into multiple separated sub-supernets via edge-wise (layer-wise) exhaustive partitioning. Since each partition of the supernet is not equally important, it necessitates the design of a more effective splitting criterion. In this work, we propose a gradient matching score (GM) that leverages gradient information at the shared weight for making informed splitting decisions. Intuitively, gradients from different child models can be used to identify whether they agree on how to update the shared modules, and subsequently to decide if they should share weight. Compared with exhaustive partitioning, the proposed criterion significantly reduces the branching factor per edge. This allows us to split more edges (layers) for a given budget, resulting in substantially improved performance as NAS search spaces usually include dozens of edges (layers). Extensive empirical evaluations of the proposed method on a wide range of search spaces (NASBench-201, DARTS, MobileNet Space), datasets (cifar10, cifar100, ImageNet) and search algorithms (DARTS, SNAS, RSPS, ProxylessNAS, OFA) demonstrate that it significantly outperforms its Few-Shot counterparts while surpassing previous comparable methods in terms of the accuracy of derived architectures. \nOur code is available at https://github.com/skhu101/GM-NAS.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hu|generalizing_fewshot_nas_with_gradient_matching", "pdf": "/pdf/ad7965c59fc903823d27880126728fb09cd8e4dd.pdf", "supplementary_material": "/attachment/bc0a1c7150e1628b9a4195e784c50d443932ce50.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.15207/code)", "_bibtex": "@inproceedings{\nhu2022generalizing,\ntitle={Generalizing Few-Shot {NAS} with Gradient Matching},\nauthor={Shoukang Hu and Ruochen Wang and Lanqing HONG and Zhenguo Li and Cho-Jui Hsieh and Jiashi Feng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_jMtny3sMKU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "VBZJ_3tz-t", "original": "d30wu1tVlC7", "number": 2122, "cdate": 1632875597595, "mdate": null, "ddate": null, "tcdate": 1632875597595, "tmdate": 1697934747233, "tddate": null, "forum": "VBZJ_3tz-t", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training", "authorids": ["~Shiwei_Liu2", "~Tianlong_Chen1", "~Xiaohan_Chen1", "~Li_Shen1", "~Decebal_Constantin_Mocanu1", "~Zhangyang_Wang1", "~Mykola_Pechenizkiy1"], "authors": ["Shiwei Liu", "Tianlong Chen", "Xiaohan Chen", "Li Shen", "Decebal Constantin Mocanu", "Zhangyang Wang", "Mykola Pechenizkiy"], "keywords": ["random pruning", "sparse training", "static sparse training", "layer-wise sparsities", "dynamic sparse training"], "abstract": "Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural networks. Without any delicate pruning criteria or carefully pursued sparsity structures, we empirically demonstrate that sparsely training a randomly pruned network from scratch can match the performance of its dense equivalent. There are two key factors that contribute to this revival: (i) $the network sizes matter$: as the original dense networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios; (ii) $appropriate layer-wise sparsity ratios$ can be pre-chosen for sparse training, which shows to be another important performance booster. Simple as it looks,  a randomly pruned subnetwork of Wide ResNet-50 can be sparsely trained to outperforming a dense Wide ResNet-50, on ImageNet. We also observed such randomly pruned networks outperform dense counterparts in other favorable aspects, such as out-of-distribution detection, uncertainty estimation, and adversarial robustness. Overall, our results strongly suggest there is larger-than-expected room for sparse training at scale, and the benefits of sparsity might be more universal beyond carefully designed pruning. Our source code can be found at https://github.com/VITA-Group/Random_Pruning.\n", "one-sentence_summary": "We revisit the most naive baseline, random pruning, in sparse training and highlight a perhaps counter-intuitive finding: random pruning can be quite powerful for the sparse training of modern neural networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|the_unreasonable_effectiveness_of_random_pruning_return_of_the_most_naive_baseline_for_sparse_training", "pdf": "/pdf/92cd52f575aab3aac6f6934474fa0fb6f56652b9.pdf", "supplementary_material": "/attachment/c273399dc2db71f26a07314992eb81c16cb9f3a0.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.02643/code)", "_bibtex": "@inproceedings{\nliu2022the,\ntitle={The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training},\nauthor={Shiwei Liu and Tianlong Chen and Xiaohan Chen and Li Shen and Decebal Constantin Mocanu and Zhangyang Wang and Mykola Pechenizkiy},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VBZJ_3tz-t}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "5HvpvYd68b", "original": "7mIT8h1zn_M", "number": 2110, "cdate": 1632875596238, "mdate": null, "ddate": null, "tcdate": 1632875596238, "tmdate": 1676330574386, "tddate": null, "forum": "5HvpvYd68b", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder", "authorids": ["~Zhenqiao_Song1", "~Hao_Zhou5", "~Lihua_Qian1", "~Jingjing_Xu1", "~Shanbo_Cheng1", "~Mingxuan_Wang1", "~Lei_Li11"], "authors": ["Zhenqiao Song", "Hao Zhou", "Lihua Qian", "Jingjing Xu", "Shanbo Cheng", "Mingxuan Wang", "Lei Li"], "keywords": ["multilingual non-autoregressive machine translation", "contextualized code-switching", "back-translation"], "abstract": "Multilingual machine translation aims to develop a single model for multiple language directions. However, existing multilingual models based on Transformer are limited in terms of both translation performance and inference speed. In this paper, we propose switch-GLAT, a non-autoregressive multilingual machine translation model with a code-switch decoder. It can generate contextual code-switched translations for a given source sentence, and perform code-switch back-translation, greatly boosting multilingual translation performance. In addition, its inference is highly efficient thanks to its parallel decoder. Experiments show that our proposed switch-GLAT outperform the multilingual Transformer with as much as 0.74 BLEU improvement and 6.2x faster decoding speed in inference.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "song|switchglat_multilingual_parallel_machine_translation_via_codeswitch_decoder", "pdf": "/pdf/24886b6f1aa837364f7c14635d0af1b9dadb0fe7.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nsong2022switchglat,\ntitle={switch-{GLAT}: Multilingual Parallel Machine Translation Via Code-Switch Decoder},\nauthor={Zhenqiao Song and Hao Zhou and Lihua Qian and Jingjing Xu and Shanbo Cheng and Mingxuan Wang and Lei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5HvpvYd68b}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "GWQWAeE9EpB", "original": "3emjWoeu3vq", "number": 2101, "cdate": 1632875595163, "mdate": null, "ddate": null, "tcdate": 1632875595163, "tmdate": 1676330574780, "tddate": null, "forum": "GWQWAeE9EpB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DictFormer: Tiny Transformer with Shared Dictionary", "authorids": ["~Qian_Lou1", "ting.hua@samsung.com", "~Yen-Chang_Hsu1", "~Yilin_Shen1", "~Hongxia_Jin1"], "authors": ["Qian Lou", "Ting Hua", "Yen-Chang Hsu", "Yilin Shen", "Hongxia Jin"], "keywords": ["Transformer", "Parameters Sharing", "Tiny", "On-device Transformer", "Machine Translation", "Attention", "Dictionary Sharing"], "abstract": "We introduce DictFormer with the efficient shared dictionary to provide a compact, fast, and accurate transformer model. DictFormer significantly reduces the redundancy in the transformer's parameters by replacing the prior transformer's parameters with a compact, shared dictionary, few unshared coefficients, and indices. Also, DictFormer enables faster computations since expensive weights multiplications are converted into cheap shared look-ups on dictionary and few linear projections. Training dictionary and coefficients are not trivial since indices used for looking up dictionary are not differentiable. We adopt a sparse-constraint training with $l_1\\,\\,norm$ relaxation to learn coefficients and indices in DictFormer. DictFormer is flexible to support different model sizes by dynamically changing dictionary size. Compared to existing lightweight Transformers, DictFormer consistently reduces model size over Transformer on multiple tasks, e.g., machine translation, abstractive summarization, and language modeling. Extensive experiments show that DictFormer reduces $3.6\\times$ to $8.9\\times$ model size with similar accuracy over multiple tasks, compared to Transformer.   ", "one-sentence_summary": "We propose DictFormer with efficient shared dictionary to provide a compact, fast, and accurate transformer model.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lou|dictformer_tiny_transformer_with_shared_dictionary", "pdf": "/pdf/63e0216a03407ae67011d9a68ba92781cd196e13.pdf", "data": "", "_bibtex": "@inproceedings{\nlou2022dictformer,\ntitle={DictFormer: Tiny Transformer with Shared Dictionary},\nauthor={Qian Lou and Ting Hua and Yen-Chang Hsu and Yilin Shen and Hongxia Jin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=GWQWAeE9EpB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 32}}, {"id": "6vkzF28Hur8", "original": "9hYTw7uFZd8", "number": 2091, "cdate": 1632875592185, "mdate": null, "ddate": null, "tcdate": 1632875592185, "tmdate": 1697934750376, "tddate": null, "forum": "6vkzF28Hur8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Training Transition Policies via Distribution Matching for Complex Tasks", "authorids": ["~JU-SEUNG_BYUN1", "~Andrew_Perrault1"], "authors": ["JU-SEUNG BYUN", "Andrew Perrault"], "keywords": ["Reinforcement Learning", "Hierarchical Reinforcement Learning", "Inverse Reinforcement Learning"], "abstract": "Humans decompose novel complex tasks into simpler ones to exploit previously learned skills. Analogously, hierarchical reinforcement learning seeks to leverage lower-level policies for simple tasks to solve complex ones. However, because each lower-level policy induces a different distribution of states, transitioning from one lower-level policy to another may fail due to an unexpected starting state. We introduce transition policies that smoothly connect lower-level policies by producing a distribution of states and actions that matches what is expected by the next policy. Training transition policies is challenging because the natural reward signal---whether the next policy can execute its subtask successfully---is sparse. By training transition policies via adversarial inverse reinforcement learning to match the distribution of expected states and actions, we avoid relying on task-based reward. To further improve performance, we use deep Q-learning with a binary action space to determine when to switch from a transition policy to the next pre-trained policy, using the success or failure of the next subtask as the reward. Although the reward is still sparse, the problem is less severe due to the simple binary action space. We demonstrate our method on continuous bipedal locomotion and arm manipulation tasks that require diverse skills. We show that it smoothly connects the lower-level policies, achieving higher success rates than previous methods that search for successful trajectories based on a reward function, but do not match the state distribution.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "byun|training_transition_policies_via_distribution_matching_for_complex_tasks", "pdf": "/pdf/b0fad4cf84b44e02b873a928fb82cf77473641fa.pdf", "one-sentence_summary": "Training transition policies via distribution matching", "supplementary_material": "/attachment/c993990fcc0e79ec9cdf23e22d7e1a058e99b0af.zip", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.04357/code)", "_bibtex": "@inproceedings{\nbyun2022training,\ntitle={Training Transition Policies via Distribution Matching for Complex Tasks},\nauthor={JU-SEUNG BYUN and Andrew Perrault},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6vkzF28Hur8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "3YqeuCVwy1d", "original": "Ou6VzCUIGcz", "number": 2078, "cdate": 1632875576160, "mdate": null, "ddate": null, "tcdate": 1632875576160, "tmdate": 1676330576098, "tddate": null, "forum": "3YqeuCVwy1d", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GDA-AM: ON THE EFFECTIVENESS OF SOLVING MIN-IMAX OPTIMIZATION VIA ANDERSON MIXING", "authorids": ["~Huan_He2", "~Shifan_Zhao1", "~Yuanzhe_Xi1", "~Joyce_Ho1", "~Yousef_Saad2"], "authors": ["Huan He", "Shifan Zhao", "Yuanzhe Xi", "Joyce Ho", "Yousef Saad"], "keywords": [], "abstract": "Many modern machine learning algorithms such as generative adversarial networks (GANs) and adversarial training can be formulated as minimax optimization.Gradient descent ascent (GDA) is the most commonly used algorithm due to its simplicity.  However, GDA can converge to non-optimal minimax points.  We propose a new minimax optimization framework,GDA-AM, that views the GDA dynamics as a fixed-point iteration and solves it using Anderson Mixing to converge to the local minimax. It addresses the diverging issue of simultaneous GDA and accelerates the convergence of alternating GDA. We show theoretically that the algorithm can achieve global convergence for bilinear problems under mildconditions. We also empirically show that GDA-AM solves a variety of minimax problems and improves GAN training on several datasets", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "he|gdaam_on_the_effectiveness_of_solving_minimax_optimization_via_anderson_mixing", "pdf": "/pdf/0c1222f9d7edda5670c919e0cbd20b7afc79e30e.pdf", "supplementary_material": "/attachment/c83c91081ac65ee2f2fd8403d50f994af4c99227.zip", "data": "", "_bibtex": "@inproceedings{\nhe2022gdaam,\ntitle={{GDA}-{AM}: {ON} {THE} {EFFECTIVENESS} {OF} {SOLVING} {MIN}-{IMAX} {OPTIMIZATION} {VIA} {ANDERSON} {MIXING}},\nauthor={Huan He and Shifan Zhao and Yuanzhe Xi and Joyce Ho and Yousef Saad},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3YqeuCVwy1d}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "PQTW3iG4sC-", "original": "chY3Qc-1R_0", "number": 2077, "cdate": 1632875575348, "mdate": null, "ddate": null, "tcdate": 1632875575348, "tmdate": 1676330576160, "tddate": null, "forum": "PQTW3iG4sC-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On feature learning in neural networks with global convergence guarantees", "authorids": ["~Zhengdao_Chen1", "~Eric_Vanden-Eijnden1", "~Joan_Bruna1"], "authors": ["Zhengdao Chen", "Eric Vanden-Eijnden", "Joan Bruna"], "keywords": ["neural networks", "feature learning", "gradient descent", "global convergence"], "abstract": "We study the gradient flow optimization of over-parameterized neural networks (NNs) in a setup that allows feature learning while admitting non-asymptotic global convergence guarantees. First, we prove that for wide shallow NNs under the mean-field (MF) scaling and with a general class of activation functions, when the input dimension is at least the size of the training set, the training loss converges to zero at a linear rate under gradient flow. Building upon this analysis, we study a model of wide multi-layer NNs with random and untrained weights in earlier layers, and also prove a linear-rate convergence of the training loss to zero, regardless of the input dimension. We also show empirically that, unlike in the Neural Tangent Kernel (NTK) regime, our multi-layer model exhibits feature learning and can achieve better generalization performance than its NTK counterpart.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|on_feature_learning_in_neural_networks_with_global_convergence_guarantees", "pdf": "/pdf/7ae8138e2ef895c84726f09da2670108d7ddfa81.pdf", "one-sentence_summary": "Gradient flow can induce feature learning in shallow and multi-layer neural networks while admitting non-asymptotic guarantees of global convergence.", "_bibtex": "@inproceedings{\nchen2022on,\ntitle={On feature learning in neural networks with global convergence guarantees},\nauthor={Zhengdao Chen and Eric Vanden-Eijnden and Joan Bruna},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PQTW3iG4sC-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "EQmAP4F859", "original": "nZAt2gCzJ7", "number": 2076, "cdate": 1632875574653, "mdate": null, "ddate": null, "tcdate": 1632875574653, "tmdate": 1676330576242, "tddate": null, "forum": "EQmAP4F859", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Three Stages of Learning Dynamics in High-dimensional Kernel Methods", "authorids": ["~Nikhil_Ghosh1", "~Song_Mei1", "~Bin_Yu1"], "authors": ["Nikhil Ghosh", "Song Mei", "Bin Yu"], "keywords": ["training dynamics", "kernels", "SGD", "deep bootstrap", "gradient flow", "random features", "high-dimensional asymptotics", "random matrix theory"], "abstract": "To understand how deep learning works, it is crucial to understand the training dynamics of neural networks. Several interesting hypotheses about these dynamics have been made based on empirically observed phenomena, but there exists a limited theoretical understanding of when and why such phenomena occur. \n\nIn this paper, we consider the training dynamics of gradient flow on kernel least-squares objectives, which is a limiting dynamics of SGD trained neural networks. Using precise high-dimensional asymptotics, we characterize the dynamics of the fitted model in two \u201cworlds\u201d: in the Oracle World the model is trained on the population distribution and in the Empirical World the model is trained on an i.i.d finite dataset. We show that under mild conditions on the kernel and $L^2$ target regression function the training dynamics have three stages that are based on the behaviors of the models in the two worlds. Our theoretical results also mathematically formalize some interesting deep learning phenomena. Specifically, in our setting we show that SGD progressively learns more complex functions and that there is a \"deep bootstrap\" phenomenon: during the second stage, the test error of both worlds remain close despite the empirical training error being much smaller. Finally, we give a concrete example comparing the dynamics of two different kernels which shows that faster training is not necessary for better generalization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ghosh|the_three_stages_of_learning_dynamics_in_highdimensional_kernel_methods", "pdf": "/pdf/273ff13b48f42a02263556d771e28712207508bc.pdf", "one-sentence_summary": "We study the training dynamics of gradient flows on the population risk and empirical risk of high-dimensional kernel least-squares problems, which we show has three learning stages.", "_bibtex": "@inproceedings{\nghosh2022the,\ntitle={The Three Stages of Learning Dynamics in High-dimensional Kernel Methods},\nauthor={Nikhil Ghosh and Song Mei and Bin Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EQmAP4F859}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "6MmiS0HUJHR", "original": "qKiefsbl16U", "number": 2073, "cdate": 1632875573167, "mdate": null, "ddate": null, "tcdate": 1632875573167, "tmdate": 1676330576425, "tddate": null, "forum": "6MmiS0HUJHR", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?", "authorids": ["~Ziang_Song1", "~Song_Mei1", "~Yu_Bai1"], "authors": ["Ziang Song", "Song Mei", "Yu Bai"], "keywords": ["reinforcement learning theory", "multi-agent RL", "Markov games", "general-sum games"], "abstract": "Multi-agent reinforcement learning has made substantial empirical progresses in solving games with a large number of players. However, theoretically, the best known sample complexity for finding a Nash equilibrium in general-sum games scales exponentially in the number of players due to the size of the joint action space, and there is a matching exponential lower bound. This paper investigates what learning goals admit better sample complexities in the setting of $m$-player general-sum Markov games with $H$ steps, $S$ states, and $A_i$ actions per player. First, we design algorithms for learning an $\\epsilon$-Coarse Correlated Equilibrium (CCE) in $\\widetilde{\\mathcal{O}}(H^5S\\max_{i\\le m} A_i / \\epsilon^2)$ episodes, and an $\\epsilon$-Correlated Equilibrium (CE) in $\\widetilde{\\mathcal{O}}(H^6S\\max_{i\\le m} A_i^2 / \\epsilon^2)$ episodes. This is the first line of results for learning CCE and CE with sample complexities polynomial in $\\max_{i\\le m} A_i$. Our algorithm for learning CE integrates an adversarial bandit subroutine which minimizes a weighted swap regret, along with several novel designs in the outer loop. Second, we consider the important special case of Markov Potential Games, and design an algorithm that learns an $\\epsilon$-approximate Nash equilibrium within $\\widetilde{\\mathcal{O}}(S\\sum_{i\\le m} A_i / \\epsilon^3)$ episodes (when only highlighting the dependence on $S$, $A_i$, and $\\epsilon$), which only depends linearly in $\\sum_{i\\le m} A_i$ and significantly improves over the existing efficient algorithm in the $\\epsilon$ dependence. Overall, our results shed light on what equilibria or structural assumptions on the game may enable sample-efficient learning with many players.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "song|when_can_we_learn_generalsum_markov_games_with_a_large_number_of_players_sampleefficiently", "pdf": "/pdf/4897aa8eae5efc42c6f0c8f6dcc2f6ce0989de42.pdf", "one-sentence_summary": "We present new algorithms for several learning goals in multi-player general-sum Markov games, with mild PAC sample complexity in terms of the number of players.", "_bibtex": "@inproceedings{\nsong2022when,\ntitle={When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?},\nauthor={Ziang Song and Song Mei and Yu Bai},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6MmiS0HUJHR}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "1NvflqAdoom", "original": "up1SPM05809", "number": 2067, "cdate": 1632875570828, "mdate": null, "ddate": null, "tcdate": 1632875570828, "tmdate": 1676330576956, "tddate": null, "forum": "1NvflqAdoom", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Networks as Kernel Learners: The Silent Alignment Effect", "authorids": ["~Alexander_Atanasov1", "~Blake_Bordelon1", "~Cengiz_Pehlevan2"], "authors": ["Alexander Atanasov", "Blake Bordelon", "Cengiz Pehlevan"], "keywords": ["Neural Tangent Kernel", "Feature Learning", "Inductive Bias of Neural Networks"], "abstract": "Neural networks in the lazy training regime converge to kernel machines. Can neural networks in the rich feature learning regime learn a kernel machine with a data-dependent kernel? We demonstrate that this can indeed happen due to a phenomenon we term silent alignment, which requires that the tangent kernel of a network evolves in eigenstructure while small and before the loss appreciably decreases, and grows only in overall scale afterwards. We show that such an effect takes place in homogenous neural networks with small initialization and whitened data. We provide an analytical treatment of this effect in the linear network case. In general, we find that the kernel develops a low-rank contribution in the early phase of training, and then evolves in overall scale, yielding a function equivalent to a kernel regression solution with the final network's tangent kernel. The early spectral learning of the kernel depends on the depth. We also demonstrate that non-whitened data can weaken the silent alignment effect.", "one-sentence_summary": "Neural networks with small initialization, trained in the rich feature learning regime, can learn kernel regression solutions for a data adaptive kernel.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "atanasov|neural_networks_as_kernel_learners_the_silent_alignment_effect", "pdf": "/pdf/06f8c9a443cfe06f78852aa3be34a2ff89b75293.pdf", "supplementary_material": "/attachment/ea777ffc3162711e4eed957df7469affa245766a.zip", "_bibtex": "@inproceedings{\natanasov2022neural,\ntitle={Neural Networks as Kernel Learners: The Silent Alignment Effect},\nauthor={Alexander Atanasov and Blake Bordelon and Cengiz Pehlevan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1NvflqAdoom}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "B6EIcyp-Rb7", "original": "UwrGNyclQHU", "number": 2065, "cdate": 1632875570158, "mdate": null, "ddate": null, "tcdate": 1632875570158, "tmdate": 1676330577047, "tddate": null, "forum": "B6EIcyp-Rb7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Object-Oriented Dynamics for Planning from Text", "authorids": ["~Guiliang_Liu1", "~Ashutosh_Adhikari1", "~Amir-massoud_Farahmand1", "~Pascal_Poupart2"], "authors": ["Guiliang Liu", "Ashutosh Adhikari", "Amir-massoud Farahmand", "Pascal Poupart"], "keywords": ["Object Oriented Markov Decision Process", "Reinforcement Learning", "Model-Based Planning", "Text-Based Games", "Knowledge Extraction"], "abstract": "The advancement of dynamics models enables model-based planning in complex environments. Existing dynamics models commonly study image-based games with fully observable states. Generalizing these models to Text-Based Games (TBGs), which commonly describe the partially observable states with noisy text observations, is challenging. In this work, we propose an Object-Oriented Text Dynamics (OOTD) model that enables planning algorithms to solve decision-making problems in text domains. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information.  To facilitate the robustness of dynamics, our OOTD model identifies the objects influenced by input actions and predicts the belief of object states with independently parameterized transition layers. We develop variational objectives under the object-supervised and self-supervised settings to model the stochasticity of predicted dynamics. Empirical results show OOTD-based planner significantly outperforms model-free baselines in terms of sample efficiency and running scores.", "one-sentence_summary": "We propose an Object-Oriented Text Dynamic (OOTD) model for solving decision-making problems in the text domain.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|learning_objectoriented_dynamics_for_planning_from_text", "pdf": "/pdf/af2da968af3f640735e0884e14b4c9f80134a9cf.pdf", "supplementary_material": "/attachment/b968a74e2830048aef8553379efb22c9302edb3f.zip", "_bibtex": "@inproceedings{\nliu2022learning,\ntitle={Learning Object-Oriented Dynamics for Planning from Text},\nauthor={Guiliang Liu and Ashutosh Adhikari and Amir-massoud Farahmand and Pascal Poupart},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=B6EIcyp-Rb7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "pWBNOgdeURp", "original": "Sqqg4Mz9P8j", "number": 2056, "cdate": 1632875567339, "mdate": null, "ddate": null, "tcdate": 1632875567339, "tmdate": 1676330577270, "tddate": null, "forum": "pWBNOgdeURp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "An Operator Theoretic View On Pruning Deep Neural Networks", "authorids": ["~William_T_Redman1", "~MARIA_FONOBEROVA1", "mohrr@aimdyn.com", "~Yannis_Kevrekidis1", "mezic@ucsb.edu"], "authors": ["William T Redman", "MARIA FONOBEROVA", "Ryan Mohr", "Yannis Kevrekidis", "Igor Mezic"], "keywords": ["deep neural network pruning", "Koopman operator theory"], "abstract": "The discovery of sparse subnetworks that are able to perform as well as full models has found broad applied and theoretical interest. While many pruning methods have been developed to this end, the na\u00efve approach of removing parameters based on their magnitude has been found to be as robust as more complex, state-of-the-art algorithms. The lack of theory behind magnitude pruning's success, especially pre-convergence, and its relation to other pruning methods, such as gradient based pruning, are outstanding open questions in the field that are in need of being addressed. We make use of recent advances in dynamical systems theory, namely Koopman operator theory, to define a new class of theoretically motivated pruning algorithms. We show that these algorithms can be equivalent to magnitude and gradient based pruning, unifying these seemingly disparate methods, and find that they can be used to shed light on magnitude pruning's performance during the early part of training.", "one-sentence_summary": "Koopman operator theoretic methods are extended to deep neural network pruning and are shown to provide new insight into existing work.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "redman|an_operator_theoretic_view_on_pruning_deep_neural_networks", "pdf": "/pdf/732335090e9773ee61b1e0375641a2c765eeaf33.pdf", "_bibtex": "@inproceedings{\nredman2022an,\ntitle={An Operator Theoretic View On Pruning Deep Neural Networks},\nauthor={William T Redman and MARIA FONOBEROVA and Ryan Mohr and Yannis Kevrekidis and Igor Mezic},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pWBNOgdeURp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "_4GFbtOuWq-", "original": "8oeLDWKDC4K", "number": 2051, "cdate": 1632875565564, "mdate": null, "ddate": null, "tcdate": 1632875565564, "tmdate": 1676330577500, "tddate": null, "forum": "_4GFbtOuWq-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?", "authorids": ["~Matthew_Farrell1", "~Blake_Bordelon1", "~Shubhendu_Trivedi2", "~Cengiz_Pehlevan2"], "authors": ["Matthew Farrell", "Blake Bordelon", "Shubhendu Trivedi", "Cengiz Pehlevan"], "keywords": ["representation learning", "perceptron capacity", "perceptual manifolds", "equivariance", "cover's theorem", "vc dimension"], "abstract": "Equivariance has emerged as a desirable property of representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. However, the expressivity of a representation constrained by group equivariance is still not fully understood. We address this gap by providing a generalization of Cover's Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects. We find that the fraction of separable dichotomies is determined by the dimension of the space that is fixed by the group action. We show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. While other operations do not change the fraction of separable dichotomies, local pooling decreases the fraction, despite being a highly nonlinear operation. Finally, we test our theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement.", "one-sentence_summary": "We compute the fraction of linearly separable dichotomies of representations formed by group actions and apply these results to intermediate representations of convolutional neural networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "farrell|capacity_of_groupinvariant_linear_readouts_from_equivariant_representations_how_many_objects_can_be_linearly_classified_under_all_possible_views", "pdf": "/pdf/6d3dd8ced80d47d56918b5e4c597c76e355e44fb.pdf", "supplementary_material": "/attachment/057dd8a28ab2af3f95979aee3b2e09f91c7ac806.zip", "_bibtex": "@inproceedings{\nfarrell2022capacity,\ntitle={Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?},\nauthor={Matthew Farrell and Blake Bordelon and Shubhendu Trivedi and Cengiz Pehlevan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_4GFbtOuWq-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "V0A5g83gdQ_", "original": "XQwUe2nP0dn", "number": 2050, "cdate": 1632875565330, "mdate": null, "ddate": null, "tcdate": 1632875565330, "tmdate": 1676330577592, "tddate": null, "forum": "V0A5g83gdQ_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency", "authorids": ["~Xiaoyu_Liu3", "~Jiahao_Su1", "~Furong_Huang1"], "authors": ["Xiaoyu Liu", "Jiahao Su", "Furong Huang"], "keywords": ["Attention Modules", "Transformers", "Data-driven Model Design", "Trainable Heads", "Expressive Power", "Tensor Methods."], "abstract": "Transformers are neural network architectures that achieve remarkable performance in many areas. However, the core component of Transformers, multi-head self-attention (MHSA), is mainly derived from heuristics, and the interactions across its components are not well understood. To address the problem, we first introduce a mathematically rigorous and yet intuitive tensor diagram representation of MHSA. Guided by tensor diagram representations, we propose a novel design, namely Tunable Transformers (Tuformers), by allowing data-driven weights across heads, whereas MHSA adopts pre-defined and fixed weights across heads, as will be explained in our paper. Tuformers naturally reveal a flexible design space that a user, depending on the needs, can choose a structure that has either improved performance (generalization error) or higher model efficiency. Any pre-trained Transformer can be an initialization of the corresponding Tuformer with trainable number of heads for efficient training and fine-tuning. Tuformers universally outperform Transformers on various tasks across multiple domains under a wide range of model sizes.", "one-sentence_summary": "We propose Tuformer, a data-driven design of theoretically guaranteed expressive Transformer with trainable heads, inspired by Tucker tensor representation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|tuformer_datadriven_design_of_transformers_for_improved_generalization_or_efficiency", "pdf": "/pdf/aab5b494ebbbe94524322c8629b269dd1c4a75fe.pdf", "supplementary_material": "/attachment/b4371d590b920b120ea336c316ea3bdc3b123faf.zip", "data": "", "_bibtex": "@inproceedings{\nliu2022tuformer,\ntitle={Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency},\nauthor={Xiaoyu Liu and Jiahao Su and Furong Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=V0A5g83gdQ_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "MSwEFaztwkE", "original": "5RG_eEIcag", "number": 2048, "cdate": 1632875564624, "mdate": null, "ddate": null, "tcdate": 1632875564624, "tmdate": 1697934754738, "tddate": null, "forum": "MSwEFaztwkE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Weakly-supervised Contrastive Representations", "authorids": ["~Yao-Hung_Hubert_Tsai1", "~Tianqin_Li2", "~Weixin_Liu2", "~Peiyuan_Liao1", "~Ruslan_Salakhutdinov1", "~Louis-Philippe_Morency1"], "authors": ["Yao-Hung Hubert Tsai", "Tianqin Li", "Weixin Liu", "Peiyuan Liao", "Ruslan Salakhutdinov", "Louis-Philippe Morency"], "keywords": ["Self-supervised Learning", "Weakly Supervised Learning", "Learning with Auxiliary Information", "Clustering-based Representation Learning"], "abstract": "We argue that a form of the valuable information provided by the auxiliary information is its implied data clustering information. For instance, considering hashtags as auxiliary information, we can hypothesize that an Instagram image will be semantically more similar with the same hashtags. With this intuition, we present a two-stage weakly-supervised contrastive learning approach. The first stage is to cluster data according to its auxiliary information. The second stage is to learn similar representations within the same cluster and dissimilar representations for data from different clusters. Our empirical experiments suggest the following three contributions. First, compared to conventional self-supervised representations, the auxiliary-information-infused representations bring the performance closer to the supervised representations, which use direct downstream labels as supervision signals. Second, our approach performs the best in most cases, when comparing our approach with other baseline representation learning methods that also leverage auxiliary data information. Third, we show that our approach also works well with unsupervised constructed clusters (e.g., no auxiliary information), resulting in a strong unsupervised representation learning approach. ", "one-sentence_summary": "We present a weakly-supervised contrastive learning framework that considers auxiliary information (additional sources of information from data).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tsai|learning_weaklysupervised_contrastive_representations", "pdf": "/pdf/08e660baa5f1b4d95675e564853ce70378ebe5fd.pdf", "supplementary_material": "/attachment/6026402aee60f4b2b0b18faa1782b47207b13efb.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.06670/code)", "_bibtex": "@inproceedings{\ntsai2022learning,\ntitle={Learning Weakly-supervised Contrastive Representations},\nauthor={Yao-Hung Hubert Tsai and Tianqin Li and Weixin Liu and Peiyuan Liao and Ruslan Salakhutdinov and Louis-Philippe Morency},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MSwEFaztwkE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "Vs5NK44aP9P", "original": "CyrMRI1wL3U", "number": 2045, "cdate": 1632875563902, "mdate": null, "ddate": null, "tcdate": 1632875563902, "tmdate": 1676330577992, "tddate": null, "forum": "Vs5NK44aP9P", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression", "authorids": ["~Bae_Seong_Park1", "~Se_Jung_Kwon1", "~Daehwan_Oh1", "~Byeongwook_Kim1", "~Dongsoo_Lee1"], "authors": ["Bae Seong Park", "Se Jung Kwon", "Daehwan Oh", "Byeongwook Kim", "Dongsoo Lee"], "keywords": ["Sparse Neural Network", "Fixed-to-fixed data compression", "Unstructured Pruning"], "abstract": "Even though fine-grained pruning techniques achieve a high compression ratio, conventional sparsity representations (such as CSR) associated with irregular sparsity degrade parallelism significantly. Practical pruning methods, thus, usually lower pruning rates (by structured pruning) to improve parallelism. In this paper, we study fixed-to-fixed (lossless) encoding architecture/algorithm to support fine-grained pruning methods such that sparse neural networks can be stored in a highly regular structure. We first estimate the maximum compression ratio of encoding-based compression using entropy. Then, as an effort to push the compression ratio to the theoretical maximum (by entropy), we propose a sequential fixed-to-fixed encoding scheme. We demonstrate that our proposed compression scheme achieves almost the maximum compression ratio for the Transformer and ResNet-50 pruned by various fine-grained pruning methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "park|encoding_weights_of_irregular_sparsity_for_fixedtofixed_model_compression", "pdf": "/pdf/ab9b739fc11bc2802199a375c6146e8512c81fa5.pdf", "one-sentence_summary": "We propose a fixed-to-fixed weight compression scheme even when weights are pruned in a fine-grained manner.", "_bibtex": "@inproceedings{\npark2022encoding,\ntitle={Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression},\nauthor={Bae Seong Park and Se Jung Kwon and Daehwan Oh and Byeongwook Kim and Dongsoo Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Vs5NK44aP9P}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "0no8Motr-zO", "original": "d6E7MdVSbVu", "number": 2037, "cdate": 1632875562205, "mdate": null, "ddate": null, "tcdate": 1632875562205, "tmdate": 1676330578399, "tddate": null, "forum": "0no8Motr-zO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "An Experimental Design Perspective on Model-Based Reinforcement Learning", "authorids": ["~Viraj_Mehta1", "~Biswajit_Paria1", "~Jeff_Schneider1", "~Stefano_Ermon1", "~Willie_Neiswanger2"], "authors": ["Viraj Mehta", "Biswajit Paria", "Jeff Schneider", "Stefano Ermon", "Willie Neiswanger"], "keywords": ["reinforcement learning", "acquisition function", "information gain"], "abstract": "In many practical applications of RL, it is expensive to observe state transitions from the environment. For example, in the problem of plasma control for nuclear fusion, computing the next state for a given state-action pair requires querying an expensive transition function which can lead to many hours of computer simulation or dollars of scientific research. Such expensive data collection prohibits application of standard RL algorithms which usually require a large number of observations to learn. In this work, we address the problem of efficiently learning a policy while making a minimal number of state-action queries to the transition function. In particular, we leverage ideas from Bayesian optimal experimental design to guide the selection of state-action queries for efficient learning. We propose an \\emph{acquisition function} that quantifies how much information a state-action pair would provide about the optimal solution to a Markov decision process. At each iteration, our algorithm maximizes this acquisition function, to choose the most informative state-action pair to be queried, thus yielding a data-efficient RL approach. We experiment with a variety of simulated continuous control problems and show that our approach learns an optimal policy with up to $5$ -- $1,000\\times$ less data than model-based RL baselines and $10^3$ -- $10^5\\times$ less data than model-free RL baselines. We also provide several ablated comparisons which point to substantial improvements arising from the principled method of obtaining data.", "pdf": "/pdf/63dd95d2070f3aa102826f2f0581987c13f5d0cf.pdf", "one-sentence_summary": "We draw a connection between Bayesian Optimal Experiment Design and RL to develop an acquisition function to guide data collection in model based RL leading to improved sample efficiency.", "supplementary_material": "/attachment/0660f347dce9114bd8bb8595b1b333fdf13e182e.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mehta|an_experimental_design_perspective_on_modelbased_reinforcement_learning", "_bibtex": "@inproceedings{\nmehta2022an,\ntitle={An Experimental Design Perspective on Model-Based Reinforcement Learning},\nauthor={Viraj Mehta and Biswajit Paria and Jeff Schneider and Willie Neiswanger and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0no8Motr-zO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "NdOoQnYPj_", "original": "1jY6PyJaIEL", "number": 2034, "cdate": 1632875561567, "mdate": null, "ddate": null, "tcdate": 1632875561567, "tmdate": 1676330578573, "tddate": null, "forum": "NdOoQnYPj_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "BAM: Bayes with Adaptive Memory", "authorids": ["~Josue_Nassar1", "~Jennifer_Rogers_Brennan1", "~Ben_Evans1", "~Kendall_Lowrey1"], "authors": ["Josue Nassar", "Jennifer Rogers Brennan", "Ben Evans", "Kendall Lowrey"], "keywords": ["Bayesian learning", "online learning"], "abstract": "Online learning via Bayes' theorem allows new data to be continuously integrated into an agent's current beliefs. However, a naive application of Bayesian methods in non-stationary environments leads to slow adaptation and results in state estimates that may converge confidently to the wrong parameter value. A common solution when learning in changing environments is to discard/downweight past data; however, this simple mechanism of \"forgetting\" fails to account for the fact that many real-world environments involve revisiting similar states. We propose a new framework, Bayes with Adaptive Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget. We demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments. Through a variety of experiments, we demonstrate the ability of BAM to continuously adapt in an ever-changing world.", "pdf": "/pdf/570533db54609ccaa349705c913de9ad4439ee1d.pdf", "one-sentence_summary": "We augment Bayes with memory to generalize many frameworks and overcome limitations of traditional methods in non-stationary settings", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nassar|bam_bayes_with_adaptive_memory", "_bibtex": "@inproceedings{\nnassar2022bam,\ntitle={{BAM}: Bayes with Adaptive Memory},\nauthor={Josue Nassar and Jennifer Rogers Brennan and Ben Evans and Kendall Lowrey},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NdOoQnYPj_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "izvwgBic9q", "original": "II-ef4RZJnR", "number": 2030, "cdate": 1632875560700, "mdate": null, "ddate": null, "tcdate": 1632875560700, "tmdate": 1676330578769, "tddate": null, "forum": "izvwgBic9q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and Partial Differential Equation in a Loop", "authorids": ["~Peng_Jin6", "~Xitong_Zhang1", "~Yinpeng_Chen1", "~Sharon_X_Huang1", "~Zicheng_Liu1", "~Youzuo_Lin1"], "authors": ["Peng Jin", "Xitong Zhang", "Yinpeng Chen", "Sharon X Huang", "Zicheng Liu", "Youzuo Lin"], "keywords": ["Unsupervised Learning", "Full-Waveform Inversion", "Partial Differential Equation", "Physics-Informed Machine Learning"], "abstract": "This paper investigates unsupervised learning of Full-Waveform Inversion (FWI), which has been widely used in geophysics to estimate subsurface velocity maps from seismic data. This problem is mathematically formulated by a second order partial differential equation (PDE), but is hard to solve. Moreover, acquiring velocity map is extremely expensive, making it impractical to scale up a supervised approach to train the mapping from seismic data to velocity maps with convolutional neural networks (CNN).We address these difficulties by $\\textit{integrating PDE and CNN in a loop}$, thus shifting the paradigm to unsupervised learning that only requires seismic data. In particular, we use finite difference to approximate the forward modeling of PDE as a differentiable operator (from velocity map to seismic data) and model its inversion by CNN (from seismic data to velocity map). Hence, we transform the supervised inversion task into an unsupervised seismic data reconstruction task. We also introduce a new large-scale dataset $\\textit{OpenFWI}$, to establish a more challenging benchmark for the community. Experiment results show that our model (using seismic data alone) yields comparable accuracy to the supervised counterpart (using both seismic data and velocity map). Furthermore, it outperforms the supervised model when involving more seismic data.", "one-sentence_summary": "We develop an unsupervised method to solve seismic full-waveform inversion in geophysics by integrating CNN and the governing partial differential equation. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jin|unsupervised_learning_of_fullwaveform_inversion_connecting_cnn_and_partial_differential_equation_in_a_loop", "pdf": "/pdf/2b601798926b31583beb679577563de37154b91f.pdf", "_bibtex": "@inproceedings{\njin2022unsupervised,\ntitle={Unsupervised Learning of Full-Waveform Inversion: Connecting {CNN} and Partial Differential Equation in a Loop},\nauthor={Peng Jin and Xitong Zhang and Yinpeng Chen and Sharon X Huang and Zicheng Liu and Youzuo Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=izvwgBic9q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "AAJLBoGt0XM", "original": "rTV_t4zDclr", "number": 2029, "cdate": 1632875560436, "mdate": null, "ddate": null, "tcdate": 1632875560436, "tmdate": 1697934756975, "tddate": null, "forum": "AAJLBoGt0XM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Conditional Contrastive Learning with Kernel", "authorids": ["~Yao-Hung_Hubert_Tsai1", "~Tianqin_Li2", "~Martin_Q._Ma1", "~Han_Zhao1", "~Kun_Zhang1", "~Louis-Philippe_Morency1", "~Ruslan_Salakhutdinov1"], "authors": ["Yao-Hung Hubert Tsai", "Tianqin Li", "Martin Q. Ma", "Han Zhao", "Kun Zhang", "Louis-Philippe Morency", "Ruslan Salakhutdinov"], "keywords": ["Contrastive Learning", "Conditional Sampling", "Kernel methods"], "abstract": "Conditional contrastive learning frameworks consider the conditional sampling procedure that constructs positive or negative data pairs conditioned on specific variables. Fair contrastive learning constructs negative pairs, for example, from the same gender (conditioning on sensitive information), which in turn reduces undesirable information from the learned representations; weakly supervised contrastive learning constructs positive pairs with similar annotative attributes (conditioning on auxiliary information), which in turn are incorporated into the representations. Although conditional contrastive learning enables many applications, the conditional sampling procedure can be challenging if we cannot obtain sufficient data pairs for some values of the conditioning variable. This paper presents Conditional Contrastive Learning with Kernel (CCL-K) that converts existing conditional contrastive objectives into alternative forms that mitigate the insufficient data problem. Instead of sampling data according to the value of the conditioning variable, CCL-K uses the Kernel Conditional Embedding Operator that samples data from all available data and assigns weights to each sampled data given the kernel similarity between the values of the conditioning variable. We conduct experiments using weakly supervised, fair, and hard negatives contrastive learning, showing CCL-K outperforms state-of-the-art baselines.\n", "one-sentence_summary": "This paper presents Conditional Contrastive Learning with Kernel (CCL-K) for conditional contrastive learning tasks under the scenario when we have insufficient data for some values of the condioning variable.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tsai|conditional_contrastive_learning_with_kernel", "pdf": "/pdf/b1faf52b79128f7491658b68cc175c78d5313668.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.05458/code)", "_bibtex": "@inproceedings{\ntsai2022conditional,\ntitle={Conditional Contrastive Learning with Kernel},\nauthor={Yao-Hung Hubert Tsai and Tianqin Li and Martin Q. Ma and Han Zhao and Kun Zhang and Louis-Philippe Morency and Ruslan Salakhutdinov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AAJLBoGt0XM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "zRJu6mU2BaE", "original": "cjQ0RV71uVK", "number": 2024, "cdate": 1632875559453, "mdate": null, "ddate": null, "tcdate": 1632875559453, "tmdate": 1676330579123, "tddate": null, "forum": "zRJu6mU2BaE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning", "authorids": ["~Debasmit_Das2", "~Sungrack_Yun1", "~Fatih_Porikli2"], "authors": ["Debasmit Das", "Sungrack Yun", "Fatih Porikli"], "keywords": [], "abstract": "Most current few-shot learning methods train a model from abundantly labeled base category data and then transfer and adapt the model to sparsely labeled novel category data. These methods mostly generalize well on novel categories from the same domain as the base categories but perform poorly for distant domain categories. In this paper, we propose a framework for few-shot learning coined as ConFeSS (Contrastive Learning and Feature Selection System) that tackles large domain shift between base and novel categories. The first step of our framework trains a feature extracting backbone with the contrastive loss on the base category data. Since the contrastive loss does not use supervision, the features can generalize better to distant target domains. For the second step, we train a masking module to select relevant features that are more suited to target domain classification. Finally, a classifier is fine-tuned along with the backbone such that the backbone produces features similar to the relevant ones. To evaluate our framework, we tested it on a recently introduced cross-domain few-shot learning benchmark. Experimental results demonstrate that our framework outperforms all meta-learning approaches and produces competitive results against recent cross-domain methods. Additional analyses are also performed to better understand our framework.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "das|confess_a_framework_for_single_source_crossdomain_fewshot_learning", "pdf": "/pdf/9a6c5c7a1d3338348f0f985794c41857eabbb501.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\ndas2022confess,\ntitle={ConFe{SS}: A Framework for Single Source Cross-Domain Few-Shot Learning},\nauthor={Debasmit Das and Sungrack Yun and Fatih Porikli},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zRJu6mU2BaE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "nZOUYEN6Wvy", "original": "Li0h_b2TJPX", "number": 2019, "cdate": 1632875558438, "mdate": null, "ddate": null, "tcdate": 1632875558438, "tmdate": 1676330579586, "tddate": null, "forum": "nZOUYEN6Wvy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Granger causal inference on DAGs identifies genomic loci regulating transcription", "authorids": ["~Alexander_P_Wu1", "~Rohit_Singh1", "~Bonnie_Berger1"], "authors": ["Alexander P Wu", "Rohit Singh", "Bonnie Berger"], "keywords": ["Granger causality", "causal inference", "graph neural networks", "gene regulation", "single-cell genomics", "chromatin accessibility", "directed acyclic graphs", "single-cell multimodal"], "abstract": "When a dynamical system can be modeled as a sequence of observations, Granger causality is a powerful approach for detecting predictive interactions between its variables. However, traditional Granger causal inference has limited utility in domains where the dynamics need to be represented as directed acyclic graphs (DAGs) rather than as a linear sequence, such as with cell differentiation trajectories. Here, we present GrID-Net, a framework based on graph neural networks with lagged message passing for Granger causal inference on DAG-structured systems. Our motivating application is the analysis of single-cell multimodal data to identify genomic loci that mediate the regulation of specific genes. To our knowledge, GrID-Net is the first single-cell analysis tool that accounts for the temporal lag between a genomic locus becoming accessible and its downstream effect on a target gene's expression. We applied GrID-Net on multimodal single-cell assays that profile chromatin accessibility (ATAC-seq) and gene expression (RNA-seq) in the same cell and show that it dramatically outperforms existing methods for inferring regulatory locus-gene links, achieving up to 71% greater agreement with independent population genetics-based estimates. By extending Granger causality to DAG-structured dynamical systems, our work unlocks new domains for causal analyses and, more specifically, opens a path towards elucidating gene regulatory interactions relevant to cellular differentiation and complex human diseases at unprecedented scale and resolution.", "pdf": "/pdf/85cdf969129ce01319414cf3e7f1ffc801bb8db0.pdf", "one-sentence_summary": "We show how to extend Granger causality to DAG-structured dynamical systems using graph neural networks, applying it to infer noncoding regions involved in gene regulation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|granger_causal_inference_on_dags_identifies_genomic_loci_regulating_transcription", "supplementary_material": "/attachment/59e32c9e174ddc61794670360dd59ebbea7cbb9c.zip", "_bibtex": "@inproceedings{\nwu2022granger,\ntitle={Granger causal inference on {DAG}s identifies genomic loci regulating transcription},\nauthor={Alexander P Wu and Rohit Singh and Bonnie Berger},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nZOUYEN6Wvy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "7QfLW-XZTl", "original": "DcU3IpSsQq", "number": 2010, "cdate": 1632875557088, "mdate": null, "ddate": null, "tcdate": 1632875557088, "tmdate": 1676330579964, "tddate": null, "forum": "7QfLW-XZTl", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Energy-Inspired Molecular Conformation Optimization", "authorids": ["~Jiaqi_Guan1", "~Wesley_Wei_Qian1", "~qiang_liu4", "~Wei-Ying_Ma2", "~Jianzhu_Ma2", "~Jian_Peng1"], "authors": ["Jiaqi Guan", "Wesley Wei Qian", "qiang liu", "Wei-Ying Ma", "Jianzhu Ma", "Jian Peng"], "keywords": [], "abstract": "This paper studies an important problem in computational chemistry: predicting a molecule's spatial atom arrangements, or a molecular conformation. We propose a neural energy minimization formulation that casts the prediction problem into an unrolled optimization process, where a neural network is parametrized to learn the gradient fields of an implicit conformational energy landscape. Assuming different forms of the underlying potential energy function, we can not only reinterpret and unify many of the existing models but also derive new variants of SE(3)-equivariant neural networks in a principled manner. In our experiments, these new variants show superior performance in molecular conformation optimization comparing to existing SE(3)-equivariant neural networks. Moreover, our energy-inspired formulation is also suitable for molecular conformation generation, where we can generate more diverse and accurate conformers comparing to existing baselines.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guan|energyinspired_molecular_conformation_optimization", "pdf": "/pdf/4a1d13d340c8727a4bc1ddee07f22d749cf0db8d.pdf", "supplementary_material": "/attachment/c711b93785348fbed6d765b5dda99d8211395e85.zip", "_bibtex": "@inproceedings{\nguan2022energyinspired,\ntitle={Energy-Inspired Molecular Conformation Optimization},\nauthor={Jiaqi Guan and Wesley Wei Qian and qiang liu and Wei-Ying Ma and Jianzhu Ma and Jian Peng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7QfLW-XZTl}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 33}}, {"id": "tT9t_ZctZRL", "original": "1u65m_5v29", "number": 2003, "cdate": 1632875555430, "mdate": null, "ddate": null, "tcdate": 1632875555430, "tmdate": 1676330580257, "tddate": null, "forum": "tT9t_ZctZRL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Deepening Graph Neural Networks: A GNTK-based Optimization Perspective", "authorids": ["~Wei_Huang6", "~Yayong_Li1", "~weitao_Du1", "~Richard_Xu1", "~Jie_Yin3", "~Ling_Chen5", "~Miao_Zhang4"], "authors": ["Wei Huang", "Yayong Li", "weitao Du", "Richard Xu", "Jie Yin", "Ling Chen", "Miao Zhang"], "keywords": ["Trainablity", "Graph Neural Tangent Kernel", "Critical DropEdge"], "abstract": "Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. Nevertheless, it is well known that deep GCNs suffer from the over-smoothing problem, where node representations tend to be indistinguishable as more layers are stacked up. The theoretical research to date on deep GCNs has focused primarily on expressive power rather than trainability, an optimization perspective. Compared to expressivity, trainability attempts to address a more fundamental question: Given a sufficiently expressive space of models, can we successfully find a good solution via gradient descent-based optimizers? This work fills this gap by exploiting the Graph Neural Tangent Kernel (GNTK), which governs the optimization trajectory under gradient descent for wide GCNs. We formulate the asymptotic behaviors of GNTK in the large depth, which enables us to reveal the dropping trainability of wide and deep GCNs at an exponential rate in the optimization process. Additionally, we extend our theoretical framework to analyze residual connection-based techniques, which are found to be merely able to mitigate the exponential decay of trainability mildly. Inspired by our theoretical insights on trainability, we propose Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, to alleviate the exponential decay problem more fundamentally. Experimental evaluation consistently confirms using our proposed method can achieve better results compared to relevant counterparts with both infinite-width and finite-width. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|towards_deepening_graph_neural_networks_a_gntkbased_optimization_perspective", "pdf": "/pdf/c18075d5d39ff0265c97344cea20791494daa455.pdf", "one-sentence_summary": "This work theoretically studies the trainability of deep graph neural networks, and is inspired to design algorithms to deepen the network.", "supplementary_material": "/attachment/8b211406b7fd41568ddaea218e2e51e50fc1dd0f.zip", "_bibtex": "@inproceedings{\nhuang2022towards,\ntitle={Towards Deepening Graph Neural Networks: A {GNTK}-based Optimization Perspective},\nauthor={Wei Huang and Yayong Li and weitao Du and Richard Xu and Jie Yin and Ling Chen and Miao Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tT9t_ZctZRL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "CJzi3dRlJE-", "original": "YZAZ7NK1uqh", "number": 2001, "cdate": 1632875555291, "mdate": null, "ddate": null, "tcdate": 1632875555291, "tmdate": 1676330580323, "tddate": null, "forum": "CJzi3dRlJE-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Connectome-constrained Latent Variable Model of Whole-Brain Neural Activity", "authorids": ["~Lu_Mi1", "~Richard_Xu2", "~Sridhama_Prakhya1", "~Albert_Lin2", "~Nir_Shavit1", "~Aravinthan_Samuel1", "~Srinivas_C_Turaga1"], "authors": ["Lu Mi", "Richard Xu", "Sridhama Prakhya", "Albert Lin", "Nir Shavit", "Aravinthan Samuel", "Srinivas C Turaga"], "keywords": ["connectome", "latent-variable model", "variational autoencoder", "biophysics", "whole-brain", "neural activity", "calcium imaging", "caenorhabditis elegans", "voltage", "generative model", "inference network"], "abstract": "The availability of both anatomical connectivity and brain-wide neural activity measurements in C. elegans make the worm a promising system for learning detailed, mechanistic models of an entire nervous system in a data-driven way. However, one faces several challenges when constructing such a model. We often do not have direct experimental access to important modeling details such as single-neuron dynamics and the signs and strengths of the synaptic connectivity. Further, neural activity can only be measured in a subset of neurons, often indirectly via calcium imaging, and significant trial-to-trial variability has been observed. To address these challenges, we introduce a connectome-constrained latent variable model (CC-LVM) of the unobserved voltage dynamics of the entire C. elegans nervous system and the observed calcium signals. We used the framework of variational autoencoders to fit parameters of the mechanistic simulation constituting the generative model of the LVM to calcium imaging observations. A variational approximate posterior distribution over latent voltage traces for all neurons is efficiently inferred using an inference network, and constrained by a prior distribution given by the biophysical simulation of neural dynamics. We applied this model to an experimental whole-brain dataset, and found that connectomic constraints enable our LVM to predict the activity of neurons whose activity were withheld significantly better than models unconstrained by a connectome. We explored models with different degrees of biophysical detail, and found that models with realistic conductance-based synapses provide markedly better predictions than current-based synapses for this system.", "one-sentence_summary": "Connectome-constrained Latent Variable Model of Whole-Brain Neural Activity.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mi|connectomeconstrained_latent_variable_model_of_wholebrain_neural_activity", "pdf": "/pdf/302ccb9e518f0da30bda16c4db8038467af5f38e.pdf", "_bibtex": "@inproceedings{\nmi2022connectomeconstrained,\ntitle={Connectome-constrained Latent Variable Model of Whole-Brain Neural Activity},\nauthor={Lu Mi and Richard Xu and Sridhama Prakhya and Albert Lin and Nir Shavit and Aravinthan Samuel and Srinivas C Turaga},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CJzi3dRlJE-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "U4uFaLyg7PV", "original": "OX2k2sqNLT6", "number": 1998, "cdate": 1632875555044, "mdate": null, "ddate": null, "tcdate": 1632875555044, "tmdate": 1676330580554, "tddate": null, "forum": "U4uFaLyg7PV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "T-WaveNet: A Tree-Structured Wavelet Neural Network for Time Series Signal Analysis", "authorids": ["~Minhao_LIU1", "~Ailing_Zeng1", "~Qiuxia_LAI1", "~Ruiyuan_Gao2", "~Min_Li7", "~Jing_Qin3", "~Qiang_Xu1"], "authors": ["Minhao LIU", "Ailing Zeng", "Qiuxia LAI", "Ruiyuan Gao", "Min Li", "Jing Qin", "Qiang Xu"], "keywords": [], "abstract": "Time series signal analysis plays an essential role in many applications, e.g., activity recognition and healthcare monitoring.\nRecently, features extracted with deep neural networks (DNNs) have shown to be more effective than conventional hand-crafted ones.\nHowever, most existing solutions rely solely on the network to extract information carried in the raw signal, regardless of its inherent physical and statistical properties, leading to sub-optimal performance particularly under a limited amount of training data.\nIn this work, we propose a novel tree-structured wavelet neural network for time series signal analysis, namely \\emph{T-WaveNet}, taking advantage of an inherent property of various types of signals, known as the \\emph{dominant frequency range}. Specifically, with \\emph{T-WaveNet}, we first conduct frequency spectrum energy analysis of the signals to get a set of dominant frequency subbands. Then, we construct a tree-structured network that iteratively decomposes the input signal into various frequency subbands with similar energies. Each node on the tree is built with an invertible neural network (INN) based wavelet transform unit. Such a disentangled representation learning method facilitates a more effective extraction of the discriminative features, as demonstrated with the comprehensive experiments on various real-life time series classification datasets. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|twavenet_a_treestructured_wavelet_neural_network_for_time_series_signal_analysis", "pdf": "/pdf/c527f1cebdb3737b281ff65a78f914a97d634bad.pdf", "supplementary_material": "/attachment/1f5132768675a9d90eaa3c6a4fcf7c3578ee7975.zip", "_bibtex": "@inproceedings{\nliu2022twavenet,\ntitle={T-WaveNet: A Tree-Structured Wavelet Neural Network for Time Series Signal Analysis},\nauthor={Minhao LIU and Ailing Zeng and Qiuxia LAI and Ruiyuan Gao and Min Li and Jing Qin and Qiang Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=U4uFaLyg7PV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "AmUhwTOHgm", "original": "MgWIY25aHu6", "number": 1987, "cdate": 1632875554273, "mdate": null, "ddate": null, "tcdate": 1632875554273, "tmdate": 1676330581254, "tddate": null, "forum": "AmUhwTOHgm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations", "authorids": ["~Fangyu_Liu1", "~Yunlong_Jiao1", "~Jordan_Massiah1", "~Emine_Yilmaz1", "~Serhii_Havrylov1"], "authors": ["Fangyu Liu", "Yunlong Jiao", "Jordan Massiah", "Emine Yilmaz", "Serhii Havrylov"], "keywords": ["self-supervised learning", "sentence embeddings", "sentence representations", "knowledge distillation"], "abstract": "In NLP, a large volume of tasks involve pairwise comparison between two sequences (e.g. sentence similarity and paraphrase identification). Predominantly, two formulations are used for sentence-pair tasks: bi-encoders and cross-encoders. Bi-encoders produce fixed-dimensional sentence representations and are computationally efficient, however, they usually underperform cross-encoders. Cross-encoders can leverage their attention heads to exploit inter-sentence interactions for better performance but they require task fine-tuning and are computationally more expensive. In this paper, we present a completely unsupervised sentence representation model termed as Trans-Encoder that combines the two learning paradigms into an iterative joint framework to simultaneously learn enhanced bi- and cross-encoders. Specifically, on top of a pre-trained Language Model (PLM), we start with converting it to an unsupervised bi-encoder, and then alternate between the bi- and cross-encoder task formulations. In each alternation, one task formulation will produce pseudo-labels which are used as learning signals for the other task formulation. We then propose an extension to conduct such self-distillation approach on multiple PLMs in parallel and use the average of their pseudo-labels for mutual distillation. Trans-Encoder creates, to the best of our knowledge, the first completely unsupervised cross-encoder and also a state-of-the-art unsupervised bi-encoder for sentence similarity. Both the bi-encoder and cross-encoder formulations of Trans-Encoder outperform recently proposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT and SimCSE by up to 5% on the sentence similarity benchmarks.", "one-sentence_summary": "Bootstrapping an unsupervised sentence encoder by self-distilling knowledge between its bi-encoder and cross-encoder forms, enhancing each other iteratively.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|transencoder_unsupervised_sentencepair_modelling_through_self_and_mutualdistillations", "pdf": "/pdf/3a6f31c7903c67d5e431aafcd98d91be36443b10.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nliu2022transencoder,\ntitle={Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations},\nauthor={Fangyu Liu and Yunlong Jiao and Jordan Massiah and Emine Yilmaz and Serhii Havrylov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AmUhwTOHgm}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "_uCb2ynRu7Y", "original": "05JOrcmvYhB", "number": 1986, "cdate": 1632875554205, "mdate": null, "ddate": null, "tcdate": 1632875554205, "tmdate": 1697934760849, "tddate": null, "forum": "_uCb2ynRu7Y", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Path Integral Sampler: A Stochastic Control Approach For Sampling", "authorids": ["~Qinsheng_Zhang1", "~Yongxin_Chen1"], "authors": ["Qinsheng Zhang", "Yongxin Chen"], "keywords": ["Sampling", "Path Integral", "Stochastic Differential Equation", "MCMC"], "abstract": "We present Path Integral Sampler~(PIS), a novel algorithm to draw samples from unnormalized probability density functions. The PIS is built on the Schr\\\"odinger bridge problem which aims to recover the most likely evolution of a diffusion process given its initial distribution and terminal distribution. The PIS draws samples from the initial distribution and then propagates the samples through the Schr\\\"odinger bridge to reach the terminal distribution. Applying the Girsanov theorem, with a simple prior diffusion, we formulate the PIS as a stochastic optimal control problem whose running cost is the control energy and terminal cost is chosen according to the target distribution. By modeling the control as a neural network, we establish a sampling algorithm that can be trained end-to-end. We provide theoretical justification of the sampling quality of PIS in terms of Wasserstein distance when sub-optimal control is used. Moreover, the path integrals theory is used to compute importance weights of the samples to compensate for the bias induced by the sub-optimality of the controller and the time-discretization. We experimentally demonstrate the advantages of PIS compared with other start-of-the-art sampling methods on a variety of tasks.", "one-sentence_summary": "We present Path Integral Sampler~(PIS), an efficient algorithm to draw samples from unnormalized probability density functions. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|path_integral_sampler_a_stochastic_control_approach_for_sampling", "pdf": "/pdf/dfed03e002c8966cbf8b7224471dc61c00f141af.pdf", "supplementary_material": "/attachment/484a2b361b234755970542e97f1eff4bb74bec3a.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2111.15141/code)", "_bibtex": "@inproceedings{\nzhang2022path,\ntitle={Path Integral Sampler: A Stochastic Control Approach For Sampling},\nauthor={Qinsheng Zhang and Yongxin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_uCb2ynRu7Y}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "WfvgGBcgbE7", "original": "_KIoCHzJKBP", "number": 1980, "cdate": 1632875553937, "mdate": null, "ddate": null, "tcdate": 1632875553937, "tmdate": 1676330581529, "tddate": null, "forum": "WfvgGBcgbE7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Model Zoo: A Growing Brain That Learns Continually", "authorids": ["~Rahul_Ramesh2", "~Pratik_Chaudhari1"], "authors": ["Rahul Ramesh", "Pratik Chaudhari"], "keywords": ["Continual Learning", "Learning Theory"], "abstract": "This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models. We use statistical learning theory and experimental analysis to show how multiple tasks can interact with each other in a non-trivial fashion when a single model is trained on them. The generalization error on a particular task can improve when it is trained with synergistic tasks, but can also deteriorate when trained with competing tasks. This theory motivates our method named Model Zoo which, inspired from the boosting literature, grows an ensemble of small models, each of which is trained during one episode of continual learning. We demonstrate that Model Zoo obtains large gains in accuracy on a wide variety of continual learning benchmark problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ramesh|model_zoo_a_growing_brain_that_learns_continually", "pdf": "/pdf/19902f663252dc5dfc680fe78ec3d27c0a55e73e.pdf", "one-sentence_summary": "Continual learning methods can benefit by splitting the capacity of the learner and we leverage this in our method Model Zoo, which demonstrates large gains in accuracy on a variety of continual learning benchmarks.", "supplementary_material": "/attachment/eff66ec7507a03e05044d3a479987d2279e8aaf6.zip", "_bibtex": "@inproceedings{\nramesh2022model,\ntitle={Model Zoo: A Growing Brain That Learns Continually},\nauthor={Rahul Ramesh and Pratik Chaudhari},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WfvgGBcgbE7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 34}}, {"id": "XctLdNfCmP", "original": "xzsvPcYB5Nt", "number": 1973, "cdate": 1632875553604, "mdate": null, "ddate": null, "tcdate": 1632875553604, "tmdate": 1676330582149, "tddate": null, "forum": "XctLdNfCmP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Predicting Physics in Mesh-reduced Space with Temporal Attention", "authorids": ["~XU_HAN6", "~Han_Gao3", "~Tobias_Pfaff1", "~Jian-Xun_Wang1", "~Liping_Liu1"], "authors": ["XU HAN", "Han Gao", "Tobias Pfaff", "Jian-Xun Wang", "Liping Liu"], "keywords": ["fluid dynamics", "graph neural network", "attention neural network"], "abstract": "Auto-regressive sequence models for physics prediction are often restricted to low-dimensional systems, as memory cost increases with both spatial extents and sequence length. On the other hand, graph-based next-step prediction models have recently been very successful in modeling complex high-dimensional physical systems on irregular meshes, but suffer from error accumulation and drift, due to their short temporal attention span. In this paper, we present a method that marries the strengths of both approaches. We use a GNN to locally summarize features and create coarsened, compact mesh representation of the system state, onto which we apply a transformer-style temporal attention module. We use a second GNN to decode these predictions back to a full-sized graph and perform fine-scale updates. Our method outperforms a competitive GNN baseline on three complex fluid dynamics prediction tasks, from sonic shocks to vascular flow. We demonstrate stable rollouts without the need for training noise and show perfectly phase-stable predictions even for very long sequences. More broadly, we believe our approach paves the way to bringing the benefits of attention-based sequence models to solving high-dimensional complex physics tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "han|predicting_physics_in_meshreduced_space_with_temporal_attention", "pdf": "/pdf/00dd7d600fb519f519f11d2cc02d0c629f487ece.pdf", "one-sentence_summary": " We use a GNN to locally summarize features and create coarsened, compact mesh representation of the system state, onto which we apply a transformer-style temporal attention module for physics prediction.", "supplementary_material": "/attachment/852978b8f8b6743955ef5f06733f3c3e8dddeeaf.zip", "_bibtex": "@inproceedings{\nhan2022predicting,\ntitle={Predicting Physics in Mesh-reduced Space with Temporal Attention},\nauthor={XU HAN and Han Gao and Tobias Pfaff and Jian-Xun Wang and Liping Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XctLdNfCmP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "qiMXBIf4NfB", "original": "1OjxKjMba9u", "number": 1970, "cdate": 1632875553388, "mdate": null, "ddate": null, "tcdate": 1632875553388, "tmdate": 1676330582437, "tddate": null, "forum": "qiMXBIf4NfB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis", "authorids": ["~Shuai_Zhang6", "~Meng_Wang4", "~Sijia_Liu1", "~Pin-Yu_Chen1", "~Jinjun_Xiong1"], "authors": ["Shuai Zhang", "Meng Wang", "Sijia Liu", "Pin-Yu Chen", "Jinjun Xiong"], "keywords": ["Self-training", "Semi-supervised learning", "Convergence analysis", "Generalization analysis"], "abstract": "Self-training, a semi-supervised learning algorithm, leverages a large amount of unlabeled data to improve learning when the labeled data are limited. Despite empirical successes, its theoretical characterization remains elusive. To the best of our knowledge, this work establishes the first theoretical analysis for the known iterative self-training paradigm and formally proves the benefits of unlabeled data in both training convergence and generalization ability. To make our theoretical analysis feasible, we focus on the case of one-hidden-layer neural networks. However, theoretical understanding of iterative self-training is non-trivial even for a shallow neural network. One of the key challenges is that existing neural network landscape analysis built upon supervised learning no longer holds in the (semi-supervised) self-training paradigm. We address this challenge and prove that iterative self-training converges linearly with both convergence rate and generalization accuracy improved in the order of $1/\\sqrt{M}$, where $M$ is the number of unlabeled samples. Extensive experiments from shallow neural networks to deep neural networks are also provided to justify the correctness of our established theoretical insights on self-training.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|how_unlabeled_data_improve_generalization_in_selftraining_a_onehiddenlayer_theoretical_analysis", "pdf": "/pdf/86dd54f483759d42c389b5c48000ced72778d937.pdf", "one-sentence_summary": "Theoretical characterization of unlabeled data in improving generalization and convergence rate via iterative self-training algorithm ", "supplementary_material": "/attachment/781bd11dc934b1794750fea689c2b3276839f5d4.zip", "_bibtex": "@inproceedings{\nzhang2022how,\ntitle={How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis},\nauthor={Shuai Zhang and Meng Wang and Sijia Liu and Pin-Yu Chen and Jinjun Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qiMXBIf4NfB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "fExcSKdDo_", "original": "mjRMTJsMdf1", "number": 1967, "cdate": 1632875553175, "mdate": null, "ddate": null, "tcdate": 1632875553175, "tmdate": 1676330582590, "tddate": null, "forum": "fExcSKdDo_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Dequantise with Truncated Flows", "authorids": ["~Shawn_Tan1", "~Chin-Wei_Huang1", "~Alessandro_Sordoni2", "~Aaron_Courville3"], "authors": ["Shawn Tan", "Chin-Wei Huang", "Alessandro Sordoni", "Aaron Courville"], "keywords": ["variational inference", "variational bayes", "dequantisation", "normalizing flows"], "abstract": "Dequantisation is a general technique used for transforming data described by a discrete random variable $x$ into a continuous (latent) random variable $z$, for the purpose of it being modeled by likelihood-based density models. Dequantisation was first introduced in the context of ordinal data, such as image pixel values.  However, when the data is categorical, the dequantisation scheme is not obvious.\nWe learn such a dequantisation scheme $q(z | x)$, using variational inference with TRUncated FLows (TRUFL) --- a novel flow-based model that allows the dequantiser to have a learnable truncated support. Unlike previous work, the TRUFL dequantiser is (i) capable of embedding the data losslessly in certain cases, since the truncation allows the conditional distributions $q(z | x)$ to have non-overlapping bounded supports, while being (ii) trainable with back-propagation. Addtionally, since the support of the marginal $q(z)$ is bounded and the support of prior $p(z)$ is not, we propose renormalising the prior distribution over the support of $q(z)$. We derive a lower bound for training, and propose a rejection sampling scheme to account for the invalid samples during generation.\nExperimentally, we benchmark TRUFL on constrained generation tasks, and find that it outperforms prior approaches. In addition, we find that rejection sampling results in higher validity for the constrained problems.", "one-sentence_summary": "Learning a variational dequantisation scheme with truncated/bounded-support distributions", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tan|learning_to_dequantise_with_truncated_flows", "pdf": "/pdf/7b8b40bfc88c9c57b2e374766d2ad6dbf203c158.pdf", "supplementary_material": "/attachment/b2cd22cbe66bf3e5020be2c1e8b0eeeeb8de523f.zip", "data": "", "_bibtex": "@inproceedings{\ntan2022learning,\ntitle={Learning to Dequantise with Truncated Flows},\nauthor={Shawn Tan and Chin-Wei Huang and Alessandro Sordoni and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fExcSKdDo_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "TpJMvo0_pu-", "original": "4Jk6-q1MCmT", "number": 1962, "cdate": 1632875552832, "mdate": null, "ddate": null, "tcdate": 1632875552832, "tmdate": 1676330582775, "tddate": null, "forum": "TpJMvo0_pu-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Curriculum learning as a tool to uncover learning principles in the brain ", "authorids": ["~Daniel_R._Kepple1", "~Rainer_Engelken1", "~Kanaka_Rajan1"], "authors": ["Daniel R. Kepple", "Rainer Engelken", "Kanaka Rajan"], "keywords": ["curriculum learning", "neuroscience"], "abstract": "We present a novel approach to use curricula to identify principles by which a system learns. Previous work in curriculum learning has focused on how curricula can be designed to improve learning of a model on particular tasks. We consider the inverse problem: what can a curriculum tell us about how a learning system acquired a task? Using recurrent neural networks (RNNs) and models of common experimental neuroscience tasks, we demonstrate that curricula can be used to differentiate learning principles using target-based and a representation-based loss functions as use cases. In particular, we compare the performance of RNNs using target-based learning rules versus those using representational learning rules on three different curricula in the context of two tasks. We show that the learned state-space trajectories of RNNs trained by these two learning rules under all curricula tested are indistinguishable. However, by comparing learning times during different curricula, we can disambiguate the learning rules and challenge traditional approaches of interrogating learning systems. Although all animals in neuroscience lab settings are trained by curriculum-based procedures called shaping, almost no behavioral or neural data are collected or published on the relative successes or training times under different curricula. Our results motivate the systematic collection and curation of data during shaping by demonstrating curriculum learning in RNNs as a tool to probe and differentiate learning principles used by biological systems, over conventional statistical analyses of learned state spaces.", "one-sentence_summary": "We present a novel approach to use curricula to identify principles by which a system learns.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kepple|curriculum_learning_as_a_tool_to_uncover_learning_principles_in_the_brain", "pdf": "/pdf/a4a7a68a3eb6363b1e9a2ccdc13a20acf84334c2.pdf", "_bibtex": "@inproceedings{\nkepple2022curriculum,\ntitle={Curriculum learning as a tool to uncover learning principles in the brain },\nauthor={Daniel R. Kepple and Rainer Engelken and Kanaka Rajan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TpJMvo0_pu-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "VqzXzA9hjaX", "original": "TD14KenGD8l", "number": 1957, "cdate": 1632875552559, "mdate": null, "ddate": null, "tcdate": 1632875552559, "tmdate": 1697934764609, "tddate": null, "forum": "VqzXzA9hjaX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Optimizer Amalgamation", "authorids": ["~Tianshu_Huang1", "~Tianlong_Chen1", "~Sijia_Liu1", "~Shiyu_Chang2", "~Lisa_Amini1", "~Zhangyang_Wang1"], "authors": ["Tianshu Huang", "Tianlong Chen", "Sijia Liu", "Shiyu Chang", "Lisa Amini", "Zhangyang Wang"], "keywords": ["Learning to Optimize", "Knowledge Amalgamation", "Stability-Aware Training"], "abstract": "Selecting an appropriate optimizer for a given problem is of major interest for researchers and practitioners. Many analytical optimizers have been proposed using a variety of theoretical and empirical approaches; however, none can offer a universal advantage over other competitive optimizers. We are thus motivated to study a new problem named Optimizer Amalgamation: how can we best combine a pool of \"teacher\" optimizers into a single \"student\" optimizer that can have stronger problem-specific performance? In this paper, we draw inspiration from the field of \"learning to optimize\" to use a learnable amalgamation target. First, we define three differentiable amalgamation mechanisms to amalgamate a pool of analytical optimizers by gradient descent. Then, in order to reduce variance of the amalgamation process, we also explore methods to stabilize the amalgamation process by perturbing the amalgamation target. Finally, we present experiments showing the superiority of our amalgamated optimizer compared to its amalgamated components and learning to optimize baselines, and the efficacy of our variance reducing perturbations.\n", "one-sentence_summary": "We study a new problem named Optimizer Amalgamation: combining a pool of \"teacher\" optimizers into a single \"student\" optimizer which can have stronger problem-specific performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|optimizer_amalgamation", "pdf": "/pdf/5f3e6351e8a4303197b51d671a2519678ac9fd21.pdf", "supplementary_material": "/attachment/0b380a955ad88e5ca5ed7eb72c61d91ff50d01d7.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.06474/code)", "_bibtex": "@inproceedings{\nhuang2022optimizer,\ntitle={Optimizer Amalgamation},\nauthor={Tianshu Huang and Tianlong Chen and Sijia Liu and Shiyu Chang and Lisa Amini and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VqzXzA9hjaX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "Xo0lbDt975", "original": "ZkkUChdmfgc", "number": 1942, "cdate": 1632875551657, "mdate": null, "ddate": null, "tcdate": 1632875551657, "tmdate": 1676330584046, "tddate": null, "forum": "Xo0lbDt975", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "An Agnostic Approach to Federated Learning with Class Imbalance", "authorids": ["~Zebang_Shen1", "~Juan_Cervino1", "~Hamed_Hassani2", "~Alejandro_Ribeiro1"], "authors": ["Zebang Shen", "Juan Cervino", "Hamed Hassani", "Alejandro Ribeiro"], "keywords": ["Federated Learning", "Class Imbalance"], "abstract": "Federated Learning (FL) has emerged as the tool of choice for training deep models over heterogeneous and decentralized datasets. \nAs a reflection of the experiences from different clients, severe class imbalance issues are observed in real-world FL problems.\nMoreover, there exists a drastic mismatch between the imbalances from the local and global perspectives, i.e. a local majority class can be the minority of the population. Additionally, the privacy requirement of FL poses an extra challenge, as one should handle class imbalance without identifying the minority class. In this paper we propose a novel agnostic constrained learning formulation to tackle the class imbalance problem in FL, without requiring further information beyond the standard FL objective. A meta algorithm, CLIMB, is designed to solve the target optimization problem, with its convergence property analyzed under certain oracle assumptions. Through an extensive empirical study over various data heterogeneity and class imbalance configurations, we showcase that CLIMB considerably improves the performance in the minority class without compromising the overall accuracy of the classifier, which significantly outperforms previous arts. \nIn fact, we observe the greatest performance boost in the most difficult scenario where every client only holds data from one class. The code can be found here https://github.com/shenzebang/Federated-Learning-Pytorch.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shen|an_agnostic_approach_to_federated_learning_with_class_imbalance", "pdf": "/pdf/5fb84fb47293dfccb811ff5d9f4c2a1861530f17.pdf", "supplementary_material": "/attachment/f8b50e1828ed0dbda6b241f7e5894d032b025e04.zip", "data": "", "_bibtex": "@inproceedings{\nshen2022an,\ntitle={An Agnostic Approach to Federated Learning with Class Imbalance},\nauthor={Zebang Shen and Juan Cervino and Hamed Hassani and Alejandro Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Xo0lbDt975}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "ckZY7DGa7FQ", "original": "4RCJJ2nP1m", "number": 1940, "cdate": 1632875551520, "mdate": null, "ddate": null, "tcdate": 1632875551520, "tmdate": 1676330584178, "tddate": null, "forum": "ckZY7DGa7FQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Fine-Tuning Approach to Belief State Modeling", "authorids": ["~Samuel_Sokota1", "~Hengyuan_Hu2", "~David_J_Wu1", "~J_Zico_Kolter1", "~Jakob_Nicolaus_Foerster1", "~Noam_Brown2"], "authors": ["Samuel Sokota", "Hengyuan Hu", "David J Wu", "J Zico Kolter", "Jakob Nicolaus Foerster", "Noam Brown"], "keywords": ["imperfect-information", "partial observability", "search", "decision-time planning"], "abstract": "We investigate the challenge of modeling the belief state of a partially observable Markov system, given sample-access to its dynamics model. This problem setting is often approached using parametric sequential generative modeling methods. However, these methods do not leverage any additional computation at inference time to increase their accuracy. Moreover, applying these methods to belief state modeling in certain multi-agent settings would require passing policies into the belief model---at the time of writing, there have been no successful demonstrations of this. Toward addressing these shortcomings, we propose an inference-time improvement framework for parametric sequential generative modeling methods called belief fine-tuning (BFT). BFT leverages approximate dynamic programming in the form of fine-tuning to determine the model parameters at each time step. It can improve the accuracy of the belief model at test time because it specializes the model to the space of local observations. Furthermore, because this specialization occurs after the action or policy has already been decided, BFT does not require the belief model to process it as input. As a result of the latter point, BFT enables, for the first time, approximate public belief state search in imperfect-information games where the number of possible information states is too large to track tabularly. We exhibit these findings on large-scale variants of the benchmark game Hanabi.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sokota|a_finetuning_approach_to_belief_state_modeling", "pdf": "/pdf/944fbcd08f3183592995e10fc1c179afad1bfc67.pdf", "one-sentence_summary": "A method for improving the accuracy of belief state models and for approximating public belief states at scale.", "_bibtex": "@inproceedings{\nsokota2022a,\ntitle={A Fine-Tuning Approach to Belief State Modeling},\nauthor={Samuel Sokota and Hengyuan Hu and David J Wu and J Zico Kolter and Jakob Nicolaus Foerster and Noam Brown},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ckZY7DGa7FQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "Q42f0dfjECO", "original": "nGEJacAQmAm", "number": 1937, "cdate": 1632875551314, "mdate": null, "ddate": null, "tcdate": 1632875551314, "tmdate": 1697934767244, "tddate": null, "forum": "Q42f0dfjECO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Differentially Private Fine-tuning of Language Models", "authorids": ["~Da_Yu1", "~Saurabh_Naik1", "~Arturs_Backurs1", "~Sivakanth_Gopi1", "~Huseyin_A_Inan1", "~Gautam_Kamath1", "~Janardhan_Kulkarni2", "~Yin_Tat_Lee1", "~Andre_Manoel1", "~Lukas_Wutschitz1", "~Sergey_Yekhanin1", "~Huishuai_Zhang3"], "authors": ["Da Yu", "Saurabh Naik", "Arturs Backurs", "Sivakanth Gopi", "Huseyin A Inan", "Gautam Kamath", "Janardhan Kulkarni", "Yin Tat Lee", "Andre Manoel", "Lukas Wutschitz", "Sergey Yekhanin", "Huishuai Zhang"], "keywords": ["differential privacy", "large language models", "fine-tuning"], "abstract": "We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commonly studied datasets, the utility of private models approaches that of non-private models. For example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using RoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of $\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large achieves an accuracy of $90.2\\%$. Our findings are similar for natural language generation when privately fine-tuning GPT-2. Our experiments also show that larger models are better suited for private fine-tuning: while they are well known to achieve superior accuracy non-privately, we find that they also better maintain their accuracy when privacy is introduced.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|differentially_private_finetuning_of_language_models", "pdf": "/pdf/a7f73a09ee1b6071d23daf0142ff02e4926db6e9.pdf", "one-sentence_summary": "We show that by combining recent advances in NLP, parameter-efficiency, privacy accounting, and using larger models, one can privately fine-tune models whose utility approaches that of non-private models.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.06500/code)", "_bibtex": "@inproceedings{\nyu2022differentially,\ntitle={Differentially Private Fine-tuning of Language Models},\nauthor={Da Yu and Saurabh Naik and Arturs Backurs and Sivakanth Gopi and Huseyin A Inan and Gautam Kamath and Janardhan Kulkarni and Yin Tat Lee and Andre Manoel and Lukas Wutschitz and Sergey Yekhanin and Huishuai Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Q42f0dfjECO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "DhzIU48OcZh", "original": "GptZ-OGz-B", "number": 1932, "cdate": 1632875550964, "mdate": null, "ddate": null, "tcdate": 1632875550964, "tmdate": 1676330584750, "tddate": null, "forum": "DhzIU48OcZh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts", "authorids": ["~Benjamin_Newman1", "~Prafulla_Kumar_Choubey2", "~Nazneen_Rajani1"], "authors": ["Benjamin Newman", "Prafulla Kumar Choubey", "Nazneen Rajani"], "keywords": ["NLP", "Prompting", "Commonsense", "information extraction", "factual extraction", "Large Language Models"], "abstract": "Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (the \"experts\") and select one to query the LLM. These require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. P-Adapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of just using natural language queries alone. Finally, we investigate what makes P-Adapters successful and conclude that a significant factor is access to the LLM's embeddings of the original natural language prompt, particularly the subject of the entity pair being queried.", "one-sentence_summary": "We make prompting large language models for factual information more robust by introducing P-Adapter models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "newman|padapters_robustly_extracting_factual_information_from_language_models_with_diverse_prompts", "pdf": "/pdf/8e2c7114bf23dadb13338c9b0dcf063a536ff1b3.pdf", "supplementary_material": "/attachment/8a56d5b5fdfed44dea41e1ef4de0741e07de905e.zip", "_bibtex": "@inproceedings{\nnewman2022padapters,\ntitle={P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts},\nauthor={Benjamin Newman and Prafulla Kumar Choubey and Nazneen Rajani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DhzIU48OcZh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "giBFoa-uS12", "original": "PEJ74pPfTe", "number": 1930, "cdate": 1632875550828, "mdate": null, "ddate": null, "tcdate": 1632875550828, "tmdate": 1676330584996, "tddate": null, "forum": "giBFoa-uS12", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming", "authorids": ["~Sachin_G_Konan1", "~Esmaeil_Seraj1", "~Matthew_Gombolay1"], "authors": ["Sachin G Konan", "Esmaeil Seraj", "Matthew Gombolay"], "keywords": ["Multi-agent Reinforcement Learning", "Cooperation and Coordination", "Policy Gradient Optimization", "Mutual Information", "Iterated Reasoning"], "abstract": "Information sharing is key in building team cognition and enables coordination and cooperation. High-performing human teams also benefit from acting strategically with hierarchical levels of iterated communication and rationalizability, meaning a human agent can reason about the actions of their teammates in their decision-making. Yet, the majority of prior work in Multi-Agent Reinforcement Learning (MARL) does not support iterated rationalizability and only encourage inter-agent communication, resulting in a suboptimal equilibrium cooperation strategy. In this work, we show that reformulating an agent's policy to be conditional on the policies of its neighboring teammates inherently maximizes Mutual Information (MI) lower-bound when optimizing under Policy Gradient (PG). Building on the idea of decision-making under bounded rationality and cognitive hierarchy theory, we show that our modified PG approach not only maximizes local agent rewards but also implicitly reasons about MI between agents without the need for any explicit ad-hoc regularization terms. Our approach, InfoPG, outperforms baselines in learning emergent collaborative behaviors and sets the state-of-the-art in decentralized cooperative MARL tasks. Our experiments validate the utility of InfoPG by achieving higher sample efficiency and significantly larger cumulative reward in several complex cooperative multi-agent domains.", "one-sentence_summary": "We propose a MARL framework with iterated and hierarchical rationalizability through mutual information for decision-making in fully-decentralized cooperative and Byzantine scenarios.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "konan|iterated_reasoning_with_mutual_information_in_cooperative_and_byzantine_decentralized_teaming", "pdf": "/pdf/6aa48ac42e21539265419c747bb0aedda2b1992d.pdf", "supplementary_material": "/attachment/67d2e8568bba880cad67f8cd5c250fb8217ebb94.zip", "_bibtex": "@inproceedings{\nkonan2022iterated,\ntitle={Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming},\nauthor={Sachin G Konan and Esmaeil Seraj and Matthew Gombolay},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=giBFoa-uS12}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "T0GpzBQ1Fg6", "original": "HT6L5lpSDrT", "number": 1924, "cdate": 1632875550413, "mdate": null, "ddate": null, "tcdate": 1632875550413, "tmdate": 1697934768941, "tddate": null, "forum": "T0GpzBQ1Fg6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Step-unrolled Denoising Autoencoders for Text Generation", "authorids": ["~Nikolay_Savinov1", "~Junyoung_Chung1", "~Mikolaj_Binkowski1", "~Erich_Elsen1", "~Aaron_van_den_Oord2"], "authors": ["Nikolay Savinov", "Junyoung Chung", "Mikolaj Binkowski", "Erich Elsen", "Aaron van den Oord"], "keywords": ["generative models", "text generation", "denoising autoencoders"], "abstract": "In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT'14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by filling in arbitrary blank patterns in a template.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "savinov|stepunrolled_denoising_autoencoders_for_text_generation", "pdf": "/pdf/17dfe8a5d0e1934ade282b40fbfe8f2fe421ef39.pdf", "one-sentence_summary": "We propose a new generative model of text that unrolls the denoising process during training.", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2112.06749/code)", "_bibtex": "@inproceedings{\nsavinov2022stepunrolled,\ntitle={Step-unrolled Denoising Autoencoders for Text Generation},\nauthor={Nikolay Savinov and Junyoung Chung and Mikolaj Binkowski and Erich Elsen and Aaron van den Oord},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=T0GpzBQ1Fg6}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "P7OVkHEoHOZ", "original": "xO2mgwKhNDM", "number": 1921, "cdate": 1632875550204, "mdate": null, "ddate": null, "tcdate": 1632875550204, "tmdate": 1697934769277, "tddate": null, "forum": "P7OVkHEoHOZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hindsight Foresight Relabeling for Meta-Reinforcement Learning", "authorids": ["~Michael_Wan1", "~Jian_Peng1", "~Tanmay_Gangwani1"], "authors": ["Michael Wan", "Jian Peng", "Tanmay Gangwani"], "keywords": ["Reinforcement Learning", "Meta-Learning"], "abstract": "Meta-reinforcement learning (meta-RL) algorithms allow for agents to learn new behaviors from small amounts of experience, mitigating the sample inefficiency problem in RL. However, while meta-RL agents can adapt quickly to new tasks at test time after experiencing only a few trajectories, the meta-training process is still sample-inefficient. Prior works have found that in the multi-task RL setting, relabeling past transitions and thus sharing experience among tasks can improve sample efficiency and asymptotic performance. We apply this idea to the meta-RL setting and devise a new relabeling method called Hindsight Foresight Relabeling (HFR). We construct a relabeling distribution using the combination of \"hindsight\", which is used to relabel trajectories using reward functions from the training task distribution, and \"foresight\", which takes the relabeled trajectories and computes the utility of each trajectory for each task. HFR is easy to implement and readily compatible with existing meta-RL algorithms. We find that HFR improves performance when compared to other relabeling methods on a variety of meta-RL tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wan|hindsight_foresight_relabeling_for_metareinforcement_learning", "pdf": "/pdf/73f685c6746fdb65b8e4aa08ea1ed0d6b49a34a8.pdf", "one-sentence_summary": "We present HFR, a relabeling method that can be applied to meta-reinforcement learning to boost sample efficiency and performance.", "supplementary_material": "/attachment/959674b7baa884f77a1d7494758be003ae5d79ec.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.09031/code)", "_bibtex": "@inproceedings{\nwan2022hindsight,\ntitle={Hindsight Foresight Relabeling for Meta-Reinforcement Learning},\nauthor={Michael Wan and Jian Peng and Tanmay Gangwani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=P7OVkHEoHOZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "nZeVKeeFYf9", "original": "cJwvWQLYWYK", "number": 1912, "cdate": 1632875549569, "mdate": null, "ddate": null, "tcdate": 1632875549569, "tmdate": 1697934770912, "tddate": null, "forum": "nZeVKeeFYf9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "authorids": ["~Edward_J_Hu1", "~yelong_shen1", "~Phillip_Wallis1", "~Zeyuan_Allen-Zhu1", "~Yuanzhi_Li1", "~Shean_Wang1", "luw@microsoft.com", "~Weizhu_Chen1"], "authors": ["Edward J Hu", "yelong shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Lu Wang", "Weizhu Chen"], "keywords": ["Transfer learning", "Adaptation", "Transformer", "Fine-tuning", "Low-rank", "RoBERTa", "DeBERTa", "GPT-2", "GPT-3"], "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible.\nUsing GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "one-sentence_summary": "Finetuning updates have a low \"intrinsic rank\" which allows us to train only the rank decomposition matrices of certain weights, yielding better performance and practical benefits.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hu|lora_lowrank_adaptation_of_large_language_models", "pdf": "/pdf/5a54aed5265cb0399c62848f44e84c4a617a354b.pdf", "supplementary_material": "/attachment/9f1ca59ff81f3339a6099be6c48a9e33db065654.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2106.09685/code)", "_bibtex": "@inproceedings{\nhu2022lora,\ntitle={Lo{RA}: Low-Rank Adaptation of Large Language Models},\nauthor={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nZeVKeeFYf9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "qRDQi3ocgR3", "original": "8baaA86BIr0", "number": 1911, "cdate": 1632875549498, "mdate": null, "ddate": null, "tcdate": 1632875549498, "tmdate": 1676330586590, "tddate": null, "forum": "qRDQi3ocgR3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective", "authorids": ["~Luca_Scimeca1", "~Seong_Joon_Oh1", "~Sanghyuk_Chun1", "~Michael_Poli1", "~Sangdoo_Yun1"], "authors": ["Luca Scimeca", "Seong Joon Oh", "Sanghyuk Chun", "Michael Poli", "Sangdoo Yun"], "keywords": ["shortcut learning", "shortcut bias", "loss geometry", "simplicity bias", "flat minima", "generalization", "wisconsin card sorting test"], "abstract": "Deep neural networks (DNNs) often rely on easy\u2013to\u2013learn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized based on their typical background scenery, such as lakes or streams. This phenomenon, also known as shortcut learning, is emerging as a key limitation of the current generation of machine learning models. In this work, we introduce a set of experiments to deepen our understanding of shortcut learning and its implications. We design a training setup with several shortcut cues, named WCST-ML, where each cue is equally conducive to the visual recognition problem at hand. Even under equal opportunities, we observe that (1) certain cues are preferred to others, (2) solutions biased to the easy\u2013to\u2013learn cues tend to converge to relatively flat minima on the loss surface, and (3) the solutions focusing on those preferred cues are far more abundant in the parameter space. We explain the abundance of certain cues via their Kolmogorov (descriptional) complexity: solutions corresponding to Kolmogorov-simple cues are abundant in the parameter space and are thus preferred by DNNs. Our studies are based on the synthetic dataset DSprites and the face dataset UTKFace. In our WCST-ML, we observe that the inborn bias of models leans toward simple cues, such as color and ethnicity. Our findings emphasize the importance of active human intervention to remove the inborn model biases that may cause negative societal impacts.", "one-sentence_summary": "When given equally likely shortcuts in data, which shortcut cue will a DNN choose, and why?", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "scimeca|which_shortcut_cues_will_dnns_choose_a_study_from_the_parameterspace_perspective", "pdf": "/pdf/3068411e384b48770e3b3fe52bc409e41a40f3f8.pdf", "data": "", "_bibtex": "@inproceedings{\nscimeca2022which,\ntitle={Which Shortcut Cues Will {DNN}s Choose? A Study from the Parameter-Space Perspective},\nauthor={Luca Scimeca and Seong Joon Oh and Sanghyuk Chun and Michael Poli and Sangdoo Yun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qRDQi3ocgR3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "tUMr0Iox8XW", "original": "H1_Wgil0Vaf", "number": 1897, "cdate": 1632875548591, "mdate": null, "ddate": null, "tcdate": 1632875548591, "tmdate": 1676330587101, "tddate": null, "forum": "tUMr0Iox8XW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient Computation of Deep Nonlinear Infinite-Width Neural Networks that Learn Features", "authorids": ["~Greg_Yang1", "michael.santacroce@microsoft.com", "~Edward_J_Hu1"], "authors": ["Greg Yang", "Michael Santacroce", "Edward J Hu"], "keywords": ["infinite-width neural network", "feature learning", "maximal update parametrization", "NTK"], "abstract": "While a popular limit of infinite-width neural networks, the Neural Tangent Kernel (NTK) often exhibits performance gaps from finite-width neural networks on standard datasets, due to lack of feature learning. Although the feature learning *maximal update limit*, or *\u03bc-limit* (Yang and Hu, 2020) of wide networks has closed the gap for 1-hidden-layer linear models, no one has been able to demonstrate this for deep nonlinear multi-layer perceptrons (MLP) because of \u03bc-limit\u2019s computational difficulty in this setting.  Here, we solve this problem by proposing a novel feature learning limit, the *\u03c0-limit*, that bypasses the computational issues. The \u03c0-limit, in short, is the limit of a form of projected gradient descent, and the \u03c0-limit of an MLP is roughly another MLP where gradients are appended to weights during training. We prove its almost sure convergence with width using the Tensor Programs technique. We evaluate it on CIFAR10 and Omniglot against NTK as well as finite networks, finding the \u03c0-limit outperform finite-width models trained normally (without projection) in both settings, closing the performance gap between finite- and infinite-width neural networks previously left by NTK. Code for this work is available at github.com/santacml/pilim.", "one-sentence_summary": "A new feature learning \u221e-width limit for deep nonlinear networks closes the performance gap between finite- and infinite-width neural networks previously left by NTK.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|efficient_computation_of_deep_nonlinear_infinitewidth_neural_networks_that_learn_features", "pdf": "/pdf/da2902df767a2a3e4eec1ef65497a37e7f8dbd2b.pdf", "data": "", "_bibtex": "@inproceedings{\nyang2022efficient,\ntitle={Efficient Computation of Deep Nonlinear Infinite-Width Neural Networks that Learn Features},\nauthor={Greg Yang and Michael Santacroce and Edward J Hu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tUMr0Iox8XW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "6q_2b6u0BnJ", "original": "IPxExn1RXf", "number": 1896, "cdate": 1632875548521, "mdate": null, "ddate": null, "tcdate": 1632875548521, "tmdate": 1697934772894, "tddate": null, "forum": "6q_2b6u0BnJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "TRAIL: Near-Optimal Imitation Learning with Suboptimal Data", "authorids": ["~Mengjiao_Yang1", "~Sergey_Levine1", "~Ofir_Nachum1"], "authors": ["Mengjiao Yang", "Sergey Levine", "Ofir Nachum"], "keywords": ["Imitation Learning", "Action Representations", "Latent Dynamics Model", "Offline Datasets"], "abstract": "In imitation learning, one aims to learn task-solving policies using access to near-optimal expert trajectories collected from the task environment. However, high-quality trajectories -- e.g., from human experts -- can be expensive to obtain in practical settings. On the contrary, it is often much easier to obtain large amounts of suboptimal trajectories which can nevertheless provide insight into the structure of the environment, showing what \\emph{could} be done in the environment even if not what \\emph{should} be done. Is it possible to formalize these conceptual benefits and devise algorithms to use offline datasets to yield \\emph{provable} improvements to the sample-efficiency of imitation learning? In this work, we answer this question affirmatively and present training objectives which use an offline dataset to learn an approximate \\emph{factored} dynamics model whose structure enables the extraction of a \\emph{latent action space}. Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the benefits suggested by our theory and show that our algorithms is able to recover near-optimal policies with fewer expert trajectories.", "one-sentence_summary": "A provably beneficial way to learn action representations for imitation learning from suboptimal auxiliary data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|trail_nearoptimal_imitation_learning_with_suboptimal_data", "pdf": "/pdf/958e9e602eeb7bacbb7b7ef495ca1126f80bed1f.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.14770/code)", "_bibtex": "@inproceedings{\nyang2022trail,\ntitle={{TRAIL}: Near-Optimal Imitation Learning with Suboptimal Data},\nauthor={Mengjiao Yang and Sergey Levine and Ofir Nachum},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6q_2b6u0BnJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "zrW-LVXj2k1", "original": "rLMBq8tgLG5", "number": 1887, "cdate": 1632875547903, "mdate": null, "ddate": null, "tcdate": 1632875547903, "tmdate": 1676330587992, "tddate": null, "forum": "zrW-LVXj2k1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the benefits of maximum likelihood estimation for Regression and Forecasting", "authorids": ["~Pranjal_Awasthi3", "~Abhimanyu_Das2", "~Rajat_Sen1", "~Ananda_Theertha_Suresh1"], "authors": ["Pranjal Awasthi", "Abhimanyu Das", "Rajat Sen", "Ananda Theertha Suresh"], "keywords": ["Forecasting", "Time-Series", "Regression", "MLE"], "abstract": "We advocate for a practical Maximum Likelihood Estimation (MLE) approach towards designing loss functions for regression and forecasting, as an alternative to the typical approach of direct empirical risk minimization on a specific target metric. The MLE approach is better suited to capture inductive biases such as prior domain knowledge in datasets, and can output post-hoc estimators at inference time that can optimize different types of target metrics. We present theoretical results to demonstrate that our approach is competitive with any estimator for the target metric under some general conditions. In two example practical settings, Poisson and Pareto regression, we show that our competitive results can be used to prove that the MLE approach has better excess risk bounds than directly minimizing the target metric. We also demonstrate empirically that our method instantiated with a well-designed general purpose mixture likelihood family can obtain superior performance for a variety of tasks across time-series forecasting and regression datasets with different data distributions.", "one-sentence_summary": "MLE + Post-Hoc Inference can be competitive with any estimator under some general assumptions ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "awasthi|on_the_benefits_of_maximum_likelihood_estimation_for_regression_and_forecasting", "pdf": "/pdf/676edee8f0bcb63695ea6a66f75477179eb324d7.pdf", "supplementary_material": "/attachment/c8e44acbe52e791f118f85a1f7b8db40d92c1ced.zip", "_bibtex": "@inproceedings{\nawasthi2022on,\ntitle={On the benefits of maximum likelihood estimation for Regression and Forecasting},\nauthor={Pranjal Awasthi and Abhimanyu Das and Rajat Sen and Ananda Theertha Suresh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zrW-LVXj2k1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "GhVS8_yPeEa", "original": "67GHSFyypp0", "number": 1874, "cdate": 1632875547052, "mdate": null, "ddate": null, "tcdate": 1632875547052, "tmdate": 1676330588379, "tddate": null, "forum": "GhVS8_yPeEa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Effect of scale on catastrophic forgetting in neural networks", "authorids": ["~Vinay_Venkatesh_Ramasesh1", "~Aitor_Lewkowycz2", "~Ethan_Dyer1"], "authors": ["Vinay Venkatesh Ramasesh", "Aitor Lewkowycz", "Ethan Dyer"], "keywords": ["Catastrophic forgetting", "continual learning", "scaling", "language modeling", "image classification"], "abstract": "Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning, i.e. learning tasks sequentially. Recently, both computer vision and natural-language processing have witnessed great progress through the use of large-scale pretrained models. In this work, we present an empirical study of catastrophic forgetting in this pretraining paradigm.\nOur experiments indicate that large, pretrained ResNets and Transformers are significantly more resistant to forgetting than randomly-initialized, trained-from-scratch models; this robustness systematically improves with scale of both model and pretraining dataset size.\nWe take initial steps towards characterizing what aspect of model representations allows them to perform continual learning so well, finding that in the pretrained models, distinct class representations grow more orthogonal with scale.  Our results suggest that, when possible, scale and a diverse pretraining dataset can be useful ingredients in mitigating catastrophic forgetting. ", "one-sentence_summary": "We find that large, pre-trained models are robust to catastrophic forgetting.  ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ramasesh|effect_of_scale_on_catastrophic_forgetting_in_neural_networks", "pdf": "/pdf/54d452543db390a26979d51399ae9c6ed01f4de1.pdf", "data": "", "_bibtex": "@inproceedings{\nramasesh2022effect,\ntitle={Effect of scale on catastrophic forgetting in neural networks},\nauthor={Vinay Venkatesh Ramasesh and Aitor Lewkowycz and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=GhVS8_yPeEa}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 6}}, {"id": "FndDxSz3LxQ", "original": "C2pbnx09mOJ", "number": 1869, "cdate": 1632875546776, "mdate": null, "ddate": null, "tcdate": 1632875546776, "tmdate": 1676330588695, "tddate": null, "forum": "FndDxSz3LxQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks", "authorids": ["~Morteza_Ramezani1", "~Weilin_Cong1", "~Mehrdad_Mahdavi2", "~Mahmut_Kandemir1", "~Anand_Sivasubramaniam1"], "authors": ["Morteza Ramezani", "Weilin Cong", "Mehrdad Mahdavi", "Mahmut Kandemir", "Anand Sivasubramaniam"], "keywords": ["Graph Neural Networks", "GNN", "GCN", "Distributed Training"], "abstract": "Despite the recent success of Graph Neural Networks (GNNs), training GNNs on large graphs remains challenging. The limited resource capacities of the existing servers, the dependency between nodes in a graph, and the privacy concern due to the centralized storage and model learning have spurred the need to design an effective distributed algorithm for GNN training. However, existing distributed GNN training methods impose either excessive communication costs or large memory overheads that hinders their scalability. To overcome these issues, we propose a communication-efficient distributed GNN training technique named $\\text{\\textit{Learn Locally, Correct Globally}}$ (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation. To solve the performance degradation, we propose to apply $\\text{\\textit{Global Server Corrections}}$ on the server to refine the locally learned models. We rigorously analyze the convergence of distributed methods  with periodic model averaging for training GNNs and show that naively applying periodic model averaging but ignoring the dependency between nodes will suffer from an irreducible residual error. However, this residual error can be eliminated  by utilizing the proposed global corrections to entail fast convergence rate. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ramezani|learn_locally_correct_globally_a_distributed_algorithm_for_training_graph_neural_networks", "pdf": "/pdf/1cb92429664f8a20c602659b4a0ca7386709ab46.pdf", "one-sentence_summary": "We propose LLCG a communication efficient distributed algorithm for training GNNs.", "supplementary_material": "/attachment/a9e70eb4cee8090fdba47e299ac2bbf91f455dd3.zip", "_bibtex": "@inproceedings{\nramezani2022learn,\ntitle={Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks},\nauthor={Morteza Ramezani and Weilin Cong and Mehrdad Mahdavi and Mahmut Kandemir and Anand Sivasubramaniam},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FndDxSz3LxQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "7MV6uLzOChW", "original": "l1ohwJ-eAVN", "number": 1868, "cdate": 1632875546706, "mdate": null, "ddate": null, "tcdate": 1632875546706, "tmdate": 1676330588926, "tddate": null, "forum": "7MV6uLzOChW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Conditional Image Generation by Conditioning Variational Auto-Encoders", "authorids": ["~William_Harvey1", "~Saeid_Naderiparizi1", "~Frank_Wood2"], "authors": ["William Harvey", "Saeid Naderiparizi", "Frank Wood"], "keywords": ["variational auto-encoders", "Bayesian inference", "variational inference", "amortized inference", "image completion"], "abstract": "We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE. To train the conditional VAE, we only need to train an artifact to perform amortized inference over the unconditional VAE's latent variables given a conditioning input. We demonstrate our approach on tasks including image inpainting, for which it outperforms state-of-the-art GAN-based approaches at faithfully representing the inherent uncertainty. We conclude by describing a possible application of our inpainting model, in which it is used to perform Bayesian experimental design for the purpose of guiding a sensor.", "one-sentence_summary": "We create fast-to-train conditional VAEs using amortized inference in pretrained unconditional VAEs, and demonstrate diverse samples on image completion tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "harvey|conditional_image_generation_by_conditioning_variational_autoencoders", "pdf": "/pdf/6a31861c3fd2a8f68a3c37fe8a7e422b0ac57cc4.pdf", "supplementary_material": "/attachment/69ea66a9ba7d75f74d81259485691d1cae872a28.zip", "data": "", "_bibtex": "@inproceedings{\nharvey2022conditional,\ntitle={Conditional Image Generation by Conditioning Variational Auto-Encoders},\nauthor={William Harvey and Saeid Naderiparizi and Frank Wood},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7MV6uLzOChW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "hm2tNDdgaFK", "original": "Qz7x4ySXQe5", "number": 1867, "cdate": 1632875546639, "mdate": null, "ddate": null, "tcdate": 1632875546639, "tmdate": 1676330588993, "tddate": null, "forum": "hm2tNDdgaFK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations", "authorids": ["~Keir_Adams1", "~Lagnajit_Pattanaik1", "~Connor_W._Coley1"], "authors": ["Keir Adams", "Lagnajit Pattanaik", "Connor W. Coley"], "keywords": ["geometric deep learning", "equivariance", "molecules"], "abstract": "Molecular chirality, a form of stereochemistry most often describing relative spatial arrangements of bonded neighbors around tetrahedral carbon centers, influences the set of 3D conformers accessible to the molecule without changing its 2D graph connectivity. Chirality can strongly alter (bio)chemical interactions, particularly protein-drug binding. Most 2D graph neural networks (GNNs) designed for molecular property prediction at best use atomic labels to na\u00efvely treat chirality, while E(3)-invariant 3D GNNs are invariant to chirality altogether. To enable representation learning on molecules with defined stereochemistry, we design an SE(3)-invariant model that processes torsion angles of a 3D molecular conformer. We explicitly model conformational flexibility by integrating a novel type of invariance to rotations about internal molecular bonds into the architecture, mitigating the need for multi-conformer data augmentation. We test our model on four benchmarks: contrastive learning to distinguish conformers of different stereoisomers in a learned latent space, classification of chiral centers as R/S, prediction of how enantiomers rotate circularly polarized light, and ranking enantiomers by their docking scores in an enantiosensitive protein pocket. We compare our model, Chiral InterRoto-Invariant Neural Network (ChIRo), with 2D and 3D GNNs to demonstrate that our model achieves state of the art performance when learning chiral-sensitive functions from molecular structures.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "adams|learning_3d_representations_of_molecular_chirality_with_invariance_to_bond_rotations", "pdf": "/pdf/0bd5d5f64a3a1afccdb03997dd1aad21fe9c2aaa.pdf", "one-sentence_summary": "We propose a method of processing the 3D torsion angles of a molecular conformer to learn tetrahedral chirality while integrating a novel invariance to rotations about internal molecular bonds directly into the model architecture.", "supplementary_material": "/attachment/8530a394c5fd12494fa781bfe7ea203a48598f29.zip", "_bibtex": "@inproceedings{\nadams2022learning,\ntitle={Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations},\nauthor={Keir Adams and Lagnajit Pattanaik and Connor W. Coley},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hm2tNDdgaFK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "tgcAoUVHRIB", "original": "S1vEeeHRQUC", "number": 1865, "cdate": 1632875546571, "mdate": null, "ddate": null, "tcdate": 1632875546571, "tmdate": 1697934776338, "tddate": null, "forum": "tgcAoUVHRIB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Methods for Logical Reasoning over Knowledge Graphs", "authorids": ["~Alfonso_Amayuelas2", "~Shuai_Zhang7", "~Xi_Susie_Rao1", "~Ce_Zhang1"], "authors": ["Alfonso Amayuelas", "Shuai Zhang", "Xi Susie Rao", "Ce Zhang"], "keywords": ["Knowledge Graphs", "Knowledge Graph Reasoning", "Graph Mining", "Data Mining", "Machine Learning", "Artificial Intelligence", "Deep Learning"], "abstract": "Reasoning is a fundamental problem for computers and deeply studied in Artificial Intelligence. In this paper, we specifically focus on answering multi-hop logical queries on Knowledge Graphs (KGs). This is a complicated task because, in real world scenarios, the graphs tend to be large and incomplete. Most previous works have been unable to create models that accept full First-Order Logical (FOL) queries, which includes negative queries, and have only been able to process a limited set of query structures. Additionally, most methods present logic operators that can only perform the logical operation they are made for. We introduce a set of models that use Neural Networks to create one-point vector embeddings to answer the queries. The versatility of neural networks allows the framework to  handle FOL queries with Conjunction, Disjunction and Negation operators. We demonstrate experimentally the performance of our models through extensive experimentation on well-known benchmarking datasets. Besides having more versatile operators, the models achieve a 10% relative increase over best performing state of the art and more than 30% over the original method based on single-point vector embeddings.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "amayuelas|neural_methods_for_logical_reasoning_over_knowledge_graphs", "pdf": "/pdf/801a4e5791b6166101b4d236bbd874bd1a6916ff.pdf", "one-sentence_summary": "Neural Network models for answering multi-hop queris on Knowledge Graphs", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2209.14464/code)", "_bibtex": "@inproceedings{\namayuelas2022neural,\ntitle={Neural Methods for Logical Reasoning over Knowledge Graphs},\nauthor={Alfonso Amayuelas and Shuai Zhang and Xi Susie Rao and Ce Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tgcAoUVHRIB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "St6eyiTEHnG", "original": "ppXuz8MLodF", "number": 1864, "cdate": 1632875546501, "mdate": null, "ddate": null, "tcdate": 1632875546501, "tmdate": 1676330589211, "tddate": null, "forum": "St6eyiTEHnG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Consistent Counterfactuals for Deep Models", "authorids": ["~Emily_Black1", "~Zifan_Wang1", "~Matt_Fredrikson1"], "authors": ["Emily Black", "Zifan Wang", "Matt Fredrikson"], "keywords": ["deep models", "deep networks", "explainability", "counterfactual explanations", "consistency", "consistent predictions", "model duplicity", "random initialization"], "abstract": "Counterfactual examples are one of the most commonly-cited methods for explaining the predictions of machine learning models in key areas such as finance and medical diagnosis. Counterfactuals are often discussed under the assumption that the model on which they will be used is static, but in deployment models may be periodically retrained or fine-tuned. This paper studies the consistency of model prediction on counterfactual examples in deep networks under small changes to initial training conditions, such as weight initialization and leave-one-out variations in data, as often occurs during model deployment. We demonstrate experimentally that counterfactual examples for deep models are often inconsistent across such small changes, and that increasing the cost of the counterfactual, a stability-enhancing mitigation suggested by prior work in the context of simpler models, is not a reliable heuristic in deep networks. Rather, our analysis shows that a model's Lipschitz continuity around the counterfactual, along with confidence of its prediction, is key to its consistency across related models. To this end, we propose Stable Neighbor Search as a way to generate more consistent counterfactual explanations, and illustrate the effectiveness of this approach on several benchmark datasets.", "one-sentence_summary": "Counterfactual explanations are often inconsistent between virtually identical deep models. We introduce a new method to increase consistency while keeping costs low relative to other fixes.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "black|consistent_counterfactuals_for_deep_models", "pdf": "/pdf/9aa3de89ad2eb688c7c39e39d0f8431eb052f3e5.pdf", "supplementary_material": "/attachment/1745d806bebfb29d2737f9a9150a976fafab2f5b.zip", "_bibtex": "@inproceedings{\nblack2022consistent,\ntitle={Consistent Counterfactuals for Deep Models},\nauthor={Emily Black and Zifan Wang and Matt Fredrikson},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=St6eyiTEHnG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "9jsZiUgkCZP", "original": "yiNwwbNPE9n", "number": 1860, "cdate": 1632875546191, "mdate": null, "ddate": null, "tcdate": 1632875546191, "tmdate": 1697934777196, "tddate": null, "forum": "9jsZiUgkCZP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Unified Visual Transformer Compression", "authorids": ["~Shixing_Yu1", "~Tianlong_Chen1", "~Jiayi_Shen1", "yuanhuan9412@163.com", "~Jianchao_Tan1", "~Sen_Yang4", "~Ji_Liu1", "~Zhangyang_Wang1"], "authors": ["Shixing Yu", "Tianlong Chen", "Jiayi Shen", "Huan Yuan", "Jianchao Tan", "Sen Yang", "Ji Liu", "Zhangyang Wang"], "keywords": ["Vision Transformer", "Model Compression", "Pruning", "Layer Skipping", "Distillation"], "abstract": "Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}.", "one-sentence_summary": " This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation, which outperforms existing competitors. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|unified_visual_transformer_compression", "pdf": "/pdf/8570214b832776a06b451827ee429b3eba50359a.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.08243/code)", "_bibtex": "@inproceedings{\nyu2022unified,\ntitle={Unified Visual Transformer Compression},\nauthor={Shixing Yu and Tianlong Chen and Jiayi Shen and Huan Yuan and Jianchao Tan and Sen Yang and Ji Liu and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9jsZiUgkCZP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "IDwN6xjHnK8", "original": "rf8kSA8GEb", "number": 1854, "cdate": 1632875545778, "mdate": null, "ddate": null, "tcdate": 1632875545778, "tmdate": 1676330589785, "tddate": null, "forum": "IDwN6xjHnK8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Transformer-based Transform Coding", "authorids": ["~Yinhao_Zhu1", "~Yang_Yang15", "~Taco_Cohen1"], "authors": ["Yinhao Zhu", "Yang Yang", "Taco Cohen"], "keywords": ["transformer", "transform coding", "image compression", "video compression"], "abstract": "Neural data compression based on nonlinear transform coding has made great progress over the last few years, mainly due to improvements in prior models, quantization methods and nonlinear transforms. A general trend in many recent works pushing the limit of rate-distortion performance is to use ever more expensive prior models that can lead to prohibitively slow decoding. Instead, we focus on more expressive transforms that result in a better rate-distortion-computation trade-off. Specifically, we show that nonlinear transforms built on Swin-transformers can achieve better compression efficiency than transforms built on convolutional neural networks (ConvNets), while requiring fewer parameters and shorter decoding time. Paired with a compute-efficient Channel-wise Auto-Regressive Model prior, our SwinT-ChARM model outperforms VTM-12.1 by $3.68\\%$ in BD-rate on Kodak with comparable decoding speed. In P-frame video compression setting, we are able to outperform the popular ConvNet-based scale-space-flow model by $12.35\\%$ in BD-rate on UVG. We provide model scaling studies to verify the computational efficiency of the proposed solutions and conduct several analyses to reveal the source of coding gain of transformers over ConvNets, including better spatial decorrelation, flexible effective receptive field, and more localized response of latent pixels during progressive decoding.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|transformerbased_transform_coding", "pdf": "/pdf/b5e3776fbe0ee70da5740c3cf525ed60629bca04.pdf", "_bibtex": "@inproceedings{\nzhu2022transformerbased,\ntitle={Transformer-based Transform Coding},\nauthor={Yinhao Zhu and Yang Yang and Taco Cohen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=IDwN6xjHnK8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "lbauk6wK2-y", "original": "-NNrJL2DDFQ", "number": 1852, "cdate": 1632875545638, "mdate": null, "ddate": null, "tcdate": 1632875545638, "tmdate": 1676330589882, "tddate": null, "forum": "lbauk6wK2-y", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Object Pursuit: Building a Space of Objects via Discriminative Weight Generation", "authorids": ["pancy17@mails.tsinghua.edu.cn", "~Yanchao_Yang1", "~Kaichun_Mo1", "~Yueqi_Duan1", "~Leonidas_Guibas1"], "authors": ["Chuanyu Pan", "Yanchao Yang", "Kaichun Mo", "Yueqi Duan", "Leonidas Guibas"], "keywords": ["object-centric", "continual learning", "representation learning", "hypernetwork"], "abstract": "We propose a framework to continuously learn object-centric representations for visual learning and understanding. Existing object-centric representations either rely on supervisions that individualize objects in the scene, or perform unsupervised disentanglement that can hardly deal with complex scenes in the real world. To mitigate the annotation burden and relax the constraints on the statistical complexity of the data, our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations. Throughout learning, objects are streamed one by one in random order with unknown identities, and are associated with latent codes that can synthesize discriminative weights for each object through a convolutional hypernetwork. Moreover, re-identification of learned objects and forgetting prevention are employed to make the learning process efficient and robust. We perform an extensive study of the key features of the proposed framework and analyze the characteristics of the learned representations. Furthermore, we demonstrate the capability of the proposed framework in learning representations that can improve label efficiency in downstream tasks. Our code and trained models are made publicly available at: https://github.com/pptrick/Object-Pursuit.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pan|object_pursuit_building_a_space_of_objects_via_discriminative_weight_generation", "pdf": "/pdf/32d3902cb1f5cf70159185cfd64ffb5c9089be73.pdf", "one-sentence_summary": "We propose a novel framework named object pursuit that can continuously learn object-centric representations using training data collected from interactions with individual objects.", "supplementary_material": "/attachment/312cd290b1e47b142e98a5764000271870cb606a.zip", "_bibtex": "@inproceedings{\npan2022object,\ntitle={Object Pursuit: Building a Space of Objects via Discriminative Weight Generation},\nauthor={Chuanyu Pan and Yanchao Yang and Kaichun Mo and Yueqi Duan and Leonidas Guibas},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lbauk6wK2-y}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "DhP9L8vIyLc", "original": "0eP6HVrd1O", "number": 1850, "cdate": 1632875545502, "mdate": null, "ddate": null, "tcdate": 1632875545502, "tmdate": 1697934778047, "tddate": null, "forum": "DhP9L8vIyLc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PAC Prediction Sets Under Covariate Shift", "authorids": ["~Sangdon_Park1", "~Edgar_Dobriban2", "~Insup_Lee1", "~Osbert_Bastani1"], "authors": ["Sangdon Park", "Edgar Dobriban", "Insup Lee", "Osbert Bastani"], "keywords": ["probably approximately correct", "prediction set", "covariate shift", "importance weight", "calibration", "Clopper-Pearson binomial interval", "rejection sampling"], "abstract": "An important challenge facing modern machine learning is how to rigorously quantify the uncertainty of model predictions. Conveying uncertainty is especially important when there are changes to the underlying data distribution that might invalidate the predictive model. Yet, most existing uncertainty quantification algorithms break down in the presence of such shifts. We propose a novel approach that addresses this challenge by constructing \\emph{probably approximately correct (PAC)} prediction sets in the presence of covariate shift. Our approach focuses on the setting where there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). Our algorithm assumes given importance weights that encode how the probabilities of the training examples change under the covariate shift. In practice, importance weights typically need to be estimated; thus, we extend our algorithm to the setting where we are given confidence intervals for the importance weights. We demonstrate the effectiveness of our approach on covariate shifts based on DomainNet and ImageNet. Our algorithm satisfies the PAC constraint, and gives prediction sets with the smallest average normalized size among approaches that always satisfy the PAC constraint.", "one-sentence_summary": "We propose a novel algorithm that constructs a prediction set with probably approximated correct (PAC) guarantee under covariate shift, while minimizing the expected prediction set size. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "park|pac_prediction_sets_under_covariate_shift", "pdf": "/pdf/c0c06e027d7b1f6b50bfe3c8ed8be38595d25d04.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.09848/code)", "_bibtex": "@inproceedings{\npark2022pac,\ntitle={{PAC} Prediction Sets Under Covariate Shift},\nauthor={Sangdon Park and Edgar Dobriban and Insup Lee and Osbert Bastani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DhP9L8vIyLc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "vJZ7dPIjip3", "original": "Pvup99hJwJ-", "number": 1848, "cdate": 1632875545366, "mdate": null, "ddate": null, "tcdate": 1632875545366, "tmdate": 1676330590421, "tddate": null, "forum": "vJZ7dPIjip3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness", "authorids": ["~Simon_Geisler1", "~Johanna_Sommer1", "~Jan_Schuchardt1", "~Aleksandar_Bojchevski1", "~Stephan_G\u00fcnnemann1"], "authors": ["Simon Geisler", "Johanna Sommer", "Jan Schuchardt", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "keywords": ["Generalization", "Neural Combinatorial Optimization", "Adversarial Robustness"], "abstract": "End-to-end (geometric) deep learning has seen first successes in approximating the solution of combinatorial optimization problems. However, generating data in the realm of NP-hard/-complete tasks brings practical and theoretical challenges, resulting in evaluation protocols that are too optimistic. Specifically, most datasets only capture a simpler subproblem and likely suffer from spurious features. We investigate these effects by studying adversarial robustness -a local generalization property- to reveal hard, model-specific instances and spurious features. For this purpose, we derive perturbation models for SAT and TSP. Unlike in other applications, where perturbation models are designed around subjective notions of imperceptibility, our perturbation models are efficient and sound, allowing us to determine the true label of perturbed samples without a solver. Surprisingly, with such perturbations, a sufficiently expressive neural solver does not suffer from the limitations of the accuracy-robustness trade-off common in supervised learning. Although such robust solvers exist, we show empirically that the assessed neural solvers do not generalize well w.r.t. small perturbations of the problem instance.", "one-sentence_summary": "We study the generalization of combinatorial optimization w.r.t. to adversarial attacks since current evaluation protocols are too optimistic and we show that neural solvers are indeed vulnerable under label-preserving perturbations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "geisler|generalization_of_neural_combinatorial_solvers_through_the_lens_of_adversarial_robustness", "pdf": "/pdf/cde50097d313f95849a0784c71b151985455962b.pdf", "_bibtex": "@inproceedings{\ngeisler2022generalization,\ntitle={Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness},\nauthor={Simon Geisler and Johanna Sommer and Jan Schuchardt and Aleksandar Bojchevski and Stephan G{\\\"u}nnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vJZ7dPIjip3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "dg79moSRqIo", "original": "p49cu4badrz", "number": 1845, "cdate": 1632875545158, "mdate": null, "ddate": null, "tcdate": 1632875545158, "tmdate": 1697934778734, "tddate": null, "forum": "dg79moSRqIo", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "One After Another: Learning Incremental Skills for a Changing World", "authorids": ["~Nur_Muhammad_Mahi_Shafiullah1", "~Lerrel_Pinto1"], "authors": ["Nur Muhammad Mahi Shafiullah", "Lerrel Pinto"], "keywords": ["Skill discovery", "Incremental reinforcement learning"], "abstract": "Reward-free, unsupervised discovery of skills is an attractive alternative to the bottleneck of hand-designing rewards in environments where task supervision is scarce or expensive. However, current skill pre-training methods, like many RL techniques, make a fundamental assumption -- stationary environments during training. Traditional methods learn all their skills simultaneously, which makes it difficult for them to both quickly adapt to changes in the environment, and to not forget earlier skills after such adaptation. On the other hand, in an evolving or expanding environment, skill learning must be able to adapt fast to new environment situations while not forgetting previously learned skills. These two conditions make it difficult for classic skill discovery to do well in an evolving environment. In this work, we propose a new framework for skill discovery, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn't forget a learned skill. We demonstrate experimentally that in both evolving and static environments, incremental skills significantly outperform current state-of-the-art skill discovery methods on both skill quality and the ability to solve downstream tasks. Videos for learned skills and code are made public on https://notmahi.github.io/disk\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shafiullah|one_after_another_learning_incremental_skills_for_a_changing_world", "pdf": "/pdf/1ee5373bdf14fb52571018799bfe53c696d99868.pdf", "one-sentence_summary": "We discover skills incrementally, without supervision, and it lets us learn skills in both static environments and environments with changing dynamics.", "supplementary_material": "/attachment/b7f8cc7442d04952ed7c8b79e67dac7dd4801ce9.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.11176/code)", "_bibtex": "@inproceedings{\nshafiullah2022one,\ntitle={One After Another: Learning Incremental Skills for a Changing World},\nauthor={Nur Muhammad Mahi Shafiullah and Lerrel Pinto},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dg79moSRqIo}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "Kwm8I7dU-l5", "original": "YEIBUhLm3tf", "number": 1839, "cdate": 1632875544741, "mdate": null, "ddate": null, "tcdate": 1632875544741, "tmdate": 1697934779312, "tddate": null, "forum": "Kwm8I7dU-l5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph-Guided Network for Irregularly Sampled Multivariate Time Series", "authorids": ["~Xiang_Zhang10", "marko.zeman@fri.uni-lj.si", "~Theodoros_Tsiligkaridis1", "~Marinka_Zitnik1"], "authors": ["Xiang Zhang", "Marko Zeman", "Theodoros Tsiligkaridis", "Marinka Zitnik"], "keywords": ["time seres", "irregular time series", "graph neural networks", "attention mechanism", "time series classification", "multivariate time series", "representation learning", "embeddings"], "abstract": "In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also learning the dynamics of sensors purely from observational data. RAINDROP represents every sample as a separate sensor graph and models time-varying dependencies between sensors with a novel message passing operator. It estimates the latent sensor graph structure and leverages the structure together with nearby observations to predict misaligned readouts. This model can be interpreted as a graph neural network that sends messages over graphs that are optimized for capturing time-varying dependencies among sensors. We use RAINDROP to classify time series and interpret temporal dynamics on three healthcare and human activity datasets. RAINDROP outperforms state-of-the-art methods by up to 11.4% (absolute F1-score points), including techniques that deal with irregular sampling using fixed discretization and set functions. RAINDROP shows superiority in diverse setups, including challenging leave-sensor-out settings. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|graphguided_network_for_irregularly_sampled_multivariate_time_series", "pdf": "/pdf/6486d4fdf5e5db05e22c8ecd34aaab8401569f24.pdf", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.05357/code)", "_bibtex": "@inproceedings{\nzhang2022graphguided,\ntitle={Graph-Guided Network for Irregularly Sampled Multivariate Time Series},\nauthor={Xiang Zhang and Marko Zeman and Theodoros Tsiligkaridis and Marinka Zitnik},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Kwm8I7dU-l5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "qI4542Y2s1D", "original": "svjQqwSzNL7", "number": 1838, "cdate": 1632875544669, "mdate": null, "ddate": null, "tcdate": 1632875544669, "tmdate": 1697934779330, "tddate": null, "forum": "qI4542Y2s1D", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FILM: Following Instructions in Language with Modular Methods", "authorids": ["~So_Yeon_Min2", "~Devendra_Singh_Chaplot2", "~Pradeep_Kumar_Ravikumar1", "~Yonatan_Bisk1", "~Ruslan_Salakhutdinov1"], "authors": ["So Yeon Min", "Devendra Singh Chaplot", "Pradeep Kumar Ravikumar", "Yonatan Bisk", "Ruslan Salakhutdinov"], "keywords": ["Instruction Following", "Visual Language Navigation", "Embodied Instruction Following", "VLN", "ALFRED"], "abstract": "Recent methods for embodied instruction following are typically trained end-to-end using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene and (2) performs exploration with a semantic search policy, to achieve the natural language goal. Our modular method achieves SOTA performance (24.46 %) with a substantial (8.17 % absolute) gap from previous work while using less data by eschewing both expert trajectories and low-level instructions. Leveraging low-level language, however, can further increase our performance (26.49 %). Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions.", "one-sentence_summary": "We propose a modular method for embodied instruction following; our method achieves SOTA on the ALFRED benchmark by a large margin while using less data by eschewing both expert trajectories and low-level instructions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "min|film_following_instructions_in_language_with_modular_methods", "pdf": "/pdf/097027dcab1f74c4d68412bd97c6299ec48d6c67.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.07342/code)", "_bibtex": "@inproceedings{\nmin2022film,\ntitle={{FILM}: Following Instructions in Language with Modular Methods},\nauthor={So Yeon Min and Devendra Singh Chaplot and Pradeep Kumar Ravikumar and Yonatan Bisk and Ruslan Salakhutdinov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qI4542Y2s1D}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "Fza94Y8VS4a", "original": "qHEf7cTWi2_", "number": 1836, "cdate": 1632875544531, "mdate": null, "ddate": null, "tcdate": 1632875544531, "tmdate": 1676330591049, "tddate": null, "forum": "Fza94Y8VS4a", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Evolution of Uncertainty of Learning in Games", "authorids": ["~Yun_Kuen_Cheung1", "~Georgios_Piliouras1", "~Yixin_Tao1"], "authors": ["Yun Kuen Cheung", "Georgios Piliouras", "Yixin Tao"], "keywords": ["learning in games", "differential entropy"], "abstract": "Learning in games has become an object of intense interest for ML due to its connections to numerous AI architectures. We study standard online learning in games but from a non-standard perspective. Instead of studying the behavior of a single initial condition and whether it converges to equilibrium or not, we study the behavior of a probability distribution/measure over a set of initial conditions. This initial uncertainty is well-motivated both from a standard game-theoretic perspective (e.g. a modeler's uncertainty about the agents' initial beliefs) as well as from a ML one (e.g. noisy measurements, system initialization from a dataset distribution). Despite this, little is formally known about whether and under what conditions uncertainty is amplified or reduced in these systems. We use the popular measure of differential entropy to quantify the evolution of uncertainty. We find that such analysis shares an intimate relationship with volume analysis, a technique which was recently used to demonstrate the occurrence of Lyapunov chaos when using Multiplicative Weights Update (MWU) or Follow-the-Regularized-Leader (FTRL) algorithms in zero-sum games. This allows us to show that the differential entropy of these learning-in-game systems increases linearly with time, formalizing their increased unpredictability over time. We showcase the power of the framework by applying it in the study of multiple related systems, including different standard online optimization algorithms in numerous games and dynamics of evolutionary game theory.", "one-sentence_summary": "We show that the differential entropy of certain learning-in-game systems increases linearly with time, formalizing their increased unpredictability over time.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cheung|the_evolution_of_uncertainty_of_learning_in_games", "pdf": "/pdf/b60f84c940dd1715e1df1ce0effeb9adde6d73b0.pdf", "_bibtex": "@inproceedings{\ncheung2022the,\ntitle={The Evolution of Uncertainty of Learning in Games},\nauthor={Yun Kuen Cheung and Georgios Piliouras and Yixin Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Fza94Y8VS4a}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "CrCvGNHAIrz", "original": "jKGtygN32r7", "number": 1827, "cdate": 1632875544051, "mdate": null, "ddate": null, "tcdate": 1632875544051, "tmdate": 1676330591433, "tddate": null, "forum": "CrCvGNHAIrz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Explainable GNN-Based Models over Knowledge Graphs", "authorids": ["~David_Jaime_Tena_Cucala1", "~Bernardo_Cuenca_Grau1", "~Egor_V._Kostylev2", "~Boris_Motik1"], "authors": ["David Jaime Tena Cucala", "Bernardo Cuenca Grau", "Egor V. Kostylev", "Boris Motik"], "keywords": [], "abstract": "Graph Neural Networks (GNNs) are often used to learn transformations of graph data. While effective in practice, such approaches make predictions via numeric manipulations so their output cannot be easily explained symbolically. We propose a new family of GNN-based transformations of graph data that can be trained effectively, but where all predictions can be explained symbolically as logical inferences in Datalog\u2014a well-known rule-based formalism. In particular, we show how to encode an input knowledge graph into a graph with numeric feature vectors, process this graph using a GNN, and decode the result into an output knowledge graph. We use a new class of monotonic GNNs (MGNNs) to ensure that this process is equivalent to a round of application of a set of Datalog rules. We also show that, given an arbitrary MGNN, we can automatically extract rules that completely characterise the transformation. We evaluate our approach by applying it to classification tasks in knowledge graph completion.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cucala|explainable_gnnbased_models_over_knowledge_graphs", "pdf": "/pdf/1865cb0305fb96472280c0ba5bf054163099c62a.pdf", "one-sentence_summary": "We propose a new family of graph neural network-based transformations of graph data that can be trained effectively and where all predictions can be explained symbolically as logical inferences in Datalog.", "supplementary_material": "/attachment/e3d778f6167712f6cecccb920f51e85ce3ccba69.zip", "data": "", "_bibtex": "@inproceedings{\ncucala2022explainable,\ntitle={Explainable {GNN}-Based Models over Knowledge Graphs},\nauthor={David Jaime Tena Cucala and Bernardo Cuenca Grau and Egor V. Kostylev and Boris Motik},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CrCvGNHAIrz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 37}}, {"id": "OY1A8ejQgEX", "original": "STKQmZJ-PK3", "number": 1823, "cdate": 1632875543767, "mdate": null, "ddate": null, "tcdate": 1632875543767, "tmdate": 1676330591866, "tddate": null, "forum": "OY1A8ejQgEX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention", "authorids": ["~Michiel_de_Jong1", "~Yury_Zemlyanskiy1", "~Nicholas_FitzGerald1", "~Fei_Sha3", "~William_W._Cohen2"], "authors": ["Michiel de Jong", "Yury Zemlyanskiy", "Nicholas FitzGerald", "Fei Sha", "William W. Cohen"], "keywords": ["NLP", "Entities and Relations", "Memory"], "abstract": "Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. \nSpecifically, our method represents knowledge with ``mention memory'', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. \nIn experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong  performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision.  Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining.", "one-sentence_summary": "Incorporate information from text corpus into Transformer model through within-model attention over table of entity mention representations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jong|mention_memory_incorporating_textual_knowledge_into_transformers_through_entity_mention_attention", "pdf": "/pdf/e112757f72e357db126aa955b1195bd923e59f19.pdf", "supplementary_material": "/attachment/bf8f041e36cd9db3d801c0cfd721f1524ad3aec6.zip", "_bibtex": "@inproceedings{\njong2022mention,\ntitle={Mention Memory: incorporating textual knowledge into Transformers through entity mention attention},\nauthor={Michiel de Jong and Yury Zemlyanskiy and Nicholas FitzGerald and Fei Sha and William W. Cohen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OY1A8ejQgEX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "dDo8druYppX", "original": "TTGYqlad101", "number": 1820, "cdate": 1632875543563, "mdate": null, "ddate": null, "tcdate": 1632875543563, "tmdate": 1676330591955, "tddate": null, "forum": "dDo8druYppX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization", "authorids": ["~Biao_Zhang5", "~Peter_Wonka1"], "authors": ["Biao Zhang", "Peter Wonka"], "keywords": ["shape reconstruction single image", "meta learning", "few-shot learning", "differentiable optimization", "bi-level optimization"], "abstract": "We propose a novel 3d shape representation for 3d shape reconstruction from a single image. Rather than predicting a shape directly, we train a network to generate a training set which will be fed into another learning algorithm to define the shape. The nested optimization problem can be modeled by bi-level optimization. Specifically, the algorithms for bi-level optimization are also being used in meta learning approaches for few-shot learning. Our framework establishes a link between 3D shape analysis and few-shot learning. We combine training data generating networks with bi-level optimization algorithms to obtain a complete framework for which all components can be jointly trained. We improve upon recent work on standard benchmarks for 3d shape reconstruction.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|training_data_generating_networks_shape_reconstruction_via_bilevel_optimization", "pdf": "/pdf/4b2efd8acee00b364d8c45ac54321d5a59b31f8c.pdf", "data": "", "_bibtex": "@inproceedings{\nzhang2022training,\ntitle={Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization},\nauthor={Biao Zhang and Peter Wonka},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dDo8druYppX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "IcUWShptD7d", "original": "nTuEWDxhqE7", "number": 1819, "cdate": 1632875543494, "mdate": null, "ddate": null, "tcdate": 1632875543494, "tmdate": 1697934781454, "tddate": null, "forum": "IcUWShptD7d", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Monotonic Differentiable Sorting Networks", "authorids": ["~Felix_Petersen1", "~Christian_Borgelt1", "~Hilde_Kuehne5", "~Oliver_Deussen1"], "authors": ["Felix Petersen", "Christian Borgelt", "Hilde Kuehne", "Oliver Deussen"], "keywords": ["differentiable sorting", "monotonic", "sorting", "ranking", "sorting networks"], "abstract": "Differentiable sorting algorithms allow training with sorting and ranking supervision, where only the ordering or ranking of samples is known. Various methods have been proposed to address this challenge, ranging from optimal transport-based differentiable Sinkhorn sorting algorithms to making classic sorting networks differentiable. One problem of current differentiable sorting methods is that they are non-monotonic. To address this issue, we propose a novel relaxation of conditional swap operations that guarantees monotonicity in differentiable sorting networks. We introduce a family of sigmoid functions and prove that they produce differentiable sorting networks that are monotonic. Monotonicity ensures that the gradients always have the correct sign, which is an advantage in gradient-based optimization. We demonstrate that monotonic differentiable sorting networks improve upon previous differentiable sorting methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "petersen|monotonic_differentiable_sorting_networks", "pdf": "/pdf/8a2a45749e55ea6d302b7f66c647d340fb2bce2a.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.09630/code)", "_bibtex": "@inproceedings{\npetersen2022monotonic,\ntitle={Monotonic Differentiable Sorting Networks},\nauthor={Felix Petersen and Christian Borgelt and Hilde Kuehne and Oliver Deussen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=IcUWShptD7d}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "qyTBxTztIpQ", "original": "jJLHPcgkzhp", "number": 1818, "cdate": 1632875543422, "mdate": null, "ddate": null, "tcdate": 1632875543422, "tmdate": 1676330592159, "tddate": null, "forum": "qyTBxTztIpQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CrowdPlay: Crowdsourcing Human Demonstrations for Offline Learning", "authorids": ["~Matthias_Gerstgrasser1", "~Rakshit_Trivedi1", "~David_C._Parkes1"], "authors": ["Matthias Gerstgrasser", "Rakshit Trivedi", "David C. Parkes"], "keywords": [], "abstract": "Crowdsourcing has been instrumental for driving AI advances that rely on large-scale data. At the same time, reinforcement learning has seen rapid progress through  benchmark environments that strike a balance between tractability and real-world complexity, such as ALE and OpenAI Gym. In this paper, we aim to fill a gap at the intersection of these two: The use of crowdsourcing to generate large-scale human demonstration data in the support of advancing research into imitation learning and offline learning.\nTo this end, we present CrowdPlay, a complete crowdsourcing pipeline for any standard RL environment including OpenAI Gym (made available under an open-source license); a large-scale publicly available crowdsourced dataset of human gameplay demonstrations in Atari 2600 games, including multimodal behavior and human-human and human-AI multiagent data; offline learning benchmarks with extensive human data evaluation; and a detailed study of incentives, including real-time feedback to drive high quality data.\nWe hope that this will drive the improvement in design of algorithms that  account for the complexity of human, behavioral data and thereby enable a step forward in direction of effective learning for real-world settings. Our code and dataset are available at https://mgerstgrasser.github.io/crowdplay/.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gerstgrasser|crowdplay_crowdsourcing_human_demonstrations_for_offline_learning", "pdf": "/pdf/b52b98ab7e0985b4d419899df029ac52613153b4.pdf", "one-sentence_summary": "We provide a crowdsourcing pipeline for arbitrary RL environments, a first dataset on Atari games, and benchmark results.", "data": "", "_bibtex": "@inproceedings{\ngerstgrasser2022crowdplay,\ntitle={CrowdPlay: Crowdsourcing Human Demonstrations for Offline Learning},\nauthor={Matthias Gerstgrasser and Rakshit Trivedi and David C. Parkes},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qyTBxTztIpQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "KSSfF5lMIAg", "original": "mpzAV5bSMLg", "number": 1816, "cdate": 1632875543276, "mdate": null, "ddate": null, "tcdate": 1632875543276, "tmdate": 1697934782039, "tddate": null, "forum": "KSSfF5lMIAg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Model Agnostic Interpretability for Multiple Instance Learning", "authorids": ["~Joseph_Early1", "~Christine_Evers1", "~SArvapali_Ramchurn1"], "authors": ["Joseph Early", "Christine Evers", "SArvapali Ramchurn"], "keywords": ["multiple instance learning", "interpretability", "model-agnostic"], "abstract": "In Multiple Instance Learning (MIL), models are trained using bags of instances, where only a single label is provided for each bag. A bag label is often only determined by a handful of key instances within a bag, making it difficult to interpret what information a classifier is using to make decisions. In this work, we establish the key requirements for interpreting MIL models. We then go on to develop several model-agnostic approaches that meet these requirements. Our methods are compared against existing inherently interpretable MIL models on several datasets, and achieve an increase in interpretability accuracy of up to 30%. We also examine the ability of the methods to identify interactions between instances and scale to larger datasets, improving their applicability to real-world problems.", "one-sentence_summary": "We propose and compare several methods for model-agnostic interpretability for multiple instance learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "early|model_agnostic_interpretability_for_multiple_instance_learning", "pdf": "/pdf/661f27fbc9eceb4bfe945b34904428e58f815b5c.pdf", "supplementary_material": "/attachment/f3d916ea00b0fb1106bb87252cb8e1fabe10c984.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.11701/code)", "_bibtex": "@inproceedings{\nearly2022model,\ntitle={Model Agnostic Interpretability for Multiple Instance Learning},\nauthor={Joseph Early and Christine Evers and SArvapali Ramchurn},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KSSfF5lMIAg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "Zq2G_VTV53T", "original": "oND4sbqIDV_", "number": 1814, "cdate": 1632875543132, "mdate": null, "ddate": null, "tcdate": 1632875543132, "tmdate": 1697934782244, "tddate": null, "forum": "Zq2G_VTV53T", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FastSHAP: Real-Time Shapley Value Estimation", "authorids": ["~Neil_Jethani1", "~Mukund_Sudarshan1", "~Ian_Connick_Covert1", "~Su-In_Lee2", "~Rajesh_Ranganath2"], "authors": ["Neil Jethani", "Mukund Sudarshan", "Ian Connick Covert", "Su-In Lee", "Rajesh Ranganath"], "keywords": ["interpretability", "shapley", "amortization", "explainability", "game theory"], "abstract": "Although Shapley values are theoretically appealing for explaining black-box models, they are costly to calculate and thus impractical in settings that involve large, high-dimensional models. To remedy this issue, we introduce FastSHAP, a new method for estimating Shapley values in a single forward pass using a learned explainer model. To enable efficient training without requiring ground truth Shapley values, we develop an approach to train FastSHAP via stochastic gradient descent using a weighted least-squares objective function. In our experiments with tabular and image datasets, we compare FastSHAP to existing estimation approaches and find that it generates accurate explanations with an orders-of-magnitude speedup.", "one-sentence_summary": "We introduce FastSHAP, a new method for estimating Shapley values in a single forward pass using an explainer model that is learned via stochastic gradient optimization using a weighted least squares-like objective function.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jethani|fastshap_realtime_shapley_value_estimation", "pdf": "/pdf/5c857a16a6f7a915997089db728adf6e56ad56c2.pdf", "supplementary_material": "/attachment/5d5c5c73c5f4d1be803abed07dcc8be48953f8ce.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2107.07436/code)", "_bibtex": "@inproceedings{\njethani2022fastshap,\ntitle={Fast{SHAP}: Real-Time Shapley Value Estimation},\nauthor={Neil Jethani and Mukund Sudarshan and Ian Connick Covert and Su-In Lee and Rajesh Ranganath},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Zq2G_VTV53T}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "4Ycr8oeCoIh", "original": "O1zdzMYoh7c", "number": 1811, "cdate": 1632875542931, "mdate": null, "ddate": null, "tcdate": 1632875542931, "tmdate": 1697934783257, "tddate": null, "forum": "4Ycr8oeCoIh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "When, Why, and Which Pretrained GANs Are Useful?", "authorids": ["~Timofey_Grigoryev1", "~Andrey_Voynov1", "~Artem_Babenko1"], "authors": ["Timofey Grigoryev", "Andrey Voynov", "Artem Babenko"], "keywords": ["GAN", "pretraining"], "abstract": "The literature has proposed several methods to finetune pretrained GANs on new datasets, which typically results in higher performance compared to training from scratch, especially in the limited-data regime. However, despite the apparent empirical benefits of GAN pretraining, its inner mechanisms were not analyzed in-depth, and understanding of its role is not entirely clear. Moreover, the essential practical details, e.g., selecting a proper pretrained GAN checkpoint, currently do not have rigorous grounding and are typically determined by trial and error. \n\nThis work aims to dissect the process of GAN finetuning. First, we show that initializing the GAN training process by a pretrained checkpoint primarily affects the model's coverage rather than the fidelity of individual samples. Second, we explicitly describe how pretrained generators and discriminators contribute to the finetuning process and explain the previous evidence on the importance of pretraining both of them. Finally, as an immediate practical benefit of our analysis, we describe a simple recipe to choose an appropriate GAN checkpoint that is the most suitable for finetuning to a particular target task. Importantly, for most of the target tasks, Imagenet-pretrained GAN, despite having poor visual quality, appears to be an excellent starting point for finetuning, resembling the typical pretraining scenario of discriminative computer vision models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "grigoryev|when_why_and_which_pretrained_gans_are_useful", "pdf": "/pdf/9895b4007914d217311fed6ca5a856553aa4384f.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2202.08937/code)", "_bibtex": "@inproceedings{\ngrigoryev2022when,\ntitle={When, Why, and Which Pretrained {GAN}s Are Useful?},\nauthor={Timofey Grigoryev and Andrey Voynov and Artem Babenko},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=4Ycr8oeCoIh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "R332S76RjxS", "original": "_jDj3RkNfI", "number": 1809, "cdate": 1632875542801, "mdate": null, "ddate": null, "tcdate": 1632875542801, "tmdate": 1676330592800, "tddate": null, "forum": "R332S76RjxS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A global convergence theory for deep ReLU implicit networks via over-parameterization", "authorids": ["~Tianxiang_Gao2", "~Hailiang_Liu1", "~Jia_Liu1", "~Hridesh_Rajan1", "~Hongyang_Gao1"], "authors": ["Tianxiang Gao", "Hailiang Liu", "Jia Liu", "Hridesh Rajan", "Hongyang Gao"], "keywords": ["Deep learning", "Deep implicit learning", "deep equilibrium model", "gradient descent", "stochastic gradient descent", "over-parameterization"], "abstract": "Implicit deep learning has received increasing attention recently due to the fact that it generalizes the recursive prediction rule of many commonly used neural network architectures. Its prediction rule is provided implicitly based on the solution of an equilibrium equation. Although a line of recent empirical studies has demonstrated its superior performances, the theoretical understanding of implicit neural networks is limited. In general, the equilibrium equation may not be well-posed during the training. As a result, there is no guarantee that a vanilla (stochastic) gradient descent (SGD) training nonlinear implicit neural networks can converge. This paper fills the gap by analyzing the gradient flow of Rectified Linear Unit (ReLU) activated implicit neural networks. For an $m$ width implicit neural network with ReLU activation and $n$ training samples, we show that a randomly initialized gradient descent converges to a global minimum at a linear rate for the square loss function if the implicit neural network is over-parameterized. It is worth noting that, unlike existing works on the convergence of (S)GD on finite-layer over-parameterized neural networks, our convergence results hold for implicit neural networks, where the number of layers is infinite.", "pdf": "/pdf/3a9349bc7a2ee29f29061dc2aa3664f9d3bed2ed.pdf", "one-sentence_summary": "For a ReLU activated implicit neural network with infinitely many layers, we prove that randomly initialized gradient descent with a fixed step-size converges to a global minimum at a linear rate if the width m is the squared sample size n.", "supplementary_material": "/attachment/eb41990fc8066d9e5f6972cabb3753f9b1c54050.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gao|a_global_convergence_theory_for_deep_relu_implicit_networks_via_overparameterization", "_bibtex": "@inproceedings{\ngao2022a,\ntitle={A global convergence theory for deep Re{LU} implicit networks via over-parameterization},\nauthor={Tianxiang Gao and Hailiang Liu and Jia Liu and Hridesh Rajan and Hongyang Gao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=R332S76RjxS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "6VpeS27viTq", "original": "RSL7xH4cQy", "number": 1807, "cdate": 1632875542662, "mdate": null, "ddate": null, "tcdate": 1632875542662, "tmdate": 1676330592962, "tddate": null, "forum": "6VpeS27viTq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations", "authorids": ["~Weiqi_Peng1", "~Jinghui_Chen1"], "authors": ["Weiqi Peng", "Jinghui Chen"], "keywords": [], "abstract": "Owing much to the revolution of information technology, recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats. Yet those publicly accessible information also raises a fundamental issue concerning Intellectual Property, that is, how to precisely control legal or illegal exploitation of a dataset for training commercial models. To tackle this issue, this paper introduces and investigates a new concept called ''learnability lock'' for securing the process of data authorization. In particular, we propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to encrypt data samples so that they become ''unlearnable'' by machine learning models with negligible loss of visual features. Meanwhile, authorized clients can use a specific key to unlock the learnability of the protected dataset and train models normally. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. We empirically demonstrate the success and practicability of our method on visual classification tasks.  ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "peng|learnability_lock_authorized_learnability_control_through_adversarial_invertible_transformations", "pdf": "/pdf/2cff7d73a77a8a9d3e795578321db17a9d903f78.pdf", "_bibtex": "@inproceedings{\npeng2022learnability,\ntitle={Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations},\nauthor={Weiqi Peng and Jinghui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6VpeS27viTq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "WHA8009laxu", "original": "AjJadNGBPA8", "number": 1799, "cdate": 1632875542135, "mdate": null, "ddate": null, "tcdate": 1632875542135, "tmdate": 1697934785095, "tddate": null, "forum": "WHA8009laxu", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Federated Learning from Only Unlabeled Data with Class-conditional-sharing Clients", "authorids": ["~Nan_Lu1", "~Zhao_Wang3", "~Xiaoxiao_Li1", "~Gang_Niu1", "~Qi_Dou2", "~Masashi_Sugiyama1"], "authors": ["Nan Lu", "Zhao Wang", "Xiaoxiao Li", "Gang Niu", "Qi Dou", "Masashi Sugiyama"], "keywords": ["unsupervised federated learning", "unlabeled data", "class prior shift"], "abstract": "Supervised federated learning (FL) enables multiple clients to share the trained model without sharing their labeled data. However, potential clients might even be reluctant to label their own data, which could limit the applicability of FL in practice. In this paper, we show the possibility of unsupervised FL whose model is still a classifier for predicting class labels, if the class-prior probabilities are shifted while the class-conditional distributions are shared among the unlabeled data owned by the clients. We propose federation of unsupervised learning (FedUL), where the unlabeled data are transformed into surrogate labeled data for each of the clients, a modified model is trained by supervised FL, and the wanted model is recovered from the modified model. FedUL is a very general solution to unsupervised FL: it is compatible with many supervised FL methods, and the recovery of the wanted model can be theoretically guaranteed as if the data have been labeled. Experiments on benchmark and real-world datasets demonstrate the effectiveness of FedUL. Code is available at https://github.com/lunanbit/FedUL.", "one-sentence_summary": "Federated learning: no label no cry", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lu|federated_learning_from_only_unlabeled_data_with_classconditionalsharing_clients", "pdf": "/pdf/9dbc6100a2a81a4d65580c913a1a7974a6fe470d.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.03304/code)", "_bibtex": "@inproceedings{\nlu2022federated,\ntitle={Federated Learning from Only Unlabeled Data with Class-conditional-sharing Clients},\nauthor={Nan Lu and Zhao Wang and Xiaoxiao Li and Gang Niu and Qi Dou and Masashi Sugiyama},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WHA8009laxu}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "Rty5g9imm7H", "original": "wDgVnNAy9bk", "number": 1792, "cdate": 1632875541675, "mdate": null, "ddate": null, "tcdate": 1632875541675, "tmdate": 1676330594133, "tddate": null, "forum": "Rty5g9imm7H", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Transformer Embeddings of Irregularly Spaced Events and Their Participants", "authorids": ["~Hongyuan_Mei1", "~Chenghao_Yang1", "~Jason_Eisner1"], "authors": ["Hongyuan Mei", "Chenghao Yang", "Jason Eisner"], "keywords": ["irregular time series", "generative Transformers", "neuro-symbolic architectures", "logic programming"], "abstract": "The neural Hawkes process (Mei & Eisner, 2017) is a generative model of irregularly spaced sequences of discrete events. To handle complex domains with many event types, Mei et al. (2020a) further consider a setting in which each event in the sequence updates a deductive database of facts (via domain-specific pattern-matching rules); future events are then conditioned on the database contents. They show how to convert such a symbolic system into a neuro-symbolic continuous-time generative model, in which each database fact and possible event has a time-varying embedding that is derived from its symbolic provenance. \n\nIn this paper, we modify both models, replacing their recurrent LSTM-based architectures with flatter attention-based architectures (Vaswani et al., 2017), which are simpler and more parallelizable. This does not appear to hurt our accuracy, which is comparable to or better than that of the original models as well as (where applicable) previous attention-based methods (Zuo et al., 2020; Zhang et al., 2020a).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mei|transformer_embeddings_of_irregularly_spaced_events_and_their_participants", "pdf": "/pdf/7d691bc93acd9cb9bd82501ce85784e489d480b0.pdf", "one-sentence_summary": "We design a generative continuous-time Transformer for embedding irregularly spaced events and their participants. ", "_bibtex": "@inproceedings{\nmei2022transformer,\ntitle={Transformer Embeddings of Irregularly Spaced Events and Their Participants},\nauthor={Hongyuan Mei and Chenghao Yang and Jason Eisner},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Rty5g9imm7H}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "0DcZxeWfOPt", "original": "Y6EfuMfjgen", "number": 1791, "cdate": 1632875541607, "mdate": null, "ddate": null, "tcdate": 1632875541607, "tmdate": 1697934786169, "tddate": null, "forum": "0DcZxeWfOPt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fast Model Editing at Scale", "authorids": ["~Eric_Mitchell1", "~Charles_Lin2", "~Antoine_Bosselut1", "~Chelsea_Finn1", "~Christopher_D_Manning1"], "authors": ["Eric Mitchell", "Charles Lin", "Antoine Bosselut", "Chelsea Finn", "Christopher D Manning"], "keywords": ["editing", "transfomers", "meta-learning"], "abstract": "While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks using Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code available at https://sites.google.com/view/mend-editing.", "one-sentence_summary": "A computationally efficient approach for learning to edit the behavior of very large pre-trained language models (10 billion+ parameters)", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mitchell|fast_model_editing_at_scale", "pdf": "/pdf/f647f6c18613fc91a31c3ef0b98dbe3b782d01f8.pdf", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.11309/code)", "_bibtex": "@inproceedings{\nmitchell2022fast,\ntitle={Fast Model Editing at Scale},\nauthor={Eric Mitchell and Charles Lin and Antoine Bosselut and Chelsea Finn and Christopher D Manning},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0DcZxeWfOPt}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "rTAclwH46Tb", "original": "pUoJ1-KMpv9", "number": 1786, "cdate": 1632875541261, "mdate": null, "ddate": null, "tcdate": 1632875541261, "tmdate": 1676330594461, "tddate": null, "forum": "rTAclwH46Tb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums", "authorids": ["~Rui_Pan4", "~Haishan_Ye2", "~Tong_Zhang2"], "authors": ["Rui Pan", "Haishan Ye", "Tong Zhang"], "keywords": ["optimization", "learning rate schedule", "optimal convergence rate"], "abstract": "Learning rate schedulers have been widely adopted in training deep neural networks. Despite their practical importance, there is a discrepancy between its practice and its theoretical analysis. For instance, it is not known what schedules of SGD achieve best convergence, even for simple problems such as optimizing quadratic objectives. In this paper, we propose Eigencurve, the first family of learning rate schedules that can achieve minimax optimal convergence rates (up to a constant) for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed. The condition is quite common in practice. Experimental results show that Eigencurve can significantly outperform step decay in image classification tasks on CIFAR-10, especially when the number of epochs is small. Moreover, the theory inspires two simple learning rate schedulers for practical applications that can approximate eigencurve.\n For some problems, the optimal shape of the proposed schedulers resembles that of cosine decay, which sheds light to the success of cosine decay for such situations. For other situations, the proposed schedulers are superior to cosine decay.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pan|eigencurve_optimal_learning_rate_schedule_for_sgd_on_quadratic_objectives_with_skewed_hessian_spectrums", "pdf": "/pdf/49443e75c10da1ddfa39b8fcaf44859bf09c842d.pdf", "one-sentence_summary": "A learning rate schedule which achieves minimax optimal convergence rate (up to a constant) for SGD on quadratic objectives with skewed Hessian spectrums.", "_bibtex": "@inproceedings{\npan2022eigencurve,\ntitle={Eigencurve: Optimal Learning Rate Schedule for {SGD} on Quadratic Objectives with Skewed Hessian Spectrums},\nauthor={Rui Pan and Haishan Ye and Tong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rTAclwH46Tb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 32}}, {"id": "C03Ajc-NS5W", "original": "dptM7aAQnH", "number": 1784, "cdate": 1632875541127, "mdate": null, "ddate": null, "tcdate": 1632875541127, "tmdate": 1676330594745, "tddate": null, "forum": "C03Ajc-NS5W", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch", "authorids": ["~Youzhi_Luo1", "~Shuiwang_Ji1"], "authors": ["Youzhi Luo", "Shuiwang Ji"], "keywords": ["3D molecular geometry generation", "flow models", "SphereNet"], "abstract": "We consider the problem of generating 3D molecular geometries from scratch. While multiple methods have been developed for generating molecular graphs, generating 3D molecular geometries from scratch is largely under-explored. In this work, we propose G-SphereNet, a novel autoregressive flow model for generating 3D molecular geometries. G-SphereNet employs a flexible sequential generation scheme by placing atoms in 3D space step-by-step. Instead of generating 3D coordinates directly, we propose to determine 3D positions of atoms by generating distances, angles and torsion angles, thereby ensuring both invariance and equivariance properties. In addition, we propose to use spherical message passing and attention mechanism for conditional information extraction. Experimental results show that G-SphereNet outperforms previous methods on random molecular geometry generation and targeted molecule discovery tasks. Our code is publicly available as part of the DIG package (https://github.com/divelab/DIG).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "luo|an_autoregressive_flow_model_for_3d_molecular_geometry_generation_from_scratch", "pdf": "/pdf/d95291386d651afeb3ab8482526cdc9564159d8e.pdf", "one-sentence_summary": "We present a novel method for 3D molecular geometry generation from scratch.", "supplementary_material": "/attachment/bf572e50a365a0ae0a5fbe28ee06f1cd4cbe6ca7.zip", "_bibtex": "@inproceedings{\nluo2022an,\ntitle={An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch},\nauthor={Youzhi Luo and Shuiwang Ji},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=C03Ajc-NS5W}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "nzvbBD_3J-g", "original": "LCgefoO3h9r", "number": 1779, "cdate": 1632875540784, "mdate": null, "ddate": null, "tcdate": 1632875540784, "tmdate": 1676330594936, "tddate": null, "forum": "nzvbBD_3J-g", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Incorporating Inductive Biases into VAEs", "authorids": ["~Ning_Miao3", "~Emile_Mathieu1", "~Siddharth_N1", "~Yee_Whye_Teh2", "~Tom_Rainforth1"], "authors": ["Ning Miao", "Emile Mathieu", "Siddharth N", "Yee Whye Teh", "Tom Rainforth"], "keywords": ["VAEs", "Variational autoencoders", "Variational auto-encoders", "Representation learning", "Inductive biases"], "abstract": "We explain why directly changing the prior can be a surprisingly ineffective mechanism for incorporating inductive biases into variational auto-encoders (VAEs), and introduce a simple and effective alternative approach: Intermediary Latent Space VAEs (InteL-VAEs). InteL-VAEs use an intermediary set of latent variables to control the stochasticity of the encoding process, before mapping these in turn to the latent representation using a parametric function that encapsulates our desired inductive bias(es). This allows us to impose properties like sparsity or clustering on learned representations, and incorporate human knowledge into the generative model. Whereas changing the prior only indirectly encourages behavior through regularizing the encoder, InteL-VAEs are able to directly enforce desired characteristics. Moreover, they bypass the computation and encoder design issues caused by non-Gaussian priors, while allowing for additional flexibility through training of the parametric mapping function. We show that these advantages, in turn, lead to both better generative models and better representations being learned.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "miao|on_incorporating_inductive_biases_into_vaes", "pdf": "/pdf/49391965f088fdd9d5dfe8de35cd9f4836edb6db.pdf", "one-sentence_summary": "A flexible and effective framework for adding inductive biases to VAEs that corrects the pathologies of previous approaches and leads to improved representations and generative models.", "supplementary_material": "/attachment/3a16bf226f62e6f590d16b3c51718a223aff218f.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nmiao2022on,\ntitle={On Incorporating Inductive Biases into {VAE}s},\nauthor={Ning Miao and Emile Mathieu and Siddharth N and Yee Whye Teh and Tom Rainforth},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nzvbBD_3J-g}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "Kef8cKdHWpP", "original": "A5R5FuajH8c", "number": 1778, "cdate": 1632875540714, "mdate": null, "ddate": null, "tcdate": 1632875540714, "tmdate": 1697934787473, "tddate": null, "forum": "Kef8cKdHWpP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools", "authorids": ["~Xingyu_Lin1", "~Zhiao_Huang1", "~Yunzhu_Li1", "~Joshua_B._Tenenbaum1", "~David_Held1", "~Chuang_Gan1"], "authors": ["Xingyu Lin", "Zhiao Huang", "Yunzhu Li", "Joshua B. Tenenbaum", "David Held", "Chuang Gan"], "keywords": ["Deformable Object Manipulation", "Differentiable Physics"], "abstract": "We consider the problem of sequential robotic manipulation of deformable objects using tools.\nPrevious works have shown that differentiable physics simulators provide gradients to the environment state and help trajectory optimization to converge orders of magnitude faster than model-free reinforcement learning algorithms for deformable object manipulation. However, such gradient-based trajectory optimization typically requires access to the full simulator states and can only solve short-horizon, single-skill tasks due to local optima. In this work, we propose a novel framework, named DiffSkill, that uses a differentiable physics simulator for skill abstraction to solve long-horizon deformable object manipulation tasks from sensory observations. In particular, we first obtain short-horizon skills using individual tools from a gradient-based optimizer, using the full state information in a differentiable simulator; we then learn a neural skill abstractor from the demonstration trajectories which takes RGBD images as input. Finally, we plan over the skills by finding the intermediate goals and then solve long-horizon tasks. We show the advantages of our method in a new set of sequential deformable object manipulation tasks compared to previous reinforcement learning algorithms and compared to the trajectory optimizer. ", "pdf": "/pdf/6beab35ef096a8de0c95debfaf6cca20cec5b33c.pdf", "one-sentence_summary": "We propose DiffSkill, a novel framework for learning skill abstraction from differentiable physics and compose them to solve long-horizontal deformable object manipulations tasks from sensory observation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lin|diffskill_skill_abstraction_from_differentiable_physics_for_deformable_object_manipulations_with_tools", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.17275/code)", "_bibtex": "@inproceedings{\nlin2022diffskill,\ntitle={DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools},\nauthor={Xingyu Lin and Zhiao Huang and Yunzhu Li and David Held and Joshua B. Tenenbaum and Chuang Gan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Kef8cKdHWpP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "SYB4WrJql1n", "original": "24CSTgIRuOU", "number": 1776, "cdate": 1632875540577, "mdate": null, "ddate": null, "tcdate": 1632875540577, "tmdate": 1697934787867, "tddate": null, "forum": "SYB4WrJql1n", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Existence of Universal Lottery Tickets", "authorids": ["~Rebekka_Burkholz1", "~Nilanjana_Laha1", "~Rajarshi_Mukherjee1", "~Alkis_Gotovos1"], "authors": ["Rebekka Burkholz", "Nilanjana Laha", "Rajarshi Mukherjee", "Alkis Gotovos"], "keywords": ["theory", "deep learning", "lottery tickets", "universality"], "abstract": "The lottery ticket hypothesis conjectures the existence of sparse subnetworks of large randomly initialized deep neural networks that can be successfully trained in isolation. Recent work has experimentally observed that some of these tickets can be practically reused across a variety of tasks, hinting at some form of universality. We formalize this concept and theoretically prove that not only do such universal tickets exist but they also do not require further training. Our proofs introduce a couple of technical innovations related to pruning for strong lottery tickets, including extensions of subset sum results and a strategy to leverage higher amounts of depth. Our explicit sparse constructions of universal function families might be of independent interest, as they highlight representational benefits induced by univariate convolutional architectures. ", "one-sentence_summary": "We formalize a notion of strong universal lottery tickets and prove their existence as subnetworks of randomly initialized neural networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "burkholz|on_the_existence_of_universal_lottery_tickets", "pdf": "/pdf/1a20f64ab36535ca4158e348dfd5510e876e1d69.pdf", "supplementary_material": "/attachment/f53431f73176b4cd4cf6e903254ff0957167d77d.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.11146/code)", "_bibtex": "@inproceedings{\nburkholz2022on,\ntitle={On the Existence of Universal Lottery Tickets},\nauthor={Rebekka Burkholz and Nilanjana Laha and Rajarshi Mukherjee and Alkis Gotovos},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SYB4WrJql1n}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "xQUe1pOKPam", "original": "UbdRDptzUGv", "number": 1772, "cdate": 1632875540302, "mdate": null, "ddate": null, "tcdate": 1632875540302, "tmdate": 1697934788077, "tddate": null, "forum": "xQUe1pOKPam", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pre-training Molecular Graph Representation with 3D Geometry", "authorids": ["~Shengchao_Liu1", "~Hanchen_Wang1", "~Weiyang_Liu1", "~Joan_Lasenby1", "~Hongyu_Guo1", "~Jian_Tang1"], "authors": ["Shengchao Liu", "Hanchen Wang", "Weiyang Liu", "Joan Lasenby", "Hongyu Guo", "Jian Tang"], "keywords": ["Pre-training", "SSL", "Molecule", "3D Geometry", "2D representation"], "abstract": "Molecular graph representation learning is a fundamental problem in modern drug and material discovery. Molecular graphs are typically modeled by their 2D topological structures, but it has been recently discovered that 3D geometric information plays a more vital role in predicting molecular functionalities. However, the lack of 3D information in real-world scenarios has significantly impeded the learning of geometric graph representation. To cope with this challenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework where self-supervised learning (SSL) is performed by leveraging the correspondence and consistency between 2D topological structures and 3D geometric views. GraphMVP effectively learns a 2D molecular graph encoder that is enhanced by richer and more discriminative 3D geometry. We further provide theoretical insights to justify the effectiveness of GraphMVP. Finally, comprehensive experiments show that GraphMVP can consistently outperform existing graph SSL methods. Code is available on GitHub: https://github.com/chao1224/GraphMVP.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|pretraining_molecular_graph_representation_with_3d_geometry", "pdf": "/pdf/a22cd0fe68ecdc04b0ffcd2cdeda6ba42dc8dffd.pdf", "one-sentence_summary": "We proposed a new SSL framework to make 3D geomety information helpful for 2D representation, in terms of the downstream tasks with 2D info only.", "supplementary_material": "/attachment/f1e6db0523eab0b32adda5f1e122f12f16a7e831.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.07728/code)", "_bibtex": "@inproceedings{\nliu2022pretraining,\ntitle={Pre-training Molecular Graph Representation with 3D Geometry},\nauthor={Shengchao Liu and Hanchen Wang and Weiyang Liu and Joan Lasenby and Hongyu Guo and Jian Tang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xQUe1pOKPam}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "-HSOjDPfhBJ", "original": "hre3vxyMPSW", "number": 1770, "cdate": 1632875540168, "mdate": null, "ddate": null, "tcdate": 1632875540168, "tmdate": 1676330595360, "tddate": null, "forum": "-HSOjDPfhBJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PER-ETD: A Polynomially Efficient Emphatic Temporal Difference Learning Method", "authorids": ["~Ziwei_Guan1", "~Tengyu_Xu1", "~Yingbin_Liang1"], "authors": ["Ziwei Guan", "Tengyu Xu", "Yingbin Liang"], "keywords": ["emphatic temporal difference", "finite-time analysis", "off-policy evaluation", "reinforcement learning"], "abstract": "Emphatic temporal difference (ETD) learning (Sutton et al., 2016) is a successful method to conduct the off-policy value function evaluation with function approximation. Although ETD has been shown to converge asymptotically to a desirable value function, it is well-known that ETD often encounters a large variance so that its sample complexity can increase exponentially fast with the number of iterations. In this work, we propose a new ETD method, called PER-ETD (i.e., PEriodically Restarted-ETD), which restarts and updates the follow-on trace only for a finite period for each iteration of the evaluation parameter. Further, PER-ETD features a design of the logarithmical increase of the restart period with the number of iterations, which guarantees the best trade-off between the variance and bias and keeps both vanishing sublinearly. We show that PER-ETD converges to the same desirable fixed point as ETD, but improves the exponential sample complexity of ETD to be polynomials. Our experiments validate the superior performance of PER-ETD and its advantage over ETD.", "pdf": "/pdf/d54fba2211a4e058d2052bf8de3fb0bdb631060b.pdf", "one-sentence_summary": "We propose a new off-policy evaluation algorithm called PER-ETD (i.e., PEriodically Restarted Emphatic TD), which improves upon its precursor ETD with reduced variance and polynomial sample efficiency.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guan|peretd_a_polynomially_efficient_emphatic_temporal_difference_learning_method", "_bibtex": "@inproceedings{\nguan2022peretd,\ntitle={{PER}-{ETD}: A Polynomially Efficient Emphatic Temporal Difference Learning Method},\nauthor={Ziwei Guan and Tengyu Xu and Yingbin Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-HSOjDPfhBJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "B72HXs80q4", "original": "djcyFEtahvk", "number": 1767, "cdate": 1632875539960, "mdate": null, "ddate": null, "tcdate": 1632875539960, "tmdate": 1697934788636, "tddate": null, "forum": "B72HXs80q4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Taming Sparsely Activated Transformer with Stochastic Experts", "authorids": ["~Simiao_Zuo1", "~Xiaodong_Liu1", "~Jian_Jiao2", "~Young_Jin_Kim1", "~Hany_Hassan1", "~Ruofei_Zhang1", "~Jianfeng_Gao1", "~Tuo_Zhao1"], "authors": ["Simiao Zuo", "Xiaodong Liu", "Jian Jiao", "Young Jin Kim", "Hany Hassan", "Ruofei Zhang", "Jianfeng Gao", "Tuo Zhao"], "keywords": [], "abstract": "Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can easily scale to have outrageously large amounts of parameters without significant increase in computational cost. However, SAMs are reported to be parameter inefficient such that larger models do not always lead to better performance. While most on-going research focuses on improving SAMs models by exploring methods of routing inputs to experts, our analysis reveals that such research might not lead to the solution we expect, i.e., the commonly-used routing methods based on gating mechanisms do not work better than randomly routing inputs to experts. In this paper, we propose a new expert-based model, THOR ($\\underline{\\textbf{T}}$ransformer wit$\\underline{\\textbf{H}}$ St$\\underline{\\textbf{O}}$chastic Expe$\\underline{\\textbf{R}}$ts). Unlike classic expert-based models, such as the Switch Transformer, experts in THOR are randomly activated for each input during training and inference. THOR models are trained using a consistency regularized loss, where experts learn not only from training data but also from other experts as teachers, such that all the experts make consistent predictions.  We validate the effectiveness of THOR on machine translation tasks. Results show that THOR models are more parameter efficient in that they significantly outperform the Transformer and MoE models across various settings. For example, in multilingual translation, THOR outperforms the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as that of a state-of-the-art MoE model that is 18 times larger. Our code is publicly available at: https://github.com/microsoft/Stochastic-Mixture-of-Experts.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zuo|taming_sparsely_activated_transformer_with_stochastic_experts", "pdf": "/pdf/55a2b1a443f2621d2199769150f3845eefe41ba6.pdf", "one-sentence_summary": "We propose a new variant of MoE, Transformer with Stochastic Experts, that is more parameter efficient.", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.04260/code)", "_bibtex": "@inproceedings{\nzuo2022taming,\ntitle={Taming Sparsely Activated Transformer with Stochastic Experts},\nauthor={Simiao Zuo and Xiaodong Liu and Jian Jiao and Young Jin Kim and Hany Hassan and Ruofei Zhang and Jianfeng Gao and Tuo Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=B72HXs80q4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "i3RI65sR7N", "original": "nfyQeU2ZYP", "number": 1759, "cdate": 1632875539547, "mdate": null, "ddate": null, "tcdate": 1632875539547, "tmdate": 1697934789308, "tddate": null, "forum": "i3RI65sR7N", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hierarchical Variational Memory for Few-shot Learning Across Domains", "authorids": ["~Yingjun_Du1", "~Xiantong_Zhen1", "~Ling_Shao1", "~Cees_G._M._Snoek1"], "authors": ["Yingjun Du", "Xiantong Zhen", "Ling Shao", "Cees G. M. Snoek"], "keywords": ["Meta-learning", "Variational hierarchical memory", "Variational hierarchical prototype", "Cross-domain few-shot learning"], "abstract": "Neural memory enables fast adaptation to new tasks with just a few training samples. Existing memory models store features only from the single last layer, which does not generalize well in presence of a domain shift between training and test distributions. Rather than relying on a flat memory, we propose a hierarchical alternative that stores features at different semantic levels. We introduce a hierarchical prototype model, where each level of the prototype fetches corresponding information from the hierarchical memory. The model is endowed with the ability to flexibly rely on features at different semantic levels if the domain shift circumstances so demand. We meta-learn the model by a newly derived hierarchical variational inference framework, where hierarchical memory and prototypes are jointly optimized. To explore and exploit the importance of different semantic levels, we further propose to learn the weights associated with the prototype at each level in a data-driven way, which enables the model to adaptively choose the most generalizable features. We conduct thorough ablation studies to demonstrate the effectiveness of each component in our model. The new state-of-the-art performance on cross-domain and competitive performance on traditional few-shot classification further substantiates the benefit of hierarchical variational memory.", "one-sentence_summary": "We propose a hierarchical memory that stores features at different semantic levels for the cross-domain few-shot learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "du|hierarchical_variational_memory_for_fewshot_learning_across_domains", "pdf": "/pdf/97127f112b9804e1962c3670a2de7a6c351ac591.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.08181/code)", "_bibtex": "@inproceedings{\ndu2022hierarchical,\ntitle={Hierarchical Variational Memory for Few-shot Learning Across Domains},\nauthor={Yingjun Du and Xiantong Zhen and Ling Shao and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=i3RI65sR7N}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "Z1Qlm11uOM", "original": "JFCbeOx3WEi", "number": 1757, "cdate": 1632875539404, "mdate": null, "ddate": null, "tcdate": 1632875539404, "tmdate": 1697934789606, "tddate": null, "forum": "Z1Qlm11uOM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction", "authorids": ["~Bowen_Shi1", "~Wei-Ning_Hsu2", "~Kushal_Lakhotia1", "~Abdelrahman_Mohamed2"], "authors": ["Bowen Shi", "Wei-Ning Hsu", "Kushal Lakhotia", "Abdelrahman Mohamed"], "keywords": ["audio-visual speech recognition", "lip reading", "speech recognition", "self-supervised learning"], "abstract": "Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker\u2019s lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/facebookresearch/av_hubert.", "one-sentence_summary": "A self-supervised learning framework for audio-visual speech data, which uses only 30h of labeled data to match the SOTA lip-reading model trained on 31k hours of data (34.6% vs 33.6% WER), and further outperforms the SOTA with 70x less data (30.6%).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shi|learning_audiovisual_speech_representation_by_masked_multimodal_cluster_prediction", "pdf": "/pdf/db3061a63dfde7babf9a5fa76d250390f56b8771.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.02184/code)", "_bibtex": "@inproceedings{\nshi2022learning,\ntitle={Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction},\nauthor={Bowen Shi and Wei-Ning Hsu and Kushal Lakhotia and Abdelrahman Mohamed},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Z1Qlm11uOM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "RdJVFCHjUMI", "original": "m8mp7UzqUBt", "number": 1753, "cdate": 1632875539186, "mdate": null, "ddate": null, "tcdate": 1632875539186, "tmdate": 1697934790246, "tddate": null, "forum": "RdJVFCHjUMI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "An Explanation of In-context Learning as Implicit Bayesian Inference", "authorids": ["~Sang_Michael_Xie1", "~Aditi_Raghunathan1", "~Percy_Liang1", "~Tengyu_Ma1"], "authors": ["Sang Michael Xie", "Aditi Raghunathan", "Percy Liang", "Tengyu Ma"], "keywords": ["in-context learning", "language modeling", "pre-training", "GPT-3"], "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xie|an_explanation_of_incontext_learning_as_implicit_bayesian_inference", "pdf": "/pdf/34aa1a37f7afecc48d15dbab476f32a40db2fe1a.pdf", "one-sentence_summary": "In-context learning emerges both theoretically and empirically when the pretraining distribution is a mixture distribution, resulting in the language model implicitly performing Bayesian inference in its forward pass.", "supplementary_material": "/attachment/ffed395625f523367ea3562d91fc323074cf8321.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.02080/code)", "_bibtex": "@inproceedings{\nxie2022an,\ntitle={An Explanation of In-context Learning as Implicit Bayesian Inference},\nauthor={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RdJVFCHjUMI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "w_drCosT76", "original": "NIemHRBkvaM", "number": 1748, "cdate": 1632875538831, "mdate": null, "ddate": null, "tcdate": 1632875538831, "tmdate": 1676330596568, "tddate": null, "forum": "w_drCosT76", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Differentiable Scaffolding Tree for Molecule Optimization", "authorids": ["~Tianfan_Fu1", "~Wenhao_Gao1", "~Cao_Xiao2", "jyasonik@mit.edu", "~Connor_W._Coley1", "~Jimeng_Sun3"], "authors": ["Tianfan Fu", "Wenhao Gao", "Cao Xiao", "Jacob Yasonik", "Connor W. Coley", "Jimeng Sun"], "keywords": [], "abstract": "The structural design of functional molecules, also called molecular optimization, is an essential chemical science and engineering task with important applications, such as drug discovery. Deep generative models and combinatorial optimization methods achieve initial success but still struggle with directly modeling discrete chemical structures and often heavily rely on brute-force enumeration. The challenge comes from the discrete and non-differentiable nature of molecule structures. To address this, we propose differentiable scaffolding tree (DST) that utilizes a learned knowledge network to convert discrete chemical structures to locally differentiable ones. DST enables a gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network (GNN). Our empirical studies show the gradient-based molecular optimizations are both effective and sample efficient (in terms of oracle calling number). Furthermore, the learned graph parameters can also provide an explanation that helps domain experts understand the model output. The code repository (including processed data, trained model, demonstration, molecules with the highest property) is available at https://github.com/futianfan/DST.", "pdf": "/pdf/af1f7e5614adea0e8575cc0ff017f48ce21789e3.pdf", "one-sentence_summary": "make the molecular optimization problem differentiable at the structure level", "supplementary_material": "/attachment/97ea77ff88a0fbd25c7dd11ed38efde8cc5e389c.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fu|differentiable_scaffolding_tree_for_molecule_optimization", "_bibtex": "@inproceedings{\nfu2022differentiable,\ntitle={Differentiable Scaffolding Tree for Molecule Optimization},\nauthor={Tianfan Fu and Wenhao Gao and Cao Xiao and Jacob Yasonik and Connor W. Coley and Jimeng Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=w_drCosT76}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "B3Nde6lvab", "original": "K5H68rMlsfe", "number": 1746, "cdate": 1632875538688, "mdate": null, "ddate": null, "tcdate": 1632875538688, "tmdate": 1697934790835, "tddate": null, "forum": "B3Nde6lvab", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise", "authorids": ["~Xingyu_Wang2", "~Sewoong_Oh1", "~Chang-Han_Rhee1"], "authors": ["Xingyu Wang", "Sewoong Oh", "Chang-Han Rhee"], "keywords": ["Stochastic Gradient Descent", "SGD", "Heavy-Tails", "Generalization"], "abstract": "The empirical success of deep learning is often attributed to SGD\u2019s mysterious ability to avoid sharp local minima in the loss landscape, as sharp minima are  known to lead to poor generalization. Recently, empirical evidence of heavy-tailed gradient noise was reported in many deep learning tasks; and it was shown in (Simsekli et al., 2019a;b) that SGD can escape sharp local minima under the presence of such heavy-tailed gradient noise, providing a partial solution to the mystery. In this work, we analyze a popular variant of SGD where gradients are truncated above a fixed threshold. We show that it achieves a stronger notion of avoiding sharp minima: it can effectively eliminate sharp local minima entirely from its training trajectory. We characterize the dynamics of truncated SGD driven by heavy-tailed noises. First, we show that the truncation threshold and width of the attraction field dictate the order of the first exit time from the associated local minimum. Moreover, when the objective function satisfies appropriate structural conditions, we prove that as the learning rate decreases, the dynamics of the heavy-tailed truncated SGD closely resemble those of a continuous-time Markov chain that never visits any sharp minima. Real data experiments on deep learning confirm our theoretical prediction that heavy-tailed SGD with gradient clipping finds a flatter local minima and achieves better generalization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|eliminating_sharp_minima_from_sgd_with_truncated_heavytailed_noise", "pdf": "/pdf/cdd56f4a4d029af79c9c45e1127747b70dee7cfb.pdf", "one-sentence_summary": "Empirical evidence and the first theoretical analysis that show SGD with truncated heavy-tailed gradient noise finds flatter minima and achieves better generalization.", "supplementary_material": "/attachment/ae4cd522622a243461f59a82a126e19e97edc66b.zip", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2102.04297/code)", "_bibtex": "@inproceedings{\nwang2022eliminating,\ntitle={Eliminating Sharp Minima from {SGD} with Truncated Heavy-tailed Noise},\nauthor={Xingyu Wang and Sewoong Oh and Chang-Han Rhee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=B3Nde6lvab}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "uxxFrDwrE7Y", "original": "6sa_wBX9gUT", "number": 1738, "cdate": 1632875538209, "mdate": null, "ddate": null, "tcdate": 1632875538209, "tmdate": 1676330596779, "tddate": null, "forum": "uxxFrDwrE7Y", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System", "authorids": ["~Elahe_Arani1", "~Fahad_Sarfraz1", "~Bahram_Zonooz1"], "authors": ["Elahe Arani", "Fahad Sarfraz", "Bahram Zonooz"], "keywords": ["Continual Learning", "Catastrophic Forgetting", "Complementary Learning Systems Theory", "Experience Replay"], "abstract": "Humans excel at continually learning from an ever-changing environment whereas it remains a challenge for deep neural networks which exhibit catastrophic forgetting. The complementary learning system (CLS) theory suggests that the interplay between rapid instance-based learning and slow structured learning in the brain is crucial for accumulating and retaining knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER) method which maintains short-term and long-term semantic memories that interact with the episodic memory. Our method employs an effective replay mechanism whereby new knowledge is acquired while aligning the decision boundaries with the semantic memories. CLS-ER does not utilize the task boundaries or make any assumption about the distribution of the data which makes it versatile and suited for ``general continual learning''. Our approach achieves state-of-the-art performance on standard benchmarks as well as more realistic general continual learning settings.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "arani|learning_fast_learning_slow_a_general_continual_learning_method_based_on_complementary_learning_system", "pdf": "/pdf/03691b7344e6968cc95744a627d25e3b5e60ca3e.pdf", "one-sentence_summary": "A dual memory experience replay method which aims to mimic the interplay between fast learning and slow learning mechanisms for enabling effective CL in DNNs.", "supplementary_material": "/attachment/f5c77b4325f9e09235df4d250d41fda1f1dd3c8c.zip", "_bibtex": "@inproceedings{\narani2022learning,\ntitle={Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System},\nauthor={Elahe Arani and Fahad Sarfraz and Bahram Zonooz},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=uxxFrDwrE7Y}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "ZaVVVlcdaN", "original": "yRYM9WsFMd", "number": 1735, "cdate": 1632875538007, "mdate": null, "ddate": null, "tcdate": 1632875538007, "tmdate": 1676330597064, "tddate": null, "forum": "ZaVVVlcdaN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning", "authorids": ["~Charlie_Hou1", "~Kiran_Koshy_Thekumparampil1", "~Giulia_Fanti1", "~Sewoong_Oh1"], "authors": ["Charlie Hou", "Kiran Koshy Thekumparampil", "Giulia Fanti", "Sewoong Oh"], "keywords": ["Federated Learning", "Optimization", "Distributed Optimization"], "abstract": "Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg).  Local methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R.  On the other hand, global methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous.  We propose FedChain, an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of R while  leveraging the similarity between clients.  Using FedChain, we instantiate algorithms that improve upon previously known rates in the general convex and PL settings, and are near-optimal (via an algorithm-independent lower bound that we show) for problems that satisfy strong convexity.  Empirical results support this theoretical gain over existing methods. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hou|fedchain_chained_algorithms_for_nearoptimal_communication_cost_in_federated_learning", "pdf": "/pdf/4b6bb8ca9713c12151f31fef5fda64b39e34e734.pdf", "_bibtex": "@inproceedings{\nhou2022fedchain,\ntitle={FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning},\nauthor={Charlie Hou and Kiran Koshy Thekumparampil and Giulia Fanti and Sewoong Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZaVVVlcdaN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "VimqQq-i_Q", "original": "JObk2W7GEx", "number": 1733, "cdate": 1632875537874, "mdate": null, "ddate": null, "tcdate": 1632875537874, "tmdate": 1697934792014, "tddate": null, "forum": "VimqQq-i_Q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "What Do We Mean by Generalization in Federated Learning?", "authorids": ["~Honglin_Yuan1", "~Warren_Richard_Morningstar1", "~Lin_Ning1", "~Karan_Singhal1"], "authors": ["Honglin Yuan", "Warren Richard Morningstar", "Lin Ning", "Karan Singhal"], "keywords": ["Federated Learning", "generalization", "heterogeneity"], "abstract": "Federated learning data is drawn from a distribution of distributions: clients are drawn from a meta-distribution, and their data are drawn from local data distributions. Generalization studies in federated learning should separate performance gaps from unseen client data (out-of-sample gap) from performance gaps from unseen client distributions (participation gap). In this work, we propose a framework for disentangling these performance gaps. Using this framework, we observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in federated learning. We propose a semantic synthesis strategy that enables realistic simulation without naturally partitioned data. Informed by our \ufb01ndings, we call out community suggestions for future federated learning works.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yuan|what_do_we_mean_by_generalization_in_federated_learning", "pdf": "/pdf/52ed8e81d3311fe4d43cf85b2faadfa3fcdaa83a.pdf", "one-sentence_summary": "We propose a framework for better measuring generalization and heterogeneity in federated learning, apply it for extensive empirical evaluation across six tasks, and make a series of recommendations for future FL works.", "supplementary_material": "/attachment/b9e9c96273103043abf82025fc71cad9a6f7e026.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.14216/code)", "_bibtex": "@inproceedings{\nyuan2022what,\ntitle={What Do We Mean by Generalization in Federated Learning?},\nauthor={Honglin Yuan and Warren Richard Morningstar and Lin Ning and Karan Singhal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VimqQq-i_Q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "ibqTBNfJmi", "original": "Pv1d_MZEI6y", "number": 1729, "cdate": 1632875537599, "mdate": null, "ddate": null, "tcdate": 1632875537599, "tmdate": 1676330597622, "tddate": null, "forum": "ibqTBNfJmi", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Frequency-aware SGD for Efficient Embedding Learning with Provable Benefits", "authorids": ["~Yan_Li9", "~Dhruv_Choudhary1", "~Xiaohan_Wei2", "~Baichuan_Yuan1", "bbhushanam@fb.com", "~Tuo_Zhao1", "~Guanghui_Lan1"], "authors": ["Yan Li", "Dhruv Choudhary", "Xiaohan Wei", "Baichuan Yuan", "Bhargav Bhushanam", "Tuo Zhao", "Guanghui Lan"], "keywords": [], "abstract": "Embedding learning has found widespread applications in recommendation systems and natural language modeling, among other domains. To learn quality embeddings efficiently, adaptive learning rate algorithms have demonstrated superior empirical performance over SGD, largely accredited to their token-dependent learning rate. However, the underlying mechanism for the efficiency of token-dependent learning rate remains underexplored. We show that incorporating frequency information of tokens in the embedding learning problems leads to provably efficient algorithms, and demonstrate that common adaptive algorithms implicitly exploit the frequency information to a large extent. Specifically, we propose (Counter-based) Frequency-aware Stochastic Gradient Descent, which applies a frequency-dependent learning rate for each token, and exhibits provable speed-up compared to SGD when the token distribution is imbalanced. Empirically, we show the proposed algorithms are able to improve or match the performance of adaptive algorithms on benchmark recommendation tasks and a large-scale industrial recommendation system,  closing the performance gap between SGD and adaptive algorithms. Our results are the first to show token-dependent learning rate provably improves convergence for non-convex embedding learning problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|frequencyaware_sgd_for_efficient_embedding_learning_with_provable_benefits", "pdf": "/pdf/7ff001fda6de89be1d9e937afb3d64fe33c689b7.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nli2022frequencyaware,\ntitle={Frequency-aware {SGD} for Efficient Embedding Learning with Provable Benefits},\nauthor={Yan Li and Dhruv Choudhary and Xiaohan Wei and Baichuan Yuan and Bhargav Bhushanam and Tuo Zhao and Guanghui Lan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ibqTBNfJmi}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "KeI9E-gsoB", "original": "MFX9SGX_IsV", "number": 1713, "cdate": 1632875536651, "mdate": null, "ddate": null, "tcdate": 1632875536651, "tmdate": 1676330598234, "tddate": null, "forum": "KeI9E-gsoB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Curves for Gaussian Process Regression with Power-Law Priors and Targets", "authorids": ["~Hui_Jin1", "~Pradeep_Kr._Banerjee2", "~Guido_Montufar1"], "authors": ["Hui Jin", "Pradeep Kr. Banerjee", "Guido Montufar"], "keywords": ["Gaussian process regression", "kernel ridge regression", "generalization error", "power law", "neural tangent kernel"], "abstract": "We characterize the power-law asymptotics of learning curves for Gaussian process regression (GPR) under the assumption that the eigenspectrum of the prior and the eigenexpansion coefficients of the target function follow a power law. Under similar assumptions, we leverage the equivalence between GPR and kernel ridge regression (KRR) to show the generalization error of KRR. Infinitely wide neural networks can be related to GPR with respect to the neural network GP kernel and the neural tangent kernel, which in several cases is known to have a power-law spectrum. Hence our methods can be applied to study the generalization error of infinitely wide neural networks. We present toy experiments demonstrating the theory.", "one-sentence_summary": "We derive the power-law decay rate of the generalization error in Gaussian process regression depending on the eigenspectrum of the prior and the target.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jin|learning_curves_for_gaussian_process_regression_with_powerlaw_priors_and_targets", "pdf": "/pdf/3adc0d00292f63c360d4960db93cefcdf6c824b7.pdf", "_bibtex": "@inproceedings{\njin2022learning,\ntitle={Learning Curves for Gaussian Process Regression with Power-Law Priors and Targets},\nauthor={Hui Jin and Pradeep Kr. Banerjee and Guido Montufar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KeI9E-gsoB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "0kPL3xO4R5", "original": "9JgCyxczez2", "number": 1709, "cdate": 1632875536380, "mdate": null, "ddate": null, "tcdate": 1632875536380, "tmdate": 1697934794010, "tddate": null, "forum": "0kPL3xO4R5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fast topological clustering with Wasserstein distance", "authorids": ["~Tananun_Songdechakraiwut1", "~Bryan_M_Krause1", "~Matthew_I_Banks1", "~Kirill_V_Nourski1", "~Barry_D_Van_Veen1"], "authors": ["Tananun Songdechakraiwut", "Bryan M Krause", "Matthew I Banks", "Kirill V Nourski", "Barry D Van Veen"], "keywords": ["Topological data analysis", "cluster analysis", "persistent homology", "Wasserstein distance", "Wasserstein barycenter", "brain networks", "intracranial electrophysiology", "consciousness"], "abstract": "The topological patterns exhibited by many real-world networks motivate the development of topology-based methods for assessing the similarity of networks. However, extracting topological structure is difficult, especially for large and dense networks whose node degrees range over multiple orders of magnitude. In this paper, we propose a novel and computationally practical topological clustering method that clusters complex networks with intricate topology using principled theory from persistent homology and optimal transport. Such networks are aggregated into clusters through a centroid-based clustering strategy based on both their topological and geometric structure, preserving correspondence between nodes in different networks. The notions of topological proximity and centroid are characterized using a novel and efficient approach to computation of the Wasserstein distance and barycenter for persistence barcodes associated with connected components and cycles. The proposed method is demonstrated to be effective using both simulated networks and measured functional brain networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "songdechakraiwut|fast_topological_clustering_with_wasserstein_distance", "pdf": "/pdf/1ca6035f2e30fb149d5edca41b2f47c7dbefcbbf.pdf", "one-sentence_summary": "In this paper, we propose a novel and computationally practical topological clustering method that clusters complex networks with intricate topology using principled theory from persistent homology and optimal transport.", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2112.00101/code)", "_bibtex": "@inproceedings{\nsongdechakraiwut2022fast,\ntitle={Fast topological clustering with Wasserstein distance},\nauthor={Tananun Songdechakraiwut and Bryan M Krause and Matthew I Banks and Kirill V Nourski and Barry D Van Veen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0kPL3xO4R5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "nkaba3ND7B5", "original": "rQZExj0bCnj", "number": 1702, "cdate": 1632875535900, "mdate": null, "ddate": null, "tcdate": 1632875535900, "tmdate": 1697934795327, "tddate": null, "forum": "nkaba3ND7B5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Autonomous Reinforcement Learning: Formalism and Benchmarking", "authorids": ["~Archit_Sharma1", "~Kelvin_Xu2", "~Nikhil_Sardana1", "~Abhishek_Gupta1", "~Karol_Hausman2", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Archit Sharma", "Kelvin Xu", "Nikhil Sardana", "Abhishek Gupta", "Karol Hausman", "Sergey Levine", "Chelsea Finn"], "keywords": ["reinforcement learning", "autonomous", "reset-free reinforcement learning", "continual reinforcement learning"], "abstract": "Reinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual, non-episodic world, whereas common benchmark tasks in RL are episodic, with the environment resetting between trials to provide the agent with multiple attempts. This discrepancy presents a major challenge when we attempt to take RL algorithms developed for episodic simulated environments and run  them on real-world platforms, such as robots. In this paper, we aim to address this discrepancy by laying out a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. We introduce a simulated benchmark EARL based on this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sharma|autonomous_reinforcement_learning_formalism_and_benchmarking", "pdf": "/pdf/f572a28516f88b10b825a32cd24ba9922c1d015e.pdf", "supplementary_material": "/attachment/b29a03fb4c6837df5c14ab66e062b49205ece33a.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2112.09605/code)", "_bibtex": "@inproceedings{\nsharma2022autonomous,\ntitle={Autonomous Reinforcement Learning: Formalism and Benchmarking},\nauthor={Archit Sharma and Kelvin Xu and Nikhil Sardana and Abhishek Gupta and Karol Hausman and Sergey Levine and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nkaba3ND7B5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "EMxu-dzvJk", "original": "kJoE4BIsLcA", "number": 1699, "cdate": 1632875535689, "mdate": null, "ddate": null, "tcdate": 1632875535689, "tmdate": 1676330599210, "tddate": null, "forum": "EMxu-dzvJk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GRAND++: Graph Neural Diffusion with A Source Term", "authorids": ["matthew.thorpe-2@manchester.ac.uk", "~Tan_Minh_Nguyen1", "~Hedi_Xia2", "~Thomas_Strohmer1", "~Andrea_Bertozzi1", "~Stanley_Osher1", "~Bao_Wang1"], "authors": ["Matthew Thorpe", "Tan Minh Nguyen", "Hedi Xia", "Thomas Strohmer", "Andrea Bertozzi", "Stanley Osher", "Bao Wang"], "keywords": ["graph deep learning", "low-labeling rates", "diffusion on graphs", "random walk"], "abstract": "We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i.e., low-labeling rate. GRAND++ is a class of continuous-depth graph deep learning architectures whose theoretical underpinning is the diffusion process on graphs with a source term. The source term guarantees two interesting theoretical properties of GRAND++: (i) the representation of graph nodes, under the dynamics of GRAND++, will not converge to a constant vector over all nodes even as the time goes to infinity, which mitigates the over-smoothing issue of graph neural networks and enables graph learning in very deep architectures. (ii) GRAND++ can provide accurate classification even when the model is trained with a very limited number of labeled training data. We experimentally verify the above two advantages on various graph deep learning benchmark tasks, showing a significant improvement over many existing graph neural networks.", "one-sentence_summary": "We propose GRAND++ for deep graph learning with limited labeled training data", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "thorpe|grand_graph_neural_diffusion_with_a_source_term", "pdf": "/pdf/d77569d8b7f28f14fd7a71bc8f199ed42934a3d4.pdf", "supplementary_material": "/attachment/617e1ec32444e0e31f6a46f16e5a6deca155f535.zip", "_bibtex": "@inproceedings{\nthorpe2022grand,\ntitle={{GRAND}++: Graph Neural Diffusion with A Source Term},\nauthor={Matthew Thorpe and Tan Minh Nguyen and Hedi Xia and Thomas Strohmer and Andrea Bertozzi and Stanley Osher and Bao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EMxu-dzvJk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "ZDaSIkWT-AP", "original": "mnQlPXMTiMc", "number": 1698, "cdate": 1632875535618, "mdate": null, "ddate": null, "tcdate": 1632875535618, "tmdate": 1676330599375, "tddate": null, "forum": "ZDaSIkWT-AP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Case-based reasoning for better generalization in textual reinforcement learning", "authorids": ["~Mattia_Atzeni1", "~Shehzaad_Zuzar_Dhuliawala3", "~Keerthiram_Murugesan1", "~Mrinmaya_Sachan3"], "authors": ["Mattia Atzeni", "Shehzaad Zuzar Dhuliawala", "Keerthiram Murugesan", "Mrinmaya Sachan"], "keywords": [], "abstract": "Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efficiently, especially under distributional shifts. In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The case-based reasoner collects instances of positive experiences from the agent's interaction with the world and later reuses the collected experiences to act efficiently. The method can be used in conjunction with any existing on-policy neural agent introduced in the literature for TBGs. Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization and achieves new state-of-the-art results on widely used environments.", "pdf": "/pdf/426ad0e0a618d416dbdc8a2fbaa7f29661e1f920.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "atzeni|casebased_reasoning_for_better_generalization_in_textual_reinforcement_learning", "_bibtex": "@inproceedings{\natzeni2022casebased,\ntitle={Case-based reasoning for better generalization in textual reinforcement learning},\nauthor={Mattia Atzeni and Shehzaad Zuzar Dhuliawala and Keerthiram Murugesan and MRINMAYA SACHAN},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZDaSIkWT-AP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "B0oHOwT5ENL", "original": "pcfWKf8P-Ta", "number": 1690, "cdate": 1632875535029, "mdate": null, "ddate": null, "tcdate": 1632875535029, "tmdate": 1676330599617, "tddate": null, "forum": "B0oHOwT5ENL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Deep Equilibrium Solvers", "authorids": ["~Shaojie_Bai1", "~Vladlen_Koltun1", "~J_Zico_Kolter1"], "authors": ["Shaojie Bai", "Vladlen Koltun", "J Zico Kolter"], "keywords": ["Deep learning", "Implicit models", "Deep equilibrium models"], "abstract": "A deep equilibrium (DEQ) model abandons traditional depth by solving for the fixed point of a single nonlinear layer $f_\\theta$. This structure enables decoupling the internal structure of the layer (which controls representational capacity) from how the fixed point is actually computed (which impacts inference-time efficiency), which is usually via classic techniques such as Broyden's method or Anderson acceleration.  In this paper, we show that one can exploit such decoupling and substantially enhance this fixed point computation using a custom neural solver. Specifically, our solver uses a parameterized network to both guess an initial value of the optimization and perform iterative updates, in a method that generalizes a learnable form of Anderson acceleration and can be trained end-to-end in an unsupervised manner. Such a solution is particularly well suited to the implicit model setting, because inference in these models requires repeatedly solving for a fixed point of the same nonlinear layer for different inputs, a task at which our network excels. Our experiments show that these neural equilibrium solvers are fast to train (only taking an extra 0.9-1.1% over the original DEQ's training time), require few additional parameters (1-3% of the original model size), yet lead to a $2\\times$ speedup in DEQ network inference without any degradation in accuracy across numerous domains and tasks.", "one-sentence_summary": "A custom and lightweight neural solver for deep equilibrium models significantly improves their efficiency with minimal training.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bai|neural_deep_equilibrium_solvers", "pdf": "/pdf/88bf23fbc8248929fab4670e786459e844cce30c.pdf", "supplementary_material": "/attachment/fbaca583f2e35426db627e36a8de0d15aee37d4a.zip", "data": "", "_bibtex": "@inproceedings{\nbai2022neural,\ntitle={Neural Deep Equilibrium Solvers},\nauthor={Shaojie Bai and Vladlen Koltun and J Zico Kolter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=B0oHOwT5ENL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "wMpS-Z_AI_E", "original": "kaCaRg6dfJm", "number": 1676, "cdate": 1632875534126, "mdate": null, "ddate": null, "tcdate": 1632875534126, "tmdate": 1676330600091, "tddate": null, "forum": "wMpS-Z_AI_E", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features", "authorids": ["~Zhenmei_Shi1", "~Junyi_Wei1", "~Yingyu_Liang1"], "authors": ["Zhenmei Shi", "Junyi Wei", "Yingyu Liang"], "keywords": ["neural networks", "feature learning", "provable advantage", "theoretical analysis"], "abstract": "An important characteristic of neural networks is their ability to learn representations of the input data with effective features for prediction, which is believed to be a key factor to their superior empirical performance. To better understand the source and benefit of feature learning in neural networks, we consider learning problems motivated by practical data, where the labels are determined by a set of class relevant patterns and the inputs are generated from these along with some background patterns. We prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, the structure of the input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no polynomial algorithm in the Statistical Query model can learn even weakly. These results provide theoretical evidence showing that feature learning in neural networks depends strongly on the input structure and leads to the superior performance. Our preliminary experimental results on synthetic and real data also provide positive support. ", "one-sentence_summary": "Theoretical analysis of feature learning in neural networks on practically motivated learning problems, showing it strongly depends on the structure of the input distribution and leads to the advantage over fixed feature methods like kernels.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shi|a_theoretical_analysis_on_feature_learning_in_neural_networks_emergence_from_inputs_and_advantage_over_fixed_features", "pdf": "/pdf/9c506df8cb3b84afcbc08295792409a5a61c3f40.pdf", "supplementary_material": "/attachment/7b6dda91b863c9135c86ffb20ad33460c9a47fca.zip", "data": "", "_bibtex": "@inproceedings{\nshi2022a,\ntitle={A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features},\nauthor={Zhenmei Shi and Junyi Wei and Yingyu Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wMpS-Z_AI_E}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "6IYp-35L-xJ", "original": "xEljzitNJd", "number": 1672, "cdate": 1632875533910, "mdate": null, "ddate": null, "tcdate": 1632875533910, "tmdate": 1676330600357, "tddate": null, "forum": "6IYp-35L-xJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals", "authorids": ["~C\u00e9dric_Rommel1", "~Thomas_Moreau2", "~Joseph_Paillard1", "~Alexandre_Gramfort1"], "authors": ["C\u00e9dric Rommel", "Thomas Moreau", "Joseph Paillard", "Alexandre Gramfort"], "keywords": ["Neuroscience", "EEG", "Sleep staging", "Automatic data augmentation"], "abstract": "Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding adequate augmentation methods and parameters for a given pipeline is however rapidly cumbersome. In particular, while intuition can guide this decision for images, the design and choice of augmentation policies remains unclear for more complex types of data, such as neuroscience signals. Besides, class-dependent augmentation strategies have been surprisingly unexplored in the literature, although it is quite intuitive: changing the color of a car image does not change the object class to be predicted, but doing the same to the picture of an orange does. This paper investigates gradient-based automatic data augmentation algorithms  amenable to class-wise policies with exponentially larger search spaces. Motivated by supervised learning applications using EEG signals for which good augmentation policies are mostly unknown, we propose a new differentiable relaxation of the problem. In the class-agnostic setting, results show that our new relaxation leads to optimal performance with faster training than competing gradient-based methods, while also outperforming gradient-free methods in the class-wise setting. This work proposes also novel differentiable augmentation operations relevant for sleep stage classification.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rommel|cadda_classwise_automatic_differentiable_data_augmentation_for_eeg_signals", "pdf": "/pdf/b32b128de32627fc518e6113f8cfb91bed89bc23.pdf", "one-sentence_summary": "We propose a new gradient-based automatic data augmentation technique where samples are transformed based on their labels, and demonstrate its relevance on EEG sleep staging data.", "supplementary_material": "/attachment/e693f48db22ca4fa82f893bfea1e6f1f0ca42b51.zip", "_bibtex": "@inproceedings{\nrommel2022cadda,\ntitle={{CADDA}: Class-wise Automatic Differentiable Data Augmentation for {EEG} Signals},\nauthor={C{\\'e}dric Rommel and Thomas Moreau and Joseph Paillard and Alexandre Gramfort},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6IYp-35L-xJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "cOtBRgsf2fO", "original": "wIoDIyHLxvP", "number": 1670, "cdate": 1632875533771, "mdate": null, "ddate": null, "tcdate": 1632875533771, "tmdate": 1676330600472, "tddate": null, "forum": "cOtBRgsf2fO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Label Leakage and Protection in Two-party Split Learning", "authorids": ["~Oscar_Li1", "~Jiankai_Sun5", "~Xin_Yang3", "~Weihao_Gao1", "~Hongyi_Zhang1", "~Junyuan_Xie1", "~Virginia_Smith1", "~Chong_Wang8"], "authors": ["Oscar Li", "Jiankai Sun", "Xin Yang", "Weihao Gao", "Hongyi Zhang", "Junyuan Xie", "Virginia Smith", "Chong Wang"], "keywords": ["Split Learning", "label leakage", "privacy", "privacy protection"], "abstract": "Two-party split learning is a popular technique for learning a model across feature-partitioned data. In this work, we explore whether it is possible for one party to steal the private label information from the other party during split training, and whether there are methods that can protect against such attacks. Specifically, we first formulate a realistic threat model and propose a privacy loss metric to quantify label leakage in split learning. We then show that there exist two simple yet effective methods within the threat model that can allow one party to accurately recover private ground-truth labels owned by the other party. To combat these attacks, we propose several random perturbation techniques, including $\\texttt{Marvell}$, an approach that strategically finds the structure of the noise perturbation by minimizing the amount of label leakage (measured through our quantification metric) of a worst-case adversary. We empirically demonstrate the effectiveness of our protection techniques against the identified attacks, and show that $\\texttt{Marvell}$ in particular has improved privacy-utility tradeoffs relative to baseline approaches.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|label_leakage_and_protection_in_twoparty_split_learning", "pdf": "/pdf/d1b9d2d34049b477cd71dab64cd68252285e2090.pdf", "one-sentence_summary": "We identify the label leakage threat in two-party split learning with concrete threat examples and propose random perturbation methods to protect against such threats.", "supplementary_material": "/attachment/7c84c98e0da2c15a881b6651e4554ff368993d66.zip", "code": "", "_bibtex": "@inproceedings{\nli2022label,\ntitle={Label Leakage and Protection in Two-party Split Learning},\nauthor={Oscar Li and Jiankai Sun and Xin Yang and Weihao Gao and Hongyi Zhang and Junyuan Xie and Virginia Smith and Chong Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cOtBRgsf2fO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "RShaMexjc-x", "original": "U-7C11wMvhy", "number": 1667, "cdate": 1632875533559, "mdate": null, "ddate": null, "tcdate": 1632875533559, "tmdate": 1676330600661, "tddate": null, "forum": "RShaMexjc-x", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Semi-relaxed Gromov-Wasserstein divergence and applications on graphs", "authorids": ["~C\u00e9dric_Vincent-Cuaz1", "~R\u00e9mi_Flamary1", "~Marco_Corneli1", "~Titouan_Vayer1", "~Nicolas_Courty1"], "authors": ["C\u00e9dric Vincent-Cuaz", "R\u00e9mi Flamary", "Marco Corneli", "Titouan Vayer", "Nicolas Courty"], "keywords": ["Optimal Transport", "Graph Learning"], "abstract": "Comparing structured objects such as graphs is a fundamental operation\ninvolved in many learning tasks. To this end, the Gromov-Wasserstein (GW)\ndistance, based on Optimal Transport (OT), has proven to be successful in\nhandling the specific nature of the associated objects. More specifically,\nthrough the nodes connectivity relations, GW operates on graphs, seen as\nprobability measures over specific spaces. At the core of OT is the idea of\nconservation of mass, which imposes a coupling between all the nodes from\nthe two considered graphs. We argue in this paper that this property can be\ndetrimental for tasks such as graph dictionary or partition learning, and we\nrelax it by proposing a new semi-relaxed Gromov-Wasserstein divergence.\nAside from immediate computational benefits, we discuss its properties, and\nshow that it can lead to an efficient graph dictionary learning algorithm.\nWe empirically demonstrate its relevance for complex tasks on graphs such as\npartitioning, clustering and completion.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "vincentcuaz|semirelaxed_gromovwasserstein_divergence_and_applications_on_graphs", "pdf": "/pdf/fe28f3e616fabe8ac2ea06baba193a597fc572a7.pdf", "one-sentence_summary": "A new transport based divergence between structured data induced by the relaxation of a mass constraint of the Gromov-Wasserstein problem, leading to new SOTA performances for unsupervised ML applications on graphs.", "supplementary_material": "/attachment/25b39f1f8a2bbad10abaaad9d7af0f6d72bf4248.zip", "data": "", "_bibtex": "@inproceedings{\nvincent-cuaz2022semirelaxed,\ntitle={Semi-relaxed Gromov-Wasserstein divergence and applications on graphs},\nauthor={C{\\'e}dric Vincent-Cuaz and R{\\'e}mi Flamary and Marco Corneli and Titouan Vayer and Nicolas Courty},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RShaMexjc-x}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "WQc075jmBmf", "original": "47pEhkf_cWy", "number": 1666, "cdate": 1632875533491, "mdate": null, "ddate": null, "tcdate": 1632875533491, "tmdate": 1676330600793, "tddate": null, "forum": "WQc075jmBmf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation", "authorids": ["~Pardis_Pashakhanloo1", "~Aaditya_Naik1", "~Yuepeng_Wang1", "~Hanjun_Dai1", "~Petros_Maniatis1", "~Mayur_Naik1"], "authors": ["Pardis Pashakhanloo", "Aaditya Naik", "Yuepeng Wang", "Hanjun Dai", "Petros Maniatis", "Mayur Naik"], "keywords": ["relational database", "code representation", "knowledge graph reasoning", "program understanding"], "abstract": "Designing a suitable representation for code-reasoning tasks is challenging in aspects such as the kinds of program information to model, how to combine them, and how much context to consider. We propose CodeTrek, a deep learning approach that addresses these challenges by representing codebases as databases that conform to rich relational schemas. The relational representation not only allows CodeTrek to uniformly represent diverse kinds of program information, but also to leverage program-analysis queries to derive new semantic relations, which can be readily incorporated without further architectural engineering. CodeTrek embeds this relational representation using a set of walks that can traverse different relations in an unconstrained fashion, and incorporates all relevant attributes along the way. We evaluate CodeTrek on four diverse and challenging Python tasks: variable misuse, exception prediction, unused definition, and variable shadowing. CodeTrek achieves an accuracy of 91%, 63%, 98%, and 94% on these tasks respectively, and outperforms state-of-the-art neural models by 2-19% points.", "one-sentence_summary": "We present a relational database representation and corresponding neural module for source code and show its potential on program understanding tasks", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pashakhanloo|codetrek_flexible_modeling_of_code_using_an_extensible_relational_representation", "pdf": "/pdf/a38e66149abc580fe950eea8b0dfd0e24a7ae1bf.pdf", "supplementary_material": "/attachment/5cf08b9b3e094fe4a95688d7975f8bea63508637.zip", "_bibtex": "@inproceedings{\npashakhanloo2022codetrek,\ntitle={CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation},\nauthor={Pardis Pashakhanloo and Aaditya Naik and Yuepeng Wang and Hanjun Dai and Petros Maniatis and Mayur Naik},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WQc075jmBmf}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "TZeArecH2Nf", "original": "0gRVQQb8pVD", "number": 1664, "cdate": 1632875533349, "mdate": null, "ddate": null, "tcdate": 1632875533349, "tmdate": 1676330600892, "tddate": null, "forum": "TZeArecH2Nf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bridging Recommendation and Marketing via Recurrent Intensity Modeling", "authorids": ["~Yifei_Ma1", "~Ge_Liu2", "~Anoop_Deoras1"], "authors": ["Yifei Ma", "Ge Liu", "Anoop Deoras"], "keywords": ["Recommender systems", "marketing", "push notifications", "temporal point processes", "sequence models"], "abstract": "This paper studies some under-explored connections between personalized recommendation and marketing systems. Obviously, these two systems are different, in two main ways. Firstly, personalized item-recommendation (ItemRec) is user-centric, whereas marketing recommends the best user-state segments (UserRec) on behalf of its item providers. (We treat different temporal states of the same user as separate marketing opportunities.) To overcome this difference, we realize a novel connection to Marked-Temporal Point Processes (MTPPs), where we view both problems as different projections from a unified temporal intensity model for all user-item pairs. Correspondingly, we derive Recurrent Intensity Models (RIMs) to extend from recurrent ItemRec models with minimal changes. The second difference between recommendation and marketing is in the temporal domains where they operate. While recommendation demands immediate responses in real-time, marketing campaigns are often long-term, setting goals to cover a given percentage of all opportunities for a given item in a given period of time. We formulate both considerations into a constrained optimization problem we call online match (OnlnMtch) and derive a solution we call Dual algorithm. Simply put, Dual modifies the real-time ItemRec scores such that the marketing constraints can be met with least compromises in user-centric utilities. Finally, our connections between recommendation and marketing may lead to novel applications. We run experiments where we use marketing as an alternative to cold-start item exploration, by setting a minimal-exposure constraint for every item in the audience base. Our experiments are available at \\url{https://github.com/awslabs/recurrent-intensity-model-experiments}", "pdf": "/pdf/cbd0578c075213db4bf240d2f69af59b6c47a4af.pdf", "one-sentence_summary": "We repurpose an item-recommender system to recommend users for item providers for the purpose of content promotion and diversity.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ma|bridging_recommendation_and_marketing_via_recurrent_intensity_modeling", "supplementary_material": "/attachment/74abd4565c71d116b82834c84c70b72962ac9834.zip", "_bibtex": "@inproceedings{\nma2022bridging,\ntitle={Bridging Recommendation and Marketing via Recurrent Intensity Modeling},\nauthor={Yifei Ma and Ge Liu and Anoop Deoras},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TZeArecH2Nf}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "VGnOJhd5Q1q", "original": "xlPzW4oQChf", "number": 1662, "cdate": 1632875533212, "mdate": null, "ddate": null, "tcdate": 1632875533212, "tmdate": 1676330601045, "tddate": null, "forum": "VGnOJhd5Q1q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sparse Attention with Learning to Hash", "authorids": ["~Zhiqing_Sun1", "~Yiming_Yang1", "~Shinjae_Yoo1"], "authors": ["Zhiqing Sun", "Yiming Yang", "Shinjae Yoo"], "keywords": ["Sparse Attention", "Transformer", "Learning-to-Hash", "Natural Language Processing"], "abstract": "Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learning-to-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sun|sparse_attention_with_learning_to_hash", "pdf": "/pdf/a9205753097eb2790a365828e3e20af6261bf8b1.pdf", "one-sentence_summary": "We propose a new strategy for sparse attention, namely LHA (Learning-to-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively.", "data": "", "_bibtex": "@inproceedings{\nsun2022sparse,\ntitle={Sparse Attention with Learning to Hash},\nauthor={Zhiqing Sun and Yiming Yang and Shinjae Yoo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VGnOJhd5Q1q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "dQ7Cy_ndl1s", "original": "FxnfOBWtJmn", "number": 1649, "cdate": 1632875532320, "mdate": null, "ddate": null, "tcdate": 1632875532320, "tmdate": 1676330601386, "tddate": null, "forum": "dQ7Cy_ndl1s", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Controlling the Complexity and Lipschitz Constant improves Polynomial Nets", "authorids": ["~Zhenyu_Zhu1", "~Fabian_Latorre1", "~Grigorios_Chrysos1", "~Volkan_Cevher1"], "authors": ["Zhenyu Zhu", "Fabian Latorre", "Grigorios Chrysos", "Volkan Cevher"], "keywords": ["Polynomial Nets", "Rademacher Complexity", "Lipschitz constant", "Coupled CP decomposition"], "abstract": "While the class of Polynomial Nets demonstrates comparable performance to neural networks (NN), it currently has neither theoretical generalization characterization nor robustness guarantees. To this end, we derive new complexity bounds for the set of Coupled CP-Decomposition (CCP) and Nested Coupled CP-decomposition (NCP) models of Polynomial Nets in terms of the $\\ell_\\infty$-operator-norm and the $\\ell_2$-operator norm. In addition, we derive bounds on the Lipschitz constant for both models to establish a theoretical certificate for their robustness. The theoretical results enable us to propose a principled regularization scheme that we also evaluate experimentally and show that it improves the accuracy as well as the robustness of the models to adversarial perturbations. We showcase how this regularization can be combined with adversarial training, resulting in further improvements.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|controlling_the_complexity_and_lipschitz_constant_improves_polynomial_nets", "pdf": "/pdf/47cd3800add7f5f04f67fe02056b0ba28b0f2af0.pdf", "one-sentence_summary": "We provide sample complexity results and bounds on the Lipschitz constant of polynomial networks, which we use to construct a regularization scheme that improves the robustness against adversarial noise.", "data": "", "_bibtex": "@inproceedings{\nzhu2022controlling,\ntitle={Controlling the Complexity and Lipschitz Constant improves Polynomial Nets},\nauthor={Zhenyu Zhu and Fabian Latorre and Grigorios Chrysos and Volkan Cevher},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dQ7Cy_ndl1s}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "Ug-bgjgSlKV", "original": "vcE-7oo8mOv", "number": 1643, "cdate": 1632875531906, "mdate": null, "ddate": null, "tcdate": 1632875531906, "tmdate": 1697934799444, "tddate": null, "forum": "Ug-bgjgSlKV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Finding an Unsupervised Image Segmenter in each of your Deep Generative Models", "authorids": ["~Luke_Melas-Kyriazi1", "~Christian_Rupprecht1", "~Iro_Laina1", "~Andrea_Vedaldi1"], "authors": ["Luke Melas-Kyriazi", "Christian Rupprecht", "Iro Laina", "Andrea Vedaldi"], "keywords": ["unsupervised", "generative", "deep learning", "segmentation", "object segmentation"], "abstract": "Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs. In this paper, we develop an automatic procedure for finding directions that lead to foreground-background image separation, and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic, producing strong segmentation results with a wide range of different GAN architectures. Furthermore, by leveraging GANs pretrained on large datasets such as ImageNet, we are able to segment images from a range of domains without further training or finetuning. Evaluating our method on image segmentation benchmarks, we compare favorably to prior work while using neither human supervision nor access to the training data. Broadly, our results demonstrate that automatically extracting foreground-background structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision.", "one-sentence_summary": "We propose an entirely unsupervised method for foreground-background image segmentation based on automatically finding a direction in the latent space of a deep generative model.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "melaskyriazi|finding_an_unsupervised_image_segmenter_in_each_of_your_deep_generative_models", "pdf": "/pdf/f19aff475bf4c294325c50324fea58c92aaa4cb3.pdf", "supplementary_material": "/attachment/6fd52cbfdf689282af64c262e039f69c270fc1d3.zip", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2105.08127/code)", "_bibtex": "@inproceedings{\nmelas-kyriazi2022finding,\ntitle={Finding an Unsupervised Image Segmenter in each of your Deep Generative Models},\nauthor={Luke Melas-Kyriazi and Christian Rupprecht and Iro Laina and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Ug-bgjgSlKV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "vaRCHVj0uGI", "original": "Mgoj1l70TB", "number": 1636, "cdate": 1632875531417, "mdate": null, "ddate": null, "tcdate": 1632875531417, "tmdate": 1676330602231, "tddate": null, "forum": "vaRCHVj0uGI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Solving Inverse Problems in Medical Imaging with Score-Based Generative Models", "authorids": ["~Yang_Song1", "~Liyue_Shen1", "~Lei_Xing1", "~Stefano_Ermon1"], "authors": ["Yang Song", "Liyue Shen", "Lei Xing", "Stefano Ermon"], "keywords": ["score-based generative modeling", "inverse problems", "sparse-view CT", "undersampled MRI", "metal artifact removal", "diffusion"], "abstract": "Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "song|solving_inverse_problems_in_medical_imaging_with_scorebased_generative_models", "pdf": "/pdf/1f9104dcb6aabcde00c31c083eb291b3d07e5ed8.pdf", "_bibtex": "@inproceedings{\nsong2022solving,\ntitle={Solving Inverse Problems in Medical Imaging with Score-Based Generative Models},\nauthor={Yang Song and Liyue Shen and Lei Xing and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vaRCHVj0uGI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "L7wzpQttNO", "original": "JPM3YuJSrK1", "number": 1626, "cdate": 1632875530719, "mdate": null, "ddate": null, "tcdate": 1632875530719, "tmdate": 1697934801900, "tddate": null, "forum": "L7wzpQttNO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis", "authorids": ["~Max_W._Y._Lam1", "~Jun_Wang21", "~Dan_Su3", "~Dong_Yu2"], "authors": ["Max W. Y. Lam", "Jun Wang", "Dan Su", "Dong Yu"], "keywords": ["Speech Synthesis", "Vocoder", "Generative Model", "Diffusion Model"], "abstract": "Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling.\nOur experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm.", "pdf": "/pdf/8ac09c0c92b7fc2905b7bbd470f4a2a4607be605.pdf", "one-sentence_summary": "In this paper, we propose a novel bilateral denoising diffusion model (BDDM), which takes significantly fewer sampling steps than the SOTA diffusion-based vocoder to generate high-quality audio samples.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lam|bddm_bilateral_denoising_diffusion_models_for_fast_and_highquality_speech_synthesis", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.13508/code)", "_bibtex": "@inproceedings{\nlam2022bddm,\ntitle={{BDDM}: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis},\nauthor={Max W. Y. Lam and Jun Wang and Dan Su and Dong Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=L7wzpQttNO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "IvepFxYRDG", "original": "dplgE4AUoE2", "number": 1619, "cdate": 1632875530306, "mdate": null, "ddate": null, "tcdate": 1632875530306, "tmdate": 1676330603411, "tddate": null, "forum": "IvepFxYRDG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sample Efficient Stochastic Policy Extragradient Algorithm for Zero-Sum Markov Game", "authorids": ["~Ziyi_Chen2", "~Shaocong_Ma1", "~Yi_Zhou2"], "authors": ["Ziyi Chen", "Shaocong Ma", "Yi Zhou"], "keywords": ["Two-player Zero-sum Markov game", "Entropy regularization", "Policy extragradient", "Nash equilibrium", "Sample complexity"], "abstract": "Two-player zero-sum Markov game is a fundamental problem in reinforcement learning and game theory. Although many algorithms have been proposed for solving zero-sum Markov games in the existing literature, many of them either require a full knowledge of the environment or are not sample-efficient. In this paper, we develop a fully decentralized and sample-efficient stochastic policy extragradient algorithm for solving tabular zero-sum Markov games. In particular, our algorithm utilizes multiple stochastic estimators to accurately estimate the value functions involved in the stochastic updates, and leverages entropy regularization to accelerate the convergence. Specifically, with a proper entropy-regularization parameter, we prove that the stochastic policy extragradient algorithm has a sample complexity of the order $\\widetilde{\\mathcal{O}}(\\frac{A_{\\max}}{\\mu_{\\text{min}}\\epsilon^{5.5}(1-\\gamma)^{13.5}})$ for finding a solution that achieves $\\epsilon$-Nash equilibrium duality gap, where $A_{\\max}$ is the maximum number of actions between the players, $\\mu_{\\min}$ is the lower bound of state stationary distribution, and $\\gamma$ is the discount factor. Such a sample complexity result substantially improves the state-of-the-art complexity result. ", "one-sentence_summary": "This paper proposes a fully decentralized, model-free, provably convergent, sample efficient stochastic policy extragradient algorithm with symmetric and private policy updates", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|sample_efficient_stochastic_policy_extragradient_algorithm_for_zerosum_markov_game", "pdf": "/pdf/a741fb9d3540a4bbeb6e7a7d823e6777a7b60fa5.pdf", "_bibtex": "@inproceedings{\nchen2022sample,\ntitle={Sample Efficient Stochastic Policy Extragradient Algorithm for Zero-Sum Markov Game},\nauthor={Ziyi Chen and Shaocong Ma and Yi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=IvepFxYRDG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "3wNcr5nq56", "original": "argIqsZ2WzX", "number": 1610, "cdate": 1632875529749, "mdate": null, "ddate": null, "tcdate": 1632875529749, "tmdate": 1676330603690, "tddate": null, "forum": "3wNcr5nq56", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Uncanny Similarity of Recurrence and Depth", "authorids": ["~Avi_Schwarzschild1", "~Arjun_Gupta2", "~Amin_Ghiasi1", "~Micah_Goldblum1", "~Tom_Goldstein1"], "authors": ["Avi Schwarzschild", "Arjun Gupta", "Amin Ghiasi", "Micah Goldblum", "Tom Goldstein"], "keywords": ["Deep learning", "recurrent networks", "depth"], "abstract": "It is widely believed that deep neural networks contain layer specialization, wherein networks extract hierarchical features representing edges and patterns in shallow layers and complete objects in deeper layers. Unlike common feed-forward models that have distinct filters at each layer, recurrent networks reuse the same parameters at various depths. In this work, we observe that recurrent models exhibit the same hierarchical behaviors and the same performance benefits as depth despite reusing the same filters at every recurrence. By training models of various feed-forward and recurrent architectures on several datasets for image classification as well as maze solving, we show that recurrent networks have the ability to closely emulate the behavior of non-recurrent deep models, often doing so with far fewer parameters.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "schwarzschild|the_uncanny_similarity_of_recurrence_and_depth", "pdf": "/pdf/c5dbd4bef9cb2e2f2c7e32f017e666939851c682.pdf", "one-sentence_summary": "We show quantitatively and qualitatively that recurrent models have the same behaviors as feed-forward networks despite reusing parameters at each recurrence.", "supplementary_material": "/attachment/0bbe14b33ac1c42b6c5e40aa0c3cee1208e971a0.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nschwarzschild2022the,\ntitle={The Uncanny Similarity of Recurrence and Depth},\nauthor={Avi Schwarzschild and Arjun Gupta and Amin Ghiasi and Micah Goldblum and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3wNcr5nq56}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "l8It-0lE5e7", "original": "plB8k7OsPsD", "number": 1607, "cdate": 1632875529537, "mdate": null, "ddate": null, "tcdate": 1632875529537, "tmdate": 1676330603887, "tddate": null, "forum": "l8It-0lE5e7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Implicit Bias of Adversarial Training for Deep Neural Networks", "authorids": ["~Bochen_Lv1", "~Zhanxing_Zhu1"], "authors": ["Bochen Lv", "Zhanxing Zhu"], "keywords": ["adversarial training", "adversarial examples"], "abstract": "We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear networks adversarially trained by gradient descent on a linearly separable dataset, we prove that the direction of the product of weight matrices converges to the direction of the max-margin solution of the original dataset. Furthermore, we generalize this result to the case of adversarial training for non-linear homogeneous deep neural networks without the linear separability of the dataset. We show that, when the neural network is adversarially trained with  $\\ell_2$ or $\\ell_{\\infty}$ FGSM, FGM and PGD perturbations, the direction of the limit point of normalized parameters of the network along the trajectory of the gradient flow converges to a KKT point of a constrained optimization problem that aims to maximize the margin for adversarial examples. Our results theoretically justify the longstanding conjecture that adversarial training modifies the decision boundary by utilizing adversarial examples to improve robustness, and potentially provides insights for designing new robust training strategies.", "one-sentence_summary": "We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without explicit regularization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lv|implicit_bias_of_adversarial_training_for_deep_neural_networks", "pdf": "/pdf/334522a6b3b9a814fb10fe5d7edc0607f92383ff.pdf", "_bibtex": "@inproceedings{\nlv2022implicit,\ntitle={Implicit Bias of Adversarial Training for Deep Neural Networks},\nauthor={Bochen Lv and Zhanxing Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=l8It-0lE5e7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "_SJ-_yyes8", "original": "UNMHnVQpUic", "number": 1597, "cdate": 1632875528899, "mdate": null, "ddate": null, "tcdate": 1632875528899, "tmdate": 1676330604348, "tddate": null, "forum": "_SJ-_yyes8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning", "authorids": ["~Denis_Yarats1", "~Rob_Fergus1", "~Alessandro_Lazaric2", "~Lerrel_Pinto1"], "authors": ["Denis Yarats", "Rob Fergus", "Alessandro Lazaric", "Lerrel Pinto"], "keywords": ["Image-based RL", "Data augmentation in RL", "Continuous Control"], "abstract": "We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel observations, previously unattained by model-free RL. DrQ-v2  is conceptually simple, easy to implement, and provides significantly better computational footprint compared to prior work, with the majority of tasks taking just 8 hours to train on a single GPU. Finally, we publicly release DrQ-v2 's implementation to  provide RL practitioners with a strong and computationally efficient baseline.", "one-sentence_summary": "We proposed a model-free off-policy algorithm for image-based continuous control that significantly outperforms previous methods both in sample and time complexity.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yarats|mastering_visual_continuous_control_improved_dataaugmented_reinforcement_learning", "pdf": "/pdf/41fe3ddc3b234237036d092d185d57e0ad50b43c.pdf", "supplementary_material": "/attachment/af7d3bb82221879ccdfe8f02e60cc5718d4dcce4.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nyarats2022mastering,\ntitle={Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning},\nauthor={Denis Yarats and Rob Fergus and Alessandro Lazaric and Lerrel Pinto},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_SJ-_yyes8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "MMAeCXIa89", "original": "Oi2M6gBlwL", "number": 1595, "cdate": 1632875528760, "mdate": null, "ddate": null, "tcdate": 1632875528760, "tmdate": 1676330604594, "tddate": null, "forum": "MMAeCXIa89", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "$\\pi$BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization", "authorids": ["~Carl_Hvarfner1", "~Danny_Stoll1", "~Artur_Souza1", "~Marius_Lindauer1", "~Frank_Hutter1", "~Luigi_Nardi1"], "authors": ["Carl Hvarfner", "Danny Stoll", "Artur Souza", "Marius Lindauer", "Frank Hutter", "Luigi Nardi"], "keywords": ["Bayesian Optimization", "Hyperparameter Optimization", "Meta-Learning"], "abstract": "Bayesian optimization (BO) has become an established framework and popular tool for hyperparameter optimization (HPO) of machine learning (ML) algorithms. While known for its sample-efficiency, vanilla BO can not utilize readily available prior beliefs the practitioner has on the potential location of the optimum.  Thus, BO disregards a valuable source of information, reducing its appeal to ML practitioners. To address this issue, we propose $\\pi$BO, an acquisition function generalization which incorporates prior beliefs about the location of the optimum in the form of a probability distribution, provided by the user. In contrast to previous approaches, $\\pi$BO is conceptually simple and can easily be integrated with existing libraries and many acquisition functions. We provide regret bounds when $\\pi$BO is applied to the common Expected Improvement acquisition function and prove convergence at regular rates independently of the prior. Further, our experiments show that $\\pi$BO outperforms competing approaches across a wide suite of benchmarks and prior characteristics. We also demonstrate that $\\pi$BO improves on the state-of-the-art performance for a popular deep learning task, with a $12.5\\times$ time-to-accuracy speedup over prominent BO approaches.", "pdf": "/pdf/e27b0600c99cc8f40e03a89e247d9b309bd479df.pdf", "one-sentence_summary": "We extend the Bayesian Optimization framework by allowing for arbitrary user priors over promising regions of the search space, to guide the search towards said regions.", "supplementary_material": "/attachment/2203a3a289d64bdf42903c2d94f031b27e7d5824.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hvarfner|\\pibo_augmenting_acquisition_functions_with_user_beliefs_for_bayesian_optimization", "_bibtex": "@inproceedings{\nhvarfner2022pibo,\ntitle={\\${\\textbackslash}pi\\${BO}: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization},\nauthor={Carl Hvarfner and Danny Stoll and Artur Souza and Luigi Nardi and Marius Lindauer and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MMAeCXIa89}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "14F3fI6MGxX", "original": "lKBh7qfWRQL", "number": 1594, "cdate": 1632875528689, "mdate": null, "ddate": null, "tcdate": 1632875528689, "tmdate": 1676330604742, "tddate": null, "forum": "14F3fI6MGxX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Generalized Weighted Optimization Method for Computational Learning and Inversion", "authorids": ["~Kui_Ren2", "~Yunan_Yang1", "engquist@oden.utexas.edu"], "authors": ["Kui Ren", "Yunan Yang", "Bj\u00f6rn Engquist"], "keywords": ["weighted optimization", "generalization error", "feature regression", "machine learning"], "abstract": "The generalization capacity of various machine learning models exhibits different phenomena in the under- and over-parameterized regimes. In this paper, we focus on regression models such as feature regression and kernel regression and analyze a generalized weighted least-squares optimization method for computational learning and inversion with noisy data. The highlight of the proposed framework is that we allow weighting in both the parameter space and the data space. The weighting scheme encodes both a priori knowledge on the object to be learned and a strategy to weight the contribution of different data points in the loss function. Here, we characterize the impact of the weighting scheme on the generalization error of the learning method, where we derive explicit generalization errors for the random Fourier feature model in both the under- and over-parameterized regimes. For more general feature maps, error bounds are provided based on the singular values of the feature matrix. We demonstrate that appropriate weighting from prior knowledge can improve the generalization capability of the learned model.", "one-sentence_summary": "This paper proposes a generalized weighted optimization method for computational learning and inversion with noisy data and derives generalization error bounds for various feature regression models, demonstrating better generalization capabilities.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ren|a_generalized_weighted_optimization_method_for_computational_learning_and_inversion", "pdf": "/pdf/50fdf2cdf1d618eebd98fd1294f4d5e3c80bd9ea.pdf", "_bibtex": "@inproceedings{\nren2022a,\ntitle={A Generalized Weighted Optimization Method for Computational Learning and Inversion},\nauthor={Kui Ren and Yunan Yang and Bj{\\\"o}rn Engquist},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=14F3fI6MGxX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "d_2lcDh0Y9c", "original": "whIngYv2HcM", "number": 1584, "cdate": 1632875527988, "mdate": null, "ddate": null, "tcdate": 1632875527988, "tmdate": 1676330605433, "tddate": null, "forum": "d_2lcDh0Y9c", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DriPP: Driven Point Processes to Model Stimuli Induced Patterns in M/EEG Signals", "authorids": ["~C\u00e9dric_Allain1", "~Alexandre_Gramfort1", "~Thomas_Moreau2"], "authors": ["C\u00e9dric Allain", "Alexandre Gramfort", "Thomas Moreau"], "keywords": ["Electrophysiology", "Neuroscience", "Temporal point processes", "Convolutional Dictionary Learning"], "abstract": "The quantitative analysis of non-invasive electrophysiology signals from electroencephalography (EEG) and magnetoencephalography (MEG) boils down to the identification of temporal patterns such as evoked responses, transient bursts of neural oscillations but also blinks or heartbeats for data cleaning. Several works have shown that these patterns can be extracted efficiently in an unsupervised way, e.g., using Convolutional Dictionary Learning. This leads to an event-based description of the data. Given these events, a natural question is to estimate how their occurrences are modulated by certain cognitive tasks and experimental manipulations. To address it, we propose a point process approach. While point processes have been used in neuroscience in the past, in particular for single cell recordings (spike trains), techniques such as Convolutional Dictionary Learning make them amenable to human studies based on EEG/MEG signals. We develop a novel statistical point process model \u2013 called driven temporal point processes (DriPP) \u2013 where the intensity function of the point process model is linked to a set of point processes corresponding to stimulation events. We derive a fast and principled expectation-maximization algorithm to estimate the parameters of this model. Simulations reveal that model parameters can be identified from long enough signals. Results on standard MEG datasets demonstrate that our methodology reveals event-related neural responses \u2013 both evoked and induced \u2013 and isolates non-task specific temporal patterns.", "one-sentence_summary": "Model for patterns' activation using temporal point processes to reveal stimulus-induced effects in brain electrophysiology.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "allain|dripp_driven_point_processes_to_model_stimuli_induced_patterns_in_meeg_signals", "pdf": "/pdf/cddccdd359e2f4b1a5def2498a394cb4cef57397.pdf", "supplementary_material": "/attachment/d706a81153f8ccfeb8e6dacc044d32a23b0fc3d6.zip", "_bibtex": "@inproceedings{\nallain2022dripp,\ntitle={Dri{PP}: Driven Point Processes to Model Stimuli Induced Patterns in M/{EEG} Signals},\nauthor={C{\\'e}dric Allain and Alexandre Gramfort and Thomas Moreau},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=d_2lcDh0Y9c}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "uVXEKeqJbNa", "original": "aDYT6YjjCAL", "number": 1582, "cdate": 1632875527853, "mdate": null, "ddate": null, "tcdate": 1632875527853, "tmdate": 1676330605562, "tddate": null, "forum": "uVXEKeqJbNa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Stiffness-aware neural network for learning Hamiltonian systems", "authorids": ["~SENWEI_Liang1", "~Zhongzhan_Huang1", "~Hong_Zhang7"], "authors": ["SENWEI Liang", "Zhongzhan Huang", "Hong Zhang"], "keywords": ["Hamiltonain systems", "Neural network", "Stiff dynamical systems", "Data-driven method"], "abstract": "We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identifies and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows us to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector fields. We evaluate SANN on complex physical systems including a three-body problem and  billiard model. We show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to significant improvement in accuracy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liang|stiffnessaware_neural_network_for_learning_hamiltonian_systems", "pdf": "/pdf/862118c84d67ec76a2491e8bff7f7d288934bc92.pdf", "_bibtex": "@inproceedings{\nliang2022stiffnessaware,\ntitle={Stiffness-aware neural network for learning Hamiltonian systems},\nauthor={SENWEI Liang and Zhongzhan Huang and Hong Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=uVXEKeqJbNa}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "PilZY3omXV2", "original": "j-xgv_qFhbY", "number": 1576, "cdate": 1632875527441, "mdate": null, "ddate": null, "tcdate": 1632875527441, "tmdate": 1676330605972, "tddate": null, "forum": "PilZY3omXV2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting", "authorids": ["~Gerald_Woo1", "chenghao.liu@salesforce.com", "~Doyen_Sahoo1", "~Akshat_Kumar2", "~Steven_Hoi2"], "authors": ["Gerald Woo", "Chenghao Liu", "Doyen Sahoo", "Akshat Kumar", "Steven Hoi"], "keywords": ["Time Series", "Representation Learning", "Forecasting", "Self-Supervised Learning"], "abstract": "Deep learning has been actively studied for time series forecasting, and the mainstream paradigm is based on the end-to-end training of neural network architectures, ranging from classical LSTM/RNNs to more recent TCNs and Transformers. Motivated by the recent success of representation learning in computer vision and natural language processing, we argue that a more promising paradigm for time series forecasting, is to first learn disentangled feature representations, followed by a simple regression fine-tuning step -- we justify such a paradigm from a causal perspective. Following this principle, we propose a new time series representation learning framework for long sequence time series forecasting named CoST, which applies contrastive learning methods to learn disentangled seasonal-trend representations. CoST comprises both time domain and frequency domain contrastive losses to learn discriminative trend and seasonal representations, respectively. Extensive experiments on real-world datasets show that CoST consistently outperforms the state-of-the-art methods by a considerable margin, achieving a 21.3% improvement in MSE on multivariate benchmarks. It is also robust to various choices of backbone encoders, as well as downstream regressors. Code is available at https://github.com/salesforce/CoST.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "woo|cost_contrastive_learning_of_disentangled_seasonaltrend_representations_for_time_series_forecasting", "pdf": "/pdf/004dc9aa27163347637dedb217441afb5184d2c0.pdf", "supplementary_material": "/attachment/5a493c5b5ff0437526b3b89dc4e96665a2dc7e1c.zip", "_bibtex": "@inproceedings{\nwoo2022cost,\ntitle={Co{ST}: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting},\nauthor={Gerald Woo and Chenghao Liu and Doyen Sahoo and Akshat Kumar and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PilZY3omXV2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "oAy7yPmdNz", "original": "mQI62Ghhc7", "number": 1573, "cdate": 1632875527235, "mdate": null, "ddate": null, "tcdate": 1632875527235, "tmdate": 1676330606103, "tddate": null, "forum": "oAy7yPmdNz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture", "authorids": ["~Ruofan_Liang1", "~Hongyi_Sun2", "nandita@cs.toronto.edu"], "authors": ["Ruofan Liang", "Hongyi Sun", "Nandita Vijaykumar"], "keywords": [], "abstract": "Implicit neural representations with multi-layer perceptrons (MLPs) have recently gained prominence for a wide variety of tasks such as novel view synthesis and 3D object representation and rendering. However, a significant challenge with these representations is that both training and inference with an MLP over a large number of input coordinates to learn and represent an image, video, or 3D object, require large amounts of computation and incur long processing times. In this work, we aim to accelerate inference and training of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, CoordX. With CoordX, the initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP. This approach thus aims at first learning functions that are a decomposition of the original signal and then fusing them to generate the learned signal. Our proposed architecture can be generally used for many implicit neural representation tasks with no additional memory overheads. We demonstrate a speedup of up to 2.92x compared to the baseline model for image, video, and 3D shape representation and rendering tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liang|coordx_accelerating_implicit_neural_representation_with_a_split_mlp_architecture", "pdf": "/pdf/883503a39b83b819141ae8569f3496924381ba8b.pdf", "_bibtex": "@inproceedings{\nliang2022coordx,\ntitle={CoordX: Accelerating Implicit Neural Representation with a Split {MLP} Architecture},\nauthor={Ruofan Liang and Hongyi Sun and Nandita Vijaykumar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oAy7yPmdNz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "9n9c8sf0xm", "original": "WwYlBk-nurg", "number": 1567, "cdate": 1632875526828, "mdate": null, "ddate": null, "tcdate": 1632875526828, "tmdate": 1697934808129, "tddate": null, "forum": "9n9c8sf0xm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Plant 'n' Seek: Can You Find the Winning Ticket?", "authorids": ["~Jonas_Fischer1", "~Rebekka_Burkholz1"], "authors": ["Jonas Fischer", "Rebekka Burkholz"], "keywords": ["lottery tickets", "ground truth", "planting", "LTH"], "abstract": "The lottery ticket hypothesis has sparked the rapid development of pruning algorithms that aim to reduce the computational costs associated with deep learning during training and model deployment. Currently, such algorithms are primarily evaluated on imaging data, for which we lack ground truth information and thus the understanding of how sparse lottery tickets could be. To fill this gap, we develop a framework that allows us to plant and hide winning tickets with desirable properties in randomly initialized neural networks. To analyze the ability of state-of-the-art pruning to identify tickets of extreme sparsity, we design and hide such tickets solving four challenging tasks. In extensive experiments, we observe similar trends as in imaging studies, indicating that our framework can provide transferable insights into realistic problems. Additionally, we can now see beyond such relative trends and highlight limitations of current pruning methods. Based on our results, we conclude that the current limitations in ticket sparsity are likely of algorithmic rather than fundamental nature. We anticipate that comparisons to planted tickets will facilitate future developments of efficient pruning algorithms.", "pdf": "/pdf/233a820014b8dad2af42950b7ca1b07460b76ebe.pdf", "one-sentence_summary": "We derive a framework to plant ground truth lottery tickets in randomly initialized deep neural networks.", "supplementary_material": "/attachment/f014e12cb4e7b4689e497c94223bb971eedd6287.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fischer|plant_n_seek_can_you_find_the_winning_ticket", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.11153/code)", "_bibtex": "@inproceedings{\nfischer2022plant,\ntitle={Plant 'n' Seek: Can You Find the Winning Ticket?},\nauthor={Jonas Fischer and Rebekka Burkholz},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9n9c8sf0xm}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "gjNcH0hj0LM", "original": "91qiq8TdU5", "number": 1566, "cdate": 1632875526761, "mdate": null, "ddate": null, "tcdate": 1632875526761, "tmdate": 1676330606852, "tddate": null, "forum": "gjNcH0hj0LM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Coherence-based Label Propagation over Time Series for Accelerated Active Learning", "authorids": ["~Yooju_Shin1", "~Susik_Yoon1", "~Sundong_Kim2", "~Hwanjun_Song2", "~Jae-Gil_Lee1", "~Byung_Suk_Lee1"], "authors": ["Yooju Shin", "Susik Yoon", "Sundong Kim", "Hwanjun Song", "Jae-Gil Lee", "Byung Suk Lee"], "keywords": ["active learning", "time series", "pseudo labeling"], "abstract": "Time-series data are ubiquitous these days, but lack of the labels in time-series data is regarded as a hurdle for its broad applicability. Meanwhile, active learning has been successfully adopted to reduce the labeling efforts in various tasks. Thus, this paper addresses an important issue, time-series active learning. Inspired by the temporal coherence in time-series data, where consecutive data points tend to have the same label, our label propagation framework, called TCLP, automatically assigns a queried label to the data points within an accurately estimated time-series segment, thereby significantly boosting the impact of an individual query. Compared with traditional time-series active learning, TCLP is shown to improve the classification accuracy by up to 7.1 times when only 0.8% of data points in the entire time series are queried for their labels.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shin|coherencebased_label_propagation_over_time_series_for_accelerated_active_learning", "pdf": "/pdf/adfec039282ce54c1aed434a034e6b291eb9364f.pdf", "one-sentence_summary": "We present a novel label propagation framework for time-series active learning, TCLP, that fully takes advantage of the temporal coherence inherent in time-series data.", "data": "", "_bibtex": "@inproceedings{\nshin2022coherencebased,\ntitle={Coherence-based Label Propagation over Time Series for Accelerated Active Learning},\nauthor={Yooju Shin and Susik Yoon and Sundong Kim and Hwanjun Song and Jae-Gil Lee and Byung Suk Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gjNcH0hj0LM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "_X90SIKbHa", "original": "cYtzHfiX5cm", "number": 1565, "cdate": 1632875526693, "mdate": null, "ddate": null, "tcdate": 1632875526693, "tmdate": 1676330606903, "tddate": null, "forum": "_X90SIKbHa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Class of Short-term Recurrence Anderson Mixing Methods and Their Applications", "authorids": ["~Fuchao_Wei1", "~Chenglong_Bao3", "~Yang_Liu19"], "authors": ["Fuchao Wei", "Chenglong Bao", "Yang Liu"], "keywords": ["Anderson mixing", "sequence acceleration", "fixed-point iteration", "nonconvex optimization", "stochastic optimization"], "abstract": "Anderson mixing (AM) is a powerful acceleration method for fixed-point iterations, but its computation requires storing many historical iterations. The extra memory footprint can be prohibitive when solving high-dimensional problems in a resource-limited machine. To reduce the memory overhead, we propose a novel class of short-term recurrence AM methods (ST-AM). The ST-AM methods only store two previous iterations with cheap corrections. We prove that the basic version of ST-AM is equivalent to the full-memory AM in strongly convex quadratic optimization, and with minor changes it has local linear convergence for solving general nonlinear fixed-point problems. We further analyze the convergence properties of the regularized ST-AM for nonconvex (stochastic) optimization. Finally, we apply ST-AM to several applications including solving root-finding problems and training neural networks. Experimental results show that ST-AM is competitive with the long-memory AM and outperforms many existing optimizers. ", "one-sentence_summary": "We develop a novel class of short-term recurrence Anderson mixing methods and validate its effectiveness in several applications including training neural networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wei|a_class_of_shortterm_recurrence_anderson_mixing_methods_and_their_applications", "pdf": "/pdf/408f39a77474837a59c452c7fbe11596507a9c6d.pdf", "supplementary_material": "/attachment/648d8dc52d7101780f7d130381e9405f823de846.zip", "data": "", "_bibtex": "@inproceedings{\nwei2022a,\ntitle={A Class of Short-term Recurrence Anderson Mixing Methods and Their Applications},\nauthor={Fuchao Wei and Chenglong Bao and Yang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_X90SIKbHa}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "A05I5IvrdL-", "original": "tRNV-l-FEG8", "number": 1564, "cdate": 1632875526625, "mdate": null, "ddate": null, "tcdate": 1632875526625, "tmdate": 1676330607000, "tddate": null, "forum": "A05I5IvrdL-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon POMDPs", "authorids": ["~Johannes_M\u00fcller1", "~Guido_Montufar1"], "authors": ["Johannes M\u00fcller", "Guido Montufar"], "keywords": ["POMDPs", "Memoryless Policies", "Critical points", "State-action frequencies", "Algebraic degree"], "abstract": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we demonstrate how the partial observability constraints can lead to multiple smooth and non-smooth local optimizers and we estimate the number of critical points.", "one-sentence_summary": "We provide an explicit description of the optimization problem and derive bounds on the number of critical points in POMDPs with memoryless stochastic policies depending on the degree of observability.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "m\u00fcller|the_geometry_of_memoryless_stochastic_policy_optimization_in_infinitehorizon_pomdps", "pdf": "/pdf/d9402e7eae55cb2e0a33dfaedb88a525a88beb52.pdf", "supplementary_material": "/attachment/b420d3d6f7b1e76b532b6293961b7b88578c5de1.zip", "_bibtex": "@inproceedings{\nm{\\\"u}ller2022the,\ntitle={The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon {POMDP}s},\nauthor={Johannes M{\\\"u}ller and Guido Montufar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=A05I5IvrdL-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "n0OeTdNRG0Q", "original": "AnkQ2OIWm-", "number": 1561, "cdate": 1632875526484, "mdate": null, "ddate": null, "tcdate": 1632875526484, "tmdate": 1676330607145, "tddate": null, "forum": "n0OeTdNRG0Q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient Sharpness-aware Minimization for Improved Training of Neural Networks", "authorids": ["~Jiawei_Du1", "~Hanshu_Yan1", "~Jiashi_Feng1", "~Joey_Tianyi_Zhou1", "~Liangli_Zhen1", "~Rick_Siow_Mong_Goh1", "~Vincent_Tan1"], "authors": ["Jiawei Du", "Hanshu Yan", "Jiashi Feng", "Joey Tianyi Zhou", "Liangli Zhen", "Rick Siow Mong Goh", "Vincent Tan"], "keywords": ["Efficient learning", "gengeralization", "training algorithm"], "abstract": "Overparametrized Deep Neural Networks (DNNs) often achieve astounding performances, but may potentially result in severe generalization error. Recently, the relation between the sharpness of the loss landscape and the generalization error has been established by Foret et al. (2020), in which the Sharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the generalization. Unfortunately, SAM\u2019s computational cost is roughly double that of base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus proposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM\u2019s efficiency at no cost to its generalization performance. ESAM includes two novel and efficient training strategies\u2014StochasticWeight Perturbation and Sharpness-Sensitive Data Selection. In the former, the sharpness measure is approximated by perturbing a stochastically chosen set of weights in each iteration; in the latter, the SAM loss is optimized using only a judiciously selected subset of data that is sensitive to the sharpness. We provide theoretical explanations as to why these strategies perform well. We also show, via extensive experiments on the CIFAR and ImageNet\ndatasets, that ESAM enhances the efficiency over SAM from requiring 100% extra computations to 40% vis-`a-vis base optimizers, while test accuracies are preserved or even improved.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "du|efficient_sharpnessaware_minimization_for_improved_training_of_neural_networks", "pdf": "/pdf/ded078d40e07efa5958c1cbeb447de7e55420ae0.pdf", "one-sentence_summary": "An efficient sharpness aware minimizer that improves the generalization ", "supplementary_material": "/attachment/019d5e9ab7e5c7ea8f9e8ecd83a75759a359623f.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\ndu2022efficient,\ntitle={Efficient Sharpness-aware Minimization for Improved Training of Neural Networks},\nauthor={Jiawei Du and Hanshu Yan and Jiashi Feng and Joey Tianyi Zhou and Liangli Zhen and Rick Siow Mong Goh and Vincent Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=n0OeTdNRG0Q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "BGvt0ghNgA", "original": "099T8IBAGqF", "number": 1560, "cdate": 1632875526414, "mdate": null, "ddate": null, "tcdate": 1632875526414, "tmdate": 1697934809105, "tddate": null, "forum": "BGvt0ghNgA", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Lipschitz-constrained Unsupervised Skill Discovery", "authorids": ["~Seohong_Park1", "~Jongwook_Choi1", "~Jaekyeom_Kim1", "~Honglak_Lee2", "~Gunhee_Kim1"], "authors": ["Seohong Park", "Jongwook Choi", "Jaekyeom Kim", "Honglak Lee", "Gunhee Kim"], "keywords": ["Reinforcement learning"], "abstract": "We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner \u2014 i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/.", "one-sentence_summary": "We propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more dynamic and diverse skills without external rewards and additional prior knowledge, enabling zero-shot control on goal-reaching downstream tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "park|lipschitzconstrained_unsupervised_skill_discovery", "pdf": "/pdf/8651c40702367a7edf4361ffff2a8a4cd82b9cba.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.00914/code)", "_bibtex": "@inproceedings{\npark2022lipschitzconstrained,\ntitle={Lipschitz-constrained Unsupervised Skill Discovery},\nauthor={Seohong Park and Jongwook Choi and Jaekyeom Kim and Honglak Lee and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BGvt0ghNgA}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "zBOI9LFpESK", "original": "m_McfSIdW7", "number": 1559, "cdate": 1632875526343, "mdate": null, "ddate": null, "tcdate": 1632875526343, "tmdate": 1676330607331, "tddate": null, "forum": "zBOI9LFpESK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Generalizable Representations for Reinforcement Learning via Adaptive Meta-learner of Behavioral Similarities", "authorids": ["~Jianda_Chen1", "~Sinno_Pan1"], "authors": ["Jianda Chen", "Sinno Pan"], "keywords": ["deep reinforcement learning", "deep learning", "representation learning"], "abstract": "How to learn an effective reinforcement learning-based model for control tasks from high-level visual observations is a practical and challenging problem. A key to solving this problem is to learn low-dimensional state representations from observations, from which an effective policy can be learned. In order to boost the learning of state encoding, recent works are focused on capturing behavioral similarities between state representations or applying data augmentation on visual observations. In this paper, we propose a novel meta-learner-based framework for representation learning regarding behavioral similarities for reinforcement learning. Specifically, our framework encodes the high-dimensional observations into two decomposed embeddings regarding reward and dynamics in a Markov Decision Process (MDP). A pair of meta-learners are developed, one of which quantifies the reward similarity and the other quantifies dynamics similarity over the correspondingly decomposed embeddings. The meta-learners are self-learned to update the state embeddings by approximating two disjoint terms in on-policy bisimulation metric. To incorporate the reward and dynamics terms, we further develop a strategy to adaptively balance their impacts based on different tasks or environments. We empirically demonstrate that our proposed framework outperforms state-of-the-art baselines on several benchmarks, including conventional DM Control Suite, Distracting DM Control Suite and a self-driving task CARLA.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|learning_generalizable_representations_for_reinforcement_learning_via_adaptive_metalearner_of_behavioral_similarities", "pdf": "/pdf/6abafd4c0174bd9684e716b9b972429d4b2ae350.pdf", "data": "", "_bibtex": "@inproceedings{\nchen2022learning,\ntitle={Learning Generalizable Representations for Reinforcement Learning via Adaptive Meta-learner of Behavioral Similarities},\nauthor={Jianda Chen and Sinno Pan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zBOI9LFpESK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 31}}, {"id": "xa6otUDdP2W", "original": "9Q1naLoJBvq", "number": 1557, "cdate": 1632875526208, "mdate": null, "ddate": null, "tcdate": 1632875526208, "tmdate": 1697934809370, "tddate": null, "forum": "xa6otUDdP2W", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods", "authorids": ["~Xiaolong_Ma2", "~Minghai_Qin1", "~Fei_Sun2", "~Zejiang_Hou1", "~Kun_Yuan4", "~Yi_Xu8", "~Yanzhi_Wang3", "~Yen-Kuang_Chen2", "~Rong_Jin1", "~Yuan_Xie9"], "authors": ["Xiaolong Ma", "Minghai Qin", "Fei Sun", "Zejiang Hou", "Kun Yuan", "Yi Xu", "Yanzhi Wang", "Yen-Kuang Chen", "Rong Jin", "Yuan Xie"], "keywords": [], "abstract": "Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning. The limitations of these algorithms reduce the level of achievable sparsity. In addition, many algorithms still require pre-trained dense models and thus suffer from large memory footprint. In this paper, we propose a novel scheduled grow-and-prune (GaP) methodology without having to pre-train a dense model. It addresses the shortcomings of the previous works by repeatedly growing a subset of layers to dense and then pruning them back to sparse after some training. Experiments show that the models pruned using the proposed methods match or beat the quality of the highly optimized dense models at 80% sparsity on a variety of tasks, such as image classification, objective detection, 3D object part segmentation, and translation. They also outperform other state-of-the-art (SOTA) methods for model sparsification. As an example, a 90% non-uniform sparse ResNet-50 model obtained via  GaP achieves 77.9% top-1 accuracy on ImageNet, improving the previous SOTA results by 1.5%. Code available at: https://github.com/boone891214/GaP.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ma|effective_model_sparsification_by_scheduled_growandprune_methods", "pdf": "/pdf/039dfe30e42cddf0da3ff88c3fbb66087d12f9a7.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.09857/code)", "_bibtex": "@inproceedings{\nma2022effective,\ntitle={Effective Model Sparsification by Scheduled Grow-and-Prune Methods},\nauthor={Xiaolong Ma and Minghai Qin and Fei Sun and Zejiang Hou and Kun Yuan and Yi Xu and Yanzhi Wang and Yen-Kuang Chen and Rong Jin and Yuan Xie},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xa6otUDdP2W}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "cpDhcsEDC2", "original": "qNGc0FBDdll", "number": 1554, "cdate": 1632875526008, "mdate": null, "ddate": null, "tcdate": 1632875526008, "tmdate": 1697934809679, "tddate": null, "forum": "cpDhcsEDC2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FILIP: Fine-grained Interactive Language-Image Pre-Training", "authorids": ["~Lewei_Yao1", "huangrh9@mail2.sysu.edu.cn", "~Lu_Hou2", "~Guansong_Lu1", "~Minzhe_Niu1", "~Hang_Xu1", "~Xiaodan_Liang2", "~Zhenguo_Li1", "~Xin_Jiang1", "~Chunjing_Xu1"], "authors": ["Lewei Yao", "Runhui Huang", "Lu Hou", "Guansong Lu", "Minzhe Niu", "Hang Xu", "Xiaodan Liang", "Zhenguo Li", "Xin Jiang", "Chunjing Xu"], "keywords": ["Visual-language pretraining", "Language-Image Pretraining", "Multi-modality model"], "abstract": "Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.", "one-sentence_summary": "We introduce a large-scale Fine-grained Interacitve Language-Image Pretraining (FILIP) to achieve finer-level alignment through a new cross-modal late interaction mechanism, which can boost the performance on more grounded vision and language tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yao|filip_finegrained_interactive_languageimage_pretraining", "pdf": "/pdf/e8f6807c88ea1d0d0090f2c381f21739b217efb9.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.07783/code)", "_bibtex": "@inproceedings{\nyao2022filip,\ntitle={{FILIP}: Fine-grained Interactive Language-Image Pre-Training},\nauthor={Lewei Yao and Runhui Huang and Lu Hou and Guansong Lu and Minzhe Niu and Hang Xu and Xiaodan Liang and Zhenguo Li and Xin Jiang and Chunjing Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cpDhcsEDC2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "DfUjyyRW90", "original": "ZlKMqe9UVoC", "number": 1551, "cdate": 1632875525871, "mdate": null, "ddate": null, "tcdate": 1632875525871, "tmdate": 1676330607847, "tddate": null, "forum": "DfUjyyRW90", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Information Prioritization through Empowerment in Visual Model-based RL", "authorids": ["~Homanga_Bharadhwaj1", "~Mohammad_Babaeizadeh1", "~Dumitru_Erhan1", "~Sergey_Levine1"], "authors": ["Homanga Bharadhwaj", "Mohammad Babaeizadeh", "Dumitru Erhan", "Sergey Levine"], "keywords": ["model-based reinforcement learning", "visual distractors", "empowerment"], "abstract": "Model-based reinforcement learning (RL) algorithms designed for handling complex visual observations typically learn some sort of latent state representation, either explicitly or implicitly. Standard methods of this sort do not distinguish between functionally relevant aspects of the state and irrelevant distractors, instead aiming to represent all available information equally. We propose a modified objective for model-based RL that, in combination with mutual information maximization, allows us to learn representations and dynamics for visual model-based RL without reconstruction in a way that explicitly prioritizes functionally relevant factors. The key principle behind our design is to integrate a term inspired by variational empowerment into a state-space learning model based on mutual information. This term prioritizes information that is correlated with action, thus ensuring that functionally relevant factors are captured first. Furthermore, the same empowerment term also promotes faster exploration during the RL process, especially for sparse-reward tasks where the reward signal is insufficient to drive exploration in the early stages of learning. We evaluate the approach on a suite of vision-based robot control tasks with natural video backgrounds, and show that the proposed prioritized information objective outperforms state-of-the-art model based RL approaches by an average of 20\\% in terms of episodic returns at 1M environment interactions with 30\\% higher sample efficiency at 100k interactions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bharadhwaj|information_prioritization_through_empowerment_in_visual_modelbased_rl", "pdf": "/pdf/13e706cf08e60f2727651883527c31c53558ed33.pdf", "one-sentence_summary": "Empowerment along with mutual information maximization helps learn functionally relevant factors in visual model-based RL", "data": "", "_bibtex": "@inproceedings{\nbharadhwaj2022information,\ntitle={Information Prioritization through Empowerment in Visual Model-based {RL}},\nauthor={Homanga Bharadhwaj and Mohammad Babaeizadeh and Dumitru Erhan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DfUjyyRW90}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "nO5caZwFwYu", "original": "o4WdTMQg-Ur", "number": 1550, "cdate": 1632875525801, "mdate": null, "ddate": null, "tcdate": 1632875525801, "tmdate": 1697934810201, "tddate": null, "forum": "nO5caZwFwYu", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient Active Search for Combinatorial Optimization Problems", "authorids": ["~Andr\u00e9_Hottung1", "~Yeong-Dae_Kwon1", "~Kevin_Tierney1"], "authors": ["Andr\u00e9 Hottung", "Yeong-Dae Kwon", "Kevin Tierney"], "keywords": ["heuristic search", "combinatorial optimization", "learning to optimize", "reinforcement learning", "traveling salesperson problem", "vehicle routing problem", "job shop scheduling problem"], "abstract": "Recently numerous machine learning based methods for combinatorial optimization problems have been proposed that learn to construct solutions in a sequential decision process via reinforcement learning. While these methods can be easily combined with search strategies like sampling and beam search, it is not straightforward to integrate them into a high-level search procedure offering strong search guidance. Bello et al. (2016) propose active search, which adjusts the weights of a (trained) model with respect to a single instance at test time using reinforcement learning. While active search is simple to implement, it is not competitive with state-of-the-art methods because adjusting all model weights for each test instance is very time and memory intensive. Instead of updating all model weights, we propose and evaluate three efficient active search strategies that only update a subset of parameters during the search. The proposed methods offer a simple way to significantly improve the search performance of a given model and outperform state-of-the-art machine learning based methods on combinatorial problems, even surpassing the well-known heuristic solver LKH3 on the capacitated vehicle routing problem. Finally, we show that (efficient) active search enables learned models to effectively solve instances that are much larger than those seen during training.", "one-sentence_summary": "We propose active search approaches for combinatorial optimization problems that search for solutions by adjusting a subset of (model) parameters to a single instance at test time.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hottung|efficient_active_search_for_combinatorial_optimization_problems", "pdf": "/pdf/80ed58845ccc4912c64aeee73748354bf61b6a13.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.05126/code)", "_bibtex": "@inproceedings{\nhottung2022efficient,\ntitle={Efficient Active Search for Combinatorial Optimization Problems},\nauthor={Andr{\\'e} Hottung and Yeong-Dae Kwon and Kevin Tierney},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nO5caZwFwYu}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "FZoZ7a31GCW", "original": "GSv5P1lOP9y", "number": 1548, "cdate": 1632875525662, "mdate": null, "ddate": null, "tcdate": 1632875525662, "tmdate": 1676330607979, "tddate": null, "forum": "FZoZ7a31GCW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Ancestral protein sequence reconstruction using a tree-structured Ornstein-Uhlenbeck variational autoencoder", "authorids": ["~Lys_Sanz_Moreta1", "~Ola_R\u00f8nning1", "~Ahmad_Salim_Al-Sibahi1", "hein@stats.ox.ac.uk", "dtheobald@brandeis.edu", "~Thomas_Hamelryck1"], "authors": ["Lys Sanz Moreta", "Ola R\u00f8nning", "Ahmad Salim Al-Sibahi", "Jotun Hein", "Douglas Theobald", "Thomas Hamelryck"], "keywords": ["biological sequences", "variational autoencoders", "latent representations", "ornstein-uhlenbeck process", "evolution"], "abstract": "We introduce a deep generative model for representation learning of biological sequences that, unlike existing models, explicitly represents the evolutionary process. The model makes use of a tree-structured Ornstein-Uhlenbeck process, obtained from a given phylogenetic tree, as an informative prior for a variational autoencoder. We show the model performs well on the task of ancestral sequence reconstruction of single protein families. Our results and ablation studies indicate that the explicit representation of evolution using a suitable tree-structured prior has the potential to improve representation learning of biological sequences considerably. Finally, we briefly discuss extensions of the model to genomic-scale data sets and the case of a latent phylogenetic tree.", "one-sentence_summary": "Ancestral protein sequence reconstruction using a tree-structured Ornstein-Uhlenbeck variational autoencoder", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "moreta|ancestral_protein_sequence_reconstruction_using_a_treestructured_ornsteinuhlenbeck_variational_autoencoder", "pdf": "/pdf/375306f629eb94307d85be89f055c937f95bde83.pdf", "_bibtex": "@inproceedings{\nmoreta2022ancestral,\ntitle={Ancestral protein sequence reconstruction using a tree-structured Ornstein-Uhlenbeck variational autoencoder},\nauthor={Lys Sanz Moreta and Ola R{\\o}nning and Ahmad Salim Al-Sibahi and Jotun Hein and Douglas Theobald and Thomas Hamelryck},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FZoZ7a31GCW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "mdUYT5QV0O", "original": "x5_HIAEVq-m", "number": 1534, "cdate": 1632875524917, "mdate": null, "ddate": null, "tcdate": 1632875524917, "tmdate": 1676330608550, "tddate": null, "forum": "mdUYT5QV0O", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Training Structured Neural Networks Through Manifold Identification and Variance Reduction", "authorids": ["~Zih-Syuan_Huang1", "~Ching-pei_Lee2"], "authors": ["Zih-Syuan Huang", "Ching-pei Lee"], "keywords": ["Structured neural networks", "variance reduction", "manifold identification", "proximal methods"], "abstract": "This paper proposes an algorithm, RMDA, for training neural networks (NNs) with a regularization term for promoting desired structures. RMDA does not incur computation additional to proximal SGD with momentum, and achieves variance reduction without requiring the objective function to be of the finite-sum form. Through the tool of manifold identification from nonlinear optimization, we prove that after a finite number of iterations, all iterates of RMDA possess a desired structure identical to that induced by the regularizer at the stationary point of asymptotic convergence, even in the presence of engineering tricks like data augmentation that complicate the training process. Experiments on training NNs with structured sparsity confirm that variance reduction is necessary for such an identification, and show that RMDA thus significantly outperforms existing methods for this task. For unstructured sparsity, RMDA also outperforms a state-of-the-art pruning method, validating the benefits of training structured NNs through regularization. \nImplementation of RMDA is available at https://www.github.com/zihsyuan1214/rmda.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|training_structured_neural_networks_through_manifold_identification_and_variance_reduction", "pdf": "/pdf/f5bc435bcb3593d7f03a33d1dc39cffc4ff125da.pdf", "one-sentence_summary": "We propose a variance-reduction method for training structured deep learning models that can provably identify the optimal structure.", "supplementary_material": "", "_bibtex": "@inproceedings{\nhuang2022training,\ntitle={Training Structured Neural Networks Through Manifold Identification and Variance Reduction},\nauthor={Zih-Syuan Huang and Ching-pei Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mdUYT5QV0O}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "KBQP4A_J1K", "original": "NM9faYi1oP", "number": 1532, "cdate": 1632875524785, "mdate": null, "ddate": null, "tcdate": 1632875524785, "tmdate": 1676330608649, "tddate": null, "forum": "KBQP4A_J1K", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization", "authorids": ["~R\u00f3bert_Csord\u00e1s1", "~Kazuki_Irie1", "~J\u00fcrgen_Schmidhuber1"], "authors": ["R\u00f3bert Csord\u00e1s", "Kazuki Irie", "J\u00fcrgen Schmidhuber"], "keywords": ["transformer", "compositionality", "systematic generalization", "algorithmic reasoning", "arithmetic"], "abstract": "Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR\u2019s attention and gating patterns tend to be interpretable as an intuitive form of neural routing", "one-sentence_summary": "We improve systematic generalization of Transformers on algorithmic tasks by introducing a novel attention mechanism and gating.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "csord\u00e1s|the_neural_data_router_adaptive_control_flow_in_transformers_improves_systematic_generalization", "pdf": "/pdf/0a8ae186717b6e3ecc30dea384724b288d4060b6.pdf", "supplementary_material": "/attachment/97706c5eabccb849dadf953eafd477639b303f77.zip", "data": "", "_bibtex": "@inproceedings{\ncsord{\\'a}s2022the,\ntitle={The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization},\nauthor={R{\\'o}bert Csord{\\'a}s and Kazuki Irie and J{\\\"u}rgen Schmidhuber},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KBQP4A_J1K}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "w-CPUXXrAj", "original": "7fi3lCAbr7", "number": 1524, "cdate": 1632875524228, "mdate": null, "ddate": null, "tcdate": 1632875524228, "tmdate": 1676330609018, "tddate": null, "forum": "w-CPUXXrAj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Limitations of Multimodal VAEs", "authorids": ["~Imant_Daunhawer2", "~Thomas_M._Sutter1", "~Kieran_Chin-Cheong1", "palumboe@student.ethz.ch", "~Julia_E_Vogt1"], "authors": ["Imant Daunhawer", "Thomas M. Sutter", "Kieran Chin-Cheong", "Emanuele Palumbo", "Julia E Vogt"], "keywords": ["multimodal learning", "variational autoencoder", "variational information bottleneck", "information theory"], "abstract": "Multimodal variational autoencoders (VAEs) have shown promise as efficient generative models for weakly-supervised data. Yet, despite their advantage of weak supervision, they exhibit a gap in generative quality compared to unimodal VAEs, which are completely unsupervised. In an attempt to explain this gap, we uncover a fundamental limitation that applies to a large family of mixture-based multimodal VAEs. We prove that the sub-sampling of modalities enforces an undesirable upper bound on the multimodal ELBO and thereby limits the generative quality of the respective models.  Empirically, we showcase the generative quality gap on both synthetic and real data and present the tradeoffs between different variants of multimodal VAEs. We find that none of the existing approaches fulfills all desired criteria of an effective multimodal generative model when applied on more complex datasets than those used in previous benchmarks. In summary, we identify, formalize, and validate fundamental limitations of VAE-based approaches for modeling weakly-supervised data and discuss implications for real-world applications.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "daunhawer|on_the_limitations_of_multimodal_vaes", "pdf": "/pdf/e25daec9628954edce262e1cda172567415510fc.pdf", "supplementary_material": "/attachment/fadf88e19ce516ceaee7a0d4fbf8ec860aa9855f.zip", "_bibtex": "@inproceedings{\ndaunhawer2022on,\ntitle={On the Limitations of Multimodal {VAE}s},\nauthor={Imant Daunhawer and Thomas M. Sutter and Kieran Chin-Cheong and Emanuele Palumbo and Julia E Vogt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=w-CPUXXrAj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "CSfcOznpDY", "original": "tgPFyr_bCrc", "number": 1522, "cdate": 1632875524091, "mdate": null, "ddate": null, "tcdate": 1632875524091, "tmdate": 1683656398523, "tddate": null, "forum": "CSfcOznpDY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Recursive Disentanglement Network", "authorids": ["~Yixuan_Chen1", "~Yubin_Shi1", "~Dongsheng_Li2", "~Yujiang_Wang1", "~Mingzhi_Dong7", "~Yingying_Zhao1", "~Robert_P._Dick1", "~Qin_Lv1", "~Fan_Yang31", "~Li_Shang3"], "authors": ["Yixuan Chen", "Yubin Shi", "Dongsheng Li", "Yujiang Wang", "Mingzhi Dong", "Yingying Zhao", "Robert P. Dick", "Qin Lv", "Fan Yang", "Li Shang"], "keywords": ["disentanglement", "representation learning", "compositional"], "abstract": "Disentangled feature representation is essential for data-efficient learning. The feature space of deep models is inherently compositional. Existing $\\beta$-VAE-based methods, which only apply disentanglement regularization to the resulting embedding space of deep models, cannot effectively regularize such compositional feature space, resulting in unsatisfactory disentangled results. In this paper, we formulate the compositional disentanglement learning problem from an information-theoretic perspective and propose a recursive disentanglement network (RecurD) that propagates regulatory inductive bias recursively across the compositional feature space during disentangled representation learning. \nExperimental studies demonstrate that RecurD outperforms $\\beta$-VAE and several of its state-of-the-art variants on disentangled representation learning and enables more data-efficient downstream machine learning tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|recursive_disentanglement_network", "pdf": "/pdf/dcb1062c5fcfc89c6726a5cb4916e3e228a85716.pdf", "one-sentence_summary": "This paper has described a solution to the compositional disentangled representation learning problem. ", "_bibtex": "@inproceedings{\nchen2022recursive,\ntitle={Recursive Disentanglement Network},\nauthor={Yixuan Chen and Yubin Shi and Dongsheng Li and Yujiang Wang and Mingzhi Dong and Yingying Zhao and Robert P. Dick and Qin Lv and Fan Yang and Li Shang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CSfcOznpDY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "CgIEctmcXx1", "original": "ITB1RUmojh-", "number": 1521, "cdate": 1632875524022, "mdate": null, "ddate": null, "tcdate": 1632875524022, "tmdate": 1676330609233, "tddate": null, "forum": "CgIEctmcXx1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models", "authorids": ["~Louis_Rouillard1", "~Demian_Wassermann1"], "authors": ["Louis Rouillard", "Demian Wassermann"], "keywords": ["Bayesian inference", "Hierarchical Bayesian Models", "structured Variational Inference", "Simulation Based Inference", "Inference amortization", "Neuroimaging"], "abstract": "Frequently, population studies feature pyramidally-organized data represented using Hierarchical Bayesian Models (HBM) enriched with plates. These models can become prohibitively large in settings such as neuroimaging, where a sample is composed of a functional MRI signal measured on 300 brain locations, across 4 measurement sessions, and 30 subjects, resulting in around 1 million latent parameters.\n\nSuch high dimensionality hampers the usage of modern, expressive flow-based techniques.\n\nTo infer parameter posterior distributions in this challenging class of problems, we designed a novel methodology that automatically produces a variational family dual to a target HBM. This variational family, represented as a neural network, consists in the combination of an attention-based hierarchical encoder feeding summary statistics to a set of normalizing flows. Our automatically-derived neural network exploits exchangeability in the plate-enriched HBM and factorizes its parameter space. The resulting architecture reduces by orders of magnitude its parameterization with respect to that of a typical flow-based representation, while maintaining expressivity.\n\nOur method performs inference on the specified HBM in an amortized setup: once trained, it can readily be applied to a new data sample to compute the parameters' full posterior.\n\nWe demonstrate the capability and scalability of our method on simulated data, as well as a challenging high-dimensional brain parcellation experiment. We also open up several questions that lie at the intersection between normalizing flows, SBI, structured Variational Inference, and inference amortization.", "one-sentence_summary": "We automatically derive a variational family dual to a plate-enriched Hierarchical Bayesian Network and perform amortized inference.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rouillard|adavi_automatic_dual_amortized_variational_inference_applied_to_pyramidal_bayesian_models", "pdf": "/pdf/5e3287e6e246a5cf89ad2b2824e72a35f115d662.pdf", "supplementary_material": "/attachment/316d3e7273e81b94cd40e69922ff34ca6863bc8f.zip", "code": "", "_bibtex": "@inproceedings{\nrouillard2022adavi,\ntitle={{ADAVI}: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models},\nauthor={Louis Rouillard and Demian Wassermann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CgIEctmcXx1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "a34GrNaYEcS", "original": "av-Du-lu_S", "number": 1519, "cdate": 1632875523888, "mdate": null, "ddate": null, "tcdate": 1632875523888, "tmdate": 1697934812927, "tddate": null, "forum": "a34GrNaYEcS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Distributionally Robust Models with Parametric Likelihood Ratios", "authorids": ["~Paul_Michel1", "~Tatsunori_Hashimoto1", "~Graham_Neubig1"], "authors": ["Paul Michel", "Tatsunori Hashimoto", "Graham Neubig"], "keywords": ["distributionally robust optimization", "fairness", "deep learning", "robustness", "adversarial learning"], "abstract": "As machine learning models are deployed ever more broadly, it becomes increasingly important that they are not only able to perform well on their training distribution, but also yield accurate predictions when confronted with distribution shift. The Distributionally Robust Optimization (DRO) framework proposes to address this issue by training models to minimize their expected risk under a collection of distributions, to imitate test-time shifts. This is most commonly achieved by instance-level re-weighting of the training objective to emulate the likelihood ratio with possible test distributions, which allows for estimating their empirical risk via importance sampling (assuming that they are subpopulations of the training distribution). However, re-weighting schemes in the literature are usually limited due to the difficulty of keeping the optimization problem tractable and the complexity of enforcing normalization constraints. In this paper, we show that three simple ideas -- mini-batch level normalization, a KL penalty and simultaneous gradient updates -- allow us to train models with DRO using a broader class of parametric likelihood ratios. In a series of experiments on both image and text classification benchmarks, we find that models trained with the resulting parametric adversaries are consistently more robust to subpopulation shifts when compared to other DRO approaches, and that the method performs reliably well with little hyper-parameter tuning.", "one-sentence_summary": "We learn adversarial parametric reweightings of the training data to reliably train more robust models", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "michel|distributionally_robust_models_with_parametric_likelihood_ratios", "pdf": "/pdf/6da76f4b4c34dc213335f4873bea59a8c0f40ec9.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.06340/code)", "_bibtex": "@inproceedings{\nmichel2022distributionally,\ntitle={Distributionally Robust Models with Parametric Likelihood Ratios},\nauthor={Paul Michel and Tatsunori Hashimoto and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=a34GrNaYEcS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "gbe1zHyA73", "original": "pkyTULAh8i", "number": 1508, "cdate": 1632875523137, "mdate": null, "ddate": null, "tcdate": 1632875523137, "tmdate": 1676330609858, "tddate": null, "forum": "gbe1zHyA73", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Constrained Physical-Statistics Models for Dynamical System Identification and Prediction", "authorids": ["~J\u00e9r\u00e9mie_DONA1", "~Marie_D\u00e9chelle1", "~patrick_gallinari1", "~Marina_Levy1"], "authors": ["J\u00e9r\u00e9mie DONA", "Marie D\u00e9chelle", "patrick gallinari", "Marina Levy"], "keywords": ["Deep Learning", "Hybrid Models", "Differential Equations"], "abstract": "Modeling dynamical systems combining prior physical knowledge and machine learning (ML) is promising in scientific problems when the underlying processes are not fully understood, e.g. when the dynamics is partially known. A common practice to identify the respective parameters of the physical and ML components is to formulate the problem as supervised learning on observed trajectories. However, this formulation leads to an infinite number of possible decompositions. To solve this ill-posedness, we reformulate the learning problem by introducing an upper bound on the prediction error of a physical-statistical model. This allows us to control the contribution of both the physical and statistical components to the overall prediction. This framework generalizes several existing hybrid schemes proposed in the literature. We provide theoretical guarantees on the well-posedness of our formulation along with a proof of convergence in a simple affine setting. For more complex dynamics, we validate our framework experimentally.", "one-sentence_summary": "We propose to incorporate constraints in the learning of hybrid physical and data driven dynamical models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dona|constrained_physicalstatistics_models_for_dynamical_system_identification_and_prediction", "pdf": "/pdf/523bf746dd928f5b2c204455c61d5a91ca94c02d.pdf", "supplementary_material": "/attachment/a49c549a0ebe0ad2f87ddb84d81897e9d9af0287.zip", "_bibtex": "@inproceedings{\ndona2022constrained,\ntitle={Constrained Physical-Statistics Models for Dynamical System Identification and Prediction},\nauthor={J{\\'e}r{\\'e}mie DONA and Marie D{\\'e}chelle and patrick gallinari and Marina Levy},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gbe1zHyA73}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "HCelXXcSEuH", "original": "ceGwVjNNUX_", "number": 1507, "cdate": 1632875523066, "mdate": null, "ddate": null, "tcdate": 1632875523066, "tmdate": 1676330609944, "tddate": null, "forum": "HCelXXcSEuH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information", "authorids": ["~Majid_Jahani2", "~Sergey_Rusakov1", "~Zheng_Shi2", "~Peter_Richt\u00e1rik1", "~Michael_W._Mahoney1", "~Martin_Takac3"], "authors": ["Majid Jahani", "Sergey Rusakov", "Zheng Shi", "Peter Richt\u00e1rik", "Michael W. Mahoney", "Martin Takac"], "keywords": ["Convex Optimization", "Non-Convex Optimization", "Stochastic Optimization", "Second-Order Optimization", "Deep Learning"], "abstract": "We present a novel adaptive optimization algorithm for large-scale machine learning problems. Equipped with a low-cost estimate of local curvature and Lipschitz smoothness, our method dynamically adapts the search direction and step-size. The search direction contains gradient information preconditioned by a well-scaled diagonal preconditioning matrix that captures the local curvature information. Our methodology does not require the tedious task of learning rate tuning, as the learning rate is updated automatically without adding an extra hyper-parameter. We provide convergence guarantees on a comprehensive collection of optimization problems, including convex, strongly convex, and nonconvex problems, in both deterministic and stochastic regimes. We also conduct an extensive empirical evaluation on standard machine learning problems, justifying our algorithm's versatility and demonstrating its strong performance compared to other start-of-the-art first-order and second-order methods.", "one-sentence_summary": "Second-Order Method for Large Scale Machine Learning Tasks", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jahani|doubly_adaptive_scaled_algorithm_for_machine_learning_using_secondorder_information", "pdf": "/pdf/329537bd0e1ff27ec646590f4e792d301baae526.pdf", "supplementary_material": "/attachment/59336c2f6b3763abd103701c8cc06d06e10e6bc2.zip", "_bibtex": "@inproceedings{\njahani2022doubly,\ntitle={Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information},\nauthor={Majid Jahani and Sergey Rusakov and Zheng Shi and Peter Richt{\\'a}rik and Michael W. Mahoney and Martin Takac},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HCelXXcSEuH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 31}}, {"id": "rI0LYgGeYaw", "original": "SLgFXHLDxH", "number": 1499, "cdate": 1632875522528, "mdate": null, "ddate": null, "tcdate": 1632875522528, "tmdate": 1676330610361, "tddate": null, "forum": "rI0LYgGeYaw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding approximate and unrolled dictionary learning for pattern recovery", "authorids": ["~Beno\u00eet_Mal\u00e9zieux1", "~Thomas_Moreau2", "~Matthieu_Kowalski1"], "authors": ["Beno\u00eet Mal\u00e9zieux", "Thomas Moreau", "Matthieu Kowalski"], "keywords": ["Dictionary learning", "bi-level optimization", "unrolling", "pattern learning"], "abstract": "Dictionary learning consists of finding a sparse representation from noisy data and is a common way to encode data-driven prior knowledge on signals. Alternating minimization (AM) is standard for the underlying optimization, where gradient descent steps alternate with sparse coding procedures. The major drawback of this method is its prohibitive computational cost, making it unpractical on large real-world data sets. This work studies an approximate formulation of dictionary learning based on unrolling and compares it to alternating minimization to find the best trade-off between speed and precision. We analyze the asymptotic behavior and convergence rate of gradients estimates in both methods. We show that unrolling performs better on the support of the inner problem solution and during the first iterations. Finally, we apply unrolling on pattern learning in magnetoencephalography (MEG) with the help of a stochastic algorithm and compare the performance to a state-of-the-art method.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mal\u00e9zieux|understanding_approximate_and_unrolled_dictionary_learning_for_pattern_recovery", "pdf": "/pdf/e456e2f1ce949acca23fba4b8e8661e5415f672a.pdf", "supplementary_material": "/attachment/ec8d0b5a9c03560c01c265be9f8d3ed907ac1ec8.zip", "code": "", "_bibtex": "@inproceedings{\nmal{\\'e}zieux2022understanding,\ntitle={Understanding approximate and unrolled dictionary learning for pattern recovery},\nauthor={Beno{\\^\\i}t Mal{\\'e}zieux and Thomas Moreau and Matthieu Kowalski},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rI0LYgGeYaw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "jbrgwbv8nD", "original": "Sh55eaYcGYW", "number": 1498, "cdate": 1632875522460, "mdate": null, "ddate": null, "tcdate": 1632875522460, "tmdate": 1676330610460, "tddate": null, "forum": "jbrgwbv8nD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Constraining Linear-chain CRFs to Regular Languages", "authorids": ["~Sean_Papay1", "~Roman_Klinger1", "~Sebastian_Pado1"], "authors": ["Sean Papay", "Roman Klinger", "Sebastian Pado"], "keywords": ["constrained training", "probabilistic graphical models", "CRF", "semantic role labeling", "sequence labeling"], "abstract": "A major challenge in structured prediction is to represent the interdependencies within output structures.  When outputs are structured as sequences, linear-chain conditional random fields (CRFs) are a widely used model class which can learn local dependencies in the output. However, the CRF's Markov assumption makes it impossible for CRFs to represent distributions with nonlocal dependencies, and standard CRFs are unable to respect nonlocal constraints of the data (such as global arity constraints on output labels).  We present a generalization of CRFs that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language $\\mathcal{L}$.  The resulting regular-constrained CRF (RegCCRF) has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.  Notably, RegCCRFs can incorporate their constraints during training, while related models only enforce constraints during decoding.  We prove that constrained training is never worse than constrained decoding, and show empirically that it can be substantially better in practice.  Additionally, we demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF into a deep neural model for semantic role labeling, exceeding state-of-the-art results on a standard dataset.", "one-sentence_summary": "CRFs can be efficiently constrained to arbitrary regular languages, enforcing nonlocal constraints on their outputs.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "papay|constraining_linearchain_crfs_to_regular_languages", "pdf": "/pdf/978e824fd3601a2093e96071691ad08ccd066da4.pdf", "supplementary_material": "/attachment/700225ac0d206eb1e771f378fca207d80a70e864.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\npapay2022constraining,\ntitle={Constraining Linear-chain {CRF}s to Regular Languages},\nauthor={Sean Papay and Roman Klinger and Sebastian Pado},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=jbrgwbv8nD}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "vHVcB-ak3Si", "original": "tqjhobyBbJQ", "number": 1497, "cdate": 1632875522397, "mdate": null, "ddate": null, "tcdate": 1632875522397, "tmdate": 1676330610551, "tddate": null, "forum": "vHVcB-ak3Si", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Dive Deeper Into Integral Pose Regression", "authorids": ["~Kerui_Gu1", "~Linlin_Yang1", "~Angela_Yao1"], "authors": ["Kerui Gu", "Linlin Yang", "Angela Yao"], "keywords": [], "abstract": "Integral pose regression combines an implicit heatmap with end-to-end training for human body and hand pose estimation. Unlike detection-based heatmap methods, which decode final joint positions from the heatmap with a non-differentiable argmax operation, integral regression methods apply a differentiable expectation operation. This paper offers a deep dive into the inference and back-propagation of integral pose regression to better understand the differences in performance and training compared to detection-based methods. For inference, we give theoretical support as to why expectation should always be better than the argmax operation, i.e. integral regression should always outperform detection.  Yet, in practice, this is observed only in hard cases because the heatmap activation for regression shrinks in easy cases. We then experimentally show that activation shrinkage is one of the leading causes for integral regression's inferior performance.  For back-propagation, we theoretically and empirically analyze the gradients to explain the slow training speed of integral regression.  Based on these findings, we incorporate the supervision of a spatial prior to speed up training and improve performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gu|dive_deeper_into_integral_pose_regression", "pdf": "/pdf/1c30c07fe0d8b037287227beefbc0d1263b462f5.pdf", "data": "", "_bibtex": "@inproceedings{\ngu2022dive,\ntitle={Dive Deeper Into Integral Pose Regression},\nauthor={Kerui Gu and Linlin Yang and Angela Yao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vHVcB-ak3Si}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "84NMXTHYe-", "original": "1tpUsfa4ll0", "number": 1484, "cdate": 1632875521540, "mdate": null, "ddate": null, "tcdate": 1632875521540, "tmdate": 1697934817552, "tddate": null, "forum": "84NMXTHYe-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Evidential Turing Processes ", "authorids": ["~Melih_Kandemir1", "~Abdullah_Akg\u00fcl1", "~Manuel_Haussmann1", "~Gozde_Unal1"], "authors": ["Melih Kandemir", "Abdullah Akg\u00fcl", "Manuel Haussmann", "Gozde Unal"], "keywords": ["Evidential Deep Learning", "Neural Processes", "Attention", "Neural Turing Machines"], "abstract": "A probabilistic classifier with reliable predictive uncertainties i) fits successfully to the target domain data, ii) provides calibrated class probabilities in difficult regions of the target domain (e.g. class overlap), and iii) accurately identifies queries coming out of the target domain and reject them. We introduce an original combination of Evidential Deep Learning, Neural Processes, and Neural Turing Machines capable of providing all three essential properties mentioned above for total uncertainty quantification. We observe our method on three image classification benchmarks to consistently improve the in-domain uncertainty quantification, out-of-domain detection, and robustness against input perturbations with one single model. Our unified solution delivers an implementation-friendly and computationally efficient recipe for safety clearance and provides intellectual economy to an investigation of algorithmic roots of epistemic awareness in deep neural nets.", "one-sentence_summary": "An original extension of evidential deep learning with neural processes and neural Turing machines makes it possible to attain both in-domain calibration and out-of-domain detection in a single model. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kandemir|evidential_turing_processes", "pdf": "/pdf/78e3a3224d06a68626d3e18fe724144646c74064.pdf", "supplementary_material": "/attachment/9dc5cd09409d8a8028f022028e94cd36b7dac563.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.01216/code)", "_bibtex": "@inproceedings{\nkandemir2022evidential,\ntitle={Evidential Turing Processes },\nauthor={Melih Kandemir and Abdullah Akg{\\\"u}l and Manuel Haussmann and Gozde Unal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=84NMXTHYe-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "vJb4I2ANmy", "original": "EWwAg-47Oy", "number": 1482, "cdate": 1632875521393, "mdate": null, "ddate": null, "tcdate": 1632875521393, "tmdate": 1697934817552, "tddate": null, "forum": "vJb4I2ANmy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Noisy Feature Mixup", "authorids": ["~Soon_Hoe_Lim1", "~N._Benjamin_Erichson1", "~Francisco_Utrera1", "~Winnie_Xu1", "~Michael_W._Mahoney1"], "authors": ["Soon Hoe Lim", "N. Benjamin Erichson", "Francisco Utrera", "Winnie Xu", "Michael W. Mahoney"], "keywords": ["Data augmentation", "implicit regularization", "mixup", "noise injection", "model robustness"], "abstract": "We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method for data augmentation that combines the best of interpolation based training and noise injection schemes. Rather than training with convex combinations of pairs of examples and their labels, we use noise-perturbed convex combinations of pairs of data points in both input and feature space. This method includes mixup and manifold mixup as special cases, but it has additional advantages, including better smoothing of decision boundaries and enabling improved model robustness. We provide theory to understand this as well as the implicit regularization effects of NFM. Our theory is supported by empirical results, demonstrating the advantage of NFM, as compared to mixup and manifold mixup. We show that residual networks and vision transformers trained with NFM have favorable trade-offs between predictive accuracy on clean data and robustness with respect to various types of data perturbation across a range of computer vision benchmark datasets.", "one-sentence_summary": "We propose and study Noisy Feature Mixup, a simple yet effective data augmentation method that leads to improved model robustness when compared to training with manifold mixup or noise injection alone.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lim|noisy_feature_mixup", "pdf": "/pdf/4a18925f14f56e9ae86ffe5ebb83e50a2d418c34.pdf", "supplementary_material": "/attachment/bafce789d72783899e364a229c350267ecb0072f.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02180/code)", "_bibtex": "@inproceedings{\nlim2022noisy,\ntitle={Noisy Feature Mixup},\nauthor={Soon Hoe Lim and N. Benjamin Erichson and Francisco Utrera and Winnie Xu and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vJb4I2ANmy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "moHCzz6D5H3", "original": "HJ9F3ZgQwQY", "number": 1477, "cdate": 1632875521043, "mdate": null, "ddate": null, "tcdate": 1632875521043, "tmdate": 1676330611771, "tddate": null, "forum": "moHCzz6D5H3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Peek-a-Boo: What (More) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently", "authorids": ["~Xiaohan_Chen1", "~Jason_Zhang2", "~Zhangyang_Wang1"], "authors": ["Xiaohan Chen", "Jason Zhang", "Zhangyang Wang"], "keywords": ["Sparse Neural Network", "Lottery Ticket Hypothesis", "Efficient Machine Leanring"], "abstract": "Sparse neural networks (NNs) are intensively investigated in literature due to their appeal in saving storage, memory, and computational costs. A recent work (Ramanujan et al., 2020) showed that, different from conventional pruning-and-finetuning pipeline, there exist hidden subnetworks in randomly initialized NNs that have good performance without training the weights. However, such \"hidden subnetworks\" have mediocre performances and require an expensive edge-popup algorithm to search for them. In this work, we define an extended class of subnetworks in randomly initialized NNs called disguised subnetworks, which are not only \"hidden\" in the random networks but also \"disguised\" -- hence can only be \"unmasked\" with certain transformations on weights. We argue that the unmasking process plays an important role in enlarging the capacity of the subnetworks and thus grants two major benefits: (i) the disguised subnetworks easily outperform the hidden counterparts; (ii) the unmasking process helps to relax the quality requirement on the sparse subnetwork mask so that the expensive edge-popup algorithm can be replaced with more efficient alternatives. On top of this new concept, we propose a novel two-stage algorithm that plays a Peek-a-Boo (PaB) game to identify the disguised subnetworks with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork's remaining weights. Furthermore, we show that the unmasking process can be efficiently implemented (a) without referring to any latent weights or scores; and (b) by only leveraging approximated gradients, so that the whole training algorithm is computationally light. Extensive experiments with several large models (ResNet-18, ResNet-50, and WideResNet-28) and datasets (CIFAR-10, CIFAR-100 and ImageNet) demonstrate the competency of PaB over edge-popup and other counterparts. Our codes are available at: https://github.com/VITA-Group/Peek-a-Boo.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|peekaboo_what_more_is_disguised_in_a_randomly_weighted_neural_network_and_how_to_find_it_efficiently", "pdf": "/pdf/60b7fbc376713b122304e2e0530ea7290974d364.pdf", "_bibtex": "@inproceedings{\nchen2022peekaboo,\ntitle={Peek-a-Boo: What (More) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently},\nauthor={Xiaohan Chen and Jason Zhang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=moHCzz6D5H3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "EwqEx5ipbOu", "original": "xRiY36C6I2-", "number": 1472, "cdate": 1632875520693, "mdate": null, "ddate": null, "tcdate": 1632875520693, "tmdate": 1676330611993, "tddate": null, "forum": "EwqEx5ipbOu", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How Well Does Self-Supervised Pre-Training Perform with Streaming Data?", "authorids": ["~Dapeng_Hu2", "~Shipeng_Yan1", "~Qizhengqiu_Lu1", "~Lanqing_HONG1", "~Hailin_Hu1", "~Yifan_Zhang1", "~Zhenguo_Li1", "~Xinchao_Wang1", "~Jiashi_Feng1"], "authors": ["Dapeng Hu", "Shipeng Yan", "Qizhengqiu Lu", "Lanqing HONG", "Hailin Hu", "Yifan Zhang", "Zhenguo Li", "Xinchao Wang", "Jiashi Feng"], "keywords": ["Pre-Training", "Representation Learning", "Continual Learning", "Self-Supervised Learning"], "abstract": "Prior works on self-supervised pre-training focus on the joint training scenario, where massive unlabeled data are assumed to be given as input all at once, and only then is a learner trained. Unfortunately, such a problem setting is often impractical if not infeasible since many real-world tasks rely on sequential learning, e.g., data are decentralized or collected in a streaming fashion. In this paper, we conduct the first thorough and dedicated investigation on self-supervised pre-training with streaming data, aiming to shed light on the model behavior under this overlooked setup. Specifically, we pre-train over 500 models on four categories of pre-training streaming data from ImageNet and DomainNet and evaluate them on three types of downstream tasks and 12 different downstream datasets. Our studies show that, somehow beyond our expectation, with simple data replay or parameter regularization, sequential self-supervised pre-training turns out to be an efficient alternative for joint pre-training, as the performances of the former are mostly on par with those of the latter. Moreover, catastrophic forgetting, a common issue in sequential supervised learning, is much alleviated in sequential self-supervised learning (SSL), which is well justified through our comprehensive empirical analysis on representations and the sharpness of minima in the loss landscape. Our findings, therefore, suggest that, in practice, for SSL, the cumbersome joint training can be replaced mainly by sequential learning, which in turn enables a much broader spectrum of potential application scenarios. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hu|how_well_does_selfsupervised_pretraining_perform_with_streaming_data", "pdf": "/pdf/a00f82e4a4b8bc53140104602610c56f5dea871e.pdf", "data": "", "_bibtex": "@inproceedings{\nhu2022how,\ntitle={How Well Does Self-Supervised Pre-Training Perform with Streaming Data?},\nauthor={Dapeng Hu and Shipeng Yan and Qizhengqiu Lu and Lanqing HONG and Hailin Hu and Yifan Zhang and Zhenguo Li and Xinchao Wang and Jiashi Feng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EwqEx5ipbOu}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "boJy41J-tnQ", "original": "--s7DB8ggm", "number": 1466, "cdate": 1632875520347, "mdate": null, "ddate": null, "tcdate": 1632875520347, "tmdate": 1697934819567, "tddate": null, "forum": "boJy41J-tnQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Subspace Regularizers for Few-Shot Class Incremental Learning", "authorids": ["~Afra_Feyza_Aky\u00fcrek1", "~Ekin_Aky\u00fcrek1", "~Derry_Wijaya1", "~Jacob_Andreas1"], "authors": ["Afra Feyza Aky\u00fcrek", "Ekin Aky\u00fcrek", "Derry Wijaya", "Jacob Andreas"], "keywords": ["few-shot class incremental learning", "incremental learning", "incremental classification", "subspace regularization", "manifold regularization", "few-shot learning"], "abstract": "Few-shot class incremental learning---the problem of updating a trained classifier to discriminate among an expanded set of classes with limited labeled data---is a key challenge for machine learning systems deployed in non-stationary environments. Existing approaches to the problem rely on complex model architectures and training procedures that are difficult to tune and re-use. In this paper, we present an extremely simple approach that enables the use of ordinary logistic regression classifiers for few-shot incremental learning. The key to this approach is a new family of \\textit{subspace regularization} schemes that encourage weight vectors for new classes to lie close to the subspace spanned by the weights of existing classes. When combined with pretrained convolutional feature extractors, logistic regression models trained with subspace regularization outperform specialized, state-of-the-art approaches to few-shot incremental image classification by up to 23\\% on the \\textit{mini}ImageNet dataset. Because of its simplicity, subspace regularization can be straightforwardly configured to incorporate additional background information about the new classes (including class names and descriptions specified in natural language); this offers additional control over the trade-off between existing and new classes. Our results show that simple geometric regularization of class representations offers an effective tool for continual learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "aky\u00fcrek|subspace_regularizers_for_fewshot_class_incremental_learning", "pdf": "/pdf/23bc15dae0714cd2d58e51d9c64892ca38433f7d.pdf", "one-sentence_summary": "We propose a simple yet highly effective set of subspace-based regularizers to address representation learning for few-shot incremental classification.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.07059/code)", "_bibtex": "@inproceedings{\naky{\\\"u}rek2022subspace,\ntitle={Subspace Regularizers for Few-Shot Class Incremental Learning},\nauthor={Afra Feyza Aky{\\\"u}rek and Ekin Aky{\\\"u}rek and Derry Wijaya and Jacob Andreas},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=boJy41J-tnQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "OtEDS2NWhqa", "original": "drp78YAw5G3", "number": 1463, "cdate": 1632875520138, "mdate": null, "ddate": null, "tcdate": 1632875520138, "tmdate": 1676330612432, "tddate": null, "forum": "OtEDS2NWhqa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Using Graph Representation Learning with Schema Encoders to Measure the Severity of Depressive Symptoms", "authorids": ["~Simin_Hong1", "~Anthony_Cohn1", "~David_Crossland_Hogg1"], "authors": ["Simin Hong", "Anthony Cohn", "David Crossland Hogg"], "keywords": ["Graph neural networks sentiment analysis node-embedding algorithm  diagnostic prediction task"], "abstract": "Graph neural networks (GNNs) are widely used in regression and classification problems applied to text, in areas such as sentiment analysis and medical decision-making processes. We propose a novel form for node attributes within a GNN based model that captures node-specific embeddings for every word in the vocabulary. This provides a global representation at each node, coupled with node-level updates according to associations among words in a transcript. We demonstrate the efficacy of the approach by augmenting the accuracy of measuring major depressive disorder (MDD). Prior research has sought to make a diagnostic prediction of depression levels from patient data using several modalities, including audio, video, and text. On the DAIC-WOZ benchmark, our method outperforms state-of-art methods by a substantial margin, including those using multiple modalities. Moreover, we also evaluate the performance of our novel model on a Twitter sentiment dataset. We show that our model outperforms a general GNN model by leveraging our novel 2-D node attributes. These results demonstrate the generality of the proposed method.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hong|using_graph_representation_learning_with_schema_encoders_to_measure_the_severity_of_depressive_symptoms", "pdf": "/pdf/9dbfdd98e925440e87789286618df5f2ae9e27ba.pdf", "one-sentence_summary": "We encode word embeddings by using graph representation learning method, which schematizes context-level depressive features for depression state prediction.", "code": "", "_bibtex": "@inproceedings{\nhong2022using,\ntitle={Using Graph Representation Learning with Schema Encoders to Measure the Severity of Depressive Symptoms},\nauthor={Simin Hong and Anthony Cohn and David Crossland Hogg},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OtEDS2NWhqa}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "DTXZqTNV5nW", "original": "j5Fv8sDErp", "number": 1461, "cdate": 1632875520000, "mdate": null, "ddate": null, "tcdate": 1632875520000, "tmdate": 1676330612536, "tddate": null, "forum": "DTXZqTNV5nW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game", "authorids": ["~Haobo_Fu2", "~Weiming_Liu3", "~Shuang_Wu3", "~Yijia_Wang1", "~Tao_Yang10", "~Kai_Li2", "~Junliang_Xing1", "~Bin_Li8", "~Bo_Ma4", "~QIANG_FU8", "~Yang_Wei2"], "authors": ["Haobo Fu", "Weiming Liu", "Shuang Wu", "Yijia Wang", "Tao Yang", "Kai Li", "Junliang Xing", "Bin Li", "Bo Ma", "QIANG FU", "Yang Wei"], "keywords": ["Policy Optimization", "Nash Equilibrium", "Mahjong AI"], "abstract": "The deep policy gradient method has demonstrated promising results in many large-scale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum games, but it usually needs domain-specific abstractions to deal with large-scale games.  Inheriting merits from both methods, in this paper we extend the actor-critic algorithm framework in deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-information game, 1-on-1 Mahjong, whose information set size and game length are much larger than poker. The proposed algorithm, named Actor-Critic Hedge (ACH), modifies the policy optimization objective from originally maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modification is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justified as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong.", "one-sentence_summary": "A new actor-critic algorithm for approximating a Nash Equilibrium in the large-scale imperfect-information game 1v1 Mahjong.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fu|actorcritic_policy_optimization_in_a_largescale_imperfectinformation_game", "pdf": "/pdf/6fe3b02efc57f5d0f92998d2d9f16fbf729ade8f.pdf", "supplementary_material": "/attachment/9c55b719595babe4f312b0161fc0b181bde70b0e.zip", "_bibtex": "@inproceedings{\nfu2022actorcritic,\ntitle={Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game},\nauthor={Haobo Fu and Weiming Liu and Shuang Wu and Yijia Wang and Tao Yang and Kai Li and Junliang Xing and Bin Li and Bo Ma and QIANG FU and Yang Wei},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DTXZqTNV5nW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "EHaUTlm2eHg", "original": "MpHHO7Mcb8_", "number": 1457, "cdate": 1632875519793, "mdate": null, "ddate": null, "tcdate": 1632875519793, "tmdate": 1676330612715, "tddate": null, "forum": "EHaUTlm2eHg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Policy Gradients Incorporating the Future", "authorids": ["~David_Venuto1", "~Elaine_Lau1", "~Doina_Precup1", "~Ofir_Nachum1"], "authors": ["David Venuto", "Elaine Lau", "Doina Precup", "Ofir Nachum"], "keywords": [], "abstract": "Reasoning about the future -- understanding how decisions in the present time affect outcomes in the future -- is one of the central challenges for reinforcement learning (RL), especially in highly-stochastic or partially observable environments. While predicting the future directly is hard, in this work we introduce a method that allows an agent to ``look into the future'' without explicitly predicting it. Namely, we propose to allow an agent, during its training on past experience, to observe what \\emph{actually} happened in the future at that time, while enforcing an information bottleneck to avoid the agent overly relying on this privileged information. Coupled with recent advances in variational inference and a latent-variable autoregressive model, this gives our agent the ability to utilize rich and \\emph{useful} information about the future trajectory dynamics in addition to the present. Our method, Policy Gradients Incorporating the Future (PGIF), is easy to implement and versatile, being applicable to virtually any policy gradient algorithm. We apply our proposed method to a number of off-the-shelf RL algorithms and show that PGIF is able to achieve higher reward faster in a variety of online and offline RL domains, as well as sparse-reward and partially observable environments. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "venuto|policy_gradients_incorporating_the_future", "pdf": "/pdf/1bc7c8d13a1713bf5e86827cb71b07a2d36496ad.pdf", "supplementary_material": "/attachment/f74ebf676ec83a8c504e8f613dfd5ff0ca2feefc.zip", "data": "", "_bibtex": "@inproceedings{\nvenuto2022policy,\ntitle={Policy Gradients Incorporating the Future},\nauthor={David Venuto and Elaine Lau and Doina Precup and Ofir Nachum},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EHaUTlm2eHg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "rzvOQrnclO0", "original": "vQ0jkyZcYM", "number": 1450, "cdate": 1632875519320, "mdate": null, "ddate": null, "tcdate": 1632875519320, "tmdate": 1676330613316, "tddate": null, "forum": "rzvOQrnclO0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Gradient Information Matters in Policy Optimization by Back-propagating through Model", "authorids": ["~Chongchong_Li1", "~Yue_Wang15", "~Wei_Chen1", "~Yuting_Liu4", "~Zhi-Ming_Ma1", "~Tie-Yan_Liu1"], "authors": ["Chongchong Li", "Yue Wang", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "keywords": ["Model-based RL", "Policy Optimization"], "abstract": "Model-based reinforcement learning provides an efficient mechanism to find the optimal policy by interacting with the learned environment. In addition to treating the learned environment like a black-box simulator, a more effective way to use the model is to exploit its differentiability. Such methods require the gradient information of the learned environment model when calculating the policy gradient. However, since the error of gradient is not considered in the model learning phase, there is no guarantee for the model's accuracy. To address this problem, we first analyze the convergence rate for the policy optimization methods when the policy gradient is calculated using the learned environment model. The theoretical results show that the model gradient error matters in the policy optimization phrase. Then we propose a two-model-based learning method to control the prediction error and the gradient error. We separate the different roles of these two models at the model learning phase and coordinate them at the policy optimization phase. After proposing the method, we introduce the directional derivative projection policy optimization (DDPPO) algorithm as a practical implementation to find the optimal policy. Finally, we empirically demonstrate the proposed algorithm has better sample efficiency when achieving a comparable or better performance on benchmark continuous control tasks.", "one-sentence_summary": "Considering the gradient information in the model learning is crucial for the model-based policy optimization according to our theoritical results. Motivated by such conclusion, we design a novel DDPPO algorithm that can achieve the SOTA performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|gradient_information_matters_in_policy_optimization_by_backpropagating_through_model", "pdf": "/pdf/f6890ab3f174fcc59f2441755d0529b472b382da.pdf", "data": "", "_bibtex": "@inproceedings{\nli2022gradient,\ntitle={Gradient Information Matters in Policy Optimization by Back-propagating through Model},\nauthor={Chongchong Li and Yue Wang and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rzvOQrnclO0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "xm6YD62D1Ub", "original": "xttwzdH9b3i", "number": 1445, "cdate": 1632875518983, "mdate": null, "ddate": null, "tcdate": 1632875518983, "tmdate": 1697934821977, "tddate": null, "forum": "xm6YD62D1Ub", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning", "authorids": ["~Adrien_Bardes1", "~Jean_Ponce1", "~Yann_LeCun1"], "authors": ["Adrien Bardes", "Jean Ponce", "Yann LeCun"], "keywords": ["self-supervised learning", "representation learning", "computer vision"], "abstract": "Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image.  The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold,  (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bardes|vicreg_varianceinvariancecovariance_regularization_for_selfsupervised_learning", "pdf": "/pdf/25a14f6fde1bb9ddf5881d141f200e0c3aaa0ccb.pdf", "one-sentence_summary": "Variance regularization prevents collapse in self-supervised representation learning", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2105.04906/code)", "_bibtex": "@inproceedings{\nbardes2022vicreg,\ntitle={{VICR}eg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},\nauthor={Adrien Bardes and Jean Ponce and Yann LeCun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xm6YD62D1Ub}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "gI7feJ9yXPz", "original": "7yuUmdIBKJB", "number": 1439, "cdate": 1632875518574, "mdate": null, "ddate": null, "tcdate": 1632875518574, "tmdate": 1676330613886, "tddate": null, "forum": "gI7feJ9yXPz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "High Probability Generalization Bounds with Fast Rates for Minimax Problems", "authorids": ["~Shaojie_Li2", "~Yong_Liu7"], "authors": ["Shaojie Li", "Yong Liu"], "keywords": [], "abstract": "Minimax problems are receiving an increasing amount of attention in a wide range of applications in machine learning (ML), for instance, reinforcement learning, robust optimization, adversarial learning, and distributed computing, to mention but a few. Current studies focus on the fundamental understanding of general minimax problems with an emphasis on convergence behavior. As a comparison, there is far less work to study the generalization performance. Additionally, existing generalization bounds are almost all derived in expectation, and the high probability bounds are all presented in the slow order $\\mathcal{O}(1/\\sqrt{n})$, where $n$ is the sample size. In this paper, we provide improved generalization analyses and obtain sharper high probability generalization bounds for most existing generalization measures of minimax problems. We then use the improved learning bounds to establish high probability generalization bounds with fast rates for classical empirical saddle point (ESP) solution and several popular gradient-based optimization algorithms, including gradient descent ascent (GDA), stochastic gradient descent ascent (SGDA), proximal point method (PPM), extra-gradient (EG), and optimistic gradient descent ascent (OGDA). In summary, we provide a systematical analysis of sharper generalization bounds of minimax problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|high_probability_generalization_bounds_with_fast_rates_for_minimax_problems", "pdf": "/pdf/e5876716f378d51a9cbe5f9d74f719c455bd4377.pdf", "_bibtex": "@inproceedings{\nli2022high,\ntitle={High Probability Generalization Bounds with Fast Rates for Minimax Problems},\nauthor={Shaojie Li and Yong Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gI7feJ9yXPz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "Z8FzvVU6_Kj", "original": "NlfdJtRKUjr", "number": 1438, "cdate": 1632875518508, "mdate": null, "ddate": null, "tcdate": 1632875518508, "tmdate": 1676330614022, "tddate": null, "forum": "Z8FzvVU6_Kj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SUMNAS: Supernet with Unbiased Meta-Features for Neural Architecture Search", "authorids": ["~Hyeonmin_Ha1", "~Ji-Hoon_Kim2", "~Semin_Park1", "~Byung-Gon_Chun1"], "authors": ["Hyeonmin Ha", "Ji-Hoon Kim", "Semin Park", "Byung-Gon Chun"], "keywords": ["Neural architecture search"], "abstract": "One-shot Neural Architecture Search (NAS) usually constructs an over-parameterized network, which we call a supernet, and typically adopts sharing parameters among the sub-models to improve computational efficiency. One-shot NAS often repeatedly samples sub-models from the supernet and trains them to optimize the shared parameters. However, this training strategy suffers from multi-model forgetting. Training a sampled sub-model overrides the previous knowledge learned by the other sub-models, resulting in an unfair performance evaluation between the sub-models. We propose Supernet with Unbiased Meta-Features for Neural Architecture Search (SUMNAS), a supernet learning strategy based on meta-learning to tackle the knowledge forgetting issue. During the training phase, we explicitly address the multi-model forgetting problem and help the supernet learn unbiased meta-features, independent from the sampled sub-models. Once training is over, sub-models can be instantly compared to get the overall ranking or the best sub-model. Our evaluation on the NAS-Bench-201 and MobileNet-based search space demonstrate that SUMNAS shows improved ranking ability and finds architectures whose performance is on par with existing state-of-the-art NAS algorithms.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ha|sumnas_supernet_with_unbiased_metafeatures_for_neural_architecture_search", "pdf": "/pdf/82215472636d191a93f9ff0e73c2fb07893068f4.pdf", "one-sentence_summary": "We propose a supernet learning strategy that learns unbiased meta-features to tackle multi-model forgetting problem of neural architecture search.", "data": "", "_bibtex": "@inproceedings{\nha2022sumnas,\ntitle={{SUMNAS}: Supernet with Unbiased Meta-Features for Neural Architecture Search},\nauthor={Hyeonmin Ha and Ji-Hoon Kim and Semin Park and Byung-Gon Chun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Z8FzvVU6_Kj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "_XNtisL32jv", "original": "dEmUAo_jOxO", "number": 1430, "cdate": 1632875517962, "mdate": null, "ddate": null, "tcdate": 1632875517962, "tmdate": 1697934823398, "tddate": null, "forum": "_XNtisL32jv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting", "authorids": ["~Shikuang_Deng1", "~Yuhang_Li1", "~Shanghang_Zhang4", "~Shi_Gu1"], "authors": ["Shikuang Deng", "Yuhang Li", "Shanghang Zhang", "Shi Gu"], "keywords": ["Spiking Neural Networks", "Direct Training", "Surrogate Gradient", "Generalizability"], "abstract": "Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efficient characteristics. It is difficult to efficiently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artificial neural networks (ANNs). Although the adoption of surrogate gradient (SG) formally allows for the back-propagation of losses, the discrete spiking mechanism actually differentiates the loss landscape of SNNs from that of ANNs, failing the surrogate gradient methods to achieve comparable accuracy as for ANNs. In this paper, we first analyze why the current direct training approach with surrogate gradient results in SNNs with poor generalizability. Then we introduce the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalizability. Meanwhile, we demonstrate that TET improves the temporal scalability of SNN and induces a temporal inheritable training for acceleration. Our method consistently outperforms the SOTA on all reported mainstream datasets, including CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained  83% top-1 accuracy, over 10% improvement compared to existing state of the art.", "one-sentence_summary": "This paper provides a novel temporal efficient training method for SNN, which significantly improves performance by modifying the optimization target.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "deng|temporal_efficient_training_of_spiking_neural_network_via_gradient_reweighting", "pdf": "/pdf/36b73d733683265023d3e40a225095942a71eef4.pdf", "supplementary_material": "/attachment/ff7589425476597733582208d6252eae70b09008.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.11946/code)", "_bibtex": "@inproceedings{\ndeng2022temporal,\ntitle={Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting},\nauthor={Shikuang Deng and Yuhang Li and Shanghang Zhang and Shi Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_XNtisL32jv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "u6TRGdzhfip", "original": "_yosvSR4OrT", "number": 1426, "cdate": 1632875517761, "mdate": null, "ddate": null, "tcdate": 1632875517761, "tmdate": 1697934824163, "tddate": null, "forum": "u6TRGdzhfip", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Reliable Adversarial Distillation with Unreliable Teachers", "authorids": ["~Jianing_Zhu2", "~Jiangchao_Yao1", "~Bo_Han1", "~Jingfeng_Zhang1", "~Tongliang_Liu1", "~Gang_Niu1", "~Jingren_Zhou1", "~Jianliang_Xu1", "~Hongxia_Yang2"], "authors": ["Jianing Zhu", "Jiangchao Yao", "Bo Han", "Jingfeng Zhang", "Tongliang Liu", "Gang Niu", "Jingren Zhou", "Jianliang Xu", "Hongxia Yang"], "keywords": [], "abstract": "In ordinary distillation, student networks are trained with soft labels (SLs) given by pretrained teacher networks, and students are expected to improve upon teachers since SLs are stronger supervision than the original hard labels. However, when considering adversarial robustness, teachers may become unreliable and adversarial distillation may not work: teachers are pretrained on their own adversarial data, and it is too demanding to require that teachers are also good at every adversarial data queried by students. Therefore, in this paper, we propose reliable introspective adversarial distillation (IAD) where students partially instead of fully trust their teachers. Specifically, IAD distinguishes between three cases given a query of a natural data (ND) and the corresponding adversarial data (AD): (a) if a teacher is good at AD, its SL is fully trusted; (b) if a teacher is good at ND but not AD, its SL is partially trusted and the student also takes its own SL into account; (c) otherwise, the student only relies on its own SL. Experiments demonstrate the effectiveness of IAD for improving upon teachers in terms of adversarial robustness.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|reliable_adversarial_distillation_with_unreliable_teachers", "pdf": "/pdf/eac811eb67d636dd867f0cf3166da4e08c31d495.pdf", "supplementary_material": "", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.04928/code)", "_bibtex": "@inproceedings{\nzhu2022reliable,\ntitle={Reliable Adversarial Distillation with Unreliable Teachers},\nauthor={Jianing Zhu and Jiangchao Yao and Bo Han and Jingfeng Zhang and Tongliang Liu and Gang Niu and Jingren Zhou and Jianliang Xu and Hongxia Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=u6TRGdzhfip}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "NyJ2KIN8P17", "original": "Y-oTIMt-T6o", "number": 1421, "cdate": 1632875517427, "mdate": null, "ddate": null, "tcdate": 1632875517427, "tmdate": 1676330614988, "tddate": null, "forum": "NyJ2KIN8P17", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Program Synthesis with Query", "authorids": ["~Di_Huang5", "~Rui_Zhang1", "~Xing_Hu3", "~Xishan_Zhang1", "~Pengwei_Jin1", "~Nan_Li3", "~Zidong_Du1", "~Qi_Guo4", "~Yunji_Chen1"], "authors": ["Di Huang", "Rui Zhang", "Xing Hu", "Xishan Zhang", "Pengwei Jin", "Nan Li", "Zidong Du", "Qi Guo", "Yunji Chen"], "keywords": [], "abstract": "Aiming to find a program satisfying the user intent given input-output examples, program synthesis has attracted increasing interest in the area of machine learning. Despite the promising performance of existing methods, most of their success comes from the privileged information of well-designed input-output examples. However, providing such input-output examples is unrealistic because it requires the users to have the ability to describe the underlying program with a few input-output examples under the training distribution. In this work, we propose a query-based framework that trains a query neural network to generate informative input-output examples automatically and interactively from a large query space. The quality of the query depends on the amount of the mutual information between the query and the corresponding program, which can guide the optimization of the query framework. To estimate the mutual information more accurately, we introduce the functional space (F-space) which models the relevance between the input-output examples and the programs in a differentiable way. We evaluate the effectiveness and generalization of the proposed query-based framework on the Karel task and the list processing task. Experimental results show that the query-based framework can generate informative input-output examples which achieve\nand even outperform well-designed input-output examples.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|neural_program_synthesis_with_query", "pdf": "/pdf/e5b3abde2c30abab84561a7eb8d74c79cdb1ddec.pdf", "one-sentence_summary": "We propose a query-based framework for the interactive program synthesis.", "supplementary_material": "/attachment/6723960ae0943718442d71de8c64a67c25fa4a89.zip", "_bibtex": "@inproceedings{\nhuang2022neural,\ntitle={Neural Program Synthesis with Query},\nauthor={Di Huang and Rui Zhang and Xing Hu and Xishan Zhang and Pengwei Jin and Nan Li and Zidong Du and Qi Guo and Yunji Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NyJ2KIN8P17}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "HTVch9AMPa", "original": "NqixWUuf_jc", "number": 1420, "cdate": 1632875517364, "mdate": null, "ddate": null, "tcdate": 1632875517364, "tmdate": 1697934824614, "tddate": null, "forum": "HTVch9AMPa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Delaunay Component Analysis for Evaluation of Data Representations", "authorids": ["~Petra_Poklukar1", "~Vladislav_Polianskii1", "~Anastasiia_Varava1", "~Florian_T._Pokorny1", "~Danica_Kragic_Jensfelt1"], "authors": ["Petra Poklukar", "Vladislav Polianskii", "Anastasiia Varava", "Florian T. Pokorny", "Danica Kragic Jensfelt"], "keywords": ["Interpretation and Evaluation of Learned Representations", "Generative Models", "Contrastive Learning"], "abstract": "Advanced representation learning techniques require reliable and general evaluation methods. Recently, several algorithms based on the common idea of geometric and topological analysis of a manifold approximated from the learned data representations have been proposed. In this work, we introduce Delaunay Component Analysis (DCA) -- an evaluation algorithm which approximates the data manifold using a more suitable neighbourhood graph called Delaunay graph. This provides a reliable manifold estimation even for challenging geometric arrangements of representations such as clusters with varying shape and density as well as outliers, which is where existing methods often fail. Furthermore, we exploit the nature of Delaunay graphs and introduce a framework for assessing the quality of individual novel data representations.  We experimentally validate the proposed DCA method on representations obtained from neural networks trained with contrastive objective, supervised and generative models, and demonstrate various use cases of our extended single point evaluation framework.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "poklukar|delaunay_component_analysis_for_evaluation_of_data_representations", "pdf": "/pdf/3df46bb46fbcb4d72d6c49b08889d954321cc9c6.pdf", "one-sentence_summary": "We present Delaunay Component Analysis (DCA) framework for evaluation of learned data representations which anayzes geometric and topological properties of representation spaces using Delaunay graphs.", "supplementary_material": "/attachment/af8f5937c9ea36621e5cde44a4273bc94bf1ea9d.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.06866/code)", "_bibtex": "@inproceedings{\npoklukar2022delaunay,\ntitle={Delaunay Component Analysis for Evaluation of Data Representations},\nauthor={Petra Poklukar and Vladislav Polianskii and Anastasiia Varava and Florian T. Pokorny and Danica Kragic Jensfelt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HTVch9AMPa}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "p0rCmDEN_-", "original": "qGUM-V00Yx_", "number": 1418, "cdate": 1632875517235, "mdate": null, "ddate": null, "tcdate": 1632875517235, "tmdate": 1676330615259, "tddate": null, "forum": "p0rCmDEN_-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Visual hyperacuity with moving sensor and recurrent neural computations", "authorids": ["~Alexander_Rivkind1", "~Or_Ram1", "~Eldad_Assa1", "~Michael_Kreiserman1", "~Ehud_Ahissar1"], "authors": ["Alexander Rivkind", "Or Ram", "Eldad Assa", "Michael Kreiserman", "Ehud Ahissar"], "keywords": ["visual system", "convolutional neural networks", "recurrent neural networks", "active vision", "active sensing", "ocular drift"], "abstract": "Dynamical phenomena, such as recurrent neuronal activity  and perpetual motion of the eye, are typically overlooked in models of bottom-up visual perception. Recent experiments suggest that tiny inter-saccadic eye motion (\"fixational drift\") enhances visual  acuity beyond the limit imposed by the density of retinal photoreceptors. Here we hypothesize that such an enhancement is enabled by recurrent neuronal computations in early visual areas. Specifically, we explore a setting involving a low-resolution dynamical sensor that moves with respect to a static scene, with drift-like tiny steps. This setting mimics a dynamical eye viewing objects in perceptually-challenging conditions. The dynamical sensory input is classified by a convolutional neural network with recurrent connectivity added to its lower layers, in analogy to recurrent connectivity in early visual areas.  Applying our system to CIFAR-10 and CIFAR-100 datasets down-sampled via 8x8 sensor, we found that (i) classification accuracy, which is drastically reduced by this down-sampling, is mostly restored to its 32x32 baseline level when using a moving sensor and recurrent connectivity, (ii) in this setting, neurons in the early layers exhibit a wide repertoire of selectivity patterns, spanning the spatiotemporal selectivity space, with neurons preferring different combinations of spatial and temporal patterning, and (iii) curved sensor's trajectories improve  visual acuity compared to straight trajectories, echoing recent experimental findings involving eye-tracking in challenging conditions. Our work sheds light on the possible role of recurrent connectivity in early vision as well as the roles of fixational drift and temporal-frequency selective cells in the visual system. It also proposes a solution for artificial image recognition in settings with limited resolution and multiple time samples, such as in edge AI applications.", "one-sentence_summary": "We show how recurrent connectivity in early vision together with eye motion helps to cope with limited sensor's spatial resolution.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rivkind|visual_hyperacuity_with_moving_sensor_and_recurrent_neural_computations", "pdf": "/pdf/74d601c5743a4f1b7e2e0c33b63ef39f695a6c4a.pdf", "supplementary_material": "/attachment/a9f19eda2ed21055fcdf20a05cd2b716c7ec47fe.zip", "data": "", "_bibtex": "@inproceedings{\nrivkind2022visual,\ntitle={Visual hyperacuity with moving sensor and recurrent neural computations},\nauthor={Alexander Rivkind and Or Ram and Eldad Assa and Michael Kreiserman and Ehud Ahissar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=p0rCmDEN_-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "2ggNjUisGyr", "original": "q6u2xvZ5Vdq", "number": 1413, "cdate": 1632875516891, "mdate": null, "ddate": null, "tcdate": 1632875516891, "tmdate": 1676330615579, "tddate": null, "forum": "2ggNjUisGyr", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration", "authorids": ["~Ziming_Wang1", "~Nan_Xue1", "~Ling_Lei1", "~Gui-Song_Xia3"], "authors": ["Ziming Wang", "Nan Xue", "Ling Lei", "Gui-Song Xia"], "keywords": ["partial Wasserstein discrepancy", "partial distribution matching", "point set registration"], "abstract": "Given two point sets, the problem of registration is to recover a transformation that matches one set to the other. This task is challenging due to the presence of large number of outliers, the unknown non-rigid deformations and the large sizes of point sets. To obtain strong robustness against outliers, we formulate the registration problem as a partial distribution matching (PDM) problem, where the goal is to partially match the distributions represented by point sets in a metric space. To handle large point sets, we propose a scalable PDM algorithm by utilizing the efficient partial Wasserstein-1 (PW) discrepancy. Specifically, we derive the Kantorovich-Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these results, we propose a partial Wasserstein adversarial network (PWAN),  which is able to approximate the PW discrepancy by a neural network, and minimize it by gradient descent. In addition,\nit also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations. We evaluate PWAN on practical point set registration tasks, and show that the proposed PWAN is robust, scalable and performs more favorably than the state-of-the-art methods.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|partial_wasserstein_adversarial_network_for_nonrigid_point_set_registration", "pdf": "/pdf/aa0f00aa3ffa33879529f0a8702f8a59097997a7.pdf", "one-sentence_summary": "We propose a method for large scale partial distribution matching problem, and apply it to non-rigid point set registration task.", "_bibtex": "@inproceedings{\nwang2022partial,\ntitle={Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration},\nauthor={Ziming Wang and Nan Xue and Ling Lei and Gui-Song Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=2ggNjUisGyr}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "xFOyMwWPkz", "original": "nvrbJENRcJm", "number": 1401, "cdate": 1632875516143, "mdate": null, "ddate": null, "tcdate": 1632875516143, "tmdate": 1676330616209, "tddate": null, "forum": "xFOyMwWPkz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation", "authorids": ["~Yang_Zhao11", "~Hao_Zhang37"], "authors": ["Yang Zhao", "Hao Zhang"], "keywords": ["interpretation of neural network units", "computational topology", "convolutional neural networks", "entropy"], "abstract": " Identifying the status of individual network units is critical for understanding the mechanism of convolutional neural networks (CNNs). However, it is still challenging to reliably give a general indication of unit status, especially for units in different network models. To this end, we propose a novel method for quantitatively clarifying the status of single unit in CNN using algebraic topological tools. Unit status is indicated via the calculation of a defined topological-based entropy, called feature entropy, which measures the degree of chaos of the global spatial pattern hidden in the unit for a category. In this way, feature entropy could provide an accurate indication of status for units in different networks with diverse situations like weight-rescaling operation. Further, we show that feature entropy decreases as the layer goes deeper and shares almost simultaneous trend with loss during training. We show that by investigating the feature entropy of units on only training data, it could give discrimination between networks with different generalization ability from the view of the effectiveness of feature representations.\n", "one-sentence_summary": "We propose a novel method for quantitatively clarifying the status of individual units in CNNs and show its value in interpreting networks with different generalization ability.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhao|quantitative_performance_assessment_of_cnn_units_via_topological_entropy_calculation", "pdf": "/pdf/20b7b30269e6c1a3819b2bb2a73bf9a836ccd1b6.pdf", "_bibtex": "@inproceedings{\nzhao2022quantitative,\ntitle={Quantitative Performance Assessment of {CNN} Units via Topological Entropy Calculation},\nauthor={Yang Zhao and Hao Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xFOyMwWPkz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "1zwleytEpYx", "original": "6Xa9piH1PL", "number": 1394, "cdate": 1632875515681, "mdate": null, "ddate": null, "tcdate": 1632875515681, "tmdate": 1697934827318, "tddate": null, "forum": "1zwleytEpYx", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Imitation Learning by Reinforcement Learning", "authorids": ["~Kamil_Ciosek1"], "authors": ["Kamil Ciosek"], "keywords": ["reinforcement learning", "imitation learning", "Markov Decision Process", "continuous control"], "abstract": "Imitation learning algorithms learn a policy from demonstrations of expert behavior. We show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Our theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. We conduct experiments which confirm that our reduction works well in practice for continuous control tasks.", "one-sentence_summary": "For deterministic experts, you can do imitation learning by calling an RL solver once, with a stationary reward signal.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ciosek|imitation_learning_by_reinforcement_learning", "pdf": "/pdf/18385148f0590e0d9a4ea379bd07c26c43414141.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2108.04763/code)", "_bibtex": "@inproceedings{\nciosek2022imitation,\ntitle={Imitation Learning by Reinforcement Learning},\nauthor={Kamil Ciosek},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1zwleytEpYx}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "81e1aeOt-sd", "original": "zL6WICTOZwV", "number": 1392, "cdate": 1632875515539, "mdate": null, "ddate": null, "tcdate": 1632875515539, "tmdate": 1697934827709, "tddate": null, "forum": "81e1aeOt-sd", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On-Policy Model Errors in Reinforcement Learning", "authorids": ["~Lukas_Froehlich1", "~Maksym_Lefarov1", "~Melanie_Zeilinger1", "~Felix_Berkenkamp1"], "authors": ["Lukas Froehlich", "Maksym Lefarov", "Melanie Zeilinger", "Felix Berkenkamp"], "keywords": ["Model-based reinforcement learning", "reinforcement learning", "model learning"], "abstract": "Model-free reinforcement learning algorithms can compute policy gradients given sampled environment transitions, but require large amounts of data. In contrast, model-based methods can use the learned model to generate new data, but model errors and bias can render learning unstable or suboptimal. In this paper, we present a novel method that combines real-world data and a learned model in order to get the best of both worlds. The core idea is to exploit the real-world data for on-policy predictions and use the learned model only to generalize to different actions. Specifically, we use the data as time-dependent on-policy correction terms on top of a learned model, to retain the ability to generate data without accumulating errors over long prediction horizons. We motivate this method theoretically and show that it counteracts an error term for model-based policy improvement. Experiments on MuJoCo- and PyBullet-benchmarks show that our method can drastically improve existing model-based approaches without introducing additional tuning parameters.", "one-sentence_summary": "We combine real-world data and a learned model for data-efficient reinforcement learning with reduced model-bias.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "froehlich|onpolicy_model_errors_in_reinforcement_learning", "pdf": "/pdf/a579d018b07ad6fa046ecc55697be2a1ea96eace.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.07985/code)", "_bibtex": "@inproceedings{\nfroehlich2022onpolicy,\ntitle={On-Policy Model Errors in Reinforcement Learning},\nauthor={Lukas Froehlich and Maksym Lefarov and Melanie Zeilinger and Felix Berkenkamp},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=81e1aeOt-sd}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "O50443AsCP", "original": "PYVyHKkcbm9", "number": 1389, "cdate": 1632875515311, "mdate": null, "ddate": null, "tcdate": 1632875515311, "tmdate": 1697934827938, "tddate": null, "forum": "O50443AsCP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor", "authorids": ["~Qian_Liu2", "~Bei_Chen3", "~Jiaqi_Guo1", "~Morteza_Ziyadi1", "~Zeqi_Lin1", "~Weizhu_Chen1", "~Jian-Guang_Lou1"], "authors": ["Qian Liu", "Bei Chen", "Jiaqi Guo", "Morteza Ziyadi", "Zeqi Lin", "Weizhu Chen", "Jian-Guang Lou"], "keywords": ["table pre-training", "sythetic pre-training", "SQL execution", "table-based question answering", "table-based fact verification"], "abstract": "Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes the improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks. Our code can be found at https://github.com/microsoft/Table-Pretraining.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|tapex_table_pretraining_via_learning_a_neural_sql_executor", "pdf": "/pdf/9abc11a326d0ad12abb958697c1ab8e0a585e62b.pdf", "one-sentence_summary": "This work performs table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries and their execution results.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2107.07653/code)", "_bibtex": "@inproceedings{\nliu2022tapex,\ntitle={{TAPEX}: Table Pre-training via Learning a Neural {SQL} Executor},\nauthor={Qian Liu and Bei Chen and Jiaqi Guo and Morteza Ziyadi and Zeqi Lin and Weizhu Chen and Jian-Guang Lou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=O50443AsCP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "9SDQB3b68K", "original": "-Zx-0XAXA6d", "number": 1378, "cdate": 1632875514571, "mdate": null, "ddate": null, "tcdate": 1632875514571, "tmdate": 1676330617562, "tddate": null, "forum": "9SDQB3b68K", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DARA: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning", "authorids": ["~Jinxin_Liu1", "~Zhang_Hongyin1", "~Donglin_Wang1"], "authors": ["Jinxin Liu", "Zhang Hongyin", "Donglin Wang"], "keywords": [], "abstract": "Offline reinforcement learning algorithms promise to be applicable in settings where a fixed dataset is available and no new experience can be acquired. However, such formulation is inevitably offline-data-hungry and, in practice, collecting a large offline dataset for one specific task over one specific environment is also costly and laborious. In this paper, we thus 1) formulate the offline dynamics adaptation by using (source) offline data collected from another dynamics to relax the requirement for the extensive (target) offline data, 2) characterize the dynamics shift problem in which prior offline methods do not scale well, and 3) derive a simple dynamics-aware reward augmentation (DARA) framework from both model-free and model-based offline settings. Specifically, DARA emphasizes learning from those source transition pairs that are adaptive for the target environment and mitigates the offline dynamics shift by characterizing state-action-next-state pairs instead of the typical state-action distribution sketched by prior offline RL methods. The experimental evaluation demonstrates that DARA, by augmenting rewards in the source offline dataset, can acquire an adaptive policy for the target environment and yet significantly reduce the requirement of target offline data. With only modest amounts of target offline data, our performance consistently outperforms the prior offline RL methods in both simulated and real-world tasks. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|dara_dynamicsaware_reward_augmentation_in_offline_reinforcement_learning", "pdf": "/pdf/b68b505b8daf0a76ad9d4d5e4cd43976ba9864db.pdf", "supplementary_material": "/attachment/b6a590b91295ea32030d827354e895948b286ddc.zip", "data": "", "_bibtex": "@inproceedings{\nliu2022dara,\ntitle={{DARA}: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning},\nauthor={Jinxin Liu and Zhang Hongyin and Donglin Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9SDQB3b68K}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "P07dq7iSAGr", "original": "JSypgk-XqN1", "number": 1371, "cdate": 1632875514090, "mdate": null, "ddate": null, "tcdate": 1632875514090, "tmdate": 1676330618084, "tddate": null, "forum": "P07dq7iSAGr", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Explaining Point Processes by Learning Interpretable Temporal Logic Rules", "authorids": ["~Shuang_Li3", "~Mingquan_Feng1", "~Lu_Wang11", "~Abdelmajid_Essofi1", "~Yufeng_Cao1", "~Junchi_Yan2", "~Le_Song1"], "authors": ["Shuang Li", "Mingquan Feng", "Lu Wang", "Abdelmajid Essofi", "Yufeng Cao", "Junchi Yan", "Le Song"], "keywords": ["Temporal Point Process", "Temporal Logic Rules", "Explainable Models"], "abstract": "We propose a principled method to learn a set of human-readable logic rules to explain temporal point processes. \nWe assume that the generative mechanisms underlying the temporal point processes are governed by a set of first-order temporal logic rules, as a compact representation of domain knowledge. Our method formulates the rule discovery process from noisy event data as a maximum likelihood problem, and designs an efficient and tractable branch-and-price algorithm to progressively search for new rules and expand existing rules. The proposed algorithm alternates between the rule generation stage and the rule evaluation stage, and uncovers the most important collection of logic rules within a fixed time limit for both synthetic and real event data. In a real healthcare application, we also had human experts (i.e., doctors) verify the learned temporal logic rules and provide further improvements. These expert-revised interpretable rules lead to a point process model which outperforms previous state-of-the-arts for symptom prediction, both in their occurrence times and types. ", "pdf": "/pdf/a417bc93488c1842edd3369524cfa9125d192f04.pdf", "one-sentence_summary": "We propose a principled method to learn a set of human-readable logic rules to explain temporal point processes. ", "supplementary_material": "/attachment/790bfaeabe127324a6bbb70801920d20370de312.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|explaining_point_processes_by_learning_interpretable_temporal_logic_rules", "data": "", "_bibtex": "@inproceedings{\nli2022explaining,\ntitle={Explaining Point Processes by Learning Interpretable Temporal Logic Rules},\nauthor={Shuang Li and Mingqaun Feng and Lu Wang and Abdelmajid Essofi and Yufeng Cao and Junchi Yan and Le Song},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=P07dq7iSAGr}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "eBCmOocUejf", "original": "6t8MH5LFR7", "number": 1365, "cdate": 1632875513691, "mdate": null, "ddate": null, "tcdate": 1632875513691, "tmdate": 1697934831175, "tddate": null, "forum": "eBCmOocUejf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Robust Prefix-Tuning for Text Classification", "authorids": ["~Zonghan_Yang1", "~Yang_Liu19"], "authors": ["Zonghan Yang", "Yang Liu"], "keywords": ["prefix-tuning", "pretrained language models", "text classification", "robustness in NLP", "optimal control"], "abstract": "Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness enhancement. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|on_robust_prefixtuning_for_text_classification", "pdf": "/pdf/02dfcabb44949137b40a59f94715b5caa4b12231.pdf", "one-sentence_summary": "We propose a robust prefix-tuning framework that improves robustness of prefix-tuning against different types of attacks while preserving its efficiency and modularity with interpretation from the perspective of optimal control.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.10378/code)", "_bibtex": "@inproceedings{\nyang2022on,\ntitle={On Robust Prefix-Tuning for Text Classification},\nauthor={Zonghan Yang and Yang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=eBCmOocUejf}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "0sgntlpKDOz", "original": "mb0ai8RV1Hc", "number": 1361, "cdate": 1632875513421, "mdate": null, "ddate": null, "tcdate": 1632875513421, "tmdate": 1697934831708, "tddate": null, "forum": "0sgntlpKDOz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Graphon Mean Field Games and Approximate Nash Equilibria", "authorids": ["~Kai_Cui3", "~Heinz_Koeppl1"], "authors": ["Kai Cui", "Heinz Koeppl"], "keywords": ["Mean Field Games", "Reinforcement Learning", "Multi Agent Systems"], "abstract": "Recent advances at the intersection of dense large graph limits and mean field games have begun to enable the scalable analysis of a broad class of dynamical sequential games with large numbers of agents. So far, results have been largely limited to graphon mean field systems with continuous-time diffusive or jump dynamics, typically without control and with little focus on computational methods. We propose a novel discrete-time formulation for graphon mean field games as the limit of non-linear dense graph Markov games with weak interaction. On the theoretical side, we give extensive and rigorous existence and approximation properties of the graphon mean field solution in sufficiently large systems. On the practical side we provide general learning schemes for graphon mean field equilibria by either introducing agent equivalence classes or reformulating the graphon mean field system as a classical mean field system. By repeatedly finding a regularized optimal control solution and its generated mean field, we successfully obtain plausible approximate Nash equilibria in otherwise infeasible large dense graph games with many agents. Empirically, we are able to demonstrate on a number of examples that the finite-agent behavior comes increasingly close to the mean field behavior for our computed equilibria as the graph or system size grows, verifying our theory. More generally, we successfully apply policy gradient reinforcement learning in conjunction with sequential Monte Carlo methods.", "one-sentence_summary": "We propose, analyze and solve a novel, theoretically well-founded graph-based mean field game for Nash equilibria in discrete-time dynamical systems on otherwise infeasibly large dense graphs. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cui|learning_graphon_mean_field_games_and_approximate_nash_equilibria", "pdf": "/pdf/b4f2ad24930753086ac1a6b4fea2f45e13771c21.pdf", "supplementary_material": "/attachment/2f6d5e026e1776237d60fe154e2222763248bb51.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.01280/code)", "_bibtex": "@inproceedings{\ncui2022learning,\ntitle={Learning Graphon Mean Field Games and Approximate Nash Equilibria},\nauthor={Kai Cui and Heinz Koeppl},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0sgntlpKDOz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "UtGtoS4CYU", "original": "96W8qvlwS4f", "number": 1350, "cdate": 1632875512689, "mdate": null, "ddate": null, "tcdate": 1632875512689, "tmdate": 1676330619311, "tddate": null, "forum": "UtGtoS4CYU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Measuring CLEVRness: Black-box Testing of Visual Reasoning Models", "authorids": ["~Spyridon_Mouselinos1", "~Henryk_Michalewski1", "~Mateusz_Malinowski1"], "authors": ["Spyridon Mouselinos", "Henryk Michalewski", "Mateusz Malinowski"], "keywords": ["Visual Reasoning", "Visual Question Answering", "Black Box Testing", "Computer Vision"], "abstract": "How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate.\nTo answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a ``human-level'', can easily be fooled by our agent. Our results \nput in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mouselinos|measuring_clevrness_blackbox_testing_of_visual_reasoning_models", "pdf": "/pdf/3eb3b1766e7d5addbbef3045662e6ad378427ae1.pdf", "one-sentence_summary": "Black box testing of visual reasoning models as a two-player game.", "data": "", "_bibtex": "@inproceedings{\nmouselinos2022measuring,\ntitle={Measuring {CLEVR}ness: Black-box Testing of Visual Reasoning Models},\nauthor={Spyridon Mouselinos and Henryk Michalewski and Mateusz Malinowski},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=UtGtoS4CYU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "qqdXHUGec9h", "original": "eMI72IpXytq", "number": 1332, "cdate": 1632875511538, "mdate": null, "ddate": null, "tcdate": 1632875511538, "tmdate": 1676330620312, "tddate": null, "forum": "qqdXHUGec9h", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Exploiting Class Activation Value for Partial-Label Learning", "authorids": ["~Fei_Zhang3", "~Lei_Feng1", "~Bo_Han1", "~Tongliang_Liu1", "~Gang_Niu1", "~Tao_Qin1", "~Masashi_Sugiyama1"], "authors": ["Fei Zhang", "Lei Feng", "Bo Han", "Tongliang Liu", "Gang Niu", "Tao Qin", "Masashi Sugiyama"], "keywords": ["Partial-label Learning", "Class Activation Map"], "abstract": "Partial-label learning (PLL) solves the multi-class classification problem, where each training instance is assigned a set of candidate labels that include the true label. Recent advances showed that PLL can be compatible with deep neural networks, which achieved state-of-the-art performance. However, most of the existing deep PLL methods focus on designing proper training objectives under various assumptions on the collected data, which may limit their performance when the collected data cannot satisfy the adopted assumptions. In this paper, we propose to exploit the learned intrinsic representation of the model to identify the true label in the training process, which does not rely on any assumptions on the collected data. We make two key contributions. As the first contribution, we empirically show that the class activation map (CAM), a simple technique for discriminating the learning patterns of each class in images, could surprisingly be utilized to make accurate predictions on selecting the true label from candidate labels. Unfortunately, as CAM is confined to image inputs with convolutional neural networks, we are yet unable to directly leverage CAM to address the PLL problem with general inputs and models. Thus, as the second contribution, we propose the class activation value (CAV), which owns similar properties of CAM, while CAV is versatile in various types of inputs and models. Building upon CAV, we propose a novel method named CAV Learning (CAVL) that selects the true label by the class with the maximum CAV for model training. Extensive experiments on various datasets demonstrate that our proposed CAVL method achieves state-of-the-art performance.", "one-sentence_summary": "Class activation value is better for partial-label learning than the basic outputs of the classifier itself.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|exploiting_class_activation_value_for_partiallabel_learning", "pdf": "/pdf/c3254feff27af6a4191d7d320290953ea6656e5d.pdf", "data": "", "_bibtex": "@inproceedings{\nzhang2022exploiting,\ntitle={Exploiting Class Activation Value for Partial-Label Learning},\nauthor={Fei Zhang and Lei Feng and Bo Han and Tongliang Liu and Gang Niu and Tao Qin and Masashi Sugiyama},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qqdXHUGec9h}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "9-Rfew334N", "original": "japH89iSAhP", "number": 1321, "cdate": 1632875510922, "mdate": null, "ddate": null, "tcdate": 1632875510922, "tmdate": 1676330620592, "tddate": null, "forum": "9-Rfew334N", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Givens Coordinate Descent Methods for Rotation Matrix Learning in Trainable Embedding Indexes", "authorids": ["~Yunjiang_Jiang1", "~Han_Zhang10", "~Yiming_Qiu1", "~Yun_Xiao2", "~Bo_Long1", "~Wen-Yun_Yang1"], "authors": ["Yunjiang Jiang", "Han Zhang", "Yiming Qiu", "Yun Xiao", "Bo Long", "Wen-Yun Yang"], "keywords": ["Search index", "Product quantization", "Block coordinate descent"], "abstract": "Product quantization (PQ) coupled with a space rotation, is widely used in modern approximate nearest neighbor (ANN) search systems to significantly compress the disk storage for embeddings and speed up the inner product computation. Existing rotation learning methods, however, minimize quantization distortion for fixed embeddings, which are not applicable to an end-to-end training scenario where embeddings are updated constantly. In this paper, based on geometric intuitions from Lie group theory,  in particular the special orthogonal groupSO(n),  we propose a family of block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. Compared to the state-of-the-art SVD method, the Givens algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably  according  to  experimental  studies.   They  further  improve  upon  vanilla product quantization significantly in an end-to-end training scenario.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|givens_coordinate_descent_methods_for_rotation_matrix_learning_in_trainable_embedding_indexes", "pdf": "/pdf/8b198d22f6cd67d27c168c61da339c81ac7d363d.pdf", "one-sentence_summary": "Learning orthonormal matrix in neural networks via Givens rotations", "data": "", "_bibtex": "@inproceedings{\njiang2022givens,\ntitle={Givens Coordinate Descent Methods for Rotation Matrix Learning in Trainable Embedding Indexes},\nauthor={Yunjiang Jiang and Han Zhang and Yiming Qiu and Yun Xiao and Bo Long and Wen-Yun Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9-Rfew334N}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "Bl8CQrx2Up4", "original": "EwNCUl1_1Hb", "number": 1311, "cdate": 1632875510313, "mdate": null, "ddate": null, "tcdate": 1632875510313, "tmdate": 1697934834934, "tddate": null, "forum": "Bl8CQrx2Up4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "cosFormer: Rethinking Softmax In Attention", "authorids": ["~Zhen_Qin6", "~Weixuan_Sun1", "~Hui_Deng2", "~Dongxu_Li1", "~Yunshen_Wei1", "~Baohong_Lv1", "~Junjie_Yan3", "~Lingpeng_Kong1", "~Yiran_Zhong1"], "authors": ["Zhen Qin", "Weixuan Sun", "Hui Deng", "Dongxu Li", "Yunshen Wei", "Baohong Lv", "Junjie Yan", "Lingpeng Kong", "Yiran Zhong"], "keywords": ["Linear Transformer", "softmax attention"], "abstract": "Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by approximating the softmax operator. Nevertheless, due to the approximation errors, their performances vary in different tasks/corpus and suffer crucial performance drops when compared with the vanilla softmax attention. In this paper, we propose a linear transformer called cosFormer that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions. cosFormer is based on two key properties of softmax attention: i). non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme that can concentrate the distribution of the attention matrix. As its linear substitute, cosFormer fulfills these properties with a linear operator and a cosine-based distance re-weighting mechanism. Extensive experiments on language modeling and text understanding tasks demonstrate the effectiveness of our method. We further examine our method on long sequences and achieve state-of-the-art performance on the Long-Range Arena benchmark. The source code is available at https://github.com/OpenNLPLab/cosFormer.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "qin|cosformer_rethinking_softmax_in_attention", "pdf": "/pdf/8d5626cec27b9e7c1a7e9c6ad0ba3b4e20fa74f9.pdf", "one-sentence_summary": "A new linear transformer.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.08791/code)", "_bibtex": "@inproceedings{\nzhen2022cosformer,\ntitle={cosFormer: Rethinking Softmax In Attention},\nauthor={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Bl8CQrx2Up4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "htWIlvDcY8", "original": "zLYr1BUJ3U6", "number": 1306, "cdate": 1632875509976, "mdate": null, "ddate": null, "tcdate": 1632875509976, "tmdate": 1676330621383, "tddate": null, "forum": "htWIlvDcY8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations", "authorids": ["~Lingjie_Mei1", "~Jiayuan_Mao1", "~Ziqi_Wang2", "~Chuang_Gan1", "~Joshua_B._Tenenbaum1"], "authors": ["Lingjie Mei", "Jiayuan Mao", "Ziqi Wang", "Chuang Gan", "Joshua B. Tenenbaum"], "keywords": ["Neuro-Symbolic Reasoning", "Concept Learning", "Meta-Learning"], "abstract": "We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images, reading sentences that describe the objects in the scene, and interpreting supplemental sentences that relate the novel concept with other concepts. The learned concepts support downstream applications, such as answering questions by reasoning about unseen images. Our model, namely FALCON, represents individual visual concepts, such as colors and shapes, as axis-aligned boxes in a high-dimensional space (the ``box embedding space''). Given an input image and its paired sentence, our model first resolves the referential expression in the sentence and associates the novel concept with particular objects in the scene. Next, our model interprets supplemental sentences to relate the novel concept with other known concepts, such as ``X has property Y'' or ``X is a kind of Y''. Finally, it infers an optimal box embedding for the novel concept that jointly 1) maximizes the likelihood of the observed instances in the image, and 2) satisfies the relationships between the novel concepts and the known ones. We demonstrate the effectiveness of our model on both synthetic and real-world datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mei|falcon_fast_visual_concept_learning_by_integrating_images_linguistic_descriptions_and_conceptual_relations", "pdf": "/pdf/074074edfbe3b59bf1651653fcf8002522df2588.pdf", "data": "", "_bibtex": "@inproceedings{\nmei2022falcon,\ntitle={{FALCON}: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations},\nauthor={Lingjie Mei and Jiayuan Mao and Ziqi Wang and Chuang Gan and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=htWIlvDcY8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "64trBbOhdGU", "original": "pccpM7giqP_", "number": 1299, "cdate": 1632875509502, "mdate": null, "ddate": null, "tcdate": 1632875509502, "tmdate": 1676330621915, "tddate": null, "forum": "64trBbOhdGU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation", "authorids": ["~Boyan_Li1", "~Hongyao_Tang1", "~YAN_ZHENG1", "~Jianye_HAO1", "~Pengyi_Li1", "w-zhen@nwpu.edu.cn", "~Zhaopeng_Meng1", "~LI_Wang13"], "authors": ["Boyan Li", "Hongyao Tang", "YAN ZHENG", "Jianye HAO", "Pengyi Li", "Zhen Wang", "Zhaopeng Meng", "LI Wang"], "keywords": [], "abstract": "Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL is to convert the hybrid action space into a unified homogeneous action space by discretization or continualization, so that conventional RL algorithms can be applied. However, this ignores the underlying structure of hybrid action space and also induces the scalability issue and additional approximation difficulties, thus leading to degenerated results. In this paper, we propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variantional Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. We evaluate HyAR in a variety of environments with discrete-continuous action space. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|hyar_addressing_discretecontinuous_action_reinforcement_learning_via_hybrid_action_representation", "pdf": "/pdf/21005c7fdccb6d12eff7a0a9deba69320225bd53.pdf", "supplementary_material": "/attachment/46f1b346494c57a2ebf9ab7a03b64c208bdc0c18.zip", "_bibtex": "@inproceedings{\nli2022hyar,\ntitle={Hy{AR}: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation},\nauthor={Boyan Li and Hongyao Tang and YAN ZHENG and Jianye HAO and Pengyi Li and Zhen Wang and Zhaopeng Meng and LI Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=64trBbOhdGU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "DesNW4-5ai9", "original": "SAe90UlVdA", "number": 1282, "cdate": 1632875508417, "mdate": null, "ddate": null, "tcdate": 1632875508417, "tmdate": 1697934838222, "tddate": null, "forum": "DesNW4-5ai9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Transferable Adversarial Attack based on Integrated Gradients", "authorids": ["~Yi_Huang2", "~Adams_Wai-Kin_Kong1"], "authors": ["Yi Huang", "Adams Wai-Kin Kong"], "keywords": [], "abstract": "The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention maps, and smoothing decision surfaces, are commonly used to craft adversarial examples. By tightly integrating the three approaches, we propose a new and simple algorithm named Transferable Attack based on Integrated Gradients (TAIG) in this paper, which can find highly transferable adversarial examples for black-box attacks. Unlike previous methods using multiple computational terms or combining with other methods, TAIG integrates the three approaches into one single term. Two versions of TAIG that compute their integrated gradients on a straight-line path and a random piecewise linear path are studied. Both versions offer strong transferability and can seamlessly work together with the previous methods. Experimental results demonstrate that TAIG outperforms the state-of-the-art methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|transferable_adversarial_attack_based_on_integrated_gradients", "pdf": "/pdf/1586562e5f640c6d1b803ae91c947824cbefde02.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2205.13152/code)", "_bibtex": "@inproceedings{\nhuang2022transferable,\ntitle={Transferable Adversarial Attack based on Integrated Gradients},\nauthor={Yi Huang and Adams Wai-Kin Kong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DesNW4-5ai9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "J7b4BCtDm4", "original": "EU8Uqz6VX0", "number": 1281, "cdate": 1632875508348, "mdate": null, "ddate": null, "tcdate": 1632875508348, "tmdate": 1676330623019, "tddate": null, "forum": "J7b4BCtDm4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How to deal with missing data in supervised deep learning?", "authorids": ["~Niels_Bruun_Ipsen1", "~Pierre-Alexandre_Mattei3", "~Jes_Frellsen1"], "authors": ["Niels Bruun Ipsen", "Pierre-Alexandre Mattei", "Jes Frellsen"], "keywords": [], "abstract": "The issue of missing data in supervised learning has been largely overlooked, especially in the deep learning community. We investigate strategies to adapt neural architectures for handling missing values. Here, we focus on regression and classification problems where the features are assumed to be missing at random. Of particular interest are schemes that allow reusing as-is a neural discriminative architecture. To address supervised deep learning with missing values, we propose to marginalize over missing values in a joint model of covariates and outcomes. Thereby, we leverage both the flexibility of deep generative models to describe the distribution of the covariates and the power of purely discriminative models to make predictions. More precisely, a deep latent variable model can be learned jointly with the discriminative model, using importance-weighted variational inference, essentially using importance sampling to mimick averaging over multiple imputations. In low-capacity regimes, or when the discriminative model has a strong inductive bias, we find that our hybrid generative/discriminative approach generally outperforms single imputations methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ipsen|how_to_deal_with_missing_data_in_supervised_deep_learning", "pdf": "/pdf/7b8b526fb7f29d7a4173ec3ad4d7c9cd67e17a63.pdf", "one-sentence_summary": "Marginalize over missing values in supervised learning using deep latent variable models.", "data": "", "_bibtex": "@inproceedings{\nipsen2022how,\ntitle={How to deal with missing data in supervised deep learning?},\nauthor={Niels Bruun Ipsen and Pierre-Alexandre Mattei and Jes Frellsen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=J7b4BCtDm4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "oxxUMeFwEHd", "original": "3Y_990p4__o", "number": 1258, "cdate": 1632875506903, "mdate": null, "ddate": null, "tcdate": 1632875506903, "tmdate": 1697934840892, "tddate": null, "forum": "oxxUMeFwEHd", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Topological Graph Neural Networks", "authorids": ["~Max_Horn1", "~Edward_De_Brouwer1", "~Michael_Moor1", "~Yves_Moreau2", "~Bastian_Rieck1", "~Karsten_Borgwardt2"], "authors": ["Max Horn", "Edward De Brouwer", "Michael Moor", "Yves Moreau", "Bastian Rieck", "Karsten Borgwardt"], "keywords": ["topology", "persistent homology", "gnn", "graph neural networks", "graph classification", "node classification", "filtrations", "topological data analysis", "tda"], "abstract": "Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler\u2013Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.", "one-sentence_summary": "We describe a new layer for graph neural networks that incorporates multi-scale (ranging from local to global) topological information.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "horn|topological_graph_neural_networks", "pdf": "/pdf/8c27790ab47c50f8661bee7b4b27becf68c62532.pdf", "supplementary_material": "", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2102.07835/code)", "_bibtex": "@inproceedings{\nhorn2022topological,\ntitle={Topological Graph Neural Networks},\nauthor={Max Horn and Edward De Brouwer and Michael Moor and Yves Moreau and Bastian Rieck and Karsten Borgwardt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oxxUMeFwEHd}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "6Pe99Juo9gd", "original": "KXNSGWEvBi", "number": 1255, "cdate": 1632875506767, "mdate": null, "ddate": null, "tcdate": 1632875506767, "tmdate": 1697934840876, "tddate": null, "forum": "6Pe99Juo9gd", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Value Functions from Undirected State-only Experience", "authorids": ["~Matthew_Chang1", "~Arjun_Gupta1", "~Saurabh_Gupta1"], "authors": ["Matthew Chang", "Arjun Gupta", "Saurabh Gupta"], "keywords": ["Reinforcement Learning", "Offline RL", "Offline RL without actions"], "abstract": "This paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i.e. (s,s',r) tuples). We first theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-learning or LAQ, an offline RL method that can learn effective value functions from state-only experience. Latent Action Q-learning (LAQ) learns value functions using Q-learning on discrete latent actions obtained through a latent-variable future prediction model. We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. Value functions learned using LAQ lead to sample efficient acquisition of goal-directed behavior, can be used with domain-specific low-level controllers, and facilitate transfer across embodiments. Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chang|learning_value_functions_from_undirected_stateonly_experience", "pdf": "/pdf/165580641e9ae16c0919513d98c7a95e8f701683.pdf", "one-sentence_summary": "We present a method for offline value learning without action labels.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.12458/code)", "_bibtex": "@inproceedings{\nchang2022learning,\ntitle={Learning Value Functions from Undirected State-only Experience},\nauthor={Matthew Chang and Arjun Gupta and Saurabh Gupta},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6Pe99Juo9gd}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "_l_QjPGN5ye", "original": "EUgNjl6_C1F", "number": 1239, "cdate": 1632875505811, "mdate": null, "ddate": null, "tcdate": 1632875505811, "tmdate": 1676330625257, "tddate": null, "forum": "_l_QjPGN5ye", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models", "authorids": ["~Cassidy_Laidlaw1", "~Anca_Dragan1"], "authors": ["Cassidy Laidlaw", "Anca Dragan"], "keywords": ["human model", "boltzmann rationality", "suboptimality", "HRI", "human-robot collaboration", "generative models", "reinforcement learning", "deep RL"], "abstract": "Models of human behavior for prediction and collaboration tend to fall into two categories: ones that learn from large amounts of data via imitation learning, and ones that assume human behavior to be noisily-optimal for some reward function. The former are very useful, but only when it is possible to gather a lot of human data in the target environment and distribution. The advantage of the latter type, which includes Boltzmann rationality, is the ability to make accurate predictions in new environments without extensive data when humans are actually close to optimal. However, these models fail when humans exhibit systematic suboptimality, i.e. when their deviations from optimal behavior are not independent, but instead consistent over time. Our key insight is that systematic suboptimality can be modeled by predicting policies, which couple action choices over time, instead of trajectories. We introduce the Boltzmann policy distribution (BPD), which serves as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but we leverage tools from generative and sequence modeling to enable efficient sampling and inference. We show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "laidlaw|the_boltzmann_policy_distribution_accounting_for_systematic_suboptimality_in_human_models", "pdf": "/pdf/441fdfcfbe0339bb96b0292455eb5acb04f4676e.pdf", "one-sentence_summary": "We propose modeling human behavior with a Boltzmann distribution over policies\u2014not trajectories\u2014and show it is more accurate and useful.", "_bibtex": "@inproceedings{\nlaidlaw2022the,\ntitle={The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models},\nauthor={Cassidy Laidlaw and Anca Dragan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_l_QjPGN5ye}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "ahi2XSHpAUZ", "original": "n1Bj5qwIVP6", "number": 1233, "cdate": 1632875505405, "mdate": null, "ddate": null, "tcdate": 1632875505405, "tmdate": 1697934843683, "tddate": null, "forum": "ahi2XSHpAUZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection", "authorids": ["~Liang_Peng3", "~Senbo_Yan1", "~Boxi_Wu1", "~Zheng_Yang2", "~Xiaofei_He2", "~Deng_Cai4"], "authors": ["Liang Peng", "Senbo Yan", "Boxi Wu", "Zheng Yang", "Xiaofei He", "Deng Cai"], "keywords": ["Computer vision", "monocular 3D object detection", "weakly supervised"], "abstract": "\tMonocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Codes are available at https://github.com/SPengLiang/WeakM3D.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "peng|weakm3d_towards_weakly_supervised_monocular_3d_object_detection", "pdf": "/pdf/0c6d24738da03503a65d924f29e0a8a8a96e1b49.pdf", "one-sentence_summary": "This paper explores the weakly supervised monocular 3D detection to dispense with the reliance on 3D box labels.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.08332/code)", "_bibtex": "@inproceedings{\npeng2022weakmd,\ntitle={WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection},\nauthor={Liang Peng and Senbo Yan and Boxi Wu and Zheng Yang and Xiaofei He and Deng Cai},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ahi2XSHpAUZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 6}}, {"id": "7gE9V9GBZaI", "original": "y2tn7o5Ja3v", "number": 1232, "cdate": 1632875505337, "mdate": null, "ddate": null, "tcdate": 1632875505337, "tmdate": 1697934843681, "tddate": null, "forum": "7gE9V9GBZaI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Exploring Memorization in Adversarial Training", "authorids": ["~Yinpeng_Dong2", "~Ke_Xu7", "~Xiao_Yang4", "~Tianyu_Pang1", "~Zhijie_Deng1", "~Hang_Su3", "~Jun_Zhu2"], "authors": ["Yinpeng Dong", "Ke Xu", "Xiao Yang", "Tianyu Pang", "Zhijie Deng", "Hang Su", "Jun Zhu"], "keywords": ["Adversarial examples", "adversarial training", "memorization", "robust overfitting"], "abstract": "Deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we explore the memorization effect in adversarial training (AT) for promoting a deeper understanding of model capacity, convergence, generalization, and especially robust overfitting of the adversarially trained models. We first demonstrate that deep networks have sufficient capacity to memorize adversarial examples of training data with completely random labels, but not all AT algorithms can converge under the extreme circumstance. Our study of AT with random labels motivates further analyses on the convergence and generalization of AT. We find that some AT approaches suffer from a gradient instability issue and the recently suggested complexity measures cannot explain robust generalization by considering models trained on random labels. Furthermore, we identify a significant drawback of memorization in AT that it could result in robust overfitting. We then propose a new mitigation algorithm motivated by detailed memorization analyses. Extensive experiments on various datasets validate the effectiveness of the proposed method. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dong|exploring_memorization_in_adversarial_training", "pdf": "/pdf/c33e24a96109ee75779a6d6cbb2bb2f7df3ddadd.pdf", "one-sentence_summary": "This paper explores the memorization effect in adversarial training and analyzes its connections with model capacity, convergence, generalization, and especially robust overfitting of the adversarially trained models.", "supplementary_material": "/attachment/8a1e51f3193fe605cadb87716a17c30a7f849603.zip", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.01606/code)", "_bibtex": "@inproceedings{\ndong2022exploring,\ntitle={Exploring Memorization in Adversarial Training},\nauthor={Yinpeng Dong and Ke Xu and Xiao Yang and Tianyu Pang and Zhijie Deng and Hang Su and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7gE9V9GBZaI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "pETy-HVvGtt", "original": "zoZeub8FN8h", "number": 1228, "cdate": 1632875505074, "mdate": null, "ddate": null, "tcdate": 1632875505074, "tmdate": 1676330626052, "tddate": null, "forum": "pETy-HVvGtt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Disentanglement Analysis with Partial Information Decomposition", "authorids": ["~Seiya_Tokui1", "sato@g.ecc.u-tokyo.ac.jp"], "authors": ["Seiya Tokui", "Issei Sato"], "keywords": ["disentangled representations", "variational autoencoders", "deep generative models"], "abstract": "We propose a framework to analyze how multivariate representations disentangle ground-truth generative factors. A quantitative analysis of disentanglement has been based on metrics designed to compare how one variable explains each generative factor. Current metrics, however, may fail to detect entanglement that involves more than two variables, e.g., representations that duplicate and rotate generative factors in high dimensional spaces. In this work, we establish a framework to analyze information sharing in a multivariate representation with Partial Information Decomposition and propose a new disentanglement metric. This framework enables us to understand disentanglement in terms of uniqueness, redundancy, and synergy. We develop an experimental protocol to assess how increasingly entangled representations are evaluated with each metric and confirm that the proposed metric correctly responds to entanglement. Through experiments on variational autoencoders, we find that models with similar disentanglement scores have a variety of characteristics in entanglement, for each of which a distinct strategy may be required to obtain a disentangled representation.", "one-sentence_summary": "We establish a framework to analyze information sharing in a multivariate representation with Partial Information Decomposition and propose a new disentanglement metric.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tokui|disentanglement_analysis_with_partial_information_decomposition", "pdf": "/pdf/9951cc9e8f5ebea47555522be7bf51da71054709.pdf", "_bibtex": "@inproceedings{\ntokui2022disentanglement,\ntitle={Disentanglement Analysis with Partial Information Decomposition},\nauthor={Seiya Tokui and Issei Sato},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pETy-HVvGtt}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "U8pbd00cCWB", "original": "wkXDCIzl_Vg", "number": 1225, "cdate": 1632875504865, "mdate": null, "ddate": null, "tcdate": 1632875504865, "tmdate": 1676330626290, "tddate": null, "forum": "U8pbd00cCWB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Differentiable Gradient Sampling for Learning Implicit 3D Scene Reconstructions from a Single Image", "authorids": ["~Shizhan_Zhu1", "~Sayna_Ebrahimi1", "~Angjoo_Kanazawa1", "~Trevor_Darrell2"], "authors": ["Shizhan Zhu", "Sayna Ebrahimi", "Angjoo Kanazawa", "Trevor Darrell"], "keywords": [], "abstract": "Implicit shape models are promising 3D representations for modeling arbitrary locations, with Signed Distance Functions (SDFs) particularly suitable for clear mesh surface reconstruction. Existing approaches for single object reconstruction impose supervision signals based on the loss of the signed distance value from all locations in a scene, posing difficulties when extending to real-world scenarios. The spatial gradient of the signed distance field, rather than the SDF value itself, has not been typically employed as a source of supervision for single-view reconstruction, in part due to the difficulties of differentiable sampling a spatial gradient from the feature map. In this study, we derive a novel closed-form gradient sampling solution for Differentialble Gradient Sampling (DGS) that enables backpropagation of the loss of the spatial gradient back to the feature map pixels, thus allowing the imposition of the loss efficiently on the spatial gradient. As a result, we achieve high-quality single view indoor scene reconstruction results learning directly from a real-world scanned dataset (e.g. ScannetV2). Our model also performs well when generalizing to unseen images downloaded directly from the internet (Fig. 1). We comfortably advanced the state-of-the-art results with several established datasets including ShapeNet and ScannetV2; extensive quantitative analysis confirmed that our proposed DGS module plays an essential role in achieving this performance improvement. Full codes are available in MaskedURL.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|differentiable_gradient_sampling_for_learning_implicit_3d_scene_reconstructions_from_a_single_image", "pdf": "/pdf/8c96aef9f871676b7212944a243a58434b44fc1a.pdf", "data": "", "_bibtex": "@inproceedings{\nzhu2022differentiable,\ntitle={Differentiable Gradient Sampling for Learning Implicit 3D Scene Reconstructions from a Single Image},\nauthor={Shizhan Zhu and Sayna Ebrahimi and Angjoo Kanazawa and Trevor Darrell},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=U8pbd00cCWB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "3ILxkQ7yElm", "original": "wpg_4rbTN4Q", "number": 1212, "cdate": 1632875504059, "mdate": null, "ddate": null, "tcdate": 1632875504059, "tmdate": 1697934846265, "tddate": null, "forum": "3ILxkQ7yElm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Continuous Environment Fields via Implicit Functions", "authorids": ["~Xueting_Li1", "~Shalini_De_Mello1", "~Xiaolong_Wang3", "~Ming-Hsuan_Yang1", "~Jan_Kautz1", "~Sifei_Liu2"], "authors": ["Xueting Li", "Shalini De Mello", "Xiaolong Wang", "Ming-Hsuan Yang", "Jan Kautz", "Sifei Liu"], "keywords": ["Continuous Scene Representation", "Implicit Neural Networks"], "abstract": "   We propose a novel scene representation that encodes reaching distance -- the distance between any position in the scene to a goal along a feasible trajectory. We demonstrate that this environment field representation can directly guide the dynamic behaviors of agents in 2D mazes or 3D indoor scenes. Our environment field is a continuous representation and learned via a neural implicit function using discretely sampled training data. We showcase its application for agent navigation in 2D mazes, and human trajectory prediction in 3D indoor environments. To produce physically plausible and natural trajectories for humans, we additionally learn a generative model that predicts regions where humans commonly appear, and enforce the environment field to be defined within such regions. Extensive experiments demonstrate that the proposed method can generate both feasible and plausible trajectories efficiently and accurately.", "pdf": "/pdf/bb9cdfc2e84adef6cb9610f21715cf048acaeaa4.pdf", "one-sentence_summary": "We propose a novel scene representation that can dynamically change behaviors of agents inside the scene.", "supplementary_material": "/attachment/0417ca1a8105d1321ff51501f619f30abbe9550b.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|learning_continuous_environment_fields_via_implicit_functions", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.13997/code)", "_bibtex": "@inproceedings{\nli2022learning,\ntitle={Learning Continuous Environment Fields via Implicit Functions},\nauthor={Xueting Li and Sifei Liu and Shalini De Mello and Xiaolong Wang and Ming-Hsuan Yang and Jan Kautz},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3ILxkQ7yElm}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "F5Em8ASCosV", "original": "Eh6C_OjVyX", "number": 1203, "cdate": 1632875503454, "mdate": null, "ddate": null, "tcdate": 1632875503454, "tmdate": 1676330627561, "tddate": null, "forum": "F5Em8ASCosV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Causal Contextual Bandits with Targeted Interventions", "authorids": ["~Chandrasekar_Subramanian1", "~Balaraman_Ravindran1"], "authors": ["Chandrasekar Subramanian", "Balaraman Ravindran"], "keywords": ["causality", "contextual bandits", "causal inference", "bandits"], "abstract": "We study a contextual bandit setting where the learning agent has the ability to perform interventions on targeted subsets of the population, apart from possessing qualitative causal side-information. This novel formalism captures intricacies in real-world scenarios such as software product experimentation where targeted experiments can be conducted. However, this fundamentally changes the set of options that the agent has, compared to standard contextual bandit settings, necessitating new techniques. This is also the first work that integrates causal side-information in a contextual bandit setting, where the agent aims to learn a policy that maps contexts to arms (as opposed to just identifying one best arm). We propose a new algorithm, which we show empirically performs better than baselines on experiments that use purely synthetic data and on real world-inspired experiments. We also prove a bound on regret that theoretically guards performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "subramanian|causal_contextual_bandits_with_targeted_interventions", "pdf": "/pdf/b77f8b2b6b86bc81bdb006750510124a95769c01.pdf", "one-sentence_summary": "A new, more realistic, formalism of contextual bandits involving causal side-information and targeted interventions, along with a novel algorithm that exploits features of the new setting such as information leakage to learn good policies quickly.", "supplementary_material": "/attachment/da21059b96b7d6d0110fcc48f3cee6ee8fc9bb80.zip", "_bibtex": "@inproceedings{\nsubramanian2022causal,\ntitle={Causal Contextual Bandits with Targeted Interventions},\nauthor={Chandrasekar Subramanian and Balaraman Ravindran},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=F5Em8ASCosV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "xS8AMYiEav3", "original": "8U9JM3a7F1m", "number": 1193, "cdate": 1632875502763, "mdate": null, "ddate": null, "tcdate": 1632875502763, "tmdate": 1676330628024, "tddate": null, "forum": "xS8AMYiEav3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sound and Complete Neural Network Repair with Minimality and Locality Guarantees", "authorids": ["~Feisi_Fu1", "~Wenchao_Li1"], "authors": ["Feisi Fu", "Wenchao Li"], "keywords": ["Neural Network Repair"], "abstract": "We present a novel methodology for repairing neural networks that use ReLU activation functions. Unlike existing methods that rely on modifying the weights of a neural network which can induce a global change in the function space, our approach applies only a localized change in the function space while still guaranteeing the removal of the buggy behavior. By leveraging the piecewise linear nature of ReLU networks, our approach can efficiently construct a patch network tailored to the linear region where the buggy input resides, which when combined with the original network, provably corrects the behavior on the buggy input. Our method is both sound and complete -- the repaired network is guaranteed to fix the buggy input, and a patch is guaranteed to be found for any buggy input. Moreover, our approach preserves the continuous piecewise linear nature of ReLU networks, automatically generalizes the repair to all the points including other undetected buggy inputs inside the repair region, is minimal in terms of changes in the function space, and guarantees that outputs on inputs away from the repair region are unaltered. On several benchmarks, we show that our approach significantly outperforms existing methods in terms of locality and limiting negative side effects.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fu|sound_and_complete_neural_network_repair_with_minimality_and_locality_guarantees", "pdf": "/pdf/33795da3be90ef32dd95bcf374b18d0f4f957766.pdf", "_bibtex": "@inproceedings{\nfu2022sound,\ntitle={Sound and Complete Neural Network Repair with Minimality and Locality Guarantees},\nauthor={Feisi Fu and Wenchao Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xS8AMYiEav3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "JJxiD-kg-oK", "original": "PHWyaUKdtS2", "number": 1191, "cdate": 1632875502631, "mdate": null, "ddate": null, "tcdate": 1632875502631, "tmdate": 1676330628110, "tddate": null, "forum": "JJxiD-kg-oK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Blaschke Product Neural Networks (BPNN): A Physics-Infused Neural Network for Phase Retrieval of Meromorphic Functions", "authorids": ["~Juncheng_Dong1", "~Simiao_Ren2", "~Yang_Deng3", "~Omar_Khatib1", "~Jordan_Malof1", "~Mohammadreza_Soltani1", "~Willie_Padilla1", "~Vahid_Tarokh1"], "authors": ["Juncheng Dong", "Simiao Ren", "Yang Deng", "Omar Khatib", "Jordan Malof", "Mohammadreza Soltani", "Willie Padilla", "Vahid Tarokh"], "keywords": ["Blaschke Product", "Neural Network", "Phase Retrieval", "Metamaterial", "Meromorphic Functions"], "abstract": "Numerous physical systems are described by ordinary or partial differential equations whose solutions are given by holomorphic or meromorphic functions in the complex domain. In many cases, only the magnitude of these functions are observed on various points on the purely imaginary $j\\omega$-axis since coherent measurement of their phases is often expensive.  However, it is desirable to retrieve the lost phases from the magnitudes when possible. To this end, we propose a physics-infused deep neural network based on the Blaschke products for phase retrieval. Inspired by the Helson and Sarason Theorem,  we recover coefficients of a rational function of Blaschke products using a Blaschke Product Neural Network (BPNN), based upon the magnitude observations as input. The resulting rational function is then used for phase retrieval. We compare the BPNN to conventional deep neural networks (NNs) on several phase retrieval problems, comprising both synthetic and contemporary real-world problems (e.g., metamaterials for which data collection requires substantial expertise and is time consuming). On each phase retrieval problem, we compare against a population of conventional NNs of varying size and hyperparameter settings. Even without any hyper-parameter search, we find that BPNNs consistently outperform the population of optimized NNs in scarce data scenarios, and do so despite being much smaller models. The results can in turn be applied to calculate the refractive index of metamaterials, which is an important problem in emerging areas of material science.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dong|blaschke_product_neural_networks_bpnn_a_physicsinfused_neural_network_for_phase_retrieval_of_meromorphic_functions", "pdf": "/pdf/76b216b1b8df2b17e46022888e66cde431a1fb26.pdf", "supplementary_material": "/attachment/b011a4845144730cb44930f023082051046c0eaf.zip", "_bibtex": "@inproceedings{\ndong2022blaschke,\ntitle={Blaschke Product Neural Networks ({BPNN}): A Physics-Infused Neural Network for Phase Retrieval of Meromorphic Functions},\nauthor={Juncheng Dong and Simiao Ren and Yang Deng and Omar Khatib and Jordan Malof and Mohammadreza Soltani and Willie Padilla and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JJxiD-kg-oK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "rFbR4Fv-D6-", "original": "YHUDV2J_4j", "number": 1189, "cdate": 1632875502497, "mdate": null, "ddate": null, "tcdate": 1632875502497, "tmdate": 1697934848985, "tddate": null, "forum": "rFbR4Fv-D6-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Automated Self-Supervised Learning for Graphs", "authorids": ["~Wei_Jin4", "~Xiaorui_Liu1", "~Xiangyu_Zhao1", "~Yao_Ma3", "~Neil_Shah2", "~Jiliang_Tang1"], "authors": ["Wei Jin", "Xiaorui Liu", "Xiangyu Zhao", "Yao Ma", "Neil Shah", "Jiliang Tang"], "keywords": ["Self-supervised learning", "Graph neural networks", "AutoML"], "abstract": "Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations. Many pretext tasks, or loss functions have been designed from distinct perspectives. However, we observe that different pretext tasks affect downstream tasks differently cross datasets, which suggests that searching pretext tasks is crucial for graph self-supervised learning.  Different from existing works focusing on designing single pretext tasks, this work aims to investigate how to automatically leverage multiple pretext tasks effectively. Nevertheless, evaluating representations derived from multiple pretext tasks without direct access to ground truth labels makes this problem challenging. To address this obstacle, we make use of a key principle of many real-world graphs, i.e., homophily, or the principle that ``like attracts like,'' as the guidance to effectively search various self-supervised pretext tasks. We provide theoretical understanding and empirical evidence to justify the flexibility of homophily in this  search task. Then we propose the AutoSSL framework which can automatically search over combinations of various self-supervised tasks. By evaluating the framework on 7 real-world datasets, our experimental results show that AutoSSL can significantly boost the performance on downstream tasks including node clustering and node classification compared with training under individual tasks. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jin|automated_selfsupervised_learning_for_graphs", "pdf": "/pdf/f5713df2ae7c42f22843d36f1aae8a36c6010b6d.pdf", "one-sentence_summary": "An automated self-supervised learning algorithm for graph neural networks.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.05470/code)", "_bibtex": "@inproceedings{\njin2022automated,\ntitle={Automated Self-Supervised Learning for Graphs},\nauthor={Wei Jin and Xiaorui Liu and Xiangyu Zhao and Yao Ma and Neil Shah and Jiliang Tang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rFbR4Fv-D6-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "m8uJvVgwRci", "original": "ZukbQIRPMU9", "number": 1188, "cdate": 1632875502429, "mdate": null, "ddate": null, "tcdate": 1632875502429, "tmdate": 1676330628413, "tddate": null, "forum": "m8uJvVgwRci", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Creating Training Sets via Weak Indirect Supervision", "authorids": ["~Jieyu_Zhang1", "~Bohan_Wang1", "~Xiangchen_Song1", "~Yujing_Wang1", "~Yaming_Yang1", "~Jing_Bai3", "~Alexander_Ratner1"], "authors": ["Jieyu Zhang", "Bohan Wang", "Xiangchen Song", "Yujing Wang", "Yaming Yang", "Jing Bai", "Alexander Ratner"], "keywords": ["weak supervision", "data programming", "training label synthesis"], "abstract": "Creating labeled training sets has become one of the major roadblocks in machine learning. To address this, recent Weak Supervision (WS) frameworks synthesize training labels from multiple potentially noisy supervision sources. However, existing frameworks are restricted to supervision sources that share the same output space as the target task. To extend the scope of usable sources, we formulate Weak Indirect Supervision (WIS), a new research problem for automatically synthesizing training labels based on indirect supervision sources that have different output label spaces. To overcome the challenge of mismatched output spaces, we develop a probabilistic modeling approach, PLRM, which uses user-provided label relations to model and leverage indirect supervision sources. Moreover, we provide a theoretically-principled test of the distinguishability of PLRM for unseen labels, along with an generalization bound. On both image and text classification tasks as well as an industrial advertising application, we demonstrate the advantages of PLRM by outperforming baselines by a margin of 2%-9%.", "one-sentence_summary": "In this work, we present a new weak supervision paradigm which automatically creates training sets for training a machine learning model given unlabeled dataset and indirect supervision sources.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|creating_training_sets_via_weak_indirect_supervision", "pdf": "/pdf/46411f07ed6843d6bacf30073524429547e8d250.pdf", "data": "", "_bibtex": "@inproceedings{\nzhang2022creating,\ntitle={Creating Training Sets via Weak Indirect Supervision},\nauthor={Jieyu Zhang and Bohan Wang and Xiangchen Song and Yujing Wang and Yaming Yang and Jing Bai and Alexander Ratner},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=m8uJvVgwRci}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "aTzMi4yV_RO", "original": "vyUhm8URTxs", "number": 1180, "cdate": 1632875501885, "mdate": null, "ddate": null, "tcdate": 1632875501885, "tmdate": 1676330628910, "tddate": null, "forum": "aTzMi4yV_RO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs", "authorids": ["~Jaewoong_Choi1", "~Junho_Lee2", "~Changyeon_Yoon1", "~Jung_Ho_Park1", "~Geonho_Hwang1", "~Myungjoo_Kang1"], "authors": ["Jaewoong Choi", "Junho Lee", "Changyeon Yoon", "Jung Ho Park", "Geonho Hwang", "Myungjoo Kang"], "keywords": ["generative adversarial network", "disentanglement", "semantic factorization", "latent space control", "image manipulation", "grassmannian"], "abstract": "The discovery of the disentanglement properties of the latent space in GANs motivated a lot of research to find the semantically meaningful directions on it. In this paper, we suggest that the disentanglement property is closely related to the geometry of the latent space. In this regard, we propose an unsupervised method for finding the semantic-factorizing directions on the intermediate latent space of GANs based on the local geometry. Intuitively, our proposed method, called $\\textit{Local Basis}$, finds the principal variation of the latent space in the neighborhood of the base latent variable. Experimental results show that the local principal variation corresponds to the semantic factorization and traversing along it provides strong robustness to image traversal. Moreover, we suggest an explanation for the limited success in finding the global traversal directions in the latent space, especially $\\mathcal{W}$-space of StyleGAN2. We show that $\\mathcal{W}$-space is warped globally by comparing the local geometry, discovered from Local Basis, through the metric on Grassmannian Manifold. The global warpage implies that the latent space is not well-aligned globally and therefore the global traversal directions are bound to show limited success on it.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "choi|do_not_escape_from_the_manifold_discovering_the_local_coordinates_on_the_latent_space_of_gans", "pdf": "/pdf/35267777d7fe835879ddec7c83aecd9d170d070d.pdf", "one-sentence_summary": "We propose a method for finding local-geometry-aware traversal directions on the intermediate latent space of Generative Adversarial Networks (GANs).", "supplementary_material": "/attachment/9bb84002ff6250be9187cacf3f1f70785c8e90f5.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nchoi2022do,\ntitle={Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of {GAN}s},\nauthor={Jaewoong Choi and Junho Lee and Changyeon Yoon and Jung Ho Park and Geonho Hwang and Myungjoo Kang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aTzMi4yV_RO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "HObMhrCeAAF", "original": "5QdHm9YqxRh", "number": 1177, "cdate": 1632875501680, "mdate": null, "ddate": null, "tcdate": 1632875501680, "tmdate": 1697934850894, "tddate": null, "forum": "HObMhrCeAAF", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GradSign: Model Performance Inference with Theoretical Insights", "authorids": ["~Zhihao_Zhang2", "~Zhihao_Jia1"], "authors": ["Zhihao Zhang", "Zhihao Jia"], "keywords": ["Model Performance Inference", "Optimization Landscape", "NAS"], "abstract": "A key challenge in neural architecture search (NAS) is quickly inferring the predictive performance of a broad spectrum of networks to discover statistically accurate and computationally efficient ones.  We refer to this task as model performance inference (MPI). The current practice for efficient MPI is gradient-based methods that leverage the gradients of a network at initialization to infer its performance. However, existing gradient-based methods rely only on heuristic metrics and lack the necessary theoretical foundations to consolidate their designs.  We propose GradSign, an accurate, simple, and flexible metric for model performance inference with theoretical insights. The key idea behind GradSign is a quantity \u03a8 to analyze the sample-wise optimization landscape of different networks. Theoretically, we show that  \u03a8 is an upper bound for both the training and true population losses of a neural network under reasonable assumptions. However, it is computationally prohibitive to directly calculate \u03a8 for modern neural networks. To\naddress this challenge, we design GradSign, an accurate and simple approximation of \u03a8 using the gradients of a network evaluated at a random initialization state. Evaluation on seven NAS benchmarks across three training datasets shows that GradSign generalizes well to real-world networks and consistently outperforms state-of-the-art gradient-based methods for MPI evaluated by Spearman\u2019s \u03c1 and Kendall\u2019s Tau.  Additionally, we integrate GradSign into four existing NAS algorithms and show that the GradSign-assisted NAS algorithms outperform their vanilla counterparts by improving the accuracies of best-discovered networks by up to 0.3%, 1.1%, and 1.0% on three real-world tasks. Code is available at https://github.com/JackFram/GradSign", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|gradsign_model_performance_inference_with_theoretical_insights", "pdf": "/pdf/f7fb958fd531503a7fafea7cd2c68e5507b12e75.pdf", "one-sentence_summary": "Model performance inference inspired by sample-wise optimization landscape analysis", "supplementary_material": "/attachment/4a7b931e285817911856fbf0506272c72ed3d847.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.08616/code)", "_bibtex": "@inproceedings{\nzhang2022gradsign,\ntitle={GradSign: Model Performance Inference with Theoretical Insights},\nauthor={Zhihao Zhang and Zhihao Jia},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HObMhrCeAAF}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "hpBTIv2uy_E", "original": "zWG6aBkWyC8", "number": 1161, "cdate": 1632875500568, "mdate": null, "ddate": null, "tcdate": 1632875500568, "tmdate": 1676330630068, "tddate": null, "forum": "hpBTIv2uy_E", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks", "authorids": ["~Eli_Chien1", "~Chao_Pan2", "~Jianhao_Peng1", "~Olgica_Milenkovic1"], "authors": ["Eli Chien", "Chao Pan", "Jianhao Peng", "Olgica Milenkovic"], "keywords": ["Hypergraph neural networks", "multiset functions", "deep sets", "set transformer"], "abstract": "Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable the efficient processing of hypergraph data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification tasks. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on benchmarking datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. The proposed AllSet framework also for the first time integrates Deep Sets and Set Transformers with hypergraph neural networks for the purpose of learning multiset functions and therefore allows for significant modeling flexibility and high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that our method has the unique ability to either match or outperform all other hypergraph neural networks across the tested datasets: As an example, the performance improvements over existing methods and a new method based on heterogeneous graph neural networks are close to $4\\%$ on the Yelp and Zoo datasets, and $3\\%$ on the Walmart dataset.", "one-sentence_summary": "We propose a multiset function framework for hypergraph neural networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chien|you_are_allset_a_multiset_function_framework_for_hypergraph_neural_networks", "pdf": "/pdf/bb6cb5f99fd0b8d93c19d31dbda4a1b5eeda57a3.pdf", "supplementary_material": "/attachment/420eafec1d6aa1ab58d9af3fa745a9e5bf43293f.zip", "data": "", "_bibtex": "@inproceedings{\nchien2022you,\ntitle={You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks},\nauthor={Eli Chien and Chao Pan and Jianhao Peng and Olgica Milenkovic},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hpBTIv2uy_E}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "KmtVD97J43e", "original": "zIecR2FSQze", "number": 1157, "cdate": 1632875500295, "mdate": null, "ddate": null, "tcdate": 1632875500295, "tmdate": 1676330630394, "tddate": null, "forum": "KmtVD97J43e", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Synchromesh: Reliable Code Generation from Pre-trained Language Models", "authorids": ["~Gabriel_Poesia1", "~Alex_Polozov1", "~Vu_Le2", "~Ashish_Tiwari2", "gsoares@microsoft.com", "~Christopher_Meek1", "~Sumit_Gulwani1"], "authors": ["Gabriel Poesia", "Alex Polozov", "Vu Le", "Ashish Tiwari", "Gustavo Soares", "Christopher Meek", "Sumit Gulwani"], "keywords": ["program synthesis", "language models", "code generation"], "abstract": "Large pre-trained language models have been used to generate code, providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation. Synchromesh comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite of differences in surface natural language features. Then, Synchromesh feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scoping and typing rules. Across all languages, we observe complementary gains from CSD and TST in prediction accuracy and in effectively preventing parsing, type and run-time errors.", "one-sentence_summary": "A framework to generate programs from large pre-trained language models (e.g. GPT-3, Codex) while satisfying syntactic and semantic constraints.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "poesia|synchromesh_reliable_code_generation_from_pretrained_language_models", "pdf": "/pdf/6ff098333e70afec46ebe0a90baf01256cacc43c.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\npoesia2022synchromesh,\ntitle={Synchromesh: Reliable Code Generation from Pre-trained Language Models},\nauthor={Gabriel Poesia and Alex Polozov and Vu Le and Ashish Tiwari and Gustavo Soares and Christopher Meek and Sumit Gulwani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KmtVD97J43e}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "tFgdrQbbaa", "original": "8jtrgaatOrh", "number": 1156, "cdate": 1632875500228, "mdate": null, "ddate": null, "tcdate": 1632875500228, "tmdate": 1676330630484, "tddate": null, "forum": "tFgdrQbbaa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting", "authorids": ["~Ryo_Karakida2", "s.akaho@aist.go.jp"], "authors": ["Ryo Karakida", "Shotaro Akaho"], "keywords": ["continual learning", "neural tangent kernel", "statistical mechanics"], "abstract": "Sequential training from task to task is becoming one of the major objects in deep learning applications such as continual learning and transfer learning. Nevertheless, it remains unclear under what conditions the trained model's performance improves or deteriorates. To deepen our understanding of sequential training, this study provides a theoretical analysis of generalization performance in a solvable case of continual learning.  We consider neural networks in the neural tangent kernel (NTK) regime that continually learn target functions from task to task, and investigate the generalization by using an established statistical mechanical analysis of kernel ridge-less regression. We first show characteristic transitions from positive to negative transfer. More similar targets above a specific critical value can achieve positive knowledge transfer for the subsequent task while catastrophic forgetting occurs even with very similar targets. Next, we investigate a variant of continual learning which supposes the same target function in multiple tasks. Even for the same target,  the trained model shows some transfer and forgetting depending on the sample size of each task.  We can guarantee that the generalization error monotonically decreases from task to task for equal sample sizes while unbalanced sample sizes  deteriorate the generalization. We respectively refer to these improvement and deterioration as self-knowledge transfer and forgetting, and empirically confirm them in realistic training of deep neural networks as well. ", "one-sentence_summary": "We analyze the generalization performance of continual learning in the NTK regime and identify key properties of knowledge transfer and forgetting. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "karakida|learning_curves_for_continual_learning_in_neural_networks_selfknowledge_transfer_and_forgetting", "pdf": "/pdf/496372cd3a258b357f5e999076aa2f2a991ca5c8.pdf", "_bibtex": "@inproceedings{\nkarakida2022learning,\ntitle={Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting},\nauthor={Ryo Karakida and Shotaro Akaho},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tFgdrQbbaa}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "xLfAgCroImw", "original": "ndKXqd4MfPT", "number": 1155, "cdate": 1632875500162, "mdate": null, "ddate": null, "tcdate": 1632875500162, "tmdate": 1676330630584, "tddate": null, "forum": "xLfAgCroImw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Energy-Based Learning for Cooperative Games, with Applications to Valuation Problems in Machine Learning", "authorids": ["~Yatao_Bian1", "~Yu_Rong1", "~Tingyang_Xu1", "~Jiaxiang_Wu1", "~Andreas_Krause1", "~Junzhou_Huang2"], "authors": ["Yatao Bian", "Yu Rong", "Tingyang Xu", "Jiaxiang Wu", "Andreas Krause", "Junzhou Huang"], "keywords": ["Valuation problems", "Shapley value", "Model interpretation", "Data valuation", "Enegy-based learning", "Attribution-based feature interpretation", "Model valuation for ensembles", "Feature attributions"], "abstract": "Valuation problems, such as feature interpretation, data valuation and model valuation for ensembles, become increasingly more important in many machine learning applications. Such problems are commonly solved by well-known game-theoretic criteria, such as Shapley value or Banzhaf value. In this work, we present a novel energy-based treatment for cooperative games, with a theoretical justification by the maximum entropy framework. Surprisingly, by conducting variational inference of the energy-based model, we recover various game-theoretic valuation criteria through conducting one-step fixed point iteration  for maximizing the mean-field ELBO objective. This observation also verifies the rationality of existing criteria, as they are all attempting to  decouple the  correlations  among  the  players  through the  mean-field approach. By running fixed point iteration for multiple steps, we achieve a trajectory of the valuations, among which we define the valuation with the best conceivable decoupling error as the Variational Index. We prove that under uniform initializations,  these variational valuations all satisfy a set of game-theoretic axioms. We experimentally demonstrate that the proposed Variational Index enjoys lower decoupling error and better valuation performance  on certain synthetic and real-world valuation problems. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bian|energybased_learning_for_cooperative_games_with_applications_to_valuation_problems_in_machine_learning", "pdf": "/pdf/52a282d00203dfc39e85be560be1315b7d3ea1e3.pdf", "one-sentence_summary": "An energy-based treatment for cooperative games provides a decoupling perspective for Shapley value and others. ", "supplementary_material": "/attachment/4f9cff66320116f1d78f4b52ce50e0d5ac90b01c.zip", "_bibtex": "@inproceedings{\nbian2022energybased,\ntitle={Energy-Based Learning for Cooperative Games, with Applications to Valuation Problems in Machine Learning},\nauthor={Yatao Bian and Yu Rong and Tingyang Xu and Jiaxiang Wu and Andreas Krause and Junzhou Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xLfAgCroImw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "tyrJsbKAe6", "original": "KQTX24VjSWf", "number": 1154, "cdate": 1632875500089, "mdate": null, "ddate": null, "tcdate": 1632875500089, "tmdate": 1676330630687, "tddate": null, "forum": "tyrJsbKAe6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage", "authorids": ["~Masatoshi_Uehara1", "~Wen_Sun1"], "authors": ["Masatoshi Uehara", "Wen Sun"], "keywords": ["Reinforcement learning Theory", "Offline reinforcement learning", "PAC Bounds"], "abstract": "We study model-based offline Reinforcement Learning with general function approximation without a full coverage assumption on the offline data distribution. We present an algorithm named Constrained Pessimistic Policy Optimization (CPPO) which leverages a general function class and uses a constraint over the models to encode pessimism. Under the assumption that the ground truth model belongs to our function class (i.e., realizability in the function class), CPPO has a PAC guarantee with offline data only providing partial coverage, i.e., it can learn a policy that competes against any policy covered by the offline data. We then demonstrate that this algorithmic framework can be applied to many specialized Markov Decision Processes where the additional structural assumptions can further refine the concept of partial coverage. Two notable examples are: (1) low- rank MDP with representation learning where the partial coverage condition is defined using a relative condition number measured by the unknown ground truth feature representation; (2) factored MDP where the partial coverage condition is defined using density-ratio based concentrability coefficients associated with individual factors.", "one-sentence_summary": "We study model-based offline Reinforcement Learning with general function approximation without a full coverage assumption on the offline data distribution.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "uehara|pessimistic_modelbased_offline_reinforcement_learning_under_partial_coverage", "pdf": "/pdf/d15bd4853bbe7787fe5fd4a9ffa5c164ac6e2d90.pdf", "supplementary_material": "/attachment/f015779503ddfbff0dfb9e9800552829795a0591.zip", "_bibtex": "@inproceedings{\nuehara2022pessimistic,\ntitle={Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage},\nauthor={Masatoshi Uehara and Wen Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tyrJsbKAe6}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "1ugNpm7W6E", "original": "qDBHZkxLiKR", "number": 1140, "cdate": 1632875499263, "mdate": null, "ddate": null, "tcdate": 1632875499263, "tmdate": 1697934853949, "tddate": null, "forum": "1ugNpm7W6E", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods", "authorids": ["~Wenqing_Zheng1", "~Edward_W_Huang1", "~Nikhil_Rao1", "~Sumeet_Katariya1", "~Zhangyang_Wang1", "~Karthik_Subbian1"], "authors": ["Wenqing Zheng", "Edward W Huang", "Nikhil Rao", "Sumeet Katariya", "Zhangyang Wang", "Karthik Subbian"], "keywords": ["Graph Neural Networks", "Cold Start", "Knowledge Distillation"], "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in node classification, regression, and recommendation tasks. GNNs work well when rich and high-quality connections are available. However, their effectiveness is often jeopardized in many real-world graphs in which node degrees have power-law distributions. The extreme case of this situation, where a node may have no neighbors, is called Strict Cold Start (SCS). SCS forces the prediction to rely completely on the node's own features. We propose Cold Brew, a teacher-student distillation approach to address the SCS and noisy-neighbor challenges for GNNs. We also introduce feature contribution ratio (FCR), a metric to quantify the behavior of inductive GNNs to solve SCS. We experimentally show that FCR disentangles the contributions of different graph data components and helps select the best architecture for SCS generalization. We further demonstrate the superior performance of Cold Brew on several public benchmark and proprietary e-commerce datasets, where many nodes have either very few or noisy connections. Our source code is available at https://github.com/amazon-research/gnn-tail-generalization.", "one-sentence_summary": "Improve strict cold start performances for graph minings with a knowledge distillation framework.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zheng|cold_brew_distilling_graph_node_representations_with_incomplete_or_missing_neighborhoods", "pdf": "/pdf/889df57fc4b767c52628e65826f4c6260f1947f6.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.04840/code)", "_bibtex": "@inproceedings{\nzheng2022cold,\ntitle={Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods},\nauthor={Wenqing Zheng and Edward W Huang and Nikhil Rao and Sumeet Katariya and Zhangyang Wang and Karthik Subbian},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1ugNpm7W6E}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "v-v1cpNNK_v", "original": "TRLvAMvOQOP", "number": 1139, "cdate": 1632875499189, "mdate": null, "ddate": null, "tcdate": 1632875499189, "tmdate": 1676330631300, "tddate": null, "forum": "v-v1cpNNK_v", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "NASI: Label- and Data-agnostic Neural Architecture Search at Initialization", "authorids": ["~Yao_Shu1", "~Shaofeng_Cai1", "~Zhongxiang_Dai1", "~Beng_Chin_Ooi1", "~Bryan_Kian_Hsiang_Low1"], "authors": ["Yao Shu", "Shaofeng Cai", "Zhongxiang Dai", "Beng Chin Ooi", "Bryan Kian Hsiang Low"], "keywords": ["Neural Architecture Search", "Initialization", "Label- and Data-agnostic", "Transferability", "Neural Tangent Kernel"], "abstract": "Recent years have witnessed a surging interest in Neural Architecture Search (NAS). Various algorithms have been proposed to improve the search efficiency and effectiveness of NAS, i.e., to reduce the search cost and improve the generalization performance of the selected architectures, respectively. However, the search efficiency of these algorithms is severely limited by the need for model training during the search process. To overcome this limitation, we propose a novel NAS algorithm called NAS at Initialization (NASI) that exploits the capability of a Neural Tangent Kernel in being able to characterize the performance of candidate architectures at initialization, hence allowing model training to be completely avoided to boost the search efficiency. Besides the improved search efficiency, NASI also achieves competitive search effectiveness on various datasets like CIFAR-10/100 and ImageNet. Further, NASI is shown to be label- and data-agnostic under mild conditions, which guarantees the transferability of architectures selected by our NASI over different datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shu|nasi_label_and_dataagnostic_neural_architecture_search_at_initialization", "pdf": "/pdf/403d8d24b5d2e39b6c55d43bda2f3c36566a45ea.pdf", "supplementary_material": "/attachment/5ef5424b94cc42911969653348a81b60d3154928.zip", "data": "", "_bibtex": "@inproceedings{\nshu2022nasi,\ntitle={{NASI}: Label- and Data-agnostic Neural Architecture Search at Initialization},\nauthor={Yao Shu and Shaofeng Cai and Zhongxiang Dai and Beng Chin Ooi and Bryan Kian Hsiang Low},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=v-v1cpNNK_v}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "49h_IkpJtaE", "original": "2Bel4tzMSyO", "number": 1135, "cdate": 1632875498915, "mdate": null, "ddate": null, "tcdate": 1632875498915, "tmdate": 1697934854849, "tddate": null, "forum": "49h_IkpJtaE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How to Train Your MAML to Excel in Few-Shot Classification", "authorids": ["~Han-Jia_Ye1", "~Wei-Lun_Chao1"], "authors": ["Han-Jia Ye", "Wei-Lun Chao"], "keywords": ["meta-learning", "few-shot learning", "classification", "MAML"], "abstract": "Model-agnostic meta-learning (MAML) is arguably one of the most popular meta-learning algorithms nowadays.\nNevertheless, its performance on few-shot classification is far behind many recent algorithms dedicated to the problem. In this paper, we point out several key facets of how to train MAML to excel in few-shot classification. First, we find that MAML needs a large number of gradient steps in its inner loop update, which contradicts its common usage in few-shot classification. Second, we find that MAML is sensitive to the class label assignments during meta-testing. Concretely, MAML meta-trains the initialization of an $N$-way classifier. These $N$ ways, during meta-testing, then have \"$N!$\" different permutations to be paired with a few-shot task of $N$ novel classes. We find that these permutations lead to a huge variance of accuracy, making MAML unstable in few-shot classification. Third, we investigate several approaches to make MAML permutation-invariant, among which meta-training a single vector to initialize all the $N$ weight vectors in the classification head performs the best. On benchmark datasets like MiniImageNet and TieredImageNet, our approach, which we name UNICORN-MAML, performs on a par with or even outperforms many recent few-shot classification algorithms, without sacrificing MAML's simplicity.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ye|how_to_train_your_maml_to_excel_in_fewshot_classification", "pdf": "/pdf/959d2177bd1cb26a51379f81a0acdd1335afe8e3.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.16245/code)", "_bibtex": "@inproceedings{\nye2022how,\ntitle={How to Train Your {MAML} to Excel in Few-Shot Classification},\nauthor={Han-Jia Ye and Wei-Lun Chao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=49h_IkpJtaE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "xy_2w3J3kH", "original": "Odn-PsyP9q", "number": 1134, "cdate": 1632875498847, "mdate": null, "ddate": null, "tcdate": 1632875498847, "tmdate": 1676330631719, "tddate": null, "forum": "xy_2w3J3kH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Communication-Efficient Actor-Critic Methods for Homogeneous Markov Games", "authorids": ["~Dingyang_Chen1", "351614132@qq.com", "~Qi_Zhang12"], "authors": ["Dingyang Chen", "Yile Li", "Qi Zhang"], "keywords": ["multi-agent reinforcement learning", "multi-agent communication"], "abstract": "Recent success in cooperative multi-agent reinforcement learning (MARL) relies on centralized training and policy sharing. Centralized training eliminates the issue of non-stationarity MARL yet induces large communication costs, and policy sharing is empirically crucial to efficient learning in certain tasks yet lacks theoretical justification. In this paper, we formally characterize a subclass of cooperative Markov games where agents exhibit a certain form of homogeneity such that policy sharing provably incurs no suboptimality. This enables us to develop the first consensus-based decentralized actor-critic method where the consensus update is applied to both the actors and the critics while ensuring convergence. We also develop practical algorithms based on our decentralized actor-critic method to reduce the communication cost during training, while still yielding policies comparable with centralized training.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|communicationefficient_actorcritic_methods_for_homogeneous_markov_games", "pdf": "/pdf/53792786d99df961509a8372e2fe61fafba57a92.pdf", "_bibtex": "@inproceedings{\nchen2022communicationefficient,\ntitle={Communication-Efficient Actor-Critic Methods for Homogeneous Markov Games},\nauthor={Dingyang Chen and Yile Li and Qi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xy_2w3J3kH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "vh-0sUt8HlG", "original": "iYFLNFNRU9", "number": 1127, "cdate": 1632875498363, "mdate": null, "ddate": null, "tcdate": 1632875498363, "tmdate": 1676330632043, "tddate": null, "forum": "vh-0sUt8HlG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer", "authorids": ["~Sachin_Mehta1", "~Mohammad_Rastegari2"], "authors": ["Sachin Mehta", "Mohammad Rastegari"], "keywords": ["Vision transformer", "Mobile", "Edge Devices", "Transformer", "CNN", "Efficient Network", "Detection", "Segmentation", "ImageNet"], "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. \n\nOur source code is open-source and available at: https://github.com/apple/ml-cvnets", "one-sentence_summary": "Light-weight and general-purpose vision transformers for mobile devices", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mehta|mobilevit_lightweight_generalpurpose_and_mobilefriendly_vision_transformer", "pdf": "/pdf/bbdc0b90daf74d7fa54066200459a863a1f5c4e0.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nmehta2022mobilevit,\ntitle={MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\nauthor={Sachin Mehta and Mohammad Rastegari},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vh-0sUt8HlG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "kavTY__jxp", "original": "UR6m0kDQhF", "number": 1126, "cdate": 1632875498292, "mdate": null, "ddate": null, "tcdate": 1632875498292, "tmdate": 1676330632139, "tddate": null, "forum": "kavTY__jxp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery", "authorids": ["~Yulun_Wu1", "~Nicholas_Choma1", "~Andrew_Deru_Chen1", "~Mikaela_Cashman1", "~Erica_Teixeira_Prates1", "~Veronica_G_Melesse_Vergara1", "~Manesh_B_Shah1", "~Austin_Clyde1", "~Thomas_Brettin1", "~Wibe_Albert_de_Jong1", "~Neeraj_Kumar4", "~Martha_S_Head1", "~Rick_L._Stevens1", "~Peter_Nugent1", "~Daniel_A_Jacobson1", "~James_B_Brown1"], "authors": ["Yulun Wu", "Nicholas Choma", "Andrew Deru Chen", "Mikaela Cashman", "Erica Teixeira Prates", "Veronica G Melesse Vergara", "Manesh B Shah", "Austin Clyde", "Thomas Brettin", "Wibe Albert de Jong", "Neeraj Kumar", "Martha S Head", "Rick L. Stevens", "Peter Nugent", "Daniel A Jacobson", "James B Brown"], "keywords": ["reinforcement learning", "graph neural network", "molecule generation", "drug discovery", "curiosity-driven policy"], "abstract": "We developed Distilled Graph Attention Policy Network (DGAPN), a reinforcement learning model to generate novel graph-structured chemical representations that optimize user-defined objectives by efficiently navigating a physically constrained domain. The framework is examined on the task of generating molecules that are designed to bind, noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial Graph Attention (sGAT) mechanism that leverages self-attention over both node and edge attributes as well as encoding the spatial structure --- this capability is of considerable interest in synthetic biology and drug discovery. An attentional policy network is introduced to learn the decision rules for a dynamic, fragment-based chemical environment, and state-of-the-art policy gradient techniques are employed to train the network with stability. Exploration is driven by the stochasticity of the action space design and the innovation reward bonuses learned and proposed by random network distillation. In experiments, our framework achieved outstanding results compared to state-of-the-art algorithms, while reducing the complexity of paths to chemical synthesis.", "one-sentence_summary": "We developed a reinforcement learning framework that advances in exploiting spatial and attributional molecular information as well as exploring novel and synthesizable chemical structures for the purpose of antiviral drug discovery.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|spatial_graph_attention_and_curiositydriven_policy_for_antiviral_drug_discovery", "pdf": "/pdf/b93cd097bbeb37eeacd8b044df00fd6143a092c6.pdf", "supplementary_material": "/attachment/edf7f47a13c0bb409641b00bcb5d89024085b72c.zip", "code": "", "_bibtex": "@inproceedings{\nwu2022spatial,\ntitle={Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery},\nauthor={Yulun Wu and Nicholas Choma and Andrew Deru Chen and Mikaela Cashman and Erica Teixeira Prates and Veronica G Melesse Vergara and Manesh B Shah and Austin Clyde and Thomas Brettin and Wibe Albert de Jong and Neeraj Kumar and Martha S Head and Rick L. Stevens and Peter Nugent and Daniel A Jacobson and James B Brown},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kavTY__jxp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "OnpFa95RVqs", "original": "Yls2qkx0dk2", "number": 1125, "cdate": 1632875498226, "mdate": null, "ddate": null, "tcdate": 1632875498226, "tmdate": 1676330632227, "tddate": null, "forum": "OnpFa95RVqs", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of Tabular NAS Benchmarks", "authorids": ["~Arber_Zela1", "~Julien_Niklas_Siems1", "~Lucas_Zimmer1", "~Jovita_Lukasik1", "~Margret_Keuper1", "~Frank_Hutter1"], "authors": ["Arber Zela", "Julien Niklas Siems", "Lucas Zimmer", "Jovita Lukasik", "Margret Keuper", "Frank Hutter"], "keywords": ["neural architecture search", "AutoML", "benchmarking", "surrogate model"], "abstract": "The most significant barrier to the advancement of Neural Architecture Search (NAS) is its demand for large computational resources, which hinders scientifically sound empirical evaluations of NAS methods. Tabular NAS benchmarks have alleviated this problem substantially, making it possible to properly evaluate NAS methods in seconds on commodity machines. However, an unintended consequence of tabular NAS benchmarks has been a focus on extremely small architectural search spaces since their construction relies on exhaustive evaluations of the space. This leads to unrealistic results that do not transfer to larger spaces. To overcome this fundamental limitation, we propose a methodology to create cheap NAS surrogate benchmarks for arbitrary search spaces. We exemplify this approach by creating surrogate NAS benchmarks on the existing tabular NAS-Bench-101 and on two widely used NAS search spaces with up to $10^{21}$ architectures ($10^{13}$ times larger than any previous tabular NAS benchmark). We show that surrogate NAS benchmarks can model the true performance of architectures better than tabular benchmarks (at a small fraction of the cost), that they lead to faithful estimates of how well different NAS methods work on the original non-surrogate benchmark, and that they can generate new scientific insight. We open-source all our code and believe that surrogate NAS benchmarks are an indispensable tool to extend scientifically sound work on NAS to large and exciting search spaces.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zela|surrogate_nas_benchmarks_going_beyond_the_limited_search_spaces_of_tabular_nas_benchmarks", "pdf": "/pdf/db2ebd6947a358b40d1236000da44615a7f04605.pdf", "one-sentence_summary": "We present surrogate benchmarks for neural architecture search and a general methodology for constructing them.", "data": "", "_bibtex": "@inproceedings{\nzela2022surrogate,\ntitle={Surrogate {NAS} Benchmarks: Going Beyond the Limited Search Spaces of Tabular {NAS} Benchmarks},\nauthor={Arber Zela and Julien Niklas Siems and Lucas Zimmer and Jovita Lukasik and Margret Keuper and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OnpFa95RVqs}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "y1PXylgrXZ", "original": "icx1ztSJ4V5", "number": 1124, "cdate": 1632875498159, "mdate": null, "ddate": null, "tcdate": 1632875498159, "tmdate": 1676330632280, "tddate": null, "forum": "y1PXylgrXZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Certified Robustness for Deep Equilibrium Models via Interval Bound Propagation", "authorids": ["~Colin_Wei1", "~J_Zico_Kolter1"], "authors": ["Colin Wei", "J Zico Kolter"], "keywords": ["deep equilibrium models", "certified robustness", "interval bound propagation"], "abstract": "Deep equilibrium layers (DEQs) have demonstrated promising performance and are competitive with standard explicit models on many benchmarks. However, little is known about certifying robustness for these models. Inspired by interval bound propagation (IBP), we propose the IBP-MonDEQ layer, a DEQ layer whose robustness can be verified by computing upper and lower interval bounds on the output. Our key insights are that these interval bounds can be obtained as the fixed-point solution to an IBP-inspired equilibrium equation, and furthermore, that this solution always exists and is unique when the layer obeys a certain parameterization. This fixed point can be interpreted as the result of applying IBP to an infinitely deep, weight-tied neural network, which may be of independent interest, as IBP bounds are typically unstable for deeper networks. Our empirical comparison reveals that models with IBP-MonDEQ layers can achieve comparable $\\ell_{\\infty}$ certified robustness to similarly-sized fully explicit networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wei|certified_robustness_for_deep_equilibrium_models_via_interval_bound_propagation", "pdf": "/pdf/56e9b880fdd57082fed8b007163cdddb3d9c50e1.pdf", "one-sentence_summary": "To develop certifiably robust deep equilibrium (DEQ) models, we propose the IBP-MonDEQ layer, a DEQ layer where interval bounds on the output can be obtained by solving an additional fixed-point equation inspired by interval bound propagation.", "supplementary_material": "/attachment/d5776f528fcd160048e57495a056e0c451cb8a41.zip", "_bibtex": "@inproceedings{\nwei2022certified,\ntitle={Certified Robustness for Deep Equilibrium Models via Interval Bound Propagation},\nauthor={Colin Wei and J Zico Kolter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=y1PXylgrXZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "03RLpj-tc_", "original": "GLt-BYtQ4Hx", "number": 1120, "cdate": 1632875497884, "mdate": null, "ddate": null, "tcdate": 1632875497884, "tmdate": 1676330632384, "tddate": null, "forum": "03RLpj-tc_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Crystal Diffusion Variational Autoencoder for Periodic Material Generation", "authorids": ["~Tian_Xie2", "~Xiang_Fu4", "~Octavian-Eugen_Ganea1", "~Regina_Barzilay1", "~Tommi_S._Jaakkola1"], "authors": ["Tian Xie", "Xiang Fu", "Octavian-Eugen Ganea", "Regina Barzilay", "Tommi S. Jaakkola"], "keywords": ["materials", "graph neural networks", "periodic", "diffusion models", "score matching", "molecule", "3D", "generative"], "abstract": "Generating the periodic structure of stable materials is a long-standing challenge for the material design community. This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also requires the structure to follow the complex, yet specific bonding preferences between different atom types. Existing methods fail to incorporate these factors and often lack proper invariances. We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability. By learning from the data distribution of stable materials, the decoder generates materials in a diffusion process that moves atomic coordinates towards a lower energy state and updates atom types to satisfy bonding preferences between neighbors. Our model also explicitly encodes interactions across periodic boundaries and respects permutation, translation, rotation, and periodic invariances. We significantly outperform past methods in three tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and realistic materials, and 3) generating materials that optimize a specific property. We also provide several standard datasets and evaluation metrics for the broader machine learning community.", "one-sentence_summary": "A generative model for the 3D periodic structure of materials", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xie|crystal_diffusion_variational_autoencoder_for_periodic_material_generation", "pdf": "/pdf/95e16c7859a352bb2fbe73d3777141e66abbd9bf.pdf", "_bibtex": "@inproceedings{\nxie2022crystal,\ntitle={Crystal Diffusion Variational Autoencoder for Periodic Material Generation},\nauthor={Tian Xie and Xiang Fu and Octavian-Eugen Ganea and Regina Barzilay and Tommi S. Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=03RLpj-tc_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "u2GZOiUTbt", "original": "nHfFUPLWnD", "number": 1118, "cdate": 1632875497746, "mdate": null, "ddate": null, "tcdate": 1632875497746, "tmdate": 1676330632659, "tddate": null, "forum": "u2GZOiUTbt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Task Affinity with Maximum Bipartite Matching in Few-Shot Learning", "authorids": ["~Cat_Phuoc_Le1", "~Juncheng_Dong1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Cat Phuoc Le", "Juncheng Dong", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Task Affinity", "Transfer Learning", "Few-Shot Learning"], "abstract": "We propose an asymmetric affinity score for representing the complexity of utilizing the knowledge of one task for learning another one. Our method is based on the maximum bipartite matching algorithm and utilizes the Fisher Information matrix. We provide theoretical analyses demonstrating that the proposed score is mathematically well-defined, and subsequently use the affinity score to propose a novel algorithm for the few-shot learning problem. In particular, using this score, we find relevant training data labels to the test data and leverage the discovered relevant data for episodically fine-tuning a few-shot model. Results on various few-shot benchmark datasets demonstrate the efficacy of the proposed approach by improving the classification accuracy over the state-of-the-art methods even when using smaller models.", "one-sentence_summary": "Task affinity and its application in few-shot learning", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "le|task_affinity_with_maximum_bipartite_matching_in_fewshot_learning", "pdf": "/pdf/9f6c1ee6225b1fcb5afc57472b4f1fa49e81df37.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nle2022task,\ntitle={Task Affinity with Maximum Bipartite Matching in Few-Shot Learning},\nauthor={Cat Phuoc Le and Juncheng Dong and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=u2GZOiUTbt}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "7r6kDq0mK_", "original": "KRsaeH477r4", "number": 1112, "cdate": 1632875497348, "mdate": null, "ddate": null, "tcdate": 1632875497348, "tmdate": 1697934857374, "tddate": null, "forum": "7r6kDq0mK_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation", "authorids": ["~Yaohui_Wang1", "~Di_Yang4", "~Francois_Bremond1", "antitza.dantcheva@inria.fr"], "authors": ["Yaohui Wang", "Di Yang", "Francois Bremond", "Antitza Dantcheva"], "keywords": ["Video generation", "Generative Adversarial Network"], "abstract": "Due to the remarkable progress of deep generative models, animating images has become increasingly efficient, whereas associated results have become increasingly realistic. Current animation-approaches commonly exploit structure representation extracted from driving videos. Such structure representation is instrumental in transferring motion from driving videos to still images. However, such approaches fail in case the source image and driving video encompass large appearance variation. Moreover, the extraction of structure information requires additional modules that endow the animation-model with increased complexity. Deviating from such models, we here introduce the Latent Image Animator (LIA), a self-supervised autoencoder that evades need for structure representation. LIA is streamlined to animate images by linear navigation in the latent space. Specifically, motion in generated video is constructed by linear displacement of codes in the latent space. Towards this, we learn a set of orthogonal motion directions simultaneously, and use their linear combination, in order to represent any displacement in the latent space. Extensive quantitative and qualitative analysis suggests that our model systematically and significantly outperforms state-of-art methods on VoxCeleb, Taichi and TED-talk datasets w.r.t. generated quality.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|latent_image_animator_learning_to_animate_images_via_latent_space_navigation", "pdf": "/pdf/4c9867f27fdc26664f6abfab9127dd3e7da49c11.pdf", "one-sentence_summary": "Image animation via latent space navigation", "supplementary_material": "/attachment/3d1b0f87e508f09c25d26e1adc283749ac89a25c.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.09043/code)", "_bibtex": "@inproceedings{\nwang2022latent,\ntitle={Latent Image Animator: Learning to Animate Images via Latent Space Navigation},\nauthor={Yaohui Wang and Di Yang and Francois Bremond and Antitza Dantcheva},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7r6kDq0mK_}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "o0ehFykKVtr", "original": "knQC17fZRQo", "number": 1100, "cdate": 1632875496603, "mdate": null, "ddate": null, "tcdate": 1632875496603, "tmdate": 1676330633481, "tddate": null, "forum": "o0ehFykKVtr", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Know Thyself: Transferable Visual Control Policies Through Robot-Awareness", "authorids": ["~Edward_S._Hu1", "~Kun_Huang6", "~Oleh_Rybkin1", "~Dinesh_Jayaraman2"], "authors": ["Edward S. Hu", "Kun Huang", "Oleh Rybkin", "Dinesh Jayaraman"], "keywords": ["visual foresight", "dynamics models", "visuomotor control", "video prediction", "planning", "transfer"], "abstract": "Training visual control policies from scratch on a new robot typically requires generating large amounts of robot-specific data. How might we leverage data previously collected on another robot to reduce or even completely remove this need for robot-specific data? We propose a \"robot-aware control\" paradigm that achieves this by exploiting readily available knowledge about the robot. We then instantiate this in a robot-aware model-based RL policy by training modular dynamics models that couple a transferable, robot-aware world dynamics module with a robot-specific, potentially analytical, robot dynamics module. This also enables us to set up visual planning costs that separately consider the robot agent and the world. Our experiments on tabletop manipulation tasks with simulated and real robots demonstrate that these plug-in improvements dramatically boost the transferability of visual model-based RL policies, even permitting zero-shot transfer of visual manipulation skills onto new robots. Project website: https://www.seas.upenn.edu/~hued/rac", "one-sentence_summary": "We closely integrate readily available knowledge about the robot and world into a learned model to facilitate transfer.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hu|know_thyself_transferable_visual_control_policies_through_robotawareness", "pdf": "/pdf/f710bc40ef26e24b70f811b0f03ad548956e27ae.pdf", "data": "", "_bibtex": "@inproceedings{\nhu2022know,\ntitle={Know Thyself: Transferable Visual Control Policies Through Robot-Awareness},\nauthor={Edward S. Hu and Kun Huang and Oleh Rybkin and Dinesh Jayaraman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=o0ehFykKVtr}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "KJggliHbs8", "original": "qP_-o-u8CGk", "number": 1099, "cdate": 1632875496537, "mdate": null, "ddate": null, "tcdate": 1632875496537, "tmdate": 1676330633581, "tddate": null, "forum": "KJggliHbs8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction", "authorids": ["~Eli_Chien1", "~Wei-Cheng_Chang1", "~Cho-Jui_Hsieh1", "~Hsiang-Fu_Yu2", "~Jiong_Zhang1", "~Olgica_Milenkovic1", "~Inderjit_S_Dhillon1"], "authors": ["Eli Chien", "Wei-Cheng Chang", "Cho-Jui Hsieh", "Hsiang-Fu Yu", "Jiong Zhang", "Olgica Milenkovic", "Inderjit S Dhillon"], "keywords": ["Self-supervised learning", "Graph Neural Networks", "Extreme multi-label classification"], "abstract": "Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take \\emph{numerical} node features and graph structure as inputs, have been shown to achieve state-of-the-art performance on various graph-related learning tasks. Recent works exploring the correlation between numerical node features and graph structure via self-supervised learning have paved the way for further performance improvements of GNNs. However, methods used for extracting numerical node features from \\emph{raw data} are still \\emph{graph-agnostic} within standard GNN pipelines. This practice is sub-optimal as it prevents one from fully utilizing potential correlations between graph topology and node attributes. To mitigate this issue, we propose a new self-supervised learning framework, Graph Information Aided Node feature exTraction (GIANT). GIANT makes use of the eXtreme Multi-label Classification (XMC) formalism, which is crucial for fine-tuning the language model based on graph information, and scales to large datasets. We also provide a theoretical analysis that justifies the use of XMC over link prediction and motivates integrating XR-Transformers, a powerful method for solving XMC problems, into the GIANT framework. We demonstrate the superior performance of GIANT over the standard GNN pipeline on Open Graph Benchmark datasets: For example, we improve the accuracy of the top-ranked method GAMLP from $68.25\\%$ to $69.67\\%$, SGC from $63.29\\%$ to $66.10\\%$ and MLP from $47.24\\%$ to $61.10\\%$ on the ogbn-papers100M dataset by leveraging GIANT.", "one-sentence_summary": "We design a self-supervised learning method for extracting node representations from raw data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chien|node_feature_extraction_by_selfsupervised_multiscale_neighborhood_prediction", "pdf": "/pdf/431334fb15f28e23e02e4a1cd1513fef6cacd2b0.pdf", "supplementary_material": "/attachment/86cc62fdd35f78ccaf2a632a18674d01f4d36698.zip", "_bibtex": "@inproceedings{\nchien2022node,\ntitle={Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction},\nauthor={Eli Chien and Wei-Cheng Chang and Cho-Jui Hsieh and Hsiang-Fu Yu and Jiong Zhang and Olgica Milenkovic and Inderjit S Dhillon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KJggliHbs8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "givsRXsOt9r", "original": "KYUPlvC0T8H", "number": 1096, "cdate": 1632875496331, "mdate": null, "ddate": null, "tcdate": 1632875496331, "tmdate": 1676330633727, "tddate": null, "forum": "givsRXsOt9r", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Spherical Message Passing for 3D Molecular Graphs", "authorids": ["~Yi_Liu12", "~Limei_Wang1", "~Meng_Liu3", "~Yuchao_Lin1", "~Xuan_Zhang3", "~Bora_Oztekin1", "~Shuiwang_Ji1"], "authors": ["Yi Liu", "Limei Wang", "Meng Liu", "Yuchao Lin", "Xuan Zhang", "Bora Oztekin", "Shuiwang Ji"], "keywords": [], "abstract": "We consider representation learning of 3D molecular graphs in which each atom is associated with a spatial position in 3D. This is an under-explored area of research, and a principled message passing framework is currently lacking. In this work, we conduct analyses in the spherical coordinate system (SCS) for the complete identification of 3D graph structures. Based on such observations, we propose the spherical message passing (SMP) as a novel and powerful scheme for 3D molecular learning. SMP dramatically reduces training complexity, enabling it to perform efficiently on large-scale molecules. In addition, SMP is capable of distinguishing almost all molecular structures, and the uncovered cases may not exist in practice. Based on meaningful physically-based representations of 3D information, we further propose the SphereNet for 3D molecular learning. Experimental results demonstrate that the use of meaningful 3D information in SphereNet leads to significant performance improvements in prediction tasks. Our results also demonstrate the advantages of SphereNet in terms of capability, efficiency, and scalability.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|spherical_message_passing_for_3d_molecular_graphs", "pdf": "/pdf/7601c409e12322f92ca5dd6beeaaab909fdd13b1.pdf", "supplementary_material": "/attachment/3941ea8156d88c0d6e5ac6fda6dc05de8739a49f.zip", "data": "", "code": "", "_bibtex": "@inproceedings{\nliu2022spherical,\ntitle={Spherical Message Passing for 3D Molecular Graphs},\nauthor={Yi Liu and Limei Wang and Meng Liu and Yuchao Lin and Xuan Zhang and Bora Oztekin and Shuiwang Ji},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=givsRXsOt9r}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "wbPObLm6ueA", "original": "vqDmT_hgSCi", "number": 1094, "cdate": 1632875496197, "mdate": null, "ddate": null, "tcdate": 1632875496197, "tmdate": 1676330633814, "tddate": null, "forum": "wbPObLm6ueA", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fairness Guarantees under Demographic Shift", "authorids": ["~Stephen_Giguere1", "~Blossom_Metevier1", "~Bruno_Castro_da_Silva1", "~Yuriy_Brun1", "~Philip_S._Thomas1", "~Scott_Niekum1"], "authors": ["Stephen Giguere", "Blossom Metevier", "Bruno Castro da Silva", "Yuriy Brun", "Philip S. Thomas", "Scott Niekum"], "keywords": ["Fairness and Bias in Artificial Intelligence", "Machine Learning"], "abstract": "Recent studies have demonstrated that using machine learning for social applications can lead to injustice in the form of racist, sexist, and otherwise unfair and discriminatory outcomes. To address this challenge, recent machine learning algorithms have been designed to limit the likelihood such unfair behaviors will occur. However, these approaches typically assume the data used for training is representative of what will be encountered once the model is deployed, thus limiting their usefulness. In particular, if certain subgroups of the population become more or less probable after the model is deployed (a phenomenon we call demographic shift), the fair-ness assurances provided by prior algorithms are often invalid. We consider the impact of demographic shift and present a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift. Shifty is the first technique of its kind and demonstrates an effective strategy for designing algorithms to overcome the challenges demographic shift poses. We evaluate Shifty-ttest, an implementation of Shifty based on Student\u2019s \ud835\udc61-test, and, using a real-world data set of university entrance exams and subsequent student success, show that the models output by our algorithm avoid unfair bias under demo-graphic shift, unlike existing methods. Our experiments demonstrate that our algorithm\u2019s high-confidence fairness guarantees are valid in practice and that our algorithm is an effective tool for training models that are fair when demographic shift occurs.", "pdf": "/pdf/d40552fbf306f7f0a8081e70e59da5be9f462a23.pdf", "one-sentence_summary": "We propose a strategy for designing classification algorithms that provide high-confidence fairness guarantees that remain valid if the distribution over observations changes after the trained model is deployed.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "giguere|fairness_guarantees_under_demographic_shift", "_bibtex": "@inproceedings{\ngiguere2022fairness,\ntitle={Fairness Guarantees under Demographic Shift},\nauthor={Stephen Giguere and Blossom Metevier and Yuriy Brun and Philip S. Thomas and Scott Niekum and Bruno Castro da Silva},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wbPObLm6ueA}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "j3krplz_4w6", "original": "vpuL2atLB9", "number": 1085, "cdate": 1632875495656, "mdate": null, "ddate": null, "tcdate": 1632875495656, "tmdate": 1676330634289, "tddate": null, "forum": "j3krplz_4w6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fooling Explanations in Text Classifiers", "authorids": ["~Adam_Ivankay1", "~Ivan_Girardi1", "chi@zurich.ibm.com", "~Pascal_Frossard1"], "authors": ["Adam Ivankay", "Ivan Girardi", "Chiara Marchiori", "Pascal Frossard"], "keywords": ["robustness", "explainability", "text classification", "natural language processing"], "abstract": "State-of-the-art text classification models are becoming increasingly reliant on deep neural networks (DNNs). Due to their black-box nature, faithful and robust explanation methods need to accompany classifiers for deployment in real-life scenarios. However, it has been shown that explanation methods in vision applications are susceptible to local, imperceptible perturbations that can significantly alter the explanations without changing the predicted classes. We show here that the existence of such perturbations extends to text classifiers as well. Specifically, we introduce TextExplanationFooler (TEF), a novel explanation attack algorithm that alters text input samples imperceptibly so that the outcome of widely-used explanation methods changes considerably while leaving classifier predictions unchanged. We evaluate the attribution robustness estimation performance of TEF on five text classification datasets, utilizing three DNN architectures and a transformer architecture for each dataset. By significantly decreasing the correlation between unchanged and perturbed input attributions, we show that all models and explanation methods are susceptible to TEF perturbations. Moreover, we evaluate how the perturbations transfer to other model architectures and attribution methods, finding better than random performance in scenarios where the exact attacked model and explanation method are unknown. Finally, we introduce a semi-universal attack that is able to compute fast, computationally light perturbations with no knowledge of the attacked classifier nor explanation method. Overall, our work shows that explanations in text classifiers are fragile and users need to carefully address their robustness before relying on them in critical applications.", "one-sentence_summary": "Our work shows that explanation methods in text classifiers are susceptible to imperceptible perturbations that alter the explanation outcomes without changing the predictions of the classifiers.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ivankay|fooling_explanations_in_text_classifiers", "pdf": "/pdf/1625b3e98423b8b43cb565c202827783d512083c.pdf", "supplementary_material": "/attachment/1d0f5af96a654304bd1af18a953c4ce2a6634845.zip", "data": "", "_bibtex": "@inproceedings{\nivankay2022fooling,\ntitle={Fooling Explanations in Text Classifiers},\nauthor={Adam Ivankay and Ivan Girardi and Chiara Marchiori and Pascal Frossard},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=j3krplz_4w6}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "y0VvIg25yk", "original": "sgYtl6S3jZd", "number": 1084, "cdate": 1632875495589, "mdate": null, "ddate": null, "tcdate": 1632875495589, "tmdate": 1697934859756, "tddate": null, "forum": "y0VvIg25yk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Learning and Learnability of Quasimetrics", "authorids": ["~Tongzhou_Wang1", "~Phillip_Isola1"], "authors": ["Tongzhou Wang", "Phillip Isola"], "keywords": ["embedding learning", "quasimetric learning", "deep learning"], "abstract": "Our world is full of asymmetries. Gravity and wind can make reaching a place easier than coming back. Social artifacts such as genealogy charts and citation graphs are inherently directed. In reinforcement learning and control, optimal goal-reaching strategies are rarely reversible (symmetrical). Distance functions supported on these asymmetrical structures are called quasimetrics. Despite their common appearance, little research has been done on the learning of quasimetrics. Our theoretical analysis reveals that a common class of learning algorithms, including unconstrained multilayer perceptrons (MLPs), provably fails to learn a quasimetric consistent with training data. In contrast, our proposed Poisson Quasimetric Embedding (PQE) is the first quasimetric learning formulation that both is learnable with gradient-based optimization and enjoys strong performance guarantees. Experiments on random graphs, social graphs, and offline Q-learning demonstrate its effectiveness over many common baselines.", "one-sentence_summary": "We theoretically analyze various algorithms on learning quasimetrics (asymmetrical metrics), and propose an embedding-based method with strong guarantees. Experiments on graph learning and Q-learning show its effectiveness over common baselines.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|on_the_learning_and_learnability_of_quasimetrics", "pdf": "/pdf/e5214f2935d36f9a385665491f63d55204633f1a.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2206.15478/code)", "_bibtex": "@inproceedings{\nwang2022on,\ntitle={On the Learning and Learnability of Quasimetrics},\nauthor={Tongzhou Wang and Phillip Isola},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=y0VvIg25yk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "WH6u2SvlLp4", "original": "R4sQi_s7H8h", "number": 1077, "cdate": 1632875495117, "mdate": null, "ddate": null, "tcdate": 1632875495117, "tmdate": 1676330634857, "tddate": null, "forum": "WH6u2SvlLp4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Prototype-oriented Set Representations for Meta-Learning ", "authorids": ["~Dan_dan_Guo1", "tianlong_xidian@163.com", "~Minghe_Zhang1", "~Mingyuan_Zhou1", "zhahy@cuhksz.edu.cn"], "authors": ["Dan dan Guo", "Long Tian", "Minghe Zhang", "Mingyuan Zhou", "Hongyuan Zha"], "keywords": ["Summary Networks", "Distribution Matching", "Optimal Transport", "Few-shot Classification", "Meta Generative Models"], "abstract": "Learning from set-structured data is a fundamental problem that has recently attracted increasing attention, where a series of summary networks are introduced to deal with the set input. In fact, many meta-learning problems can be treated as set-input tasks. Most existing summary networks aim to design different architectures for the input set in order to enforce permutation invariance. However, scant attention has been paid to the common cases where different sets in a meta distribution are closely related and share certain statistical properties. Viewing each set as a distribution over a set of global prototypes, this paper provides a novel prototype-oriented optimal transport (POT) framework to improve existing summary networks. To learn the distribution over the global prototypes, we minimize its regularized optimal transport distance to the set empirical distribution over data points, providing a natural unsupervised way to improve the summary network. Since our plug-and-play framework can be applied to many meta learning problems, we further instantiate it to the cases of few-shot classification and implicit meta generative modeling. Extensive experiments demonstrate that our framework significantly improves the existing summary networks on learning more powerful summary statistics from sets and can be successfully integrated into metric-based few-shot classification and generative modeling applications, providing a promising tool for addressing set-input and meta-learning problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guo|learning_prototypeoriented_set_representations_for_metalearning", "pdf": "/pdf/68183379fa9bdd81c24c017f6e6ab5b9720d1c15.pdf", "one-sentence_summary": "A plug-and-play framework for set-structured input tasks", "supplementary_material": "/attachment/89608051254b08a2a2a6f0efa2b547ff278996aa.zip", "_bibtex": "@inproceedings{\nguo2022learning,\ntitle={Learning Prototype-oriented Set Representations for Meta-Learning },\nauthor={Dan dan Guo and Long Tian and Minghe Zhang and Mingyuan Zhou and Hongyuan Zha},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WH6u2SvlLp4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "9pEJSVfDbba", "original": "b7bhgIU-OoC", "number": 1071, "cdate": 1632875494693, "mdate": null, "ddate": null, "tcdate": 1632875494693, "tmdate": 1676330635261, "tddate": null, "forum": "9pEJSVfDbba", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling", "authorids": ["~Gianluigi_Silvestri1", "~Emily_Fertig1", "~Dave_Moore1", "~Luca_Ambrogioni1"], "authors": ["Gianluigi Silvestri", "Emily Fertig", "Dave Moore", "Luca Ambrogioni"], "keywords": ["Normalizing Flows", "Probabilistic model", "Probabilistic programming", "Generative modeling", "Variational Inference"], "abstract": "Normalizing flows have shown great success as general-purpose density estimators. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality and continuity. Furthermore, we show that EMFs enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms a large number of alternative methods in common structured inference problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "silvestri|embeddedmodel_flows_combining_the_inductive_biases_of_modelfree_deep_learning_and_explicit_probabilistic_modeling", "pdf": "/pdf/ea5ccb4619c2399f3a652126f47196d475ee7c9c.pdf", "one-sentence_summary": "We introduce bijective transformations that embed domain-specific inductive biases in Normalizing Flow architectures.", "supplementary_material": "/attachment/7be181156cc235233c5e50f654984623293f2e89.zip", "_bibtex": "@inproceedings{\nsilvestri2022embeddedmodel,\ntitle={Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling},\nauthor={Gianluigi Silvestri and Emily Fertig and Dave Moore and Luca Ambrogioni},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9pEJSVfDbba}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "YRq0ZUnzKoZ", "original": "luIHNiHaxu2", "number": 1063, "cdate": 1632875494119, "mdate": null, "ddate": null, "tcdate": 1632875494119, "tmdate": 1697934862517, "tddate": null, "forum": "YRq0ZUnzKoZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning", "authorids": ["~Jiaxian_Guo2", "~Mingming_Gong1", "~Dacheng_Tao1"], "authors": ["Jiaxian Guo", "Mingming Gong", "Dacheng Tao"], "keywords": ["Model-Based Reinforcement Learning", "Unsupervised Dynamics Generalization"], "abstract": "The generalization of model-based reinforcement learning (MBRL) methods to environments with unseen transition dynamics is an important yet challenging problem.\nExisting methods try to extract environment-specified information $Z$ from past transition segments to make the dynamics prediction model generalizable to different dynamics. However, because environments are not labelled, the extracted information inevitably contains redundant information unrelated to the dynamics in transition segments and thus fails to maintain a crucial property of $Z$: $Z$ should be similar in the same environment and dissimilar in different ones. As a result, the learned dynamics prediction function will deviate from the true one, which undermines the generalization ability. To tackle this problem, we introduce an interventional prediction module to estimate the probability of two estimated $\\hat{z}_i, \\hat{z}_j$ belonging to the same environment.\nFurthermore, by utilizing the $Z$'s invariance within a single environment, a relational head is proposed to enforce the similarity between $\\hat{{Z}}$ from the same environment. As a result, the redundant information will be reduced in $\\hat{Z}$. We empirically show that $\\hat{{Z}}$ estimated by our method enjoy less redundant information than previous methods, and such $\\hat{{Z}}$  can significantly reduce dynamics prediction errors and improve the performance of model-based RL methods on zero-shot new environments with unseen dynamics. The codes of this method are available at \\url{https://github.com/CR-Gjx/RIA}.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guo|a_relational_intervention_approach_for_unsupervised_dynamics_generalization_in_modelbased_reinforcement_learning", "pdf": "/pdf/a6c6a600f9e89fe92c0e2d8df1d09d0a78dd39ad.pdf", "one-sentence_summary": "This paper proposes a new model-based RL that could generalize to new environments.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2206.04551/code)", "_bibtex": "@inproceedings{\nguo2022a,\ntitle={A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning},\nauthor={Jiaxian Guo and Mingming Gong and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YRq0ZUnzKoZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "2f1z55GVQN", "original": "6mhwXXsrYW_", "number": 1051, "cdate": 1632875493348, "mdate": null, "ddate": null, "tcdate": 1632875493348, "tmdate": 1676330636194, "tddate": null, "forum": "2f1z55GVQN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Critical Points in Quantum Generative Models", "authorids": ["~Eric_Ricardo_Anschuetz1"], "authors": ["Eric Ricardo Anschuetz"], "keywords": ["loss landscapes", "quantum", "Wishart spin-glass model"], "abstract": "One of the most important properties of neural networks is the clustering of local minima of the loss function near the global minimum, enabling efficient training. Though generative models implemented on quantum computers are known to be more expressive than their traditional counterparts, it has empirically been observed that these models experience a transition in the quality of their local minima. Namely, below some critical number of parameters, all local minima are far from the global minimum in function value; above this critical parameter count, all local minima are good approximators of the global minimum. Furthermore, for a certain class of quantum generative models, this transition has empirically been observed to occur at parameter counts exponentially large in the problem size, meaning practical training of these models is out of reach. Here, we give the first proof of this transition in trainability, specializing to this latter class of quantum generative model. We use techniques inspired by those used to study the loss landscapes of classical neural networks. We also verify that our analytic results hold experimentally even at modest model sizes.", "one-sentence_summary": "We show using techniques from random matrix theory that, unlike typical neural networks, quantum generative models often have poor quality local minima.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "anschuetz|critical_points_in_quantum_generative_models", "pdf": "/pdf/425fdfa2b1be0636b4b3ab1636a8eaac0ea179ea.pdf", "_bibtex": "@inproceedings{\nanschuetz2022critical,\ntitle={Critical Points in Quantum Generative Models},\nauthor={Eric Ricardo Anschuetz},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=2f1z55GVQN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "TW7d65uYu5M", "original": "yf9QbTtiz36", "number": 1050, "cdate": 1632875493278, "mdate": null, "ddate": null, "tcdate": 1632875493278, "tmdate": 1697934863713, "tddate": null, "forum": "TW7d65uYu5M", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "VOS: Learning What You Don't Know by Virtual Outlier Synthesis", "authorids": ["~Xuefeng_Du1", "~Zhaoning_Wang2", "~Mu_Cai1", "~Yixuan_Li1"], "authors": ["Xuefeng Du", "Zhaoning Wang", "Mu Cai", "Yixuan Li"], "keywords": [], "abstract": "Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside,  we introduce a novel unknown-aware training objective,  which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves competitive performance on both object detection and image classification models, reducing the  FPR95 by up to 9.36% compared to the previous best method on object detectors. Code is available at https://github.com/deeplearning-wisc/vos.", "pdf": "/pdf/6faea9b02b55e27b924aea7fe1a92365b3b12a27.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "du|vos_learning_what_you_dont_know_by_virtual_outlier_synthesis", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.01197/code)", "_bibtex": "@inproceedings{\ndu2022towards,\ntitle={Towards Unknown-aware Learning with Virtual Outlier Synthesis},\nauthor={Xuefeng Du and Zhaoning Wang and Mu Cai and Sharon Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TW7d65uYu5M}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "EcGGFkNTxdJ", "original": "pYyZWZNbiD", "number": 1046, "cdate": 1632875492987, "mdate": null, "ddate": null, "tcdate": 1632875492987, "tmdate": 1697934864203, "tddate": null, "forum": "EcGGFkNTxdJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning", "authorids": ["~Jakub_Grudzien_Kuba1", "~Ruiqing_Chen1", "~Muning_Wen2", "~Ying_Wen1", "~Fanglei_Sun1", "~Jun_Wang2", "~Yaodong_Yang1"], "authors": ["Jakub Grudzien Kuba", "Ruiqing Chen", "Muning Wen", "Ying Wen", "Fanglei Sun", "Jun Wang", "Yaodong Yang"], "keywords": ["Multi-Agent Reinforcement Learning", "trust-region method", "policy gradient method"], "abstract": "Trust region methods rigorously enabled reinforcement learning (RL) agents to learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning (MARL),  the property of monotonic improvement may not simply apply; this is because agents, even in cooperative games, could have conflicting directions of policy updates. As a result, achieving a guaranteed improvement on the joint policy where each agent acts individually remains an open challenge. In this paper, we extend the theory of trust region learning to MARL. Central to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. Based on these, we develop Heterogeneous-Agent Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL algorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they need any restrictive assumptions on decomposibility of the joint value function. Most importantly, we justify in theory the monotonic improvement property of HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all tested tasks, thereby establishing a new state of the art. ", "pdf": "/pdf/3909fb38c37d6d25dca74d884b891baf99754ff3.pdf", "one-sentence_summary": "This paper introduces the first trust region method for multi-agent reinforcement learning that enjoys theoretically-justified monotonic improvement guarantee and demonstrates the state-of-the-art performance on Mujoco benchmarks.", "supplementary_material": "", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kuba|trust_region_policy_optimisation_in_multiagent_reinforcement_learning", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.11251/code)", "_bibtex": "@inproceedings{\nkuba2022trust,\ntitle={Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning},\nauthor={Jakub Grudzien Kuba and Ruiqing Chen and Muning Wen and Ying Wen and Fanglei Sun and Jun Wang and Yaodong Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EcGGFkNTxdJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "neqU3HWDgE", "original": "CiebIOHOkA", "number": 1043, "cdate": 1632875492763, "mdate": null, "ddate": null, "tcdate": 1632875492763, "tmdate": 1697934864461, "tddate": null, "forum": "neqU3HWDgE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Disentanglement with Tensor Product Representations on the Torus", "authorids": ["~Michael_Rotman1", "~Amit_Dekel1", "~Shir_Gur1", "~Yaron_Oz1", "~Lior_Wolf1"], "authors": ["Michael Rotman", "Amit Dekel", "Shir Gur", "Yaron Oz", "Lior Wolf"], "keywords": ["Variational Auto-Encoder", "Disentanglement Learning"], "abstract": "The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations.  In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally distributed features, the latent space in our representation is distributed uniformly over a set of unit circles. We argue that the torus structure of the latent space captures the generative factors effectively. We employ recent tools for measuring unsupervised disentanglement, and in an extensive set of experiments demonstrate the advantage of our method in terms of disentanglement, completeness, and informativeness. The code for our proposed method is available at https://github.com/rotmanmi/Unsupervised-Disentanglement-Torus.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rotman|unsupervised_disentanglement_with_tensor_product_representations_on_the_torus", "pdf": "/pdf/c3d91cb3b118da4fa03135f118cc0fdc619e5210.pdf", "one-sentence_summary": "Decomposition of a latent space on a torus leads to a disentangled representation", "supplementary_material": "/attachment/40d8a3d6a5f52a58c91870187a926f231723c1a0.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.06201/code)", "_bibtex": "@inproceedings{\nrotman2022unsupervised,\ntitle={Unsupervised Disentanglement with Tensor Product Representations on the Torus},\nauthor={Michael Rotman and Amit Dekel and Shir Gur and Yaron Oz and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=neqU3HWDgE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "_hszZbt46bT", "original": "TwVX9AiaNU9", "number": 1041, "cdate": 1632875492692, "mdate": null, "ddate": null, "tcdate": 1632875492692, "tmdate": 1676330636958, "tddate": null, "forum": "_hszZbt46bT", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Anomaly Detection for Tabular Data with Internal Contrastive Learning", "authorids": ["~Tom_Shenkar1", "~Lior_Wolf1"], "authors": ["Tom Shenkar", "Lior Wolf"], "keywords": ["Anomaly detection", "Tabular data"], "abstract": " We consider the task of finding out-of-class samples in tabular data, where little can be assumed on the structure of the data. In order to capture the structure of the samples of the single training class, we learn mappings that maximize the mutual information between each sample and the part that is masked out. The mappings are learned by employing a contrastive loss, which considers only one sample at a time. Once learned, we can score a test sample by measuring whether the learned mappings lead to a small contrastive loss using the masked parts of this sample. Our experiments show that our method leads by a sizable accuracy gap in comparison to the literature and that the same default set of hyperparameters provides state-of-the-art results across benchmarks.", "one-sentence_summary": " An anomaly detection method based on the ability to predict the masked out part in a vector. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shenkar|anomaly_detection_for_tabular_data_with_internal_contrastive_learning", "pdf": "/pdf/067e5071eee6bb0a62c48953456a5c12e7469b55.pdf", "supplementary_material": "/attachment/f470e517aa77f89e6cbbf43efaea0ae20ba46248.zip", "_bibtex": "@inproceedings{\nshenkar2022anomaly,\ntitle={Anomaly Detection for Tabular Data with Internal Contrastive Learning},\nauthor={Tom Shenkar and Lior Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_hszZbt46bT}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "CpTuR2ECuW", "original": "pVThLsl3tKC", "number": 1038, "cdate": 1632875492474, "mdate": null, "ddate": null, "tcdate": 1632875492474, "tmdate": 1676330637201, "tddate": null, "forum": "CpTuR2ECuW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning ", "authorids": ["~David_Henry_Mguni1", "~Taher_Jafferjee1", "~Jianhong_Wang1", "~Nicolas_Perez-Nieves1", "~Oliver_Slumbers1", "~Feifei_Tong1", "liyang2@shanghaitech.edu.cn", "~Jiangcheng_Zhu1", "~Yaodong_Yang1", "~Jun_Wang2"], "authors": ["David Henry Mguni", "Taher Jafferjee", "Jianhong Wang", "Nicolas Perez-Nieves", "Oliver Slumbers", "Feifei Tong", "Yang Li", "Jiangcheng Zhu", "Yaodong Yang", "Jun Wang"], "keywords": ["multi-agent", "reinforcement learning", "intrinsic rewards", "exploration"], "abstract": "Efficient exploration is important for reinforcement learners (RL) to achieve high rewards. In multi-agent systems, coordinated exploration and behaviour is critical for agents to jointly achieve optimal outcomes. In this paper, we introduce a new general framework for improving coordination and performance of multi-agent reinforcement learners (MARL). Our framework, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS) introduces an adaptive learner, Generator that observes the agents and learns to construct intrinsic rewards online that coordinate the agents\u2019 joint exploration and joint behaviour. Using a novel combination of reinforcement learning (RL) and switching controls, LIGS determines the best states to learn to add intrinsic rewards which leads to a highly efficient learning process. LIGS can subdivide complex tasks making them easier to solve and enables systems of RL agents to quickly solve environments with sparse rewards. LIGS can seamlessly adopt existing multi-agent RL algorithms and our theory shows that it ensures convergence to joint policies that deliver higher system performance. We demonstrate the superior performance of the LIGS framework in challenging tasks in Foraging and StarCraft II and show LIGS is capable of tackling tasks previously unsolvable by MARL methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mguni|ligs_learnable_intrinsicreward_generation_selection_for_multiagent_learning", "pdf": "/pdf/e236eaa5e5c72be5b36faf18dc589c2d09c9f470.pdf", "supplementary_material": "/attachment/b823113899439f18f37f06f2b8037d3fb3001610.zip", "_bibtex": "@inproceedings{\nmguni2022ligs,\ntitle={{LIGS}: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning },\nauthor={David Henry Mguni and Taher Jafferjee and Jianhong Wang and Nicolas Perez-Nieves and Oliver Slumbers and Feifei Tong and Yang Li and Jiangcheng Zhu and Yaodong Yang and Jun Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CpTuR2ECuW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "EVVadRFRgL7", "original": "LaptqRBMOf3", "number": 1037, "cdate": 1632875492400, "mdate": null, "ddate": null, "tcdate": 1632875492400, "tmdate": 1676330637252, "tddate": null, "forum": "EVVadRFRgL7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How", "authorids": ["~Yuning_You1", "~Yue_Cao4", "~Tianlong_Chen1", "~Zhangyang_Wang1", "~Yang_Shen4"], "authors": ["Yuning You", "Yue Cao", "Tianlong Chen", "Zhangyang Wang", "Yang Shen"], "keywords": [], "abstract": "Optimizing an objective function with uncertainty awareness is well-known to improve the accuracy and confidence of optimization solutions. Meanwhile, another relevant but very different question remains yet open: how to model and quantify the uncertainty of an optimization algorithm (a.k.a., optimizer) itself? To close such a gap, the prerequisite is to consider the optimizers as sampled from a distribution, rather than a few prefabricated and fixed update rules. We first take the novel angle to consider the algorithmic space of optimizers, and provide definitions for the optimizer prior and likelihood, that intrinsically determine the posterior and therefore uncertainty. We then leverage the recent advance of learning to optimize (L2O) for the space parameterization, with the end-to-end training pipeline built via variational inference, referred to as uncertainty-aware L2O (UA-L2O). Our study represents the first effort to recognize and quantify the uncertainty of the optimization algorithm. The extensive numerical results show that, UA-L2O achieves superior uncertainty calibration with accurate confidence estimation and tight confidence intervals, suggesting the improved posterior estimation thanks to considering optimizer uncertainty. Intriguingly, UA-L2O even improves optimization performances for two out of three test functions, the loss function in data privacy attack, and four of five cases of the energy function in protein docking. Our codes are released at https://github.com/Shen-Lab/Bayesian-L2O.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "you|bayesian_modeling_and_uncertainty_quantification_for_learning_to_optimize_what_why_and_how", "pdf": "/pdf/19e1837e276ae2dc367c9117f1039ab41b0e7bc4.pdf", "supplementary_material": "/attachment/b9de7afbe02ec0247ab7d1580b00e7df501eed0c.zip", "_bibtex": "@inproceedings{\nyou2022bayesian,\ntitle={Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How},\nauthor={Yuning You and Yue Cao and Tianlong Chen and Zhangyang Wang and Yang Shen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EVVadRFRgL7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "RLtqs6pzj1-", "original": "XmNiq9_iemX", "number": 1030, "cdate": 1632875491978, "mdate": null, "ddate": null, "tcdate": 1632875491978, "tmdate": 1676330637546, "tddate": null, "forum": "RLtqs6pzj1-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity", "authorids": ["~Shiwei_Liu2", "~Tianlong_Chen1", "~Zahra_Atashgahi1", "~Xiaohan_Chen1", "~Ghada_Sokar1", "~Elena_Mocanu1", "~Mykola_Pechenizkiy1", "~Zhangyang_Wang1", "~Decebal_Constantin_Mocanu1"], "authors": ["Shiwei Liu", "Tianlong Chen", "Zahra Atashgahi", "Xiaohan Chen", "Ghada Sokar", "Elena Mocanu", "Mykola Pechenizkiy", "Zhangyang Wang", "Decebal Constantin Mocanu"], "keywords": ["efficient ensemble", "FreeTickets", "dynamic sparse training", "deep ensemble", "dynamic sparsity"], "abstract": "The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called $FreeTickets$. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, $FreeTickets$, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, $FreeTickets$ has even fewer parameters and training FLOPs than a single dense model. This seemingly counter-intuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. $FreeTickets$ surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, $FreeTickets$ outperforms the naive deep ensemble with ResNet50 on ImageNet using around only $1/5$ of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets.", "one-sentence_summary": "We propose an efficient ensemble learning framework FreeTickets via dynamic spasity, which is more efficient to train and inference than a single dense model, while matching the performance of the naive dense ensemble.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|deep_ensembling_with_no_overhead_for_either_training_or_testing_the_allround_blessings_of_dynamic_sparsity", "pdf": "/pdf/0108f1e168989b51e9c31bd9e515fde8e7473904.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nliu2022deep,\ntitle={Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity},\nauthor={Shiwei Liu and Tianlong Chen and Zahra Atashgahi and Xiaohan Chen and Ghada Sokar and Elena Mocanu and Mykola Pechenizkiy and Zhangyang Wang and Decebal Constantin Mocanu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RLtqs6pzj1-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "X0nrKAXu7g-", "original": "qYbZRXi0ycU", "number": 1014, "cdate": 1632875490993, "mdate": null, "ddate": null, "tcdate": 1632875490993, "tmdate": 1676330638414, "tddate": null, "forum": "X0nrKAXu7g-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning", "authorids": ["~Ziniu_Li1", "~Yingru_Li1", "~Yushun_Zhang1", "~Tong_Zhang2", "~Zhi-Quan_Luo1"], "authors": ["Ziniu Li", "Yingru Li", "Yushun Zhang", "Tong Zhang", "Zhi-Quan Luo"], "keywords": ["exploration", "reinforcement learning"], "abstract": "Randomized least-square value iteration (RLSVI) is a provably efficient exploration method. However, it is limited to the case where (1) a good feature is known in advance and (2) this feature is fixed during the training. If otherwise, RLSVI suffers an unbearable computational burden to obtain the posterior samples. In this work, we present a practical algorithm named HyperDQN to address the above issues under deep RL. In addition to a non-linear neural network (i.e., base model) that predicts Q-values, our method employs a probabilistic hypermodel (i.e., meta model), which outputs the parameter of the base model. When both models are jointly optimized under a specifically designed objective, three purposes can be achieved. First, the hypermodel can generate approximate posterior samples regarding the parameter of the Q-value function. As a result, diverse Q-value functions are sampled to select exploratory action sequences. This retains the punchline of RLSVI for efficient exploration. Second, a good feature is learned to approximate Q-value functions. This addresses limitation (1). Third, the posterior samples of the Q-value function can be obtained in a more efficient way than the existing methods, and the changing feature does not affect the efficiency. This deals with limitation (2). On the Atari suite, HyperDQN with 20M frames outperforms DQN with 200M frames in terms of the maximum human-normalized score. For SuperMarioBros, HyperDQN outperforms several exploration bonus and randomized exploration methods on 5 out of 9 games.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|hyperdqn_a_randomized_exploration_method_for_deep_reinforcement_learning", "pdf": "/pdf/30b6b4ed73060d9df3e0281db73cefda2f68ce5d.pdf", "one-sentence_summary": "We design a practical randomized exploration method to address the sample efficiency issue in online reinforcement learning.", "_bibtex": "@inproceedings{\nli2022hyperdqn,\ntitle={Hyper{DQN}: A Randomized Exploration Method for Deep Reinforcement Learning},\nauthor={Ziniu Li and Yingru Li and Yushun Zhang and Tong Zhang and Zhi-Quan Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=X0nrKAXu7g-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "3rULBvOJ8D2", "original": "h51M0UQp4Fn", "number": 1013, "cdate": 1632875490924, "mdate": null, "ddate": null, "tcdate": 1632875490924, "tmdate": 1676330638580, "tddate": null, "forum": "3rULBvOJ8D2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Unraveling Model-Agnostic Meta-Learning via The Adaptation Learning Rate", "authorids": ["~Yingtian_Zou1", "~Fusheng_Liu1", "~Qianxiao_Li1"], "authors": ["Yingtian Zou", "Fusheng Liu", "Qianxiao Li"], "keywords": ["Meta-Learning", "Learning rate", "Optimization"], "abstract": "Model-Agnostic Meta-Learning (MAML) aims to find initial weights that allow fast adaptation to new tasks. The adaptation (inner loop) learning rate in MAML plays a central role in enabling such fast adaptation. However, how to choose this value in practice and how this choice affects the adaptation error remains less explored. In this paper, we study the effect of the adaptation learning rate in meta-learning with mixed linear regression. First, we present a principled way to estimate optimal adaptation learning rates that minimize the population risk of MAML. Second, we interpret the underlying dependence between the optimal adaptation learning rate and the input data. Finally, we prove that compared with empirical risk minimization (ERM), MAML produces an initialization with a smaller average distance to the task optima, consistent with previous practical findings. These results are corroborated with numerical experiments.", "one-sentence_summary": "Theoretical analysis of Model-Agnostic Meta-Learning (MAML) through the inner loop (adaptation) learning rate.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zou|unraveling_modelagnostic_metalearning_via_the_adaptation_learning_rate", "pdf": "/pdf/f7252f69a2c6aea15401a405687f27c41c8a1489.pdf", "supplementary_material": "/attachment/6f15c6cd58af940add5e3ca0c1fe98353cde88c9.zip", "_bibtex": "@inproceedings{\nzou2022unraveling,\ntitle={Unraveling Model-Agnostic Meta-Learning via The Adaptation Learning Rate},\nauthor={Yingtian Zou and Fusheng Liu and Qianxiao Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3rULBvOJ8D2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "MsHnJPaBUZE", "original": "9ZXJBjL4EQ3", "number": 992, "cdate": 1632875489807, "mdate": null, "ddate": null, "tcdate": 1632875489807, "tmdate": 1676330639486, "tddate": null, "forum": "MsHnJPaBUZE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "iFlood: A Stable and Effective Regularizer", "authorids": ["~Yuexiang_Xie1", "~Zhen_WANG2", "~Yaliang_Li1", "~Ce_Zhang1", "~Jingren_Zhou1", "~Bolin_Ding3"], "authors": ["Yuexiang Xie", "Zhen WANG", "Yaliang Li", "Ce Zhang", "Jingren Zhou", "Bolin Ding"], "keywords": ["overfitting", "regularizer"], "abstract": "Various regularization methods have been designed to prevent overfitting of machine learning models. Among them, a surprisingly simple yet effective one, called Flooding, is proposed recently, which directly constrains the training loss on average to stay at a given level. However, our further studies uncover that the design of the loss function of Flooding can lead to a discrepancy between its objective and implementation, and cause the instability issue. To resolve these issues, in this paper, we propose a new regularizer, called individual Flood (denoted as iFlood). With instance-level constraints on training loss, iFlood encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones. We theoretically show that the design of iFlood can be intrinsically connected with removing the noise or bias in training data, which makes it suitable for a variety of applications to improve the generalization performances of learned models. We also theoretically link iFlood to some other regularizers by comparing the inductive biases they introduce. Our experimental results on both image classification and language understanding tasks confirm that models learned with iFlood can stably converge to solutions with better generalization ability, and behave consistently at instance-level.", "one-sentence_summary": "We propose a novel regularizer named iFlood, which encourages the trained models to better fit the under-fitted instances while suppressing the confidence on over-fitted ones.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xie|iflood_a_stable_and_effective_regularizer", "pdf": "/pdf/3492f8920549a63c8c1bcadbc6e3c5daad349247.pdf", "data": "", "_bibtex": "@inproceedings{\nxie2022iflood,\ntitle={iFlood: A Stable and Effective Regularizer},\nauthor={Yuexiang Xie and Zhen WANG and Yaliang Li and Ce Zhang and Jingren Zhou and Bolin Ding},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MsHnJPaBUZE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "3jooF27-0Wy", "original": "JFb1veq1l0W", "number": 990, "cdate": 1632875489666, "mdate": null, "ddate": null, "tcdate": 1632875489666, "tmdate": 1676330639655, "tddate": null, "forum": "3jooF27-0Wy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes", "authorids": ["~David_W._Romero1", "~Robert-Jan_Bruintjes1", "~Jakub_Mikolaj_Tomczak1", "~Erik_J_Bekkers1", "~Mark_Hoogendoorn2", "~Jan_van_Gemert1"], "authors": ["David W. Romero", "Robert-Jan Bruintjes", "Jakub Mikolaj Tomczak", "Erik J Bekkers", "Mark Hoogendoorn", "Jan van Gemert"], "keywords": ["Convolutional neural networks", "learnable kernel size", "continuous convolutional kernels", "alias-free convolutional networks", "implicit neural representations", "resolution-agnostic representations", "time series", "sequential data", "computer vision"], "abstract": "When designing Convolutional Neural Networks (CNNs), one must select the size of the convolutional kernels before training. Recent works show CNNs benefit from different kernel sizes at different layers, but exploring all possible combinations is unfeasible in practice. A more efficient approach is to learn the kernel size during training. However, existing works that learn the kernel size have a limited bandwidth. These approaches scale kernels by dilation, and thus the detail they can describe is limited. In this work, we propose FlexConv, a novel convolutional operation with which high bandwidth convolutional kernels of learnable kernel size can be learned at a fixed parameter cost. FlexNets model long-term dependencies without the use of pooling, achieve state-of-the-art performance on several sequential datasets, outperform recent works with learned kernel sizes, and are competitive with much deeper ResNets on image benchmark datasets. Additionally, FlexNets can be deployed at higher resolutions than those seen during training. To avoid aliasing, we propose a novel kernel parameterization with which the frequency of the kernels can be analytically controlled. Our novel kernel parameterization shows higher descriptive power and faster convergence speed than existing parameterizations. This leads to important improvements in classification accuracy.", "one-sentence_summary": "We provide a high bandwidth, alias-free convolutional kernel parameterization with learnable kernel size and constant parameter cost.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "romero|flexconv_continuous_kernel_convolutions_with_differentiable_kernel_sizes", "pdf": "/pdf/e9c130ce97afaaa4ab570dfc544d41b573a90491.pdf", "supplementary_material": "/attachment/04b4fb892858d3bdb2302dae5548c62f30e6e7ab.zip", "_bibtex": "@inproceedings{\nromero2022flexconv,\ntitle={FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes},\nauthor={David W. Romero and Robert-Jan Bruintjes and Jakub Mikolaj Tomczak and Erik J Bekkers and Mark Hoogendoorn and Jan van Gemert},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3jooF27-0Wy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "nxcABL7jbQh", "original": "lJp54UultBQ", "number": 979, "cdate": 1632875488893, "mdate": null, "ddate": null, "tcdate": 1632875488893, "tmdate": 1697934870439, "tddate": null, "forum": "nxcABL7jbQh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Zero Pixel Directional Boundary by Vector Transform", "authorids": ["~Edoardo_Mello_Rella1", "~Ajad_Chhatkuli1", "~Yun_Liu1", "~Ender_Konukoglu1", "~Luc_Van_Gool1"], "authors": ["Edoardo Mello Rella", "Ajad Chhatkuli", "Yun Liu", "Ender Konukoglu", "Luc Van Gool"], "keywords": [], "abstract": "Boundaries or contours are among the primary visual cues used by human and computer vision systems. One of the key problems in boundary detection is the loss formulation, which typically leads to class imbalance and, as a consequence, to thick boundaries which require non-differential post-processing steps to be thinned.\nIn this paper, we re-interpret boundaries as 1-D surfaces and formulate a one-to-one vector transform function that allows for training of boundary prediction completely avoiding the class imbalance issue. Specifically, we define the boundary representation at any point as the unit vector pointing to the closest boundary surface.\nOur problem formulation leads to the estimation of direction as well as richer contextual information of the boundary, and, if desired, the availability of zero-pixel thin boundaries also at training time. Our method uses no hyper-parameter in the training loss and a fixed stable hyper-parameter at inference. We provide theoretical justification/discussions of the vector transform representation. We evaluate the proposed loss method using a standard architecture and show the excellent performance over other losses and representations on several datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rella|zero_pixel_directional_boundary_by_vector_transform", "pdf": "/pdf/d11c4f7b7945e6df8efcf8fff9a75ccdd5fc848c.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.08795/code)", "_bibtex": "@inproceedings{\nrella2022zero,\ntitle={Zero Pixel Directional Boundary by Vector Transform},\nauthor={Edoardo Mello Rella and Ajad Chhatkuli and Yun Liu and Ender Konukoglu and Luc Van Gool},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nxcABL7jbQh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "wqD6TfbYkrn", "original": "qIybbCMHkLg", "number": 971, "cdate": 1632875488337, "mdate": null, "ddate": null, "tcdate": 1632875488337, "tmdate": 1676330640715, "tddate": null, "forum": "wqD6TfbYkrn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion", "authorids": ["~Zhaoyang_Lyu1", "~Zhifeng_Kong1", "~Xudong_XU1", "~Liang_Pan2", "~Dahua_Lin1"], "authors": ["Zhaoyang Lyu", "Zhifeng Kong", "Xudong XU", "Liang Pan", "Dahua Lin"], "keywords": ["Point Cloud Completion", "Denoising Diffusion Pobabilistic Model", "Conditional Generation"], "abstract": "3D point clouds are an important data format that captures 3D information for real world objects.  Since 3D point clouds scanned in the real world are often incomplete, it is important to recover the complete point cloud for many downstreaming applications. Most existing point cloud completion methods use the Chamfer Distance (CD) loss for training. The CD loss estimates correspondences between two point clouds by searching nearest neighbors, which does not capture the overall point distribution on the generated shape, and therefore likely leads to non-uniform point cloud generation. To tackle this problem, we propose a novel Point Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The CGNet uses a conditional generative model called the denoising diffusion probabilistic model (DDPM) to generate a coarse completion conditioned on the partial observation. DDPM establishes a one-to-one pointwise mapping between the generated point cloud and the uniform ground truth, and then optimizes the mean squared error loss to realize uniform generation. The RFNet refines the coarse output of the CGNet and further improves quality of the completed point cloud.  In terms of the architecture, we develop a novel dual-path architecture for both networks. The architecture can (1) effectively and efficiently extract multi-level features from partially observed point clouds to guide completion, and (2) accurately manipulate spatial locations of 3D points to obtain smooth surfaces and sharp details. Extensive experimental results on various benchmark datasets show that our PDR paradigm outperforms previous state-of-the-art methods for point cloud completion. In addition, with the help of the RFNet,  we can accelerate the iterative generation process of the DDPM by up to 50 times without much performance drop.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lyu|a_conditional_point_diffusionrefinement_paradigm_for_3d_point_cloud_completion", "pdf": "/pdf/b9a7b67a887496b06e6334d71f7fd5780f4a0c3b.pdf", "_bibtex": "@inproceedings{\nlyu2022a,\ntitle={A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion},\nauthor={Zhaoyang Lyu and Zhifeng Kong and Xudong XU and Liang Pan and Dahua Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wqD6TfbYkrn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "SIKV0_MrZlr", "original": "jdv8vFQ-Erc", "number": 970, "cdate": 1632875488263, "mdate": null, "ddate": null, "tcdate": 1632875488263, "tmdate": 1676330640761, "tddate": null, "forum": "SIKV0_MrZlr", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Auto-Transfer: Learning to Route Transferable Representations", "authorids": ["~Keerthiram_Murugesan1", "~Vijay_Sadashivaiah1", "~Ronny_Luss1", "~Karthikeyan_Shanmugam1", "~Pin-Yu_Chen1", "~Amit_Dhurandhar1"], "authors": ["Keerthiram Murugesan", "Vijay Sadashivaiah", "Ronny Luss", "Karthikeyan Shanmugam", "Pin-Yu Chen", "Amit Dhurandhar"], "keywords": ["Feature routing", "Transferable Representations"], "abstract": "Knowledge transfer between heterogeneous source and target networks and tasks has received a lot of attention in recent times as large amounts of quality labeled data can be difficult to obtain in many applications. Existing approaches typically constrain the target deep neural network (DNN) feature representations to be close to the source DNNs feature representations, which can be limiting. We, in this paper, propose a novel adversarial multi-armed bandit approach that automatically learns to route source representations to appropriate target representations following which they are combined in meaningful ways to produce accurate target models. We see upwards of 5\\% accuracy improvements compared with the state-of-the-art knowledge transfer methods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67, and Stanford40 where the source dataset is ImageNet. We qualitatively analyze the goodness of our transfer scheme by showing individual examples of the important features focused on by our target network at different layers compared with the (closest) competitors. We also observe that our improvement over other methods is higher for smaller target datasets making it an effective tool for small data applications that may benefit from transfer learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "murugesan|autotransfer_learning_to_route_transferable_representations", "pdf": "/pdf/18eb4920f959a323535f04fc7cd7b4509a3dfa40.pdf", "one-sentence_summary": "This paper offers a novel transfer method that uses a routing function to select source-target pairs as well as aggregation functions that combine source and target features (as opposed to matching source to target features).", "supplementary_material": "/attachment/a94a644fa94ecbb54c00fec4bb1bbc1fcd8c1a49.zip", "data": "", "_bibtex": "@inproceedings{\nmurugesan2022autotransfer,\ntitle={Auto-Transfer: Learning to Route Transferable Representations},\nauthor={Keerthiram Murugesan and Vijay Sadashivaiah and Ronny Luss and Karthikeyan Shanmugam and Pin-Yu Chen and Amit Dhurandhar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SIKV0_MrZlr}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "9jInD9JjicF", "original": "VK9rTmrVgJ", "number": 964, "cdate": 1632875487908, "mdate": null, "ddate": null, "tcdate": 1632875487908, "tmdate": 1697934872113, "tddate": null, "forum": "9jInD9JjicF", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences", "authorids": ["~Chao-Hong_Tan1", "~Qian_Chen1", "~Wen_Wang6", "~Qinglin_Zhang1", "~Siqi_Zheng1", "~Zhen-Hua_Ling1"], "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Qinglin Zhang", "Siqi Zheng", "Zhen-Hua Ling"], "keywords": ["Transformer", "Efficient Transformers", "Token Mixing", "Pooling", "Linear", "Long Range Arena", "Transfer Learning", "BERT", "GLUE"], "abstract": "Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also conduct systematic studies on the transfer learning capability of PoNet and observe that PoNet achieves 95.7 percent of the accuracy of BERT on the GLUE benchmark, outperforming FNet by 4.5 percent relative. Comprehensive ablation analysis demonstrates effectiveness of the designed multi-granularity pooling and pooling fusion for token mixing in long sequences and efficacy of the designed pre-training tasks for PoNet to learn transferable contextualized language representations.", "pdf": "/pdf/9907180d33781573fba9842560bb54e6b685e1e8.pdf", "one-sentence_summary": "We propose a novel Pooling Network for token mixing with linear complexity, achieve competitive performance on the Long Range Arena benchmark, and 95.7% of the accuracy of BERT on the GLUE demonstrating its transferability.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tan|ponet_pooling_network_for_efficient_token_mixing_in_long_sequences", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 8 code implementations](https://www.catalyzex.com/paper/arxiv:2110.02442/code)", "_bibtex": "@inproceedings{\ntan2022ponet,\ntitle={PoNet: Pooling Network for Efficient Token Mixing in Long Sequences},\nauthor={Chao-Hong Tan and Qian Chen and Wen Wang and Qinglin Zhang and Siqi Zheng and Zhen-Hua Ling},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9jInD9JjicF}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "9kpuB2bgnim", "original": "NipfEn7ylo6", "number": 962, "cdate": 1632875487766, "mdate": null, "ddate": null, "tcdate": 1632875487766, "tmdate": 1676330641577, "tddate": null, "forum": "9kpuB2bgnim", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Huber Additive Models for Non-stationary Time Series Analysis", "authorids": ["~Yingjie_Wang1", "~Xianrui_Zhong1", "~Fengxiang_He1", "~Hong_Chen1", "~Dacheng_Tao1"], "authors": ["Yingjie Wang", "Xianrui Zhong", "Fengxiang He", "Hong Chen", "Dacheng Tao"], "keywords": ["Sparse additive models", "variable selection", "Huber", "non-stationary", "robust forecasting"], "abstract": "Sparse additive models have shown promising \ufb02exibility and interpretability in processing time series data. However, existing methods usually assume the time series data to be stationary and the innovation is sampled from a Gaussian distribution. Both assumptions are too stringent for heavy-tailed and non-stationary time series data that frequently arise in practice, such as \ufb01nance and medical \ufb01elds. To address these problems, we propose an adaptive sparse Huber additive model for robust forecasting in both non-Gaussian data and (non)stationary data. In theory, the generalization bounds of our estimator are established for both stationary and nonstationary time series data, which are independent of the widely used mixing conditions in learning theory of dependent observations. Moreover, the error bound for non-stationary time series contains a discrepancy measure for the shifts of the data distributions over time. Such a discrepancy measure can be estimated empirically and used as a penalty in our method. Experimental results on both synthetic and real-world benchmark datasets validate the effectiveness of the proposed method. The code is available at https://github.com/xianruizhong/SpHAM.", "one-sentence_summary": "An adaptive sparse Huber additive model for robust forecasting and  variable selection in  non-Gaussian  and (non)stationary time series data ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|huber_additive_models_for_nonstationary_time_series_analysis", "pdf": "/pdf/b179999c2c435d6ce0c920f2fd597bc6b7cfe26e.pdf", "_bibtex": "@inproceedings{\nwang2022huber,\ntitle={Huber Additive Models for Non-stationary Time Series Analysis},\nauthor={Yingjie Wang and Xianrui Zhong and Fengxiang He and Hong Chen and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9kpuB2bgnim}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "WuEiafqdy9H", "original": "XNOFQRAeCdT", "number": 959, "cdate": 1632875487557, "mdate": null, "ddate": null, "tcdate": 1632875487557, "tmdate": 1676330641725, "tddate": null, "forum": "WuEiafqdy9H", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Model-augmented Prioritized Experience Replay", "authorids": ["~Youngmin_Oh2", "~Jinwoo_Shin1", "~Eunho_Yang1", "~Sung_Ju_Hwang1"], "authors": ["Youngmin Oh", "Jinwoo Shin", "Eunho Yang", "Sung Ju Hwang"], "keywords": ["RL", "Reinforcement Learning", "Replay Buffer"], "abstract": "Experience replay is an essential component in off-policy model-free reinforcement learning (MfRL). Due to its effectiveness, various methods for calculating priority scores on experiences have been proposed for sampling. Since critic networks are crucial to policy learning, TD-error, directly correlated to $Q$-values, is one of the most frequently used features to compute the scores. However, critic networks often under- or overestimate $Q$-values, so it is often ineffective to learn to predict $Q$-values by sampled experiences based heavily on TD-error. Accordingly, it is valuable to find auxiliary features, which positively support TD-error in calculating the scores for efficient sampling. Motivated by this, we propose a novel experience replay method, which we call model-augmented prioritized experience replay (MaPER), that employs new learnable features driven from components in model-based RL (MbRL) to calculate the scores on experiences. The proposed MaPER brings the effect of curriculum learning for predicting $Q$-values better by the critic network with negligible memory and computational overhead compared to the vanilla PER. Indeed, our experimental results on various tasks demonstrate that MaPER can significantly improve the performance of the state-of-the-art off-policy MfRL and MbRL which includes off-policy MfRL algorithms in its policy optimization procedure.", "one-sentence_summary": "We propose a novel experience replay which employs additional auxiliary learnable features as well as TD-errors for prioritizing experiences", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "oh|modelaugmented_prioritized_experience_replay", "pdf": "/pdf/b9c58544a23c255f84c3ebd3f597d7e15dd8f541.pdf", "data": "", "supplementary_material": "/attachment/c2a384a11b9cb27425cf957305bfd14ad2c69a7c.zip", "_bibtex": "@inproceedings{\noh2022modelaugmented,\ntitle={Model-augmented Prioritized Experience Replay},\nauthor={Youngmin Oh and Jinwoo Shin and Eunho Yang and Sung Ju Hwang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WuEiafqdy9H}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "MSgB8D4Hy51", "original": "yg-8kI17v0a", "number": 953, "cdate": 1632875487208, "mdate": null, "ddate": null, "tcdate": 1632875487208, "tmdate": 1676330641900, "tddate": null, "forum": "MSgB8D4Hy51", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios", "authorids": ["~Zhen_Xiang1", "~David_Miller8", "~George_Kesidis1"], "authors": ["Zhen Xiang", "David Miller", "George Kesidis"], "keywords": ["backdoor", "Trojan", "adversarial learning", "deep neural network"], "abstract": "Backdoor attacks (BAs) are an emerging threat to deep neural network classifiers. A victim classifier will predict to an attacker-desired target class whenever a test sample is embedded with the same backdoor pattern (BP) that was used to poison the classifier's training set. Detecting whether a classifier is backdoor attacked is not easy in practice, especially when the defender is, e.g., a downstream user without access to the classifier's training set. This challenge is addressed here by a reverse-engineering defense (RED), which has been shown to yield state-of-the-art performance in several domains. However, existing REDs are not applicable when there are only two classes or when multiple attacks are present. These scenarios are first studied in the current paper, under the practical constraints that the defender neither has access to the classifier's training set nor to supervision from clean reference classifiers trained for the same domain. We propose a detection framework based on BP reverse-engineering and a novel expected transferability (ET) statistic. We show that our ET statistic is effective using the same detection threshold, irrespective of the classification domain, the attack configuration, and the BP reverse-engineering algorithm that is used. The excellent performance of our method is demonstrated on six benchmark datasets. Notably, our detection framework is also applicable to multi-class scenarios with multiple attacks. Code is available at https://github.com/zhenxianglance/2ClassBADetection.", "one-sentence_summary": "We proposed a detection framework against backdoor attacks for two-class and multi-attack scenarios, without access to the classifier's training set or any supervision from clean classifiers trained for the same domain.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xiang|posttraining_detection_of_backdoor_attacks_for_twoclass_and_multiattack_scenarios", "pdf": "/pdf/ab4bf90af1442414ba5fa816448b5b73d44ecb92.pdf", "_bibtex": "@inproceedings{\nxiang2022posttraining,\ntitle={Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios},\nauthor={Zhen Xiang and David Miller and George Kesidis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MSgB8D4Hy51}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "9otKVlgrpZG", "original": "63iMkyzx3uI", "number": 952, "cdate": 1632875487137, "mdate": null, "ddate": null, "tcdate": 1632875487137, "tmdate": 1697934873137, "tddate": null, "forum": "9otKVlgrpZG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multi-Task Processes", "authorids": ["~Donggyun_Kim1", "~Seongwoong_Cho1", "~Wonkwang_Lee2", "~Seunghoon_Hong2"], "authors": ["Donggyun Kim", "Seongwoong Cho", "Wonkwang Lee", "Seunghoon Hong"], "keywords": ["stochastic processes", "neural processes", "multi-task learning", "incomplete data"], "abstract": "Neural Processes (NPs) consider a task as a function realized from a stochastic process and flexibly adapt to unseen tasks through inference on functions. However, naive NPs can model data from only a single stochastic process and are designed to infer each task independently. Since many real-world data represent a set of correlated tasks from multiple sources (e.g., multiple attributes and multi-sensor data), it is beneficial to infer them jointly and exploit the underlying correlation to improve the predictive performance.\nTo this end, we propose Multi-Task Neural Processes (MTNPs), an extension of NPs designed to jointly infer tasks realized from multiple stochastic processes. We build MTNPs in a hierarchical way such that inter-task correlation is considered by conditioning all per-task latent variables on a single global latent variable. In addition, we further design our MTNPs so that they can address multi-task settings with incomplete data (i.e., not all tasks share the same set of input points), which has high practical demands in various applications.\nExperiments demonstrate that MTNPs can successfully model multiple tasks jointly by discovering and exploiting their correlations in various real-world data such as time series of weather attributes and pixel-aligned visual modalities. We release our code at https://github.com/GitGyun/multi_task_neural_processes.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kim|multitask_processes", "pdf": "/pdf/37cf7d07aad3da1d05b566f738c88764b8d55db2.pdf", "one-sentence_summary": "We propose a new family of stochastic processes that can infer multiple heterogeneous functions jointly given a few incomplete observations (i.e., some functions may not be observed at each input).", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.14953/code)", "_bibtex": "@inproceedings{\nkim2022multitask,\ntitle={Multi-Task Processes},\nauthor={Donggyun Kim and Seongwoong Cho and Wonkwang Lee and Seunghoon Hong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9otKVlgrpZG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 37}}, {"id": "f9MHpAGUyMn", "original": "9WqzwgBZnln", "number": 945, "cdate": 1632875486642, "mdate": null, "ddate": null, "tcdate": 1632875486642, "tmdate": 1676330642236, "tddate": null, "forum": "f9MHpAGUyMn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Dynamic Token Normalization improves Vision Transformers", "authorids": ["~Wenqi_Shao2", "~Yixiao_Ge2", "~Zhaoyang_Zhang1", "~XUYUAN_XU1", "~Xiaogang_Wang2", "~Ying_Shan2", "~Ping_Luo2"], "authors": ["Wenqi Shao", "Yixiao Ge", "Zhaoyang Zhang", "XUYUAN XU", "Xiaogang Wang", "Ying Shan", "Ping Luo"], "keywords": ["classification", "Normalization", "transformer"], "abstract": "Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN  makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, and PVT. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN on small ViT by $1.1\\%$ top-1 accuracy on ImageNet.", "one-sentence_summary": "The proposed DTN is a simple yet effective normalizer for vision transformers.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shao|dynamic_token_normalization_improves_vision_transformers", "pdf": "/pdf/078f0ee170896cd9a0cb8a5b1aeb1b6c0c0556b3.pdf", "data": "", "_bibtex": "@inproceedings{\nshao2022dynamic,\ntitle={Dynamic Token Normalization improves Vision Transformers},\nauthor={Wenqi Shao and Yixiao Ge and Zhaoyang Zhang and XUYUAN XU and Xiaogang Wang and Ying Shan and Ping Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=f9MHpAGUyMn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "ef0nInZHKIC", "original": "ijI4A3suds", "number": 944, "cdate": 1632875486575, "mdate": null, "ddate": null, "tcdate": 1632875486575, "tmdate": 1697934873717, "tddate": null, "forum": "ef0nInZHKIC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Symbolic Learning to Optimize: Towards Interpretability and Scalability", "authorids": ["~Wenqing_Zheng1", "~Tianlong_Chen1", "~Ting-Kuei_Hu1", "~Zhangyang_Wang1"], "authors": ["Wenqing Zheng", "Tianlong Chen", "Ting-Kuei Hu", "Zhangyang Wang"], "keywords": ["Symbolic Regression", "Learning To Optimize", "Interpretability"], "abstract": "Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural networks create extra memory overhead for applying L2O models, and limits their applicability to optimizing larger tasks; (2) interpretability: it is unclear what each L2O model has learned in its black-box optimization rule, nor is it straightforward to compare different L2O models in an explainable way. To avoid both pitfalls, this paper proves the concept that we can \"kill two birds by one stone\", by introducing the powerful tool of symbolic regression to L2O. In this paper, we establish a holistic symbolic representation and analysis framework for L2O, which yields a series of insights for learnable optimizers. Leveraging our findings, we further propose a lightweight L2O model that can be meta-trained on large-scale problems and outperformed human-designed and tuned optimizers. Our work is set to supply a brand-new perspective to L2O research. Codes are available at: https://github.com/VITA-Group/Symbolic-Learning-To-Optimize.", "one-sentence_summary": "Learning to distill learned optimization rule into symbolic math equations that bears better interpretability and scales better.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zheng|symbolic_learning_to_optimize_towards_interpretability_and_scalability", "pdf": "/pdf/278475a0ef39ae7c0ff2e358f9723083c0b6b342.pdf", "supplementary_material": "/attachment/26ace5ae1da2325e4a24664d3cfb44cdb31ebb8e.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.06578/code)", "_bibtex": "@inproceedings{\nzheng2022symbolic,\ntitle={Symbolic Learning to Optimize: Towards Interpretability and Scalability},\nauthor={Wenqing Zheng and Tianlong Chen and Ting-Kuei Hu and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ef0nInZHKIC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 36}}, {"id": "ivQruZvXxtz", "original": "yP6_zAmtoXK", "number": 935, "cdate": 1632875485939, "mdate": null, "ddate": null, "tcdate": 1632875485939, "tmdate": 1676330642970, "tddate": null, "forum": "ivQruZvXxtz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning", "authorids": ["~Seanie_Lee1", "~Hae_Beom_Lee1", "~Juho_Lee2", "~Sung_Ju_Hwang1"], "authors": ["Seanie Lee", "Hae Beom Lee", "Juho Lee", "Sung Ju Hwang"], "keywords": ["multilingual language model", "gradient alignment"], "abstract": "Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize knowledge transfer while minimizing negative transfer. Despite its importance, the existing methods for gradient alignment either have a completely different purpose, ignore inter-task alignment, or aim to solve continual learning problems in rather inefficient ways. As a result of the misaligned gradients between tasks, the model suffers from severe negative transfer in the form of catastrophic forgetting of the knowledge acquired from the pretraining. To overcome the limitations, we propose a simple yet effective method that can efficiently align gradients between tasks. Specifically, we perform each inner-optimization by sequentially sampling batches from all the tasks, followed by a Reptile outer update. Thanks to the gradients aligned between tasks by our method, the model becomes less vulnerable to negative transfer and catastrophic forgetting. We extensively validate our method on various multi-task learning and zero-shot cross-lingual transfer tasks, where our method largely outperforms all the relevant baselines we consider.", "one-sentence_summary": "We propose a simple yet effective gradient alignment method for finetuning multilingual pretrained language models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|sequential_reptile_intertask_gradient_alignment_for_multilingual_learning", "pdf": "/pdf/a902aca6de0e705d273cc4715fde1741a44e649e.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nlee2022sequential,\ntitle={Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning},\nauthor={Seanie Lee and Hae Beom Lee and Juho Lee and Sung Ju Hwang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ivQruZvXxtz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "PlKWVd2yBkY", "original": "WaH7NcCTChH", "number": 933, "cdate": 1632875485801, "mdate": null, "ddate": null, "tcdate": 1632875485801, "tmdate": 1697934874985, "tddate": null, "forum": "PlKWVd2yBkY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pseudo Numerical Methods for Diffusion Models on Manifolds", "authorids": ["~Luping_Liu2", "~Yi_Ren2", "~Zhijie_Lin1", "~Zhou_Zhao2"], "authors": ["Luping Liu", "Yi Ren", "Zhijie Lin", "Zhou Zhao"], "keywords": ["diffusion model", "generative model", "numerical method", "manifold"], "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce a sample. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a new perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that pseudo linear multi-step method is the best method in most situations. According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|pseudo_numerical_methods_for_diffusion_models_on_manifolds", "pdf": "/pdf/08c071f50f706076294158af529e4f1a2556df41.pdf", "one-sentence_summary": "We propose PNDMs, a new kind of numerical method, to accelerate diffusion models on manifolds.", "supplementary_material": "/attachment/99d3a44be1ad5b9430c9d90c9802de6867a40279.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.09778/code)", "_bibtex": "@inproceedings{\nliu2022pseudo,\ntitle={Pseudo Numerical Methods for Diffusion Models on Manifolds},\nauthor={Luping Liu and Yi Ren and Zhijie Lin and Zhou Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PlKWVd2yBkY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "zq1iJkNk3uN", "original": "b0OIPFUEPW", "number": 931, "cdate": 1632875485655, "mdate": null, "ddate": null, "tcdate": 1632875485655, "tmdate": 1676330643317, "tddate": null, "forum": "zq1iJkNk3uN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image  Pre-training Paradigm", "authorids": ["~Yangguang_Li1", "~Feng_Liang3", "~Lichen_Zhao1", "~Yufeng_Cui2", "~Wanli_Ouyang1", "~Jing_Shao3", "~Fengwei_Yu1", "~Junjie_Yan4"], "authors": ["Yangguang Li", "Feng Liang", "Lichen Zhao", "Yufeng Cui", "Wanli Ouyang", "Jing Shao", "Fengwei Yu", "Junjie Yan"], "keywords": [], "abstract": "Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to downstream tasks. However, CLIP is quite data-hungry and requires 400M image-text pairs for pre-training, thereby restricting its adoption. This work proposes a novel training paradigm, Data efficient CLIP (DeCLIP), to alleviate this limitation. We demonstrate that by carefully utilizing the widespread supervision among the image-text pairs, our De-CLIP can learn generic visual features more efficiently. Instead of using the single image-text contrastive supervision, we fully exploit data potential through the use of (1) self-supervision within each modality; (2) multi-view supervision across modalities; (3) nearest-neighbor supervision from other similar pairs. Benefiting from intrinsic supervision, our DeCLIP-ResNet50 can achieve 60.4% zero-shot top1 accuracy on ImageNet, which is 0.8% above the CLIP-ResNet50 while using 7.1\u00d7fewer data. Our DeCLIP-ResNet50 outperforms its counterpart in 8 out of 11 visual datasets when transferred to downstream tasks. Moreover, Scaling up the model and computing also works well in our framework.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|supervision_exists_everywhere_a_data_efficient_contrastive_languageimage_pretraining_paradigm", "pdf": "/pdf/f51fab8b3b00ecb4e2888cbad4efe7e779ef9b8b.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nli2022supervision,\ntitle={Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image  Pre-training Paradigm},\nauthor={Yangguang Li and Feng Liang and Lichen Zhao and Yufeng Cui and Wanli Ouyang and Jing Shao and Fengwei Yu and Junjie Yan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zq1iJkNk3uN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "DBiQQYWykyy", "original": "IjCANM3fk9U", "number": 927, "cdate": 1632875485447, "mdate": null, "ddate": null, "tcdate": 1632875485447, "tmdate": 1676330643397, "tddate": null, "forum": "DBiQQYWykyy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Environment Predictive Coding for Visual Navigation", "authorids": ["~Santhosh_Kumar_Ramakrishnan1", "~Tushar_Nagarajan1", "~Ziad_Al-Halah2", "~Kristen_Grauman1"], "authors": ["Santhosh Kumar Ramakrishnan", "Tushar Nagarajan", "Ziad Al-Halah", "Kristen Grauman"], "keywords": ["Self-supervised learning", "visual navigation", "representation learning"], "abstract": "We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for individual images, we aim to encode a 3D environment using a series of images observed by an agent moving in it. We learn these representations via a masked-zone prediction task, which segments an agent\u2019s trajectory into zones and then predicts features of randomly masked zones, conditioned on the agent\u2019s camera poses. This explicit spatial conditioning encourages learning representations that capture the geometric and semantic regularities of 3D environments. We learn such representations on a collection of video walkthroughs and demonstrate successful transfer to multiple downstream navigation tasks. Our experiments on the real-world scanned 3D environments of Gibson and Matterport3D show that our method obtains 2 - 6\u00d7 higher sample-ef\ufb01ciency and up to 57% higher performance over standard image-representation learning.", "one-sentence_summary": "We introduce environment predicting coding, a self-supervised approach for learning environment-level representations for navigation-like tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ramakrishnan|environment_predictive_coding_for_visual_navigation", "pdf": "/pdf/a4d281fcacaf03321d8029271263ac6207b29784.pdf", "supplementary_material": "/attachment/00b1bea34971cfafc0c359201b1bc9930ef55c83.zip", "data": "", "_bibtex": "@inproceedings{\nramakrishnan2022environment,\ntitle={Environment Predictive Coding for Visual Navigation},\nauthor={Santhosh Kumar Ramakrishnan and Tushar Nagarajan and Ziad Al-Halah and Kristen Grauman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DBiQQYWykyy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "OXRZeMmOI7a", "original": "4Dwc4USmxjL", "number": 920, "cdate": 1632875484940, "mdate": null, "ddate": null, "tcdate": 1632875484940, "tmdate": 1697934875794, "tddate": null, "forum": "OXRZeMmOI7a", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Topological Experience Replay", "authorids": ["~Zhang-Wei_Hong1", "~Tao_Chen1", "~Yen-Chen_Lin1", "~Joni_Pajarinen2", "~Pulkit_Agrawal1"], "authors": ["Zhang-Wei Hong", "Tao Chen", "Yen-Chen Lin", "Joni Pajarinen", "Pulkit Agrawal"], "keywords": ["Deep reinforcement learning", "experience replay"], "abstract": "State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function since a state's correct Q-value preconditions on the accurate successor states' Q-value. Disregarding such a successor's value dependency leads to useless updates and even learning wrong values.\nTo expedite Q-learning, we maintain states' dependency by organizing the agent's experience into a graph. Each edge in the graph represents a transition between two connected states. We perform value backups via a breadth-first search that expands vertices in the graph starting from the set of terminal states successively moving backward. We empirically show that our method is substantially more data-efficient than several baselines on a diverse range of goal-reaching tasks. Notably, the proposed method also outperforms baselines that consume more batches of training experience. ", "one-sentence_summary": "We rearrange the update order of experience for training the Q-function by a dependency graph.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hong|topological_experience_replay", "pdf": "/pdf/67478db1775a09947be98e8721cebb8b1e453692.pdf", "supplementary_material": "/attachment/2b0f7b6044a4284d62d45f86e1df224be5056232.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.15845/code)", "_bibtex": "@inproceedings{\nhong2022topological,\ntitle={Topological Experience Replay},\nauthor={Zhang-Wei Hong and Tao Chen and Yen-Chen Lin and Joni Pajarinen and Pulkit Agrawal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OXRZeMmOI7a}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "SYuJXrXq8tw", "original": "xbRG8SPKPYd", "number": 918, "cdate": 1632875484768, "mdate": null, "ddate": null, "tcdate": 1632875484768, "tmdate": 1697934876178, "tddate": null, "forum": "SYuJXrXq8tw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sparsity Winning Twice: Better Robust Generalization from More Efficient Training", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~pengjun_wang1", "~Santosh_Balachandra1", "~Haoyu_Ma1", "~Zehao_Wang4", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "pengjun wang", "Santosh Balachandra", "Haoyu Ma", "Zehao Wang", "Zhangyang Wang"], "keywords": [], "abstract": "Recent studies demonstrate the deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., $\\textit{injecting appropriate forms of sparsity}$ during adversarial training. We introduce two alternatives for sparse adversarial training: (i) $\\textit{static sparsity}$, by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) $\\textit{dynamic sparsity}$, by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training. We find both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overfitting, meanwhile significantly saving training and inference FLOPs. Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet. For example, our methods reduce robust generalization gap and overfitting by $34.44\\%$ and $4.02\\%$, with comparable robust/standard accuracy boosts and $87.83\\%$/$87.82\\%$ training/inference FLOPs savings on CIFAR-100 with ResNet-18. Besides, our approaches can be organically combined with existing regularizers, establishing new state-of-the-art results in AT. All codes are included.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|sparsity_winning_twice_better_robust_generalization_from_more_efficient_training", "pdf": "/pdf/be814865ef70b50ab161933745ec83c51f8ce075.pdf", "supplementary_material": "/attachment/d712be4fcd8609c9d5ca643780eeed0ca3bbd18f.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.09844/code)", "_bibtex": "@inproceedings{\nchen2022sparsity,\ntitle={Sparsity Winning Twice: Better Robust Generalization from More Efficient Training},\nauthor={Tianlong Chen and Zhenyu Zhang and pengjun wang and Santosh Balachandra and Haoyu Ma and Zehao Wang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SYuJXrXq8tw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "48RBsJwGkJf", "original": "AMm1e-fz-S", "number": 916, "cdate": 1632875484631, "mdate": null, "ddate": null, "tcdate": 1632875484631, "tmdate": 1676330643792, "tddate": null, "forum": "48RBsJwGkJf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CrossMatch: Cross-Classifier Consistency Regularization for Open-Set Single Domain Generalization", "authorids": ["~Ronghang_Zhu2", "~Sheng_Li3"], "authors": ["Ronghang Zhu", "Sheng Li"], "keywords": ["Single Domain Generalization", "Open-Set Recognition"], "abstract": "Single domain generalization (SDG) is a challenging scenario of domain generalization, where only one source domain is available to train the model. Typical SDG methods are based on the adversarial data augmentation strategy, which complements the diversity of source domain to learn a robust model. Existing SDG methods require the source and target domains to have the same label space. However, as target domains may contain novel categories unseen in source label space, this assumption is not practical in many real-world applications. In this paper, we propose a challenging and untouched problem: \\textit{Open-Set Single Domain Generalization} (OS-SDG), where target domains include unseen categories out of source label space. The goal of OS-SDG is to learn a model, with only one source domain, to classify a target sample with correct class if it belongs to source label space, or assign it to unknown classes. We design a \\textit{CrossMatch} approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. We also adopt a consistency regularization on generated auxiliary samples between multi-binary classifiers and the model trained by SDG methods, to improve the model\u2019s capability on unknown class identification. Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance of SDG methods in the OS-SDG setting.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|crossmatch_crossclassifier_consistency_regularization_for_openset_single_domain_generalization", "pdf": "/pdf/3e9c8f0127840613ea019e2229f86378b1cc0e91.pdf", "data": "", "_bibtex": "@inproceedings{\nzhu2022crossmatch,\ntitle={CrossMatch: Cross-Classifier Consistency Regularization for Open-Set Single Domain Generalization},\nauthor={Ronghang Zhu and Sheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=48RBsJwGkJf}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "baUQQPwQiAg", "original": "U1PpyG6bU_I", "number": 912, "cdate": 1632875484357, "mdate": null, "ddate": null, "tcdate": 1632875484357, "tmdate": 1676330644023, "tddate": null, "forum": "baUQQPwQiAg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning", "authorids": ["~Shaopeng_Fu1", "~Fengxiang_He1", "~Yang_Liu59", "~Li_Shen1", "~Dacheng_Tao1"], "authors": ["Shaopeng Fu", "Fengxiang He", "Yang Liu", "Li Shen", "Dacheng Tao"], "keywords": ["unlearnable examples", "adversarial training", "privacy"], "abstract": "The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \\url{https://github.com/fshp971/robust-unlearnable-examples}.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fu|robust_unlearnable_examples_protecting_data_privacy_against_adversarial_learning", "pdf": "/pdf/cb11d28b469a29f68dc6043214ae3f4f579b360a.pdf", "one-sentence_summary": "This paper proposes an robust error-minimizing noise that can protect data from being learned under adversarial training.", "data": "", "_bibtex": "@inproceedings{\nfu2022robust,\ntitle={Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning},\nauthor={Shaopeng Fu and Fengxiang He and Yang Liu and Li Shen and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=baUQQPwQiAg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "bZJbzaj_IlP", "original": "uUPh-ko4GKJ", "number": 909, "cdate": 1632875484149, "mdate": null, "ddate": null, "tcdate": 1632875484149, "tmdate": 1676330644228, "tddate": null, "forum": "bZJbzaj_IlP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A NON-PARAMETRIC REGRESSION VIEWPOINT : GENERALIZATION OF OVERPARAMETRIZED DEEP RELU NETWORK UNDER NOISY OBSERVATIONS", "authorids": ["~Namjoon_Suh1", "~Hyunouk_Ko1", "~Xiaoming_Huo1"], "authors": ["Namjoon Suh", "Hyunouk Ko", "Xiaoming Huo"], "keywords": ["Overparametrized Deep Neural Network", "Neural Tangent Kernel", "Minimax", "Non-parametric regression"], "abstract": "We study the generalization properties of the overparameterized deep neural network (DNN) with Rectified Linear Unit (ReLU) activations.\nUnder the non-parametric regression framework, it is assumed that the ground-truth function is from a reproducing kernel Hilbert space (RKHS) induced by a neural tangent kernel (NTK) of ReLU DNN, and a dataset is given with the noises. Without a delicate adoption of early stopping, we prove that the overparametrized DNN trained by vanilla gradient descent does not recover the ground-truth function. It turns out that the estimated DNN's $L_{2}$ prediction error is bounded away from $0$. As a complement of the above result, we show that the $\\ell_{2}$-regularized gradient descent enables the overparametrized DNN achieve the minimax optimal convergence rate of the $L_{2}$ prediction error, without early stopping. Notably, the rate we obtained is faster than $\\mathcal{O}(n^{-1/2})$ known in the literature.", "pdf": "/pdf/7375bfe36d699ee163cb7e9a3dde320908fe2afb.pdf", "one-sentence_summary": "Study the generalization of overparametrized deep neural network with relu activation function with noisy dataset.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "suh|a_nonparametric_regression_viewpoint_generalization_of_overparametrized_deep_relu_network_under_noisy_observations", "_bibtex": "@inproceedings{\nsuh2022a,\ntitle={A {NON}-{PARAMETRIC} {REGRESSION} {VIEWPOINT} : {GENERALIZATION} {OF} {OVERPARAMETRIZED} {DEEP} {RELU} {NETWORK} {UNDER} {NOISY} {OBSERVATIONS}},\nauthor={Namjoon Suh and Hyunouk Ko and Xiaoming Huo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bZJbzaj_IlP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "sNuFKTMktcY", "original": "n2TBqUBchmy", "number": 898, "cdate": 1632875483451, "mdate": null, "ddate": null, "tcdate": 1632875483451, "tmdate": 1676330644972, "tddate": null, "forum": "sNuFKTMktcY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Active Hierarchical Exploration with Stable Subgoal Representation Learning", "authorids": ["~Siyuan_Li1", "~Jin_Zhang6", "~Jianhao_Wang1", "~Yang_Yu5", "~Chongjie_Zhang1"], "authors": ["Siyuan Li", "Jin Zhang", "Jianhao Wang", "Yang Yu", "Chongjie Zhang"], "keywords": ["Hierarchical Reinforcement Learning", "Exploration", "Representation Learning"], "abstract": "Goal-conditioned hierarchical reinforcement learning (GCHRL) provides a promising approach to solving long-horizon tasks. Recently, its success has been extended to more general settings by concurrently learning hierarchical policies and subgoal representations. Although GCHRL possesses superior exploration ability by decomposing tasks via subgoals, existing GCHRL methods struggle in temporally extended tasks with sparse external rewards, since the high-level policy learning relies on external rewards. As the high-level policy selects subgoals in an online learned representation space, the dynamic change of the subgoal space severely hinders effective high-level exploration. In this paper, we propose a novel regularization that contributes to both stable and efficient subgoal representation learning. Building upon the stable representation, we design measures of novelty and potential for subgoals, and develop an active hierarchical exploration strategy that seeks out new promising subgoals and states without intrinsic rewards. Experimental results show that our approach significantly outperforms state-of-the-art baselines in continuous control tasks with sparse rewards. ", "one-sentence_summary": "We propose a regularization to stabilize subgoal representation learning in goal-conditioned HRL and develop an active exploration strategy upon this stable representation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|active_hierarchical_exploration_with_stable_subgoal_representation_learning", "pdf": "/pdf/ab3c3ef2320e8d9c7eb88f44d5945f5e22a93480.pdf", "supplementary_material": "/attachment/41023f25ea1e2a3a06924bc8e58888fefe5de39d.zip", "_bibtex": "@inproceedings{\nli2022active,\ntitle={Active Hierarchical Exploration with Stable Subgoal Representation Learning},\nauthor={Siyuan Li and Jin Zhang and Jianhao Wang and Yang Yu and Chongjie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=sNuFKTMktcY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "St-53J9ZARf", "original": "BNA8qtNZA8d", "number": 891, "cdate": 1632875482957, "mdate": null, "ddate": null, "tcdate": 1632875482957, "tmdate": 1697934878780, "tddate": null, "forum": "St-53J9ZARf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Deep AutoAugment", "authorids": ["~Yu_Zheng3", "~Zhi_Zhang4", "~Shen_Yan2", "~Mi_Zhang1"], "authors": ["Yu Zheng", "Zhi Zhang", "Shen Yan", "Mi Zhang"], "keywords": ["automated machine learning", "data augmentation"], "abstract": "While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this work, instead of fixing a set of hand-picked default augmentations alongside the searched data augmentations, we propose a fully automated approach for data augmentation search named Deep AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data augmentation pipeline from scratch by stacking augmentation layers one at a time until reaching convergence. For each augmentation layer, the policy is optimized to maximize the cosine similarity between the gradients of the original and augmented data along the direction with low variance. Our experiments show that even without default augmentations, we can learn an augmentation policy that achieves strong performance with that of previous works. Extensive ablation studies show that the regularized gradient matching is an effective search method for data augmentation policies. Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA .", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zheng|deep_autoaugment", "pdf": "/pdf/4b43ec70e4b68159b4b57b68a3bf89106e693152.pdf", "one-sentence_summary": "We propose Deep AutoAugment (DeepAA), a fully automated automated data augmentation methods that outperforms previous automated data augmentation methods.", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.06172/code)", "_bibtex": "@inproceedings{\nzheng2022deep,\ntitle={Deep AutoAugment},\nauthor={Yu Zheng and Zhi Zhang and Shen Yan and Mi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=St-53J9ZARf}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 31}}, {"id": "p3DKPQ7uaAi", "original": "PzQOcny-U_", "number": 890, "cdate": 1632875482889, "mdate": null, "ddate": null, "tcdate": 1632875482889, "tmdate": 1676330645375, "tddate": null, "forum": "p3DKPQ7uaAi", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification", "authorids": ["~Bing_Su1", "~Ji-Rong_Wen1"], "authors": ["Bing Su", "Ji-Rong Wen"], "keywords": ["Temporal Alignment", "Supervised Representation Learning", "Few-shot Action Recognition", "Alignment Prediction", "Sequence Classification"], "abstract": "Explainable distances for sequence data depend on temporal alignment to tackle sequences with different lengths and local variances. Most sequence alignment methods infer the optimal alignment by solving an optimization problem under pre-defined feasible alignment constraints, which not only is time-consuming, but also makes end-to-end sequence learning intractable. In this paper, we propose a learnable sequence distance called Temporal Alignment Prediction (TAP). TAP employs a lightweight convolutional neural network to directly predict the optimal alignment between two sequences, so that only straightforward calculations are required and no optimization is involved in inference. TAP can be applied in different distance-based machine learning tasks. For supervised sequence representation learning, we show that TAP trained with various metric learning losses achieves completive performances with much faster inference speed. For few-shot action classification, we apply TAP as the distance measure in the metric learning-based episode-training paradigm. This simple strategy achieves comparable results with state-of-the-art few-shot action recognition methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "su|temporal_alignment_prediction_for_supervised_representation_learning_and_fewshot_sequence_classification", "pdf": "/pdf/d44a93ec98adde3d3f16d65b055eaca6e7e14bb1.pdf", "one-sentence_summary": "We propose a learnable sequence distance by predicting the temporal alignment and show its application in supervised representation learning for sequence data and few-shot action recognition.", "data": "", "_bibtex": "@inproceedings{\nsu2022temporal,\ntitle={Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification},\nauthor={Bing Su and Ji-Rong Wen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=p3DKPQ7uaAi}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 4}}, {"id": "O476oWmiNNp", "original": "mKsc9tmCRiM", "number": 887, "cdate": 1632875482689, "mdate": null, "ddate": null, "tcdate": 1632875482689, "tmdate": 1697934879515, "tddate": null, "forum": "O476oWmiNNp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice", "authorids": ["~Peihao_Wang1", "~Wenqing_Zheng1", "~Tianlong_Chen1", "~Zhangyang_Wang1"], "authors": ["Peihao Wang", "Wenqing Zheng", "Tianlong Chen", "Zhangyang Wang"], "keywords": ["Deep ViT", "Spectral Analysis", "Attention Collapse", "Patch Diversity"], "abstract": "Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a  rigorous theory framework to analyze ViT features from the Fourier spectrum domain. We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation. The first technique, termed AttnScale, decomposes a self-attention block into low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed FeatScale, re-weights feature maps on separate frequency bands to amplify the high-frequency signals. Both techniques are efficient and hyperparameter-free, while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple ViT variants, we demonstrate that they consistently help ViTs benefit from deeper architectures, bringing up to 1.1% performance gains \"for free\" (e.g., with little parameter overhead). We publicly release our codes and pre-trained models at https://github.com/VITA-Group/ViT-Anti-Oversmoothing.", "one-sentence_summary": "In this paper, we investigate the scalability issue with ViT via Fourier domain analysis and propose two practical solutions by scaling different frequency components of attention and feature maps.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|antioversmoothing_in_deep_vision_transformers_via_the_fourier_domain_analysis_from_theory_to_practice", "pdf": "/pdf/b248407c10a8f43f933b8eb52a6221cc6f0680b2.pdf", "supplementary_material": "/attachment/d22bd8355ab7038de47ea1de25d54752449585ad.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.05962/code)", "_bibtex": "@inproceedings{\nwang2022antioversmoothing,\ntitle={Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice},\nauthor={Peihao Wang and Wenqing Zheng and Tianlong Chen and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=O476oWmiNNp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "oU3aTsmeRQV", "original": "48sXtUPTLxz", "number": 884, "cdate": 1632875482473, "mdate": null, "ddate": null, "tcdate": 1632875482473, "tmdate": 1697934879759, "tddate": null, "forum": "oU3aTsmeRQV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Self-ensemble Adversarial Training for Improved Robustness", "authorids": ["~Hongjun_Wang2", "~Yisen_Wang1"], "authors": ["Hongjun Wang", "Yisen Wang"], "keywords": ["Adversarial Example", "Adversarial Training"], "abstract": "Due to numerous breakthroughs in real-world applications brought by machine intelligence, deep neural networks (DNNs) are widely employed in critical applications. However, predictions of DNNs are easily manipulated with imperceptible adversarial perturbations, which impedes the further deployment of DNNs and may result in profound security and privacy implications. By incorporating adversarial samples into the training data pool, adversarial training is the strongest principled strategy against various adversarial attacks among all sorts of defense methods. Recent works mainly focus on developing new loss functions or regularizers, attempting to find the unique optimal point in the weight space. But none of them taps the potentials of classifiers obtained from standard adversarial training, especially states on the searching trajectory of training. In this work, we are dedicated to the weight states of models through the training process and devise a simple but powerful \\emph{Self-Ensemble Adversarial Training} (SEAT) method for yielding a robust classifier by averaging weights of history models. This considerably improves the robustness of the target model against several well known adversarial attacks, even merely utilizing the naive cross-entropy loss to supervise. We also discuss the relationship between the ensemble of predictions from different adversarially trained models and the prediction of weight-ensembled models, as well as provide theoretical and empirical evidence that the proposed self-ensemble method provides a smoother loss landscape and better robustness than both individual models and the ensemble of predictions from different classifiers. We further analyze a subtle but fatal issue in the general settings for the self-ensemble model, which causes the deterioration of the weight-ensembled method in the late phases. ", "one-sentence_summary": "This paper proposes an efficient self-ensemble method for adversarial trained classifiers and significantly improve their adversarial robustness", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|selfensemble_adversarial_training_for_improved_robustness", "pdf": "/pdf/65a9ab42c5b42098e96011686b3f498df9749c11.pdf", "supplementary_material": "/attachment/bb8a1f1f6741c848adbf090e350e2dc51fa16950.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2203.09678/code)", "_bibtex": "@inproceedings{\nwang2022selfensemble,\ntitle={Self-ensemble Adversarial Training for Improved Robustness},\nauthor={Hongjun Wang and Yisen Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oU3aTsmeRQV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "Fn7i_r5rR0q", "original": "yXFq74kXCnO", "number": 879, "cdate": 1632875482128, "mdate": null, "ddate": null, "tcdate": 1632875482128, "tmdate": 1697934880908, "tddate": null, "forum": "Fn7i_r5rR0q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Do deep networks transfer invariances across classes?", "authorids": ["~Allan_Zhou1", "~Fahim_Tajwar1", "~Alexander_Robey1", "tknowles@stanford.edu", "~George_J._Pappas1", "~Hamed_Hassani2", "~Chelsea_Finn1"], "authors": ["Allan Zhou", "Fahim Tajwar", "Alexander Robey", "Tom Knowles", "George J. Pappas", "Hamed Hassani", "Chelsea Finn"], "keywords": ["invariance", "augmentation", "nuisance transformation", "imbalance", "long tail"], "abstract": "In order to generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input's class. Many problems have \"class-agnostic\" nuisance transformations that apply similarly to all classes, such as lighting and background changes for image classification. Neural networks can learn these invariances given sufficient data, but many real-world datasets are heavily class imbalanced and contain only a few examples for most of the classes. We therefore pose the question: how well do neural networks transfer class-agnostic invariances learned from the large classes to the small ones? Through careful experimentation, we observe that invariance to class-agnostic transformations is still heavily dependent on class size, with the networks being much less invariant on smaller classes. This result holds even when using data balancing techniques, and suggests poor invariance transfer across classes. Our results provide one explanation for why classifiers generalize poorly on unbalanced and long-tailed distributions. Based on this analysis, we show how a generative approach for learning the nuisance transformations can help transfer invariances across classes and improve performance on a set of imbalanced image classification benchmarks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhou|do_deep_networks_transfer_invariances_across_classes", "pdf": "/pdf/6e8bc14f228c823731c79815d22ed61d236f189c.pdf", "one-sentence_summary": "Study how well classifiers learn invariances in the imbalanced setting, and methods for improvement.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.09739/code)", "_bibtex": "@inproceedings{\nzhou2022do,\ntitle={Do deep networks transfer invariances across classes?},\nauthor={Allan Zhou and Fahim Tajwar and Alexander Robey and Tom Knowles and George J. Pappas and Hamed Hassani and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Fn7i_r5rR0q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "XOh5x-vxsrV", "original": "Xhsvc5BeoQx", "number": 876, "cdate": 1632875481919, "mdate": null, "ddate": null, "tcdate": 1632875481919, "tmdate": 1676330646364, "tddate": null, "forum": "XOh5x-vxsrV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL", "authorids": ["~Bogdan_Mazoure1", "~Ahmed_M_Ahmed1", "~R_Devon_Hjelm1", "~Andrey_Kolobov1", "~Patrick_MacAlpine1"], "authors": ["Bogdan Mazoure", "Ahmed M Ahmed", "R Devon Hjelm", "Andrey Kolobov", "Patrick MacAlpine"], "keywords": ["reinforcement learning", "representation learning", "self-supervised learning", "procgen"], "abstract": "A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions simultaneously: a complex nonlinear encoder that maps high-dimensional observations to a latent representation space, and a simple linear policy over this space. We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder (Raileanu et al., 2021). We propose Cross-Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL can be viewed as having the same effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use of rewards and associated overfitting risks. Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite (Cobbe et al., 2020).  ", "one-sentence_summary": "Cross-trajectory self-supervised learning for better zero-shot generalization in RL", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mazoure|crosstrajectory_representation_learning_for_zeroshot_generalization_in_rl", "pdf": "/pdf/64c9cd96c4583a048d9effe87d02766bc94a2e07.pdf", "supplementary_material": "/attachment/57ad6e19097b0d2f44e552317e58ee1d656c8f3b.zip", "data": "", "code": "", "_bibtex": "@inproceedings{\nmazoure2022crosstrajectory,\ntitle={Cross-Trajectory Representation Learning for Zero-Shot Generalization in {RL}},\nauthor={Bogdan Mazoure and Ahmed M Ahmed and R Devon Hjelm and Andrey Kolobov and Patrick MacAlpine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XOh5x-vxsrV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "w01vBAcewNX", "original": "WpSJl4Y-ib", "number": 857, "cdate": 1632875480616, "mdate": null, "ddate": null, "tcdate": 1632875480616, "tmdate": 1676330647311, "tddate": null, "forum": "w01vBAcewNX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Covariate Shift of Latent Confounders in Imitation and Reinforcement Learning", "authorids": ["~Guy_Tennenholtz2", "~Assaf_Hallak1", "~Gal_Dalal2", "~Shie_Mannor2", "~Gal_Chechik1", "~Uri_Shalit1"], "authors": ["Guy Tennenholtz", "Assaf Hallak", "Gal Dalal", "Shie Mannor", "Gal Chechik", "Uri Shalit"], "keywords": ["imitation learning", "reinforcement learning", "expert data", "hidden confounding", "causal inference", "covariate shift"], "abstract": "We consider the problem of using expert data with unobserved confounders for imitation and reinforcement learning. We begin by defining the problem of learning from confounded expert data in a contextual MDP setup. We analyze the limitations of learning from such data with and without external reward and propose an adjustment of standard imitation learning algorithms to fit this setup. In addition, we discuss the problem of distribution shift between the expert data and the online environment when partial observability is present in the data. We prove possibility and impossibility results for imitation learning under arbitrary distribution shift of the missing covariates. When additional external reward is provided, we propose a sampling procedure that addresses the unknown shift and prove convergence to an optimal solution. Finally, we validate our claims empirically on challenging assistive healthcare and recommender system simulation tasks.", "one-sentence_summary": "We use expert data with unobserved confounders for both imitation and reinforcement learning. Such hidden confounding is prone to a shifted distribution, which may severely hurt performance unless accounted for.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tennenholtz|on_covariate_shift_of_latent_confounders_in_imitation_and_reinforcement_learning", "pdf": "/pdf/34d8c65ffe15df7deaa72adf4835d3f9164844de.pdf", "supplementary_material": "/attachment/8f27cd29fd1ddb0fd2d5f4b112ff2501a9da086e.zip", "_bibtex": "@inproceedings{\ntennenholtz2022on,\ntitle={On Covariate Shift of Latent Confounders in Imitation and Reinforcement Learning},\nauthor={Guy Tennenholtz and Assaf Hallak and Gal Dalal and Shie Mannor and Gal Chechik and Uri Shalit},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=w01vBAcewNX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "S874XAIpkR-", "original": "deIHDgWdzsA", "number": 855, "cdate": 1632875480473, "mdate": null, "ddate": null, "tcdate": 1632875480473, "tmdate": 1697934883518, "tddate": null, "forum": "S874XAIpkR-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "RvS: What is Essential for Offline RL via Supervised Learning?", "authorids": ["~Scott_Emmons1", "~Benjamin_Eysenbach1", "~Ilya_Kostrikov1", "~Sergey_Levine1"], "authors": ["Scott Emmons", "Benjamin Eysenbach", "Ilya Kostrikov", "Sergey Levine"], "keywords": ["reinforcement learning", "deep reinforcement learning", "offline reinforcement learning"], "abstract": "Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin RvS learning). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems.", "one-sentence_summary": "Experimentally evaluating when and why supervised learning solves offline RL", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "emmons|rvs_what_is_essential_for_offline_rl_via_supervised_learning", "pdf": "/pdf/e72a195a733bc43adb5968aa6500c934806fa486.pdf", "supplementary_material": "/attachment/28f2fde4e20d2f3da4a7d76aea11928ba93365ef.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2112.10751/code)", "_bibtex": "@inproceedings{\nemmons2022rvs,\ntitle={RvS: What is Essential for Offline {RL} via Supervised Learning?},\nauthor={Scott Emmons and Benjamin Eysenbach and Ilya Kostrikov and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=S874XAIpkR-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "dpXL6lz4mOQ", "original": "c_Omw_u8_R7", "number": 841, "cdate": 1632875479517, "mdate": null, "ddate": null, "tcdate": 1632875479517, "tmdate": 1676330648230, "tddate": null, "forum": "dpXL6lz4mOQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "LEARNING GUARANTEES FOR GRAPH CONVOLUTIONAL NETWORKS ON THE STOCHASTIC BLOCK MODEL", "authorids": ["~Wei_Lu8"], "authors": ["Wei Lu"], "keywords": [], "abstract": "An abundance of neural network models and algorithms for diverse tasks on graphs have been developed in the past five years. However, very few provable guarantees have been available for the performance of graph neural network models. This state of affairs is in contrast with the steady progress on the theoretical underpinnings of traditional dense and convolutional neural networks. In this paper we present the first provable guarantees for one of the best-studied families of graph neural network models, Graph Convolutional Networks (GCNs), for semi- supervised community detection tasks. We show that with high probability over the initialization and training data, a GCN will efficiently learn to detect communities on graphs drawn from a stochastic block model. Our proof relies on a fine-grained analysis of the training dynamics in order to overcome the complexity of a non-convex optimization landscape with many poorly-performing local minima.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lu|learning_guarantees_for_graph_convolutional_networks_on_the_stochastic_block_model", "pdf": "/pdf/6ecea89eabc4da5e65cce1c3eeb900876578488f.pdf", "_bibtex": "@inproceedings{\nlu2022learning,\ntitle={{LEARNING} {GUARANTEES} {FOR} {GRAPH} {CONVOLUTIONAL} {NETWORKS} {ON} {THE} {STOCHASTIC} {BLOCK} {MODEL}},\nauthor={Wei Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dpXL6lz4mOQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "KEQl-MZ5fg7", "original": "MJ8_6E42sQW", "number": 837, "cdate": 1632875479244, "mdate": null, "ddate": null, "tcdate": 1632875479244, "tmdate": 1697934885028, "tddate": null, "forum": "KEQl-MZ5fg7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Versatile Neural Architectures by Propagating Network Codes", "authorids": ["~Mingyu_Ding1", "~Yuqi_Huo1", "~Haoyu_Lu1", "~Linjie_Yang4", "~Zhe_Wang2", "~Zhiwu_Lu1", "~Jingdong_Wang1", "~Ping_Luo2"], "authors": ["Mingyu Ding", "Yuqi Huo", "Haoyu Lu", "Linjie Yang", "Zhe Wang", "Zhiwu Lu", "Jingdong Wang", "Ping Luo"], "keywords": ["Multitask NAS", "Task-Transferable Architecture", "Neural Predictor", "NAS Benchmark"], "abstract": "This work explores how to design a single neural network capable of adapting to multiple heterogeneous vision tasks, such as image segmentation, 3D detection, and video recognition. This goal is challenging because both network architecture search (NAS) spaces and methods in different tasks are inconsistent. We solve this challenge from both sides. We first introduce a unified design space for multiple tasks and build a multitask NAS benchmark (NAS-Bench-MR) on many widely used datasets, including ImageNet, Cityscapes, KITTI, and HMDB51. We further propose Network Coding Propagation (NCP), which back-propagates gradients of neural predictors to directly update architecture codes along the desired gradient directions to solve various tasks. In this way, optimal architecture configurations can be found by NCP in our large search space in seconds.\n\nUnlike prior arts of NAS that typically focus on a single task, NCP has several unique benefits. (1) NCP transforms architecture optimization from data-driven to architecture-driven, enabling joint search an architecture among multitasks with different data distributions. (2) NCP learns from network codes but not original data, enabling it to update the architecture efficiently across datasets. (3) In addition to our NAS-Bench-MR, NCP performs well on other NAS benchmarks, such as NAS-Bench-201. (4) Thorough studies of NCP on inter-, cross-, and intra-tasks highlight the importance of cross-task neural architecture design, i.e., multitask neural architectures and architecture transferring between different tasks. Code is available at https://github.com/dingmyu/NCP.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ding|learning_versatile_neural_architectures_by_propagating_network_codes", "pdf": "/pdf/38a5d09013b567af9b21b0c0a10e51dc018b377a.pdf", "one-sentence_summary": "An efficient NAS method by inverting neural predictors to directly update architectures and a multitask NAS benchmark for cross-task architecture design and analysis.", "supplementary_material": "/attachment/64eae3f538c20cf46cbf65310537e96ade75b081.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2103.13253/code)", "_bibtex": "@inproceedings{\nding2022learning,\ntitle={Learning Versatile Neural Architectures by Propagating Network Codes},\nauthor={Mingyu Ding and Yuqi Huo and Haoyu Lu and Linjie Yang and Zhe Wang and Zhiwu Lu and Jingdong Wang and Ping Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KEQl-MZ5fg7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "OzyXtIZAzFv", "original": "BUJO7MRlzop", "number": 832, "cdate": 1632875479042, "mdate": null, "ddate": null, "tcdate": 1632875479042, "tmdate": 1676330648704, "tddate": null, "forum": "OzyXtIZAzFv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Task-Induced Representation Learning", "authorids": ["~Jun_Yamada1", "~Karl_Pertsch1", "~Anisha_Gunjal1", "~Joseph_J_Lim1"], "authors": ["Jun Yamada", "Karl Pertsch", "Anisha Gunjal", "Joseph J Lim"], "keywords": ["representation learning", "reinforcement learning", "transfer learning", "visually complex observations"], "abstract": "In this work, we evaluate the effectiveness of representation learning approaches for decision making in visually complex environments. Representation learning is essential for effective reinforcement learning (RL) from high-dimensional in- puts. Unsupervised representation learning approaches based on reconstruction, prediction or contrastive learning have shown substantial learning efficiency gains. Yet, they have mostly been evaluated in clean laboratory or simulated settings. In contrast, real environments are visually complex and contain substantial amounts of clutter and distractors. Unsupervised representations will learn to model such distractors, potentially impairing the agent\u2019s learning efficiency. In contrast, an alternative class of approaches, which we call task-induced representation learning, leverages task information such as rewards or demonstrations from prior tasks to focus on task-relevant parts of the scene and ignore distractors. We investi- gate the effectiveness of unsupervised and task-induced representation learning approaches on four visually complex environments, from Distracting DMControl to the CARLA driving simulator. For both, RL and imitation learning, we find that representation learning generally improves sample efficiency on unseen tasks even in visually complex scenes and that task-induced representations can double learning efficiency compared to unsupervised alternatives.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yamada|taskinduced_representation_learning", "pdf": "/pdf/6ffc4d2e51676a89b8a8dc75c13ac8b37bc4d6ad.pdf", "one-sentence_summary": "We introduce task-induced representation learning, which leverages task information in offline data from prior tasks to learn representations of visually complex scenes that model only task-relevant aspects and enable efficient learning of new tasks.", "supplementary_material": "/attachment/1d15687656fe3dec165b650bf7d614c58433b286.zip", "data": "", "_bibtex": "@inproceedings{\nyamada2022taskinduced,\ntitle={Task-Induced Representation Learning},\nauthor={Jun Yamada and Karl Pertsch and Anisha Gunjal and Joseph J Lim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OzyXtIZAzFv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "USIgIY6TNDe", "original": "i5_jS3Gsc9i", "number": 828, "cdate": 1632875478774, "mdate": null, "ddate": null, "tcdate": 1632875478774, "tmdate": 1676330649030, "tddate": null, "forum": "USIgIY6TNDe", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph-based Nearest Neighbor Search in Hyperbolic Spaces", "authorids": ["~Liudmila_Prokhorenkova1", "~Dmitry_Baranchuk2", "~Nikolay_Bogachev1", "~Yury_Demidovich1", "~Alexander_Kolpakov1"], "authors": ["Liudmila Prokhorenkova", "Dmitry Baranchuk", "Nikolay Bogachev", "Yury Demidovich", "Alexander Kolpakov"], "keywords": ["similarity search", "nearest neighbor search", "hyperbolic space", "graph-based nearest neighbor search"], "abstract": "The nearest neighbor search (NNS) problem is widely studied in Euclidean space, and graph-based algorithms are known to outperform other approaches for this task. However, hyperbolic geometry often allows for better data representation in various domains, including graphs, words, and images. In this paper, we show that graph-based approaches are also well suited for hyperbolic geometry. From a theoretical perspective, we rigorously analyze the time and space complexity of graph-based NNS, assuming that an $n$-element dataset is uniformly distributed within a $d$-dimensional ball of radius $R$ in the hyperbolic space of curvature $-1$. Under some conditions on $R$ and $d$, we derive the time and space complexity of graph-based NNS and compare the obtained results with known guarantees for the Euclidean case. Interestingly, in the dense setting ($d \\ll \\log n$) and under some assumptions on the radius $R$, graph-based NNS has lower time complexity in the hyperbolic space. This agrees with our experiments: we consider datasets embedded in hyperbolic and Euclidean spaces and show that graph-based NNS can be more efficient in the hyperbolic space. We also demonstrate that graph-based methods outperform other existing baselines for hyperbolic NNS. Overall, our theoretical and empirical analysis suggests that graph-based NNS can be considered a default approach for similarity search in hyperbolic spaces.", "one-sentence_summary": "Analyze graph-based NNS for hyperbolic spaces theoretically and empirically and show that this method outperforms other existing approaches.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "prokhorenkova|graphbased_nearest_neighbor_search_in_hyperbolic_spaces", "pdf": "/pdf/2b964b236bad8988bed9b9b8c8ec33f189b77f1b.pdf", "supplementary_material": "/attachment/db70a3c0809ee20ee70c1c18c5c93f32406c2ff9.zip", "_bibtex": "@inproceedings{\nprokhorenkova2022graphbased,\ntitle={Graph-based Nearest Neighbor Search in Hyperbolic Spaces},\nauthor={Liudmila Prokhorenkova and Dmitry Baranchuk and Nikolay Bogachev and Yury Demidovich and Alexander Kolpakov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=USIgIY6TNDe}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "qhAeZjs7dCL", "original": "2hUmHhXmUoH", "number": 820, "cdate": 1632875478219, "mdate": null, "ddate": null, "tcdate": 1632875478219, "tmdate": 1697934887804, "tddate": null, "forum": "qhAeZjs7dCL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generative Models as a Data Source for Multiview Representation Learning", "authorids": ["~Ali_Jahanian1", "~Xavier_Puig1", "~Yonglong_Tian1", "~Phillip_Isola1"], "authors": ["Ali Jahanian", "Xavier Puig", "Yonglong Tian", "Phillip Isola"], "keywords": ["Generative models", "GANs", "Contrastive Learning", "Representation Learning"], "abstract": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \"views\" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \"model zoos\" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jahanian|generative_models_as_a_data_source_for_multiview_representation_learning", "pdf": "/pdf/c520635a84953747f1435603e72a866d4b28bac7.pdf", "one-sentence_summary": "State of the art visual representations are learned by aligning multiple \u2018views\u2019 of the training data; we show how GANs can be used to generate synthetic multiview data that yields effective visual representations.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2106.05258/code)", "_bibtex": "@inproceedings{\njahanian2022generative,\ntitle={Generative Models as a Data Source for Multiview Representation Learning},\nauthor={Ali Jahanian and Xavier Puig and Yonglong Tian and Phillip Isola},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qhAeZjs7dCL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "cBu4ElJfneV", "original": "aSCVseQnD-Y", "number": 815, "cdate": 1632875477872, "mdate": null, "ddate": null, "tcdate": 1632875477872, "tmdate": 1697934888152, "tddate": null, "forum": "cBu4ElJfneV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GiraffeDet: A Heavy-Neck Paradigm for Object Detection", "authorids": ["~yiqi_jiang1", "~Zhiyu_Tan2", "~Junyan_Wang5", "~Xiuyu_Sun1", "~Ming_Lin4", "~Hao_Li16"], "authors": ["yiqi jiang", "Zhiyu Tan", "Junyan Wang", "Xiuyu Sun", "Ming Lin", "Hao Li"], "keywords": ["Object Detection", "fpn", "space-to-depth", "representation"], "abstract": "In conventional object detection frameworks, a backbone body inherited from image recognition models extracts deep latent features and then a neck module fuses these latent features to capture information at different scales. As the resolution in object detection is much larger than in image recognition, the computational cost of the backbone often dominates the total inference cost. This heavy-backbone design paradigm is mostly due to the historical legacy when transferring image recognition models to object detection rather than an end-to-end optimized design for object detection. In this work, we show that such  paradigm indeed leads to sub-optimal object detection models. To this end, we propose a novel heavy-neck paradigm, GiraffeDet, a giraffe-like network for efficient object detection. The GiraffeDet uses an extremely lightweight backbone and a very deep and large neck module which encourages dense information exchange among different spatial scales as well as different levels of latent semantics simultaneously. This design paradigm allows detectors to process the high-level semantic information and low-level spatial information at the same priority even in the early stage of the network, making it more effective in detection tasks.  Numerical evaluations on multiple popular object detection benchmarks show that GiraffeDet consistently outperforms previous SOTA models across a wide spectrum of resource constraints. The source code is available at\nhttps://github.com/jyqi/GiraffeDet.", "one-sentence_summary": "we propose a novel heavy-neck paradigm(GiraffeDet) for detection task, which allows detectors to process the high-level categorical information and low-level spatial information uniformly, making it more effective in detection tasks.  ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|giraffedet_a_heavyneck_paradigm_for_object_detection", "pdf": "/pdf/3d6317eaf73ed0c9901788fc53ea45bdc78d6286.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.04256/code)", "_bibtex": "@inproceedings{\njiang2022giraffedet,\ntitle={GiraffeDet: A Heavy-Neck Paradigm for Object Detection},\nauthor={yiqi jiang and Zhiyu Tan and Junyan Wang and Xiuyu Sun and Ming Lin and Hao Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cBu4ElJfneV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "Dzpe9C1mpiv", "original": "9I19x5zF4WA", "number": 812, "cdate": 1632875477665, "mdate": null, "ddate": null, "tcdate": 1632875477665, "tmdate": 1676330650234, "tddate": null, "forum": "Dzpe9C1mpiv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Unified Wasserstein Distributional Robustness Framework for Adversarial Training", "authorids": ["~Anh_Tuan_Bui2", "~Trung_Le2", "~Quan_Hung_Tran1", "~He_Zhao1", "~Dinh_Phung2"], "authors": ["Anh Tuan Bui", "Trung Le", "Quan Hung Tran", "He Zhao", "Dinh Phung"], "keywords": ["Adversarial Machine Learning", "Distributional Robustness"], "abstract": "It is well-known that deep neural networks (DNNs) are susceptible to adversarial attacks, exposing a severe fragility of deep learning systems. As the result, adversarial training (AT) method, by incorporating adversarial examples during training, represents a natural and effective approach to strengthen the robustness of a DNN-based classifier. However, most AT-based methods, notably PGD-AT and TRADES, typically seek a pointwise adversary that generates the worst-case adversarial example by independently perturbing each data sample, as a way to ``probe'' the vulnerability of the classifier. Arguably, there are unexplored benefits in considering such adversarial effects from an entire distribution. To this end, this paper presents a unified framework that connects Wasserstein distributional robustness with current state-of-the-art AT methods. We introduce a new Wasserstein cost function and a new series of risk functions, with which we show that standard AT methods are special cases of their counterparts in our framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distributional robustness AT-based algorithms. Extensive experiments show that our distributional robustness AT algorithms robustify further their standard AT counterparts in various settings.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bui|a_unified_wasserstein_distributional_robustness_framework_for_adversarial_training", "pdf": "/pdf/7572638f2a47392a13461e8530c3c1cb8786742c.pdf", "one-sentence_summary": "A Unified Wasserstein Distributional Robustness Framework for Adversarial Training", "data": "", "_bibtex": "@inproceedings{\nbui2022a,\ntitle={A Unified Wasserstein Distributional Robustness Framework for Adversarial Training},\nauthor={Anh Tuan Bui and Trung Le and Quan Hung Tran and He Zhao and Dinh Phung},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Dzpe9C1mpiv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "9ZPegFuFTFv", "original": "p5Tf06nbaZ", "number": 810, "cdate": 1632875477525, "mdate": null, "ddate": null, "tcdate": 1632875477525, "tmdate": 1697934889332, "tddate": null, "forum": "9ZPegFuFTFv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "miniF2F: a cross-system benchmark for formal Olympiad-level mathematics", "authorids": ["~Kunhao_Zheng1", "~Jesse_Michael_Han1", "~Stanislas_Polu1"], "authors": ["Kunhao Zheng", "Jesse Michael Han", "Stanislas Polu"], "keywords": ["Neural theorem proving", "Benchmark dataset"], "abstract": "We present $\\textsf{miniF2F}$, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The $\\textsf{miniF2F}$ benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of its performance. We intend for $\\textsf{miniF2F}$ to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.", "pdf": "/pdf/74334724110d37109013ea669f8ffec08b3498a1.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zheng|minif2f_a_crosssystem_benchmark_for_formal_olympiadlevel_mathematics", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2109.00110/code)", "_bibtex": "@inproceedings{\nzheng2022miniff,\ntitle={miniF2F: a cross-system benchmark for formal Olympiad-level mathematics},\nauthor={Kunhao Zheng and Jesse Michael Han and Stanislas Polu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9ZPegFuFTFv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "lQI_mZjvBxj", "original": "HU22Bkmtopd", "number": 799, "cdate": 1632875476832, "mdate": null, "ddate": null, "tcdate": 1632875476832, "tmdate": 1676330650737, "tddate": null, "forum": "lQI_mZjvBxj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Model Agnostic Federated Learning Using Knowledge Distillation", "authorids": ["~Andrei_Afonin1", "~Sai_Praneeth_Karimireddy1"], "authors": ["Andrei Afonin", "Sai Praneeth Karimireddy"], "keywords": ["Federated Learning", "Knowledge Distillation", "Model Agnostic Communication", "Kernel Regression"], "abstract": "Is it possible to design an universal API for federated learning using which an ad-hoc group of data-holders (agents) collaborate with each other and perform federated learning? Such an API would necessarily need to be model-agnostic i.e. make no assumption about the model architecture being used by the agents, and also cannot rely on having representative public data at hand. Knowledge distillation (KD) is the obvious tool of choice to design such protocols. However, surprisingly, we show that most natural KD-based federated learning protocols have poor performance.\n    \n    To investigate this, we propose a new theoretical framework, Federated Kernel ridge regression, which can capture both model heterogeneity as well as data heterogeneity. Our analysis shows that the degradation is largely due to a fundamental limitation of knowledge distillation under data heterogeneity. We further validate our framework by analyzing and designing new protocols based on KD. Their performance on real world experiments using neural networks, though still unsatisfactory, closely matches our theoretical predictions. ", "one-sentence_summary": "We develop a rich yet tractable framework for analyzing distillation based federated learning algorithms, using which we draw some surprising insights.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "afonin|towards_model_agnostic_federated_learning_using_knowledge_distillation", "pdf": "/pdf/f344afa0b3f559c81737bef7821131844aa1feb1.pdf", "supplementary_material": "/attachment/0cd2dbddff5125736a190d6c94a898e61d277dcc.zip", "_bibtex": "@inproceedings{\nafonin2022towards,\ntitle={Towards Model Agnostic Federated Learning Using Knowledge Distillation},\nauthor={Andrei Afonin and Sai Praneeth Karimireddy},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lQI_mZjvBxj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "541PxiEKN3F", "original": "PgVrZCLkmZL", "number": 794, "cdate": 1632875476559, "mdate": null, "ddate": null, "tcdate": 1632875476559, "tmdate": 1697934890371, "tddate": null, "forum": "541PxiEKN3F", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Acceleration of Federated Learning with Alleviated Forgetting in Local Training", "authorids": ["~Chencheng_Xu1", "~Zhiwei_Hong1", "~Minlie_Huang1", "~Tao_Jiang2"], "authors": ["Chencheng Xu", "Zhiwei Hong", "Minlie Huang", "Tao Jiang"], "keywords": ["Federated learning", "non-i.i.d. data"], "abstract": "Federated learning (FL) enables distributed optimization of machine learning models while protecting privacy by independently training local models on each client and then aggregating parameters on a central server, thereby producing an effective global model. Although a variety of FL algorithms have been proposed, their training efficiency remains low when the data are not independently and identically distributed (non-i.i.d.) across different clients. We observe that the slow convergence rates of the existing methods are (at least partially) caused by the catastrophic forgetting issue during the local training stage on each individual client, which leads to a large increase in the loss function concerning the previous training data provided at other clients. Here, we propose FedReg, an algorithm to accelerate FL with alleviated knowledge forgetting in the local training stage by regularizing locally trained parameters with the loss on generated pseudo data, which encode the knowledge of previous training data learned by the global model. Our comprehensive experiments demonstrate that FedReg not only significantly improves the convergence rate of FL, especially when the neural network architecture is deep and the clients' data are extremely non-i.i.d., but is also able to protect privacy better in classification problems and more robust against gradient inversion attacks.", "pdf": "/pdf/7bfcce1814c09fc250d134309321dc8e0995db05.pdf", "supplementary_material": "/attachment/178dd540d239e2ae7938e477519234f589258051.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|acceleration_of_federated_learning_with_alleviated_forgetting_in_local_training", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.02645/code)", "_bibtex": "@inproceedings{\nxu2022acceleration,\ntitle={Acceleration of Federated Learning with Alleviated Forgetting in Local Training},\nauthor={Chencheng Xu and Zhiwei Hong and Minlie Huang and Tao Jiang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=541PxiEKN3F}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "hGXij5rfiHw", "original": "EBzd9mcEfpy", "number": 792, "cdate": 1632875476416, "mdate": null, "ddate": null, "tcdate": 1632875476416, "tmdate": 1697934890374, "tddate": null, "forum": "hGXij5rfiHw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Discovering Invariant Rationales for Graph Neural Networks", "authorids": ["~Yingxin_Wu1", "~Xiang_Wang6", "~An_Zhang2", "~Xiangnan_He1", "~Tat-Seng_Chua2"], "authors": ["Yingxin Wu", "Xiang Wang", "An Zhang", "Xiangnan He", "Tat-Seng Chua"], "keywords": ["Interpretability", "Graph Neural Networks", "Causal Discovery", "Invariant Learning"], "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features --- rationale --- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.", "one-sentence_summary": "We propose a novel invariant learning algorithm, Discovering Invariant Rationale (DIR), for intrinsically interpretable models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|discovering_invariant_rationales_for_graph_neural_networks", "pdf": "/pdf/52dbe9ae47ee8b2442aae9863d89049938a772ba.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.12872/code)", "_bibtex": "@inproceedings{\nwu2022discovering,\ntitle={Discovering Invariant Rationales for Graph Neural Networks},\nauthor={Yingxin Wu and Xiang Wang and An Zhang and Xiangnan He and Tat-Seng Chua},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hGXij5rfiHw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "IYMuTbGzjFU", "original": "aN5aIjW6g5-", "number": 786, "cdate": 1632875475995, "mdate": null, "ddate": null, "tcdate": 1632875475995, "tmdate": 1676330651570, "tddate": null, "forum": "IYMuTbGzjFU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Representing Mixtures of Word Embeddings with Mixtures of Topic Embeddings", "authorids": ["~dongsheng_wang3", "~Dan_dan_Guo1", "~He_Zhao1", "~Huangjie_Zheng1", "~Korawat_Tanwisuth1", "~Bo_Chen1", "~Mingyuan_Zhou1"], "authors": ["dongsheng wang", "Dan dan Guo", "He Zhao", "Huangjie Zheng", "Korawat Tanwisuth", "Bo Chen", "Mingyuan Zhou"], "keywords": ["topic model", "text mining", "distribution matching"], "abstract": "A topic model is often formulated as a generative model that explains how each word of a document is generated given a set of topics and document-specific topic proportions.  It is focused on capturing the word co-occurrences in a document and hence often suffers from poor performance in analyzing short documents. In addition, its parameter estimation often relies on approximate posterior inference that is either not scalable or suffering from large approximation error. This paper introduces a new topic-modeling framework where each document is viewed as a set of word embedding vectors and each topic is modeled as an embedding vector in the same embedding space. Embedding the words and topics in the same vector space, we define a method to measure the semantic difference between the embedding vectors of the words of a document and these of the topics, and optimize the topic embeddings to minimize the expected difference over all documents. Experiments on text analysis demonstrate that the proposed method, which is amenable to mini-batch stochastic gradient descent based optimization and hence scalable to big corpora, provides competitive performance in discovering more coherent and diverse topics and extracting better document representations. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|representing_mixtures_of_word_embeddings_with_mixtures_of_topic_embeddings", "pdf": "/pdf/2a3d46ae6b15a25df349b615b36a5454b906d4c8.pdf", "one-sentence_summary": "A novel method to learn in the word embedding space a set of globally shared topic embedding vectors that are manifested differently in each document", "_bibtex": "@inproceedings{\nwang2022representing,\ntitle={Representing Mixtures of Word Embeddings with Mixtures of Topic Embeddings},\nauthor={dongsheng wang and Dan dan Guo and He Zhao and Huangjie Zheng and Korawat Tanwisuth and Bo Chen and Mingyuan Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=IYMuTbGzjFU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "5JdLZg346Lw", "original": "h89ykUuyG4", "number": 785, "cdate": 1632875475925, "mdate": null, "ddate": null, "tcdate": 1632875475925, "tmdate": 1697934891435, "tddate": null, "forum": "5JdLZg346Lw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generative Modeling with Optimal Transport Maps", "authorids": ["~Litu_Rout1", "~Alexander_Korotin2", "~Evgeny_Burnaev1"], "authors": ["Litu Rout", "Alexander Korotin", "Evgeny Burnaev"], "keywords": ["Optimal Transport Map", "Generative Modeling", "Unpaired Image Restoration"], "abstract": "With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a powerful tool for large-scale generative modeling tasks. In these tasks, OT cost is typically used as the loss for training GANs. In contrast to this approach, we show that the OT map itself can be used as a generative model, providing comparable performance. Previous analogous approaches consider OT maps as generative models only in the latent spaces due to their poor performance in the original high-dimensional ambient space. In contrast, we apply OT maps directly in the ambient space, e.g., a space of high-dimensional images. First, we derive a min-max optimization algorithm to efficiently compute OT maps for the quadratic cost (Wasserstein-2 distance). Next, we extend the approach to the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. We evaluate the algorithm on image generation and unpaired image restoration tasks. In particular, we consider denoising, colorization, and inpainting, where the optimality of the restoration map is a desired attribute, since the output (restored) image is expected to be close to the input (degraded) one.", "pdf": "/pdf/ed46c377aaa94afa77296e14de28b219a2a901bc.pdf", "one-sentence_summary": "While optimal transport cost serves as the loss for popular generative models, we demonstrate that the optimal transport map can be used as the generative model itself.", "supplementary_material": "/attachment/eebc7b2dde2be3ef527556c77b4d711a02cb4ac5.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rout|generative_modeling_with_optimal_transport_maps", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/arxiv:2110.02999/code)", "_bibtex": "@inproceedings{\nrout2022generative,\ntitle={Generative Modeling with Optimal Transport Maps},\nauthor={Litu Rout and Alexander Korotin and Evgeny Burnaev},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5JdLZg346Lw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "irARV_2VFs4", "original": "jJ2dEbUAzM", "number": 784, "cdate": 1632875475857, "mdate": null, "ddate": null, "tcdate": 1632875475857, "tmdate": 1697934891685, "tddate": null, "forum": "irARV_2VFs4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Focus on the Common Good: Group Distributional Robustness Follows", "authorids": ["~Vihari_Piratla1", "~Praneeth_Netrapalli1", "~Sunita_Sarawagi1"], "authors": ["Vihari Piratla", "Praneeth Netrapalli", "Sunita Sarawagi"], "keywords": ["sub-population shift", "robust optimization", "domain generalization"], "abstract": "We consider the problem of training a classification model with group annotated training data. Recent work has established that, if there is distribution shift across different groups, models trained using the standard empirical risk minimization (ERM) objective suffer from poor performance on minority groups and that group distributionally robust optimization (Group-DRO) objective is a better alternative. The starting point of this paper is the observation that though Group-DRO performs better than ERM on minority groups for some benchmark datasets, there are several other datasets where it performs much worse than ERM. Inspired by ideas from the closely related problem of domain generalization, this paper proposes a new and simple algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind our proposed algorithm is that while Group-DRO focuses on groups with worst regularized loss, focusing instead, on groups that enable better performance even on other groups, could lead to learning of shared/common features, thereby enhancing minority performance beyond what is achieved by Group-DRO. Empirically, we show that our proposed algorithm matches or achieves better performance compared to strong contemporary baselines including ERM and Group-DRO on standard benchmarks on both minority groups and across all groups.  Theoretically, we show that the proposed algorithm is a descent method and finds first order stationary points of smooth nonconvex functions.", "one-sentence_summary": "We propose a new and simple algorithm for the sub-population shift problem that enables learning of shared features and performed consistently well over several standard, and real-world, benchmarks of the problem.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "piratla|focus_on_the_common_good_group_distributional_robustness_follows", "pdf": "/pdf/69946032216ff35b996fb358e9cf8779822de6bb.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.02619/code)", "_bibtex": "@inproceedings{\npiratla2022focus,\ntitle={Focus on the Common Good: Group Distributional Robustness Follows},\nauthor={Vihari Piratla and Praneeth Netrapalli and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=irARV_2VFs4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "PDYs7Z2XFGv", "original": "J9MhOW07TDZ", "number": 778, "cdate": 1632875475454, "mdate": null, "ddate": null, "tcdate": 1632875475454, "tmdate": 1676330651997, "tddate": null, "forum": "PDYs7Z2XFGv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification", "authorids": ["~Wensi_Tang1", "~Guodong_Long2", "~Lu_Liu7", "~Tianyi_Zhou1", "michael.blumenstein@uts.edu.au", "~Jing_Jiang6"], "authors": ["Wensi Tang", "Guodong Long", "Lu Liu", "Tianyi Zhou", "Michael Blumenstein", "Jing Jiang"], "keywords": ["Time series classification"], "abstract": "The size of the receptive field has been one of the most important factors for One Dimensional Convolutional Neural Networks (1D-CNNs) on time series classification tasks. Large efforts have been taken to choose the appropriate receptive field size, for it has a huge influence on the performance and differs significantly for each dataset. In this paper, we propose an Omni-Scale block (OS-block) for 1D-CNNs, where the kernel sizes are set by a simple and universal rule. OS-block can efficiently cover the best size of the receptive field across different datasets. This set of kernel sizes consists of multiple prime numbers according to the length of the time series. We experimentally show 1D-CNNs built from OS-block can consistently achieve the state-of-the-art accuracy with a smaller model size on five time series benchmarks, including both univariate and multivariate data from multiple domains. Comprehensive analysis and ablation studies shed light on how our rule finds the best receptive field size and demonstrate the consistency of our OS-block for multiple 1D-CNN structures.", "one-sentence_summary": "To extract features from time series data in proper time scales, many complicated scales searching or weighting methods have been proposed, but we will show that this could have been done via a very simple structure.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tang|omniscale_cnns_a_simple_and_effective_kernel_size_configuration_for_time_series_classification", "pdf": "/pdf/bc236de1c9c7e988f2851ff83b8062b23a50547c.pdf", "supplementary_material": "/attachment/d21bb2a7402abaf120f148afdfd646f344167ba5.zip", "_bibtex": "@inproceedings{\ntang2022omniscale,\ntitle={Omni-Scale {CNN}s: a simple and effective kernel size configuration for time series classification},\nauthor={Wensi Tang and Guodong Long and Lu Liu and Tianyi Zhou and Michael Blumenstein and Jing Jiang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PDYs7Z2XFGv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "QJWVP4CTmW4", "original": "CPwLWeDAbb-", "number": 773, "cdate": 1632875475105, "mdate": null, "ddate": null, "tcdate": 1632875475105, "tmdate": 1676330652197, "tddate": null, "forum": "QJWVP4CTmW4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space", "authorids": ["~Yaohua_Wang2", "~Yaobin_Zhang1", "~Fangyi_Zhang4", "~Senzhang_Wang2", "~Ming_Lin4", "~YuQi_Zhang5", "~Xiuyu_Sun1"], "authors": ["Yaohua Wang", "Yaobin Zhang", "Fangyi Zhang", "Senzhang Wang", "Ming Lin", "YuQi Zhang", "Xiuyu Sun"], "keywords": ["Face Clustering", "Graph Convolutional Networks (GCN)", "Computer Vision"], "abstract": "Face clustering has attracted rising research interest recently to take advantage of massive amounts of face images on the web. State-of-the-art performance has been achieved by Graph Convolutional Networks (GCN) due to their powerful representation capacity. However, existing GCN-based methods build face graphs mainly according to $k$NN relations in the feature space, which may lead to a lot of noise edges connecting two faces of different classes. The face features will be polluted when messages pass along these noise edges, thus degrading the performance of GCNs. In this paper, a novel algorithm named Ada-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In Ada-NETS, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCNs to cluster faces. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods, proving its superiority and generalization. Code is available at https://github.com/damo-cv/Ada-NETS.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|adanets_face_clustering_via_adaptive_neighbour_discovery_in_the_structure_space", "pdf": "/pdf/651a49b1013956d42280a94b59d7b13aa23e2f72.pdf", "one-sentence_summary": "A novel algorithm named Ada-NETS is proposed to construct the clean graph for GCNs to cluster faces in this paper.", "_bibtex": "@inproceedings{\nwang2022adanets,\ntitle={Ada-{NETS}: Face Clustering via Adaptive Neighbour Discovery in the Structure Space},\nauthor={Yaohua Wang and Yaobin Zhang and Fangyi Zhang and Senzhang Wang and Ming Lin and YuQi Zhang and Xiuyu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=QJWVP4CTmW4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "VNqaB1g9393", "original": "3jSw0B7KcSP", "number": 772, "cdate": 1632875475038, "mdate": null, "ddate": null, "tcdate": 1632875475038, "tmdate": 1697934892789, "tddate": null, "forum": "VNqaB1g9393", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Decoupled Adaptation for Cross-Domain Object Detection", "authorids": ["~Junguang_Jiang2", "~Baixu_Chen2", "~Jianmin_Wang1", "~Mingsheng_Long5"], "authors": ["Junguang Jiang", "Baixu Chen", "Jianmin Wang", "Mingsheng Long"], "keywords": ["Object Detection", "Domain Adaptation", "Object Localization", "Deep Learning", "Transfer Learning"], "abstract": "Cross-domain object detection is more challenging than object classification since multiple objects exist in an image and the location of each object is unknown in the unlabeled target domain. As a result, when we adapt features of different objects to enhance the transferability of the detector, the features of the foreground and the background are easy to be confused, which may hurt the discriminability of the detector. Besides, previous methods focused on category adaptation but ignored another important part for object detection, i.e., the adaptation on bounding box regression. To this end, we propose D-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation and the training of the detector. Besides, we fill the blank of regression domain adaptation in object detection by introducing a bounding box adaptor. Experiments show that \\textit{D-adapt} achieves state-of-the-art results on four cross-domain object detection tasks and yields 17\\%  and 21\\% relative improvement on benchmark datasets Clipart1k and Comic2k in particular.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|decoupled_adaptation_for_crossdomain_object_detection", "pdf": "/pdf/88ba4a7c16c8aa0fe22280dd84bf63ff1eccdbee.pdf", "one-sentence_summary": "To deal with the challenges in cross-domain object detection, we propose D-adapt to decouple the adversarial adaptation and the training of the detector, and also decouple the category adaptation and the bounding box adaptation.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02578/code)", "_bibtex": "@inproceedings{\njiang2022decoupled,\ntitle={Decoupled Adaptation for Cross-Domain Object Detection},\nauthor={Junguang Jiang and Baixu Chen and Jianmin Wang and Mingsheng Long},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VNqaB1g9393}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "3Pbra-_u76D", "original": "DFXyXdTPsrZ", "number": 770, "cdate": 1632875474900, "mdate": null, "ddate": null, "tcdate": 1632875474900, "tmdate": 1697934893049, "tddate": null, "forum": "3Pbra-_u76D", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework", "authorids": ["~Xu_Ma2", "~Can_Qin1", "~Haoxuan_You1", "~Haoxi_Ran1", "~Yun_Fu1"], "authors": ["Xu Ma", "Can Qin", "Haoxuan You", "Haoxi Ran", "Yun Fu"], "keywords": ["point cloud representation", "local relation", "mlp"], "abstract": "Point cloud analysis is challenging due to irregularity and unordered data structure. To capture the 3D geometries, prior works mainly rely on exploring sophisticated local geometric extractors, using convolution, graph, or attention mechanisms. These methods, however, incur unfavorable latency during inference and the performance saturates over the past few years. In this paper, we present an ovel perspective on this task. We find detailed local geometrical informationprobably is not the key to point cloud analysis \u2013 we introduce a pure residual MLP network, called PointMLP, which integrates no local geometrical extractors but still performs very competitively. Equipped with a proposed lightweight geometric-affine module to stabilize the training, PointMLP delivers the new state-of-the-art on multiple datasets. On the real-world ScanObjectNN dataset, our method even surpasses the prior best method by 3.3% accuracy. We emphasize PointMLP achieves this strong performance without any sophisticated operations, hence leading to a prominent inference speed. Compared to most recent CurveNet, PointMLP trains 2\u00d7 faster, tests 7\u00d7 faster, and is more accurate on ModelNet40 benchmark. We hope our PointMLP may help the community towards a better understanding of point cloud analysis. The code is available at https://github.com/ma-xu/pointMLP-pytorch.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ma|rethinking_network_design_and_local_geometry_in_point_cloud_a_simple_residual_mlp_framework", "pdf": "/pdf/163d9f0c1b4b43642724b70658bca42821c622e9.pdf", "one-sentence_summary": "In this paper, we present a new design for point cloud analysis , dubbed as PointMLP, which delivers new state-of-the-art results on multiple benchmarks and exhibits gratifying inference speed.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.07123/code)", "_bibtex": "@inproceedings{\nma2022rethinking,\ntitle={Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual {MLP} Framework},\nauthor={Xu Ma and Can Qin and Haoxuan You and Haoxi Ran and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3Pbra-_u76D}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "N8MaByOzUfb", "original": "PDRDI7xJHEr", "number": 767, "cdate": 1632875474689, "mdate": null, "ddate": null, "tcdate": 1632875474689, "tmdate": 1697934893631, "tddate": null, "forum": "N8MaByOzUfb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "New Insights on Reducing Abrupt Representation Change in Online Continual Learning", "authorids": ["~Lucas_Caccia1", "~Rahaf_Aljundi1", "~Nader_Asadi1", "~Tinne_Tuytelaars1", "~Joelle_Pineau1", "~Eugene_Belilovsky1"], "authors": ["Lucas Caccia", "Rahaf Aljundi", "Nader Asadi", "Tinne Tuytelaars", "Joelle Pineau", "Eugene Belilovsky"], "keywords": ["continual learning"], "abstract": "In the online continual learning paradigm, agents must learn from a changing distribution while respecting memory and compute constraints. Experience Replay (ER), where a small subset of past data is stored and replayed alongside new data, has emerged as a simple and effective learning strategy. In this work, we focus on the change in representations of observed data that arises when previously unobserved classes appear in the incoming data stream, and new classes must be distinguished from previous ones. We shed new light on this question by showing that applying ER causes the newly added classes\u2019 representations to overlap significantly with the previous classes, leading to highly disruptive parameter updates.  Based on this empirical analysis, we propose a new method which mitigates this issue by shielding the learned representations from drastic adaptation to accommodate new classes. We show that using an asymmetric update rule pushes new classes to adapt to the older ones (rather than the reverse), which is more effective especially at task boundaries, where much of the forgetting typically occurs. Empirical results show significant gains over strong baselines on standard continual learning benchmarks.", "one-sentence_summary": "We study how representations shift at task boundaries in the single-head online continual learning setting, leading to a simple high performance method", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "caccia|new_insights_on_reducing_abrupt_representation_change_in_online_continual_learning", "pdf": "/pdf/83b22fca57bfe89e11778c687ac0a3b7bb503c15.pdf", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.03798/code)", "_bibtex": "@inproceedings{\ncaccia2022new,\ntitle={New Insights on Reducing Abrupt Representation Change in Online Continual Learning},\nauthor={Lucas Caccia and Rahaf Aljundi and Nader Asadi and Tinne Tuytelaars and Joelle Pineau and Eugene Belilovsky},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=N8MaByOzUfb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "6XGgutacQ0B", "original": "ak9VRo-ZotF", "number": 765, "cdate": 1632875474552, "mdate": null, "ddate": null, "tcdate": 1632875474552, "tmdate": 1676330652896, "tddate": null, "forum": "6XGgutacQ0B", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization", "authorids": ["~Tolga_Ergen1", "~Arda_Sahiner1", "~Batu_Ozturkler1", "~John_M._Pauly1", "~Morteza_Mardani1", "~Mert_Pilanci3"], "authors": ["Tolga Ergen", "Arda Sahiner", "Batu Ozturkler", "John M. Pauly", "Morteza Mardani", "Mert Pilanci"], "keywords": ["batch normalization", "ReLU networks", "deep networks", "convex optimization", "whitening", "implicit regularization", "algorithmic bias"], "abstract": "Batch Normalization (BN) is a commonly used technique to accelerate and stabilize training of deep neural networks. Despite its empirical success, a full theoretical understanding of BN is yet to be developed. In this work, we analyze BN through the lens of convex optimization. We introduce an analytic framework based on convex duality to obtain exact convex representations of weight-decay regularized ReLU networks with BN, which can be trained in polynomial-time. Our analyses also show that optimal layer weights can be obtained as simple closed-form formulas in the high-dimensional and/or overparameterized regimes. Furthermore, we find that Gradient Descent provides an algorithmic bias effect on the standard non-convex BN network, and we design an approach to explicitly encode this implicit regularization into the convex objective. Experiments with CIFAR image classification highlight the effectiveness of this explicit regularization for mimicking and substantially improving the performance of standard BN networks. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ergen|demystifying_batch_normalization_in_relu_networks_equivalent_convex_optimization_models_and_implicit_regularization", "pdf": "/pdf/af5d5ce93fce5052caf45cbcdf0da6ca08cad524.pdf", "one-sentence_summary": "We introduce an analytic framework based on convex duality to obtain exact and polynomial-time trainable convex representations of weight-decay regularized ReLU networks with BN.", "supplementary_material": "/attachment/b7b0f3ee814f6eaac63a4651b0807b370e6a625e.zip", "_bibtex": "@inproceedings{\nergen2022demystifying,\ntitle={Demystifying Batch Normalization in Re{LU} Networks: Equivalent Convex Optimization Models and Implicit Regularization},\nauthor={Tolga Ergen and Arda Sahiner and Batu Ozturkler and John M. Pauly and Morteza Mardani and Mert Pilanci},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6XGgutacQ0B}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 4}}, {"id": "4N-17dske79", "original": "DdjPBig_UB", "number": 764, "cdate": 1632875474480, "mdate": null, "ddate": null, "tcdate": 1632875474480, "tmdate": 1676330653007, "tddate": null, "forum": "4N-17dske79", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Associated Learning: an Alternative to End-to-End Backpropagation that Works on CNN, RNN, and Transformer", "authorids": ["~Dennis_Y.H._Wu1", "~Dinan_Lin1", "~Vincent_Chen4", "~Hung-Hsuan_Chen1"], "authors": ["Dennis Y.H. Wu", "Dinan Lin", "Vincent Chen", "Hung-Hsuan Chen"], "keywords": ["pipeline training", "parallel training", "backpropagation", "associated learning"], "abstract": "This paper studies Associate Learning (AL), an alternative methodology to the end-to-end backpropagation (BP).  We introduce the workflow to convert a neural network into a proper structure such that AL can be used to learn the weights for various types of neural networks.  We compared AL and BP on some of the most successful types of neural networks -- Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Transformer.  Experimental results show that AL consistently outperforms BP on various open datasets.  We discuss possible reasons for AL's success and its limitations.", "one-sentence_summary": "This paper studies Associate Learning, an alternative methodology to the end-to-end backpropagation", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|associated_learning_an_alternative_to_endtoend_backpropagation_that_works_on_cnn_rnn_and_transformer", "pdf": "/pdf/ce77110ad5e5ed0f1d142f313b702412a79e408e.pdf", "supplementary_material": "/attachment/00037c7af16d786121a99947b26e8fd4a8a9fede.zip", "data": "", "_bibtex": "@inproceedings{\nwu2022associated,\ntitle={Associated Learning: an Alternative to End-to-End Backpropagation that Works on {CNN}, {RNN}, and Transformer},\nauthor={Dennis Y.H. Wu and Dinan Lin and Vincent Chen and Hung-Hsuan Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=4N-17dske79}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "MTex8qKavoS", "original": "BJ4s93UUm1", "number": 763, "cdate": 1632875474409, "mdate": null, "ddate": null, "tcdate": 1632875474409, "tmdate": 1697934894810, "tddate": null, "forum": "MTex8qKavoS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution Shifts and Training Conflicts", "authorids": ["~Weixin_Liang1", "~James_Zou1"], "authors": ["Weixin Liang", "James Zou"], "keywords": ["benchmark dataset", "distribution shift", "out-of-domain generalization"], "abstract": "Understanding the performance of machine learning models across diverse data distributions is critically important for reliable applications. Motivated by this, there is a growing focus on curating benchmark datasets that capture distribution shifts. While valuable, the existing benchmarks are limited in that many of them only contain a small number of shifts and they lack systematic annotation about what is different across different shifts. We present MetaShift\u2014a collection of 12,868 sets of natural images across 410 classes\u2014to address this challenge. We leverage the natural heterogeneity of Visual Genome and its annotations to construct MetaShift. The key construction idea is to cluster images using its metadata, which provides context for each image (e.g. \u201ccats with cars\u201d or \u201ccats in bathroom\u201d) that represent distinct data distributions. MetaShift has two important benefits: first, it contains orders of magnitude more natural data shifts than previously available. Second, it provides explicit explanations of what is unique about each of its data sets and a distance score that measures the amount of distribution shift between any two of its data sets. We demonstrate the utility of MetaShift in benchmarking several recent proposals for training models to be robust to data shifts. We find that the simple empirical risk minimization performs the best when shifts are moderate and no method had a systematic advantage for large shifts. We also show how MetaShift can help to visualize conflicts between data subsets during model training. ", "one-sentence_summary": "We leverage annotated subsets within a heterogeneous dataset to evaluate the performance of learning algorithms to distribution shifts and to visualize training dynamics. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liang|metashift_a_dataset_of_datasets_for_evaluating_contextual_distribution_shifts_and_training_conflicts", "pdf": "/pdf/a72720590882b735ca852bb280cdb6545e03794d.pdf", "supplementary_material": "/attachment/bbc1b2ddeb555f510a7c9e5cb29c636a5cf93c73.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.06523/code)", "_bibtex": "@inproceedings{\nliang2022metashift,\ntitle={MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution Shifts and Training Conflicts},\nauthor={Weixin Liang and James Zou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MTex8qKavoS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "yjMQuLLcGWK", "original": "KfvznH0O0zD", "number": 750, "cdate": 1632875473492, "mdate": null, "ddate": null, "tcdate": 1632875473492, "tmdate": 1676330653713, "tddate": null, "forum": "yjMQuLLcGWK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FP-DETR: Detection Transformer Advanced by Fully Pre-training", "authorids": ["~Wen_Wang7", "~Yang_Cao5", "~Jing_Zhang17", "~Dacheng_Tao1"], "authors": ["Wen Wang", "Yang Cao", "Jing Zhang", "Dacheng Tao"], "keywords": ["Object Detection", "Detection Transformer", "Pre-training", "Visual Prompt"], "abstract": "Large-scale pre-training has proven to be effective for visual representation learning on downstream tasks, especially for improving robustness and generalization. However, the recently developed detection transformers only employ pre-training on its backbone while leaving the key component, i.e., a 12-layer transformer, being trained from scratch, which prevents the model from above benefits. This separated training paradigm is mainly caused by the discrepancy between the upstream and downstream tasks. To mitigate the issue, we propose FP-DETR, a new method that Fully Pre-Trains an encoder-only transformer and smoothly fine-tunes it for object detection via a task adapter. Inspired by the success of textual prompts in NLP, we treat query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. To this end, we propose the task adapter which leverages self-attention to model the contextual relation between object query embedding. Experiments on the challenging COCO dataset demonstrate that our FP-DETR achieves competitive performance. Moreover, it enjoys better robustness to common corruptions and generalization to small-size datasets than state-of-the-art detection transformers. Code will be made publicly available at $\\url{https://github.com/encounter1997/FP-DETR}$.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|fpdetr_detection_transformer_advanced_by_fully_pretraining", "pdf": "/pdf/e9eb5c626773238a576771113c31684900fad1b7.pdf", "data": "", "_bibtex": "@inproceedings{\nwang2022fpdetr,\ntitle={{FP}-{DETR}: Detection Transformer Advanced by Fully Pre-training},\nauthor={Wen Wang and Yang Cao and Jing Zhang and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=yjMQuLLcGWK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "Ht85_jyihxp", "original": "UrqpkZDYy3b", "number": 746, "cdate": 1632875473279, "mdate": null, "ddate": null, "tcdate": 1632875473279, "tmdate": 1676330653897, "tddate": null, "forum": "Ht85_jyihxp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient and Differentiable Conformal Prediction with General Function Classes", "authorids": ["~Yu_Bai1", "~Song_Mei1", "~Huan_Wang1", "~Yingbo_Zhou1", "~Caiming_Xiong1"], "authors": ["Yu Bai", "Song Mei", "Huan Wang", "Yingbo Zhou", "Caiming Xiong"], "keywords": ["uncertainty quantification", "conformal prediction", "prediction sets"], "abstract": "  Quantifying the data uncertainty in learning tasks is often done by learning a prediction interval or prediction set of the label given the input. Two commonly desired properties for learned prediction sets are \\emph{valid coverage} and \\emph{good efficiency} (such as low length or low cardinality). Conformal prediction is a powerful technique for learning prediction sets with valid coverage, yet by default its conformalization step only learns a single parameter, and does not optimize the efficiency over more expressive function classes.\n  In this paper, we propose a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of finding the most efficient prediction set subject to valid empirical coverage. This meta-algorithm generalizes existing conformal prediction algorithms, and we show that it achieves approximate valid population coverage and near-optimal efficiency within class, whenever the function class in the conformalization step is low-capacity in a certain sense. Next, this ERM problem is challenging to optimize as it involves a non-differentiable coverage constraint. We develop a gradient-based algorithm for it by approximating the original constrained ERM using differentiable surrogate losses and Lagrangians. Experiments show that our algorithm is able to learn valid prediction sets and improve the efficiency significantly over existing approaches in several applications such as prediction intervals with improved length, minimum-volume prediction sets for multi-output regression, and label prediction sets for image classification.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bai|efficient_and_differentiable_conformal_prediction_with_general_function_classes", "pdf": "/pdf/7faac3481a1c40bd4b7dc004699477c8b2aeb572.pdf", "one-sentence_summary": "We generalize conformal prediction to learning multiple parameters within a general function class, to obtain an improved efficiency subject to valid coverage.", "supplementary_material": "/attachment/b1d5ff1b5876e5ac511edd9ef98d2abf2c9bd41e.zip", "data": "", "_bibtex": "@inproceedings{\nbai2022efficient,\ntitle={Efficient and Differentiable Conformal Prediction with General Function Classes},\nauthor={Yu Bai and Song Mei and Huan Wang and Yingbo Zhou and Caiming Xiong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Ht85_jyihxp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "NYBmJN4MyZ", "original": "4YACHcbhIwZ", "number": 741, "cdate": 1632875472907, "mdate": null, "ddate": null, "tcdate": 1632875472907, "tmdate": 1697934897001, "tddate": null, "forum": "NYBmJN4MyZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Safe Neurosymbolic Learning with Differentiable Symbolic Execution", "authorids": ["~Chenxi_Yang1", "~Swarat_Chaudhuri1"], "authors": ["Chenxi Yang", "Swarat Chaudhuri"], "keywords": ["Verified Learning", "Neurosymbolic Programs", "Safe Learning", "Symbolic Execution"], "abstract": "We study the problem of learning verifiably safe parameters for programs that use neural networks as well as symbolic, human-written code. Such neurosymbolic programs arise in many safety-critical domains. However, because they need not be differentiable, it is hard to learn their parameters using existing gradient-based approaches to safe learning. Our method, Differentiable Symbolic Execution (DSE), samples control flow paths in a program, symbolically constructs worst-case \"safety loss\" along these paths, and backpropagates the gradients of these losses through program operations using a generalization of the REINFORCE estimator. We evaluate the method on a mix of synthetic tasks and real-world benchmarks. Our experiments show that DSE significantly outperforms the state-of-the-art DiffAI method on these tasks.  ", "one-sentence_summary": "We present DSE, the first approach to worst-case-safe parameter learning for potentially non-differentiable neurosymbolic programs where we bridge symbolic execution and stochastic gradient estimator to learn the loss of safety properties.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|safe_neurosymbolic_learning_with_differentiable_symbolic_execution", "pdf": "/pdf/690814f434fc18ef5083c8fa5370d2421cb68514.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.07671/code)", "_bibtex": "@inproceedings{\nyang2022safe,\ntitle={Safe Neurosymbolic Learning with Differentiable Symbolic Execution},\nauthor={Chenxi Yang and Swarat Chaudhuri},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NYBmJN4MyZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 29}}, {"id": "GUrhfTuf_3", "original": "dQcAQlvYZpD", "number": 730, "cdate": 1632875472201, "mdate": null, "ddate": null, "tcdate": 1632875472201, "tmdate": 1676330654654, "tddate": null, "forum": "GUrhfTuf_3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "authorids": ["~Zirui_Wang1", "~Jiahui_Yu1", "~Adams_Wei_Yu1", "~Zihang_Dai1", "~Yulia_Tsvetkov1", "~Yuan_Cao2"], "authors": ["Zirui Wang", "Jiahui Yu", "Adams Wei Yu", "Zihang Dai", "Yulia Tsvetkov", "Yuan Cao"], "keywords": ["Vision-Language Pretraining", "Multimodal Language Model", "Weak Supervision"], "abstract": "With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|simvlm_simple_visual_language_model_pretraining_with_weak_supervision", "pdf": "/pdf/6212c3fec8ad1418306ef5cebb5e0b76abbc4c26.pdf", "data": "", "_bibtex": "@inproceedings{\nwang2022simvlm,\ntitle={Sim{VLM}: Simple Visual Language Model Pretraining with Weak Supervision},\nauthor={Zirui Wang and Jiahui Yu and Adams Wei Yu and Zihang Dai and Yulia Tsvetkov and Yuan Cao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=GUrhfTuf_3}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "aBXzcPPOuX", "original": "84lrxJZV4GG", "number": 728, "cdate": 1632875472063, "mdate": null, "ddate": null, "tcdate": 1632875472063, "tmdate": 1676330654783, "tddate": null, "forum": "aBXzcPPOuX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps", "authorids": ["~Nico_Courts1", "~Henry_Kvinge1"], "authors": ["Nico Courts", "Henry Kvinge"], "keywords": ["generative models", "applications of topology to deep learning", "many-to-one maps", "invertible neural nets"], "abstract": "Many-to-one maps are ubiquitous in machine learning, from the image recognition model that assigns a multitude of distinct images to the concept of \u201ccat\u201d to the time series forecasting model which assigns a range of distinct time-series to a single scalar regression value. While the primary use of such models is naturally to associate correct output to each input, in many problems it is also useful to be able to explore, understand, and sample from a model's fibers, which are the set of input values $x$ such that $f(x) = y,$ for fixed $y$ in the output space. In this paper we show that popular generative architectures are ill-suited to such tasks. Motivated by this, we introduce a novel generative architecture, Bundle Networks, based on the concept of a fiber bundle from (differential) topology. BundleNets exploit the idea of a local trivialization wherein a space can be locally decomposed into a product space that cleanly encodes the many-to-one nature of the map. By enforcing this decomposition in BundleNets and by utilizing state-of-the-art invertible components, investigating a network's fibers becomes natural.", "one-sentence_summary": "We draw from the theory of fiber bundles in (differential) topology to create a principled approach to generative models that allow us to learn and sample from the \"fiber\" over a point in a many-to-one map.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "courts|bundle_networks_fiber_bundles_local_trivializations_and_a_generative_approach_to_exploring_manytoone_maps", "pdf": "/pdf/26d909f2b14d40f2fc1445fa1f2bf766a91d4784.pdf", "_bibtex": "@inproceedings{\ncourts2022bundle,\ntitle={Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps},\nauthor={Nico Courts and Henry Kvinge},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aBXzcPPOuX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "5i2f-aR6B8H", "original": "a11lqNddOvt", "number": 725, "cdate": 1632875471862, "mdate": null, "ddate": null, "tcdate": 1632875471862, "tmdate": 1676330654969, "tddate": null, "forum": "5i2f-aR6B8H", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Privacy Implications of Shuffling", "authorids": ["~Casey_Meehan1", "~Amrita_Roy_Chowdhury1", "~Kamalika_Chaudhuri1", "~Somesh_Jha1"], "authors": ["Casey Meehan", "Amrita Roy Chowdhury", "Kamalika Chaudhuri", "Somesh Jha"], "keywords": ["local differential privacy", "shuffle DP model"], "abstract": "\\ldp deployments are vulnerable to inference attacks as an adversary can link the noisy responses to their identity and subsequently, auxiliary information using the \\textit{order} of the data. An alternative model, shuffle \\textsf{DP}, prevents this by shuffling the noisy responses uniformly at random.  However, this limits the data learnability -- only symmetric functions (input order agnostic) can be learned. In this paper, we strike a balance and show that systematic shuffling of the noisy responses can thwart specific inference attacks while retaining some meaningful data learnability. To this end, we propose a novel privacy guarantee, \\name-privacy, that captures the privacy of the order of a data sequence. \\name-privacy allows tuning the granularity at which the ordinal information is maintained, which formalizes the degree the resistance to inference attacks trading it off with data learnability.  Additionally, we propose a novel shuffling mechanism that can achieve \\name-privacy and demonstrate the practicality of our mechanism via evaluation on real-world datasets. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "meehan|privacy_implications_of_shuffling", "pdf": "/pdf/173bb7fd934e874c2567373bc9917f7ca85707b1.pdf", "one-sentence_summary": "a novel formalization of the privacy offered by shuffling ", "supplementary_material": "/attachment/58c62f90ba8a0ca1711113b01bdfb0417ab0e931.zip", "_bibtex": "@inproceedings{\nmeehan2022privacy,\ntitle={Privacy Implications of Shuffling},\nauthor={Casey Meehan and Amrita Roy Chowdhury and Kamalika Chaudhuri and Somesh Jha},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5i2f-aR6B8H}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "5Qkd7-bZfI", "original": "wivyaoiR6z4", "number": 722, "cdate": 1632875471657, "mdate": null, "ddate": null, "tcdate": 1632875471657, "tmdate": 1676330655255, "tddate": null, "forum": "5Qkd7-bZfI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the role of population heterogeneity in emergent communication", "authorids": ["~Mathieu_Rita1", "~Florian_Strub1", "~Jean-Bastien_Grill2", "~Olivier_Pietquin1", "emmanuel.dupoux@gmail.com"], "authors": ["Mathieu Rita", "Florian Strub", "Jean-Bastien Grill", "Olivier Pietquin", "Emmanuel Dupoux"], "keywords": [], "abstract": "Populations have often been perceived as a structuring component for language to emerge and evolve: the larger the population, the more systematic the language. While this observation is widespread in the sociolinguistic literature, it has not been reproduced in computer simulations with neural agents. In this paper, we thus aim to clarify this apparent contradiction. We explore emergent language properties by varying agent population size in the speaker-listener Lewis Game. After reproducing the experimental paradox, we challenge the simulation assumption that the agent community is homogeneous. We first investigate how speaker-listener asymmetry alters language structure to examine two potential diversity factors: training speed and network capacity. We find out that emergent language properties are only altered by the relative difference of factors between speaker and listener, and not by their absolute values. From then, we leverage this observation to control population heterogeneity without introducing confounding factors. We finally show that introducing such training speed heterogeneities naturally sort out the initial paradox: larger simulated communities start developing more systematic and structured languages.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rita|on_the_role_of_population_heterogeneity_in_emergent_communication", "pdf": "/pdf/58631dde9cf5c1e786259a90eb129ea9f0e5b5b3.pdf", "one-sentence_summary": "This paper discusses the role of population heterogeneities in structuring emergent languages, partially resolving an apparent contraction between the psycho-linguistic and AI literature.", "_bibtex": "@inproceedings{\nrita2022on,\ntitle={On the role of population heterogeneity in emergent communication},\nauthor={Mathieu Rita and Florian Strub and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5Qkd7-bZfI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "qsZoGvFiJn1", "original": "BPmA7D5G9gw", "number": 721, "cdate": 1632875471588, "mdate": null, "ddate": null, "tcdate": 1632875471588, "tmdate": 1697934899276, "tddate": null, "forum": "qsZoGvFiJn1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception", "authorids": ["~Yurong_You1", "~Katie_Z_Luo1", "~Xiangyu_Chen1", "~Junan_Chen1", "~Wei-Lun_Chao1", "~Wen_Sun1", "~Bharath_Hariharan3", "~Mark_Campbell1", "~Kilian_Q_Weinberger1"], "authors": ["Yurong You", "Katie Z Luo", "Xiangyu Chen", "Junan Chen", "Wei-Lun Chao", "Wen Sun", "Bharath Hariharan", "Mark Campbell", "Kilian Q Weinberger"], "keywords": ["3D object detection", "perception with historical context"], "abstract": "Self-driving cars must detect vehicles, pedestrians, and other traf\ufb01c participants accurately to operate safely. Small, far-away, or highly occluded objects are particularly challenging because there is limited information in the LiDAR point clouds for detecting them. To address this challenge, we leverage valuable information from the past: in particular, data collected in past traversals of the same scene. We posit that these past data, which are typically discarded, provide rich contextual information for disambiguating the above-mentioned challenging cases. To this end, we propose a novel end-to-end trainable Hindsight framework to extract this contextual information from past traversals and store it in an easy-to-query data structure, which can then be leveraged to aid future 3D object detection of the same scene. We show that this framework is compatible with most modern 3D detection architectures and can substantially improve their average precision on multiple autonomous driving datasets, most notably by more than 300% on the challenging cases. Our code is available at https://github.com/YurongYou/Hindsight.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "you|hindsight_is_2020_leveraging_past_traversals_to_aid_3d_perception", "pdf": "/pdf/63a8427b5cbf3d30656d84f715bc0982ca9cd409.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.11405/code)", "_bibtex": "@inproceedings{\nyou2022hindsight,\ntitle={Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception},\nauthor={Yurong You and Katie Z Luo and Xiangyu Chen and Junan Chen and Wei-Lun Chao and Wen Sun and Bharath Hariharan and Mark Campbell and Kilian Q Weinberger},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qsZoGvFiJn1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "RriDjddCLN", "original": "YgcBrfh5Jlt", "number": 718, "cdate": 1632875471385, "mdate": null, "ddate": null, "tcdate": 1632875471385, "tmdate": 1697934899943, "tddate": null, "forum": "RriDjddCLN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Language-driven Semantic Segmentation", "authorids": ["~Boyi_Li1", "~Kilian_Q_Weinberger1", "~Serge_Belongie1", "~Vladlen_Koltun1", "~Rene_Ranftl1"], "authors": ["Boyi Li", "Kilian Q Weinberger", "Serge Belongie", "Vladlen Koltun", "Rene Ranftl"], "keywords": ["language-driven", "semantic segmentation", "zero-shot", "transformer"], "abstract": "We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., ``grass'' or ``building'') together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., ``cat'' and ``furry''). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided. Code and demo are available at https://github.com/isl-org/lang-seg.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|languagedriven_semantic_segmentation", "pdf": "/pdf/9214428c3de4dee047f2ade9ea22c54498592843.pdf", "one-sentence_summary": "We present a language-driven approach that enables synthesis of zero-shot semantic segmentation models from arbitrary label sets at test time.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.03546/code)", "_bibtex": "@inproceedings{\nli2022languagedriven,\ntitle={Language-driven Semantic Segmentation},\nauthor={Boyi Li and Kilian Q Weinberger and Serge Belongie and Vladlen Koltun and Rene Ranftl},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RriDjddCLN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "ydopy-e6Dg", "original": "e8Nsb-8wtz6", "number": 716, "cdate": 1632875471247, "mdate": null, "ddate": null, "tcdate": 1632875471247, "tmdate": 1676330655750, "tddate": null, "forum": "ydopy-e6Dg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Image BERT Pre-training with Online Tokenizer", "authorids": ["~Jinghao_Zhou1", "~Chen_Wei2", "~Huiyu_Wang1", "~Wei_Shen2", "~Cihang_Xie3", "~Alan_Yuille1", "~Tao_Kong3"], "authors": ["Jinghao Zhou", "Chen Wei", "Huiyu Wang", "Wei Shen", "Cihang Xie", "Alan Yuille", "Tao Kong"], "keywords": ["online tokenizer", "masked image modeling", "vision transformer"], "abstract": "The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces.\nIn this work, we study masked image modeling (MIM) and indicate the necessity and challenges of using a semantically meaningful visual tokenizer.\nWe present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. \nSpecifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics.\nThe online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pre-trained beforehand.\nWe show the prominence of iBOT by achieving an 82.3% linear probing accuracy and an 87.8% fine-tuning accuracy evaluated on ImageNet-1K.\nBeyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, e.g., object detection, instance segmentation, and semantic segmentation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhou|image_bert_pretraining_with_online_tokenizer", "pdf": "/pdf/12096c8a70fdac02ab174d59593366e17a8f8e3b.pdf", "one-sentence_summary": "We present a self-supervised framework iBOT that can perform masked image modeling with an online tokenizer, achieving the state-of-the-art results in downstream tasks.", "data": "", "_bibtex": "@inproceedings{\nzhou2022image,\ntitle={Image {BERT} Pre-training with Online Tokenizer},\nauthor={Jinghao Zhou and Chen Wei and Huiyu Wang and Wei Shen and Cihang Xie and Alan Yuille and Tao Kong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ydopy-e6Dg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "ZSKRQMvttc", "original": "aRnzBhpyfkB", "number": 708, "cdate": 1632875470688, "mdate": null, "ddate": null, "tcdate": 1632875470688, "tmdate": 1697934901235, "tddate": null, "forum": "ZSKRQMvttc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Accelerated Policy Learning with Parallel Differentiable Simulation", "authorids": ["~Jie_Xu7", "~Viktor_Makoviychuk1", "~Yashraj_Narang1", "~Fabio_Ramos1", "~Wojciech_Matusik2", "~Animesh_Garg1", "~Miles_Macklin1"], "authors": ["Jie Xu", "Viktor Makoviychuk", "Yashraj Narang", "Fabio Ramos", "Wojciech Matusik", "Animesh Garg", "Miles Macklin"], "keywords": ["Robot Control", "Policy Learning", "Differentiable Simulation", "Reinforcement Learning"], "abstract": "Deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively. Recent work has attempted to address this issue by leveraging differentiable simulators. However, inherent problems such as local minima and exploding/vanishing numerical gradients prevent these methods from being generally applied to control tasks with complex contact-rich dynamics, such as humanoid locomotion in classical RL benchmarks. In this work we present a high-performance differentiable simulator and a new policy learning algorithm (SHAC) that can effectively leverage simulation gradients, even in the presence of non-smoothness. Our learning algorithm alleviates problems with local minima through a smooth critic function, avoids vanishing/exploding gradients through a truncated learning window, and allows many physical environments to be run in parallel. We evaluate our method on classical RL control tasks, and show substantial improvements in sample efficiency and wall-clock time over state-of-the-art RL and differentiable simulation-based algorithms. In addition, we demonstrate the scalability of our method by applying it to the challenging high-dimensional problem of muscle-actuated locomotion with a large action space, achieving a greater than $17\\times$ reduction in training time over the best-performing established RL algorithm. More visual results are provided at: https://short-horizon-actor-critic.github.io/.", "pdf": "/pdf/114df250fe35e26582332a1e3f398f2f33148603.pdf", "one-sentence_summary": "We propose an efficient policy learning method leveraging the recent advance of differentiable simulation, and our method outperforms state-of-the-art algorithms in both sample efficiency and wall clock time on multiple challenging control tasks.", "supplementary_material": "/attachment/6e91626fdb66dc4d9a2be62ea0642fd595f6b723.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|accelerated_policy_learning_with_parallel_differentiable_simulation", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.07137/code)", "_bibtex": "@inproceedings{\nxu2022accelerated,\ntitle={Accelerated Policy Learning with Parallel Differentiable Simulation},\nauthor={Jie Xu and Miles Macklin and Viktor Makoviychuk and Yashraj Narang and Animesh Garg and Fabio Ramos and Wojciech Matusik},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZSKRQMvttc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "hl9ePdHO4_s", "original": "k9Um0o9jAo_", "number": 705, "cdate": 1632875470468, "mdate": null, "ddate": null, "tcdate": 1632875470468, "tmdate": 1676330656612, "tddate": null, "forum": "hl9ePdHO4_s", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Do We Need Anisotropic Graph Neural Networks?", "authorids": ["~Shyam_A._Tailor1", "~Felix_Opolka1", "~Pietro_Lio1", "~Nicholas_Donald_Lane1"], "authors": ["Shyam A. Tailor", "Felix Opolka", "Pietro Lio", "Nicholas Donald Lane"], "keywords": ["graph neural networks", "efficiency", "latency reduction", "memory reduction", "architecture design", "benchmarking", "hardware-aware"], "abstract": "Common wisdom in the graph neural network (GNN) community dictates that anisotropic models---in which messages sent between nodes are a function of both the source and target node---are required to achieve state-of-the-art performance. Benchmarks to date have demonstrated that these models perform better than comparable isotropic models---where messages are a function of the source node only. In this work we provide empirical evidence challenging this narrative: we propose an isotropic GNN, which we call Efficient Graph Convolution (EGC), that consistently outperforms comparable anisotropic models, including the popular GAT or PNA architectures by using spatially-varying adaptive filters. In addition to raising important questions for the GNN community, our work has significant real-world implications for efficiency. EGC achieves higher model accuracy, with lower memory consumption and latency, along with characteristics suited to accelerator implementation, while being a drop-in replacement for existing architectures. As an isotropic model, it requires memory proportional to the number of vertices in the graph ($\\mathcal{O}(V)$); in contrast, anisotropic models require memory proportional to the number of edges ($\\mathcal{O}(E)$). We demonstrate that EGC outperforms existing approaches across 6 large and diverse benchmark datasets, and conclude by discussing questions that our work raise for the community going forward. Code and pretrained models for our experiments are provided at https://github.com/shyam196/egc.", "pdf": "/pdf/497c4355d426c672bb9611ee54d17fe0d737dad2.pdf", "one-sentence_summary": "We find that using a simple adaptive filtering approach for GNNs improves performance over SOTA, while reducing model memory consumption and latency.", "supplementary_material": "/attachment/7f8f481307bd6e5156cd01390f5b59064b078d33.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tailor|do_we_need_anisotropic_graph_neural_networks", "_bibtex": "@inproceedings{\ntailor2022adaptive,\ntitle={Adaptive Filters for Low-Latency and Memory-Efficient Graph Neural Networks},\nauthor={Shyam A. Tailor and Felix Opolka and Pietro Lio and Nicholas Donald Lane},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hl9ePdHO4_s}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "9xhgmsNVHu", "original": "nv0uh02VWaq", "number": 704, "cdate": 1632875470398, "mdate": null, "ddate": null, "tcdate": 1632875470398, "tmdate": 1697934901953, "tddate": null, "forum": "9xhgmsNVHu", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Is High Variance Unavoidable in RL? A Case Study in Continuous Control", "authorids": ["~Johan_Bjorck2", "~Carla_P_Gomes1", "~Kilian_Q_Weinberger1"], "authors": ["Johan Bjorck", "Carla P Gomes", "Kilian Q Weinberger"], "keywords": ["reinforcement learning", "continuous control"], "abstract": "Reinforcement learning (RL) experiments have notoriously high variance, and minor details can have disproportionately large effects on measured outcomes. This is problematic for creating reproducible research and also serves as an obstacle when applying RL to sensitive real-world applications. In this paper, we investigate causes for this perceived instability. To allow for an in-depth analysis, we focus on a specifically popular setup with high variance -- continuous control from pixels with an actor-critic agent. In this setting, we demonstrate that poor outlier runs which completely fail to learn are an important source of variance, but that weight initialization and initial exploration are not at fault. We show that one cause for these outliers is unstable network parametrization which leads to saturating nonlinearities. We investigate several fixes to this issue and find that simply normalizing penultimate features is surprisingly effective. For sparse tasks, we also find that partially disabling clipped double Q-learning decreases variance. By combining fixes we significantly decrease variances, lowering the average standard deviation across 21 tasks by a factor >3 for a state-of-the-art agent. This demonstrates that the perceived variance is not necessarily inherent to RL. Instead, it may be addressed via simple modifications and we argue that developing low-variance agents is an important goal for the RL community.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bjorck|is_high_variance_unavoidable_in_rl_a_case_study_in_continuous_control", "pdf": "/pdf/231cc619ca2bc7ec04038f562613fdfa22ef8634.pdf", "one-sentence_summary": "we study sources of variance in RL and propose methods to decrease it.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.11222/code)", "_bibtex": "@inproceedings{\nbjorck2022is,\ntitle={Is High Variance Unavoidable in {RL}? A Case Study in Continuous Control},\nauthor={Johan Bjorck and Carla P Gomes and Kilian Q Weinberger},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9xhgmsNVHu}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "1wVvweK3oIb", "original": "AtnSbdnPrT", "number": 701, "cdate": 1632875470172, "mdate": null, "ddate": null, "tcdate": 1632875470172, "tmdate": 1676330656991, "tddate": null, "forum": "1wVvweK3oIb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond", "authorids": ["~Jonathan_Godwin1", "~Michael_Schaarschmidt1", "~Alexander_L_Gaunt1", "~Alvaro_Sanchez-Gonzalez1", "~Yulia_Rubanova2", "~Petar_Veli\u010dkovi\u01071", "~James_Kirkpatrick1", "~Peter_Battaglia1"], "authors": ["Jonathan Godwin", "Michael Schaarschmidt", "Alexander L Gaunt", "Alvaro Sanchez-Gonzalez", "Yulia Rubanova", "Petar Veli\u010dkovi\u0107", "James Kirkpatrick", "Peter Battaglia"], "keywords": ["Graph Neural Networks", "GNNs", "Deep Learning", "Molecular Property Prediction"], "abstract": "In this paper we show that simple noisy regularisation can be an effective way to address oversmoothing. We first argue that regularisers ad-dressing oversmoothing should both penalise node latent similarity and encourage meaningful node representations. From this observation we derive \u201cNoisy Nodes\u201d,a simple technique in which we corrupt the input graph with noise, and add a noise correcting node-level loss.  The diverse node level loss encourages latent node diversity, and the denoising objective encourages graph manifold learning.  Our regulariser applies well-studied methods in simple, straightforward ways which allow even generic architectures to overcome oversmoothing and achieve state of the art results on quantum chemistry tasks such as QM9 and Open Catalyst, and improve results significantly on Open Graph Benchmark (OGB) datasets.  Our results suggest Noisy Nodes can serve as a complementary building block in the GNN toolkit.", "pdf": "/pdf/435f7c82f234394254199216f44bf92910d8c237.pdf", "one-sentence_summary": "A simple regularisation technique for GNNs applied to 3D molecular property prediction & beyond.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "godwin|simple_gnn_regularisation_for_3d_molecular_property_prediction_and_beyond", "data": "", "_bibtex": "@inproceedings{\ngodwin2022simple,\ntitle={Simple {GNN} Regularisation for 3D Molecular Property Prediction and Beyond},\nauthor={Jonathan Godwin and Michael Schaarschmidt and Alexander L Gaunt and Alvaro Sanchez-Gonzalez and Yulia Rubanova and Petar Veli{\\v{c}}kovi{\\'c} and James Kirkpatrick and Peter Battaglia},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1wVvweK3oIb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "2bO2x8NAIMB", "original": "S_Ws1c_k5gZ", "number": 700, "cdate": 1632875470104, "mdate": null, "ddate": null, "tcdate": 1632875470104, "tmdate": 1676330657079, "tddate": null, "forum": "2bO2x8NAIMB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative", "authorids": ["~Lucio_M._Dery1", "~Paul_Michel1", "~Ameet_Talwalkar1", "~Graham_Neubig1"], "authors": ["Lucio M. Dery", "Paul Michel", "Ameet Talwalkar", "Graham Neubig"], "keywords": ["pre-training", "multitask learning", "meta-learning", "deeplearning", "end-task aware training", "NLP"], "abstract": "In most settings of practical concern, machine learning practitioners know in advance what end-task they wish to boost with auxiliary tasks. However, widely used methods for leveraging auxiliary data like pre-training and its continued-pretraining variant are end-task agnostic: they rarely, if ever, exploit knowledge of the target task. We study replacing end-task agnostic continued training of pre-trained language models with end-task aware training of said models. We argue that for sufficiently important end-tasks, the benefits of leveraging auxiliary data in a task-aware fashion can justify forgoing the traditional approach of obtaining generic, end-task agnostic representations as with (continued) pre-training. On three different low-resource NLP tasks from two domains, we demonstrate that  multi-tasking the end-task and auxiliary objectives results in significantly better downstream task performance than the widely-used task-agnostic continued pre-training paradigm of Gururangan et al. (2020).\nWe next introduce an online meta-learning algorithm that learns  a set of multi-task weights to better balance among our multiple auxiliary objectives, achieving further improvements on end-task performance and data efficiency.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dery|should_we_be_pretraining_an_argument_for_endtask_aware_training_as_an_alternative", "pdf": "/pdf/691ed677722bace4faaff6140d7c50eac5c22122.pdf", "one-sentence_summary": "When we know the end-task objective in advance, instead of pre-training on auxiliary objectives  and then fine-tuning, we advocate for it to be introduced directly in training with auxiliary objectives ", "supplementary_material": "/attachment/95d6f185a3830744171e2f6cf96e2bd4b79f1b9e.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\ndery2022should,\ntitle={Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative},\nauthor={Lucio M. Dery and Paul Michel and Ameet Talwalkar and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=2bO2x8NAIMB}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "wogsFPHwftY", "original": "7RIH-PYmgvI", "number": 695, "cdate": 1632875469747, "mdate": null, "ddate": null, "tcdate": 1632875469747, "tmdate": 1697934903795, "tddate": null, "forum": "wogsFPHwftY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Super-Features for Image Retrieval", "authorids": ["~Philippe_Weinzaepfel1", "~Thomas_Lucas1", "~Diane_Larlus1", "~Yannis_Kalantidis2"], "authors": ["Philippe Weinzaepfel", "Thomas Lucas", "Diane Larlus", "Yannis Kalantidis"], "keywords": ["image retrieval", "landmark retrieval", "mid-level features"], "abstract": "Methods that combine local and global features have recently shown excellent performance on multiple challenging deep image retrieval benchmarks, but their use of local features raises at least two issues. First, these local features simply boil down to the localized map activations of a neural network, and hence can be extremely redundant. Second, they are typically trained with a global loss that only acts on top of an aggregation of local features; by contrast, testing is based on local feature matching, which creates a discrepancy between training and testing. In this paper, we propose a novel architecture for deep image retrieval, based solely on mid-level features that we call Super-features. These Super-features are constructed by an iterative attention module and constitute an ordered set in which each element focuses on a localized and discriminant image pattern. For training, they require only image labels. A contrastive loss operates directly at the level of Super-features and focuses on those that match across images. A second complementary loss encourages diversity. Experiments on common landmark retrieval benchmarks validate that Super-features substantially outperform state-of-the-art methods when using the same number of features, and only require a significantly smaller memory footprint to match their performance. Code and models are available at: https://github.com/naver/FIRe.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "weinzaepfel|learning_superfeatures_for_image_retrieval", "pdf": "/pdf/e14496dfee7726a2e7d9d921f56cca54fe6f8528.pdf", "one-sentence_summary": "A novel image retrieval framework that learns mid-level features performing better and more compact that standard local ones", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2201.13182/code)", "_bibtex": "@inproceedings{\nweinzaepfel2022learning,\ntitle={Learning Super-Features for Image Retrieval},\nauthor={Philippe Weinzaepfel and Thomas Lucas and Diane Larlus and Yannis Kalantidis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wogsFPHwftY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "DSQHjibtgKR", "original": "-2FjvPhw7ck", "number": 682, "cdate": 1632875468960, "mdate": null, "ddate": null, "tcdate": 1632875468960, "tmdate": 1676330658253, "tddate": null, "forum": "DSQHjibtgKR", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Online Facility Location with Predictions", "authorids": ["~Shaofeng_H.-C._Jiang1", "~Erzhi_Liu1", "~You_Lyu1", "~Zhihao_Gavin_Tang1", "~Yubo_Zhang4"], "authors": ["Shaofeng H.-C. Jiang", "Erzhi Liu", "You Lyu", "Zhihao Gavin Tang", "Yubo Zhang"], "keywords": ["online algorithms", "facility location", "prediction", "learning-augmented"], "abstract": "We provide nearly optimal algorithms for online facility location (OFL) with predictions. In OFL, $n$ demand points arrive in order and the algorithm must irrevocably assign each demand point to an open facility upon its arrival. The objective is to minimize the total connection costs from demand points to assigned facilities plus the facility opening cost. We further assume the algorithm is additionally given for each demand point $x_i$ a natural prediction $f_{x_i}^{\\mathrm{pred}}$ which is supposed to be the facility $f_{x_i}^{\\mathrm{opt}}$ that serves $x_i$ in the offline optimal solution.\n\nOur main result is an $O(\\min\\{\\log {\\frac{n\\eta_\\infty}{\\mathrm{OPT}}}, \\log{n} \\})$-competitive algorithm where $\\eta_\\infty$ is the maximum prediction error (i.e., the distance between $f_{x_i}^{\\mathrm{pred}}$ and $f_{x_i}^{\\mathrm{opt}}$). Our algorithm overcomes the fundamental $\\Omega(\\frac{\\log n}{\\log \\log n})$ lower bound of OFL (without predictions) when $\\eta_\\infty$ is small, and it still maintains $O(\\log n)$ ratio even when $\\eta_\\infty$ is unbounded. Furthermore, our theoretical analysis is supported by empirical evaluations for the tradeoffs between $\\eta_\\infty$ and the competitive ratio on various real datasets of different types.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|online_facility_location_with_predictions", "pdf": "/pdf/e6b7ec4392916092fad88aaf52f9e10a20de1162.pdf", "one-sentence_summary": "We give a nearly optimal robust algorithm for online facility location with predictions.", "supplementary_material": "/attachment/348bec02cc49ca65158df822f2ef8909b22eebc9.zip", "_bibtex": "@inproceedings{\njiang2022online,\ntitle={Online Facility Location with Predictions},\nauthor={Shaofeng H.-C. Jiang and Erzhi Liu and You Lyu and Zhihao Gavin Tang and Yubo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DSQHjibtgKR}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "qSV5CuSaK_a", "original": "pQ6ECor--K", "number": 681, "cdate": 1632875468885, "mdate": null, "ddate": null, "tcdate": 1632875468885, "tmdate": 1697934905631, "tddate": null, "forum": "qSV5CuSaK_a", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Few-Shot Backdoor Attacks on Visual Object Tracking", "authorids": ["~Yiming_Li1", "~Haoxiang_Zhong1", "~Xingjun_Ma1", "~Yong_Jiang3", "~Shu-Tao_Xia1"], "authors": ["Yiming Li", "Haoxiang Zhong", "Xingjun Ma", "Yong Jiang", "Shu-Tao Xia"], "keywords": ["Backdoor Attack", "Visual Object Tracking", "AI Security", "Deep Learning"], "abstract": "Visual object tracking (VOT) has been widely adopted in mission-critical applications, such as autonomous driving and intelligent surveillance systems. In current practice, third-party resources such as datasets, backbone networks, and training platforms are frequently used to train high-performance VOT models. Whilst these resources bring certain convenience, they also introduce new security threats into VOT models. In this paper, we reveal such a threat where an adversary can easily implant hidden backdoors into VOT models by tempering with the training process. Specifically, we propose a simple yet effective few-shot backdoor attack (FSBA) that optimizes two losses alternately: 1) a \\emph{feature loss} defined in the hidden feature space, and 2) the standard \\emph{tracking loss}. We show that, once the backdoor is embedded into the target model by our FSBA, it can trick the model to lose track of specific objects even when the \\emph{trigger} only appears in one or a few frames. We examine our attack in both digital and physical-world settings and show that it can significantly degrade the performance of state-of-the-art VOT trackers. We also show that our attack is resistant to potential defenses, highlighting the vulnerability of VOT models to potential backdoor attacks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|fewshot_backdoor_attacks_on_visual_object_tracking", "pdf": "/pdf/132d1b18d6c8d837cebbdb801781870f713295cb.pdf", "one-sentence_summary": "We propose a simple yet effective backdoor attack against visual object tracking, which is effective in both digital and physical-world scenarios even if the trigger only appears in a few frames and resistant to potential defenses.", "supplementary_material": "/attachment/9276c97033583e9790e0143165d7dc35d09ded58.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2201.13178/code)", "_bibtex": "@inproceedings{\nli2022fewshot,\ntitle={Few-Shot Backdoor Attacks on Visual Object Tracking},\nauthor={Yiming Li and Haoxiang Zhong and Xingjun Ma and Yong Jiang and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qSV5CuSaK_a}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "TySnJ-0RdKI", "original": "IGFVSH4Jede", "number": 674, "cdate": 1632875468376, "mdate": null, "ddate": null, "tcdate": 1632875468376, "tmdate": 1697934906851, "tddate": null, "forum": "TySnJ-0RdKI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Backdoor Defense via Decoupling the Training Process", "authorids": ["hkunzhe@zju.edu.cn", "~Yiming_Li1", "~Baoyuan_Wu1", "qinzhan@zju.edu.cn", "kuiren@zju.edu.cn"], "authors": ["Kunzhe Huang", "Yiming Li", "Baoyuan Wu", "Zhan Qin", "Kui Ren"], "keywords": ["Backdoor Defense", "Backdoor Learning"], "abstract": "Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end-to-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via \\emph{self-supervised learning} based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some `low-credible' samples determined based on the learned model and conduct a \\emph{semi-supervised fine-tuning} of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at \\url{https://github.com/SCLBD/DBD}.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|backdoor_defense_via_decoupling_the_training_process", "pdf": "/pdf/825a2fee50fe494bcf13085113d2a7565af192b6.pdf", "one-sentence_summary": "We reveal that the hidden backdoors are embedded in the feature space mostly due to the end-to-end supervised training paradigm, based on which we propose a simple yet effective decoupling-based training method for backdoor defense. ", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 9 code implementations](https://www.catalyzex.com/paper/arxiv:2202.03423/code)", "_bibtex": "@inproceedings{\nhuang2022backdoor,\ntitle={Backdoor Defense via Decoupling the Training Process},\nauthor={Kunzhe Huang and Yiming Li and Baoyuan Wu and Zhan Qin and Kui Ren},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TySnJ-0RdKI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "q79uMSC6ZBT", "original": "vBKXuzjoUBK", "number": 664, "cdate": 1632875467725, "mdate": null, "ddate": null, "tcdate": 1632875467725, "tmdate": 1676330659358, "tddate": null, "forum": "q79uMSC6ZBT", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Complete Code with Sketches", "authorids": ["~Daya_Guo3", "~Alexey_Svyatkovskiy1", "~Jian_Yin3", "~Nan_Duan1", "~Marc_Brockschmidt1", "~Miltiadis_Allamanis1"], "authors": ["Daya Guo", "Alexey Svyatkovskiy", "Jian Yin", "Nan Duan", "Marc Brockschmidt", "Miltiadis Allamanis"], "keywords": ["sketch", "generative model", "ml4code"], "abstract": "Code completion is usually cast as a language modelling problem, i.e., continuing an input in a left-to-right fashion. However, in practice, some parts of the completion (e.g., string literals) may be very hard to predict, whereas subsequent parts directly follow from the context.\nTo handle this, we instead consider the scenario of generating code completions with \"holes\" inserted in places where a model is uncertain. We develop Grammformer, a Transformer-based model that guides the code generation by the programming language grammar, and compare it to a variety of more standard sequence models.\n\nWe train the models on code completion for C# and Python given partial code context. To evaluate models, we consider both ROUGE as well as a new metric RegexAcc that measures success of generating completions matching long outputs with as few holes as possible.\nIn our experiments, Grammformer generates 10-50% more accurate completions compared to traditional generative models and 37-50% longer sketches compared to sketch-generating baselines trained with similar techniques.", "one-sentence_summary": "Autocomplete code sketches, placing holes where ambiguity prevents us predicting terminal tokens.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guo|learning_to_complete_code_with_sketches", "pdf": "/pdf/721a38d7a69acd79d164ead05430e5912c4c0721.pdf", "_bibtex": "@inproceedings{\nguo2022learning,\ntitle={Learning to Complete Code with Sketches},\nauthor={Daya Guo and Alexey Svyatkovskiy and Jian Yin and Nan Duan and Marc Brockschmidt and Miltiadis Allamanis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=q79uMSC6ZBT}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "gpp7cf0xdfN", "original": "BluCI30gXY5", "number": 663, "cdate": 1632875467653, "mdate": null, "ddate": null, "tcdate": 1632875467653, "tmdate": 1697934907988, "tddate": null, "forum": "gpp7cf0xdfN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Reverse Engineering of Imperceptible Adversarial Image Perturbations", "authorids": ["~Yifan_Gong2", "~Yuguang_Yao1", "~Yize_Li1", "~Yimeng_Zhang2", "~Xiaoming_Liu2", "~Xue_Lin1", "~Sijia_Liu1"], "authors": ["Yifan Gong", "Yuguang Yao", "Yize Li", "Yimeng Zhang", "Xiaoming Liu", "Xue Lin", "Sijia Liu"], "keywords": ["Reverse Engineering of Deceptions", "adversarial examples", "denoising", "neural networks", "interpretability"], "abstract": "It has been well recognized that neural network based image classifiers are easily fooled by images with tiny perturbations crafted by an adversary. There has been a vast volume of research to generate and defend such adversarial attacks. However, the following problem is left unexplored: How to reverse-engineer adversarial perturbations from an adversarial image? This leads to a new adversarial learning paradigm\u2014Reverse Engineering of Deceptions (RED). If successful, RED allows us to estimate adversarial perturbations and recover the original images. However, carefully crafted, tiny adversarial perturbations are difficult to recover by optimizing a unilateral RED objective. For example, the pure image denoising method may overfit to minimizing the reconstruction error but hardly preserve the classification properties of the true adversarial perturbations.  To tackle this challenge, we formalize the RED problem and identify a set of principles crucial to the RED approach design. Particularly, we find that prediction alignment and proper data augmentation (in terms of spatial transformations) are two criteria to achieve a generalizable RED approach. By integrating these RED principles with image denoising, we propose a new Class-Discriminative Denoising based RED framework, termed CDD-RED. Extensive experiments demonstrate the effectiveness of CDD-RED under different evaluation metrics (ranging from the pixel-level, prediction-level to the attribution-level alignment) and a variety of attack generation methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gong|reverse_engineering_of_imperceptible_adversarial_image_perturbations", "pdf": "/pdf/0cd0b682a03374a0d8e8ed22bcabcbb5befb3bbc.pdf", "one-sentence_summary": "Reverse engineer adversarial image perturbations with a denoiser-based framework.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.14145/code)", "_bibtex": "@inproceedings{\ngong2022reverse,\ntitle={Reverse Engineering of Imperceptible Adversarial Image Perturbations},\nauthor={Yifan Gong and Yuguang Yao and Yize Li and Yimeng Zhang and Xiaoming Liu and Xue Lin and Sijia Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gpp7cf0xdfN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "oMI9PjOb9Jl", "original": "vCKi1lrxOx", "number": 660, "cdate": 1632875467438, "mdate": null, "ddate": null, "tcdate": 1632875467438, "tmdate": 1697934908367, "tddate": null, "forum": "oMI9PjOb9Jl", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR", "authorids": ["~Shilong_Liu1", "~Feng_Li9", "~Hao_Zhang39", "~Xiao_Yang4", "~Xianbiao_Qi2", "~Hang_Su3", "~Jun_Zhu2", "~Lei_Zhang23"], "authors": ["Shilong Liu", "Feng Li", "Hao Zhang", "Xiao Yang", "Xianbiao Qi", "Hang Su", "Jun Zhu", "Lei Zhang"], "keywords": ["Object detection", "Transformer"], "abstract": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer by layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer by layer in a cascade manner. As a result, it leads to the best performance on the MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \\url{https://github.com/IDEA-opensource/DAB-DETR}.", "one-sentence_summary": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR and offer a deeper understanding of the role of queries in DETR. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|dabdetr_dynamic_anchor_boxes_are_better_queries_for_detr", "pdf": "/pdf/3222fb33c40d6a03db797feaa7cded67412477bf.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2201.12329/code)", "_bibtex": "@inproceedings{\nliu2022dabdetr,\ntitle={{DAB}-{DETR}: Dynamic Anchor Boxes are Better Queries for {DETR}},\nauthor={Shilong Liu and Feng Li and Hao Zhang and Xiao Yang and Xianbiao Qi and Hang Su and Jun Zhu and Lei Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oMI9PjOb9Jl}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "tUa4REjGjTf", "original": "OrGCs2HJyyt", "number": 658, "cdate": 1632875467292, "mdate": null, "ddate": null, "tcdate": 1632875467292, "tmdate": 1697934908599, "tddate": null, "forum": "tUa4REjGjTf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Certified Robustness for Ensemble Models and Beyond", "authorids": ["~Zhuolin_Yang1", "~Linyi_Li1", "~Xiaojun_Xu1", "~Bhavya_Kailkhura1", "~Tao_Xie4", "~Bo_Li19"], "authors": ["Zhuolin Yang", "Linyi Li", "Xiaojun Xu", "Bhavya Kailkhura", "Tao Xie", "Bo Li"], "keywords": ["robustness", "ensemble", "certified robustness"], "abstract": "Recent studies show that deep neural networks (DNN) are vulnerable to adversarial examples, which aim to mislead DNNs by adding perturbations with small magnitude. To defend against such attacks, both empirical and theoretical defense approaches have been extensively studied for a single ML model. In this work, we aim to analyze and provide the certified robustness for ensemble ML models, together with the sufficient and necessary conditions of robustness for different ensemble protocols. Although ensemble models are shown more robust than a single model empirically; surprisingly, we find that in terms of the certified robustness the standard ensemble models only achieve marginal improvement compared to a single model. Thus, to explore the conditions that guarantee to provide certifiably robust ensemble ML models, we first prove that diversified gradient and large confidence margin are sufficient and necessary conditions for certifiably robust ensemble models under the model-smoothness assumption. We then provide the bounded model-smoothness analysis based on the proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble model can always achieve higher certified robustness than a single base model under mild conditions. Inspired by the theoretical findings, we propose the lightweight Diversity Regularized Training (DRT) to train certifiably robust ensemble ML models. Extensive experiments show that our DRT enhanced ensembles can consistently achieve higher certified robustness than existing single and ensemble ML models, demonstrating the state-of-the-art certified $L_2$-robustness on MNIST, CIFAR-10, and ImageNet datasets.", "one-sentence_summary": "Inspired by theoretical analysis, we propose Diversity Regularized Training to enhance the certified robustness of ensemble models and DRT significantly outperforms existing methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|on_the_certified_robustness_for_ensemble_models_and_beyond", "pdf": "/pdf/ff715dc7ef2af7f93fff118ec95ee5c8f6b7c3b9.pdf", "supplementary_material": "/attachment/1d17f01507583bb24234968daf2daf5bcc0829f6.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.10873/code)", "_bibtex": "@inproceedings{\nyang2022on,\ntitle={On the Certified Robustness for Ensemble Models and Beyond},\nauthor={Zhuolin Yang and Linyi Li and Xiaojun Xu and Bhavya Kailkhura and Tao Xie and Bo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tUa4REjGjTf}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "eYciPrLuUhG", "original": "LctCwPXvaiW", "number": 657, "cdate": 1632875467218, "mdate": null, "ddate": null, "tcdate": 1632875467218, "tmdate": 1697934908627, "tddate": null, "forum": "eYciPrLuUhG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Efficient Neural Causal Discovery without Acyclicity Constraints", "authorids": ["~Phillip_Lippe1", "~Taco_Cohen1", "~Efstratios_Gavves1"], "authors": ["Phillip Lippe", "Taco Cohen", "Efstratios Gavves"], "keywords": ["Causal discovery", "structure learning"], "abstract": "Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous optimization for score-based methods, which, however, require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an efficient structure learning method for directed, acyclic causal graphs leveraging observational and interventional data. ENCO formulates the graph search as an optimization of independent edge likelihoods, with the edge orientation being modeled as a separate parameter. Consequently, we provide for ENCO convergence guarantees under mild conditions, without having to constrain the score function with respect to acyclicity. In experiments, we show that ENCO can efficiently recover graphs with hundreds of nodes, an order of magnitude larger than what was previously possible, while handling deterministic variables and discovering latent confounders.", "one-sentence_summary": "We present ENCO, an efficient structure learning method that leverages observational and interventional data and scales to graphs with a thousand variables.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lippe|efficient_neural_causal_discovery_without_acyclicity_constraints", "pdf": "/pdf/5cf966d1e6fc357e1082cb17294a14750c01b92e.pdf", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2107.10483/code)", "_bibtex": "@inproceedings{\nlippe2022efficient,\ntitle={Efficient Neural Causal Discovery without Acyclicity Constraints},\nauthor={Phillip Lippe and Taco Cohen and Efstratios Gavves},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=eYciPrLuUhG}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "6Q52pZ-Th7N", "original": "jOQqiAaiI-v", "number": 648, "cdate": 1632875466568, "mdate": null, "ddate": null, "tcdate": 1632875466568, "tmdate": 1676330660310, "tddate": null, "forum": "6Q52pZ-Th7N", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization", "authorids": ["~Can_Wang6", "~Sheng_Jin1", "~Yingda_Guan1", "~Wentao_Liu1", "~Chen_Qian1", "~Ping_Luo2", "~Wanli_Ouyang1"], "authors": ["Can Wang", "Sheng Jin", "Yingda Guan", "Wentao Liu", "Chen Qian", "Ping Luo", "Wanli Ouyang"], "keywords": ["Keypoint Localization", "Semi-Supervised Learning", "Curriculum Learning"], "abstract": "Localizing keypoints of an object is a basic visual problem. However, supervised learning of a keypoint localization network often requires a large amount of data, which is expensive and time-consuming to obtain. To remedy this, there is an ever-growing interest in semi-supervised learning (SSL), which leverages a small set of labeled data along with a large set of unlabeled data. Among these SSL approaches, pseudo-labeling (PL) is one of the most popular. PL approaches apply pseudo-labels to unlabeled data, and then train the model with a combination of the labeled and pseudo-labeled data iteratively. The key to the success of PL is the selection of high-quality pseudo-labeled samples. Previous works mostly select training samples by manually setting a single confidence threshold. We propose to automatically select reliable pseudo-labeled samples with a series of dynamic thresholds, which constitutes a learning curriculum.Extensive experiments on five keypoint localization benchmark datasets demonstrate that the proposed approach significantly outperforms the previous state-of-the-art SSL approaches. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|pseudolabeled_autocurriculum_learning_for_semisupervised_keypoint_localization", "pdf": "/pdf/cef8db1f5cfa760e6196e4a13ea01cea4b2da3fa.pdf", "one-sentence_summary": "The first work that explores automatic curriculum learning for semi-supervised keypoint localization", "_bibtex": "@inproceedings{\nwang2022pseudolabeled,\ntitle={Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization},\nauthor={Can Wang and Sheng Jin and Yingda Guan and Wentao Liu and Chen Qian and Ping Luo and Wanli Ouyang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6Q52pZ-Th7N}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "e0jtGTfPihs", "original": "u_iQugqrDAg", "number": 647, "cdate": 1632875466496, "mdate": null, "ddate": null, "tcdate": 1632875466496, "tmdate": 1697934909651, "tddate": null, "forum": "e0jtGTfPihs", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Signing the Supermask: Keep, Hide, Invert", "authorids": ["~Nils_Koster1", "~Oliver_Grothe1", "~Achim_Rettinger1"], "authors": ["Nils Koster", "Oliver Grothe", "Achim Rettinger"], "keywords": ["Neural Networks", "Supermask", "Lottery Ticket Hypothesis", "Pruning", "Weight Initialization", "Interpretation", "Subnetworks"], "abstract": "The exponential growth in numbers of parameters of neural networks over the past years has been accompanied by an increase in performance across several fields. However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications, since hardware requirements increased accordingly. \nTackling both issues, we present a novel approach that either drops a neural network's initial weights or inverts their respective sign. \nPut simply, a network is trained by weight selection and inversion without changing their absolute values.\nOur contribution extends previous work on masking by additionally sign-inverting the initial weights and follows the findings of the Lottery Ticket Hypothesis.\nThrough this extension and adaptations of initialization methods, we achieve a pruning rate of up to 99%, while still matching or exceeding the performance of various baseline and previous models.\nOur approach has two main advantages.\nFirst, and most notable, signed Supermask models drastically simplify a model's structure, while still performing well on given tasks.\nSecond, by reducing the neural network to its very foundation, we gain insights into which weights matter for performance. \nThe code is available on GitHub.", "one-sentence_summary": "We train neural networks by weight selection and sign inversion instead of optimizing the values of weights which achieves consistent pruning rates of up to 99% while maintaining competitive performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "koster|signing_the_supermask_keep_hide_invert", "pdf": "/pdf/973a36160c20dcbdbb3dfc000f6803773fa95d8e.pdf", "supplementary_material": "/attachment/2f412381258d6b54da2d60c36d9178823d1de8f9.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2201.13361/code)", "_bibtex": "@inproceedings{\nkoster2022signing,\ntitle={Signing the Supermask: Keep, Hide, Invert},\nauthor={Nils Koster and Oliver Grothe and Achim Rettinger},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=e0jtGTfPihs}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "6u6N8WWwYSM", "original": "kDnz-SvBeWP", "number": 643, "cdate": 1632875466261, "mdate": null, "ddate": null, "tcdate": 1632875466261, "tmdate": 1697934910001, "tddate": null, "forum": "6u6N8WWwYSM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bootstrapping Semantic Segmentation with Regional Contrast", "authorids": ["~Shikun_Liu1", "~Shuaifeng_Zhi2", "~Edward_Johns1", "~Andrew_Davison1"], "authors": ["Shikun Liu", "Shuaifeng Zhi", "Edward Johns", "Andrew Davison"], "keywords": ["semi-supervised learning", "semantic segmentation", "contrastive learning"], "abstract": "We present ReCo, a contrastive learning framework designed at a regional level to assist learning in semantic segmentation. ReCo performs pixel-level contrastive learning on a sparse set of hard negative pixels, with minimal additional memory footprint. ReCo is easy to implement, being built on top of off-the-shelf segmentation networks, and consistently improves performance, achieving more accurate segmentation boundaries and faster convergence. The strongest effect is in semi-supervised learning with very few labels. With ReCo, we achieve high quality semantic segmentation model, requiring only 5 examples of each semantic class. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|bootstrapping_semantic_segmentation_with_regional_contrast", "pdf": "/pdf/c13a05eb5dbd64c9e46b0245f88f170e7f974da4.pdf", "one-sentence_summary": "We present a pixel-level contrastive learning framework to achieve a high-quality semantic segmeantation model trained with very few human annotations.", "supplementary_material": "/attachment/dff06f48fbfad44993e5565640c139a38bfa3b3f.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2104.04465/code)", "_bibtex": "@inproceedings{\nliu2022bootstrapping,\ntitle={Bootstrapping Semantic Segmentation with Regional Contrast},\nauthor={Shikun Liu and Shuaifeng Zhi and Edward Johns and Andrew Davison},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6u6N8WWwYSM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "pgir5f7ekAL", "original": "FcSfU93-CEf", "number": 641, "cdate": 1632875466115, "mdate": null, "ddate": null, "tcdate": 1632875466115, "tmdate": 1697934910551, "tddate": null, "forum": "pgir5f7ekAL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generative Principal Component Analysis", "authorids": ["~Zhaoqiang_Liu1", "~Jiulong_Liu1", "~Subhroshekhar_Ghosh1", "~Jun_Han4", "~Jonathan_Scarlett1"], "authors": ["Zhaoqiang Liu", "Jiulong Liu", "Subhroshekhar Ghosh", "Jun Han", "Jonathan Scarlett"], "keywords": ["Principal component analysis", "generative models", "sparse principal component analysis", "projected power methods", "optimal statistical rates"], "abstract": "In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the first principal eigenvector lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\\sqrt{\\frac{k\\log L}{m}}$, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges exponentially fast to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis.", "one-sentence_summary": "We study the problem of principal component analysis with generative modeling assumptions, and provide a corresponding efficient algorithm with provable guarantees.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|generative_principal_component_analysis", "pdf": "/pdf/0d613cf22b7aac05ac214c335bb03801a56f9c06.pdf", "supplementary_material": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.09693/code)", "_bibtex": "@inproceedings{\nliu2022generative,\ntitle={Generative Principal Component Analysis},\nauthor={Zhaoqiang Liu and Jiulong Liu and Subhroshekhar Ghosh and Jun Han and Jonathan Scarlett},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pgir5f7ekAL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "OqcZu8JIIzS", "original": "nmEMDKxBq-Q", "number": 640, "cdate": 1632875466044, "mdate": null, "ddate": null, "tcdate": 1632875466044, "tmdate": 1676330660993, "tddate": null, "forum": "OqcZu8JIIzS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning", "authorids": ["~Yijun_Yang3", "~Jing_Jiang6", "~Tianyi_Zhou1", "~Jie_Ma4", "shiyh@sustech.edu.cn"], "authors": ["Yijun Yang", "Jing Jiang", "Tianyi Zhou", "Jie Ma", "Yuhui Shi"], "keywords": ["model-based offline RL", "Pareto front", "multi-objective optimization", "policy pool", "model return-uncertainty trade-off"], "abstract": "Online reinforcement learning (RL) can suffer from poor exploration, sparse reward, insufficient data, and overhead caused by inefficient interactions between an immature policy and a complicated environment. Model-based offline RL instead trains an environment model using a dataset of pre-collected experiences so online RL methods can learn in an offline manner by solely interacting with the model. However, the uncertainty and accuracy of the environment model can drastically vary across different state-action pairs so the RL agent may achieve high model return but perform poorly in the true environment. Unlike previous works that need to carefully tune the trade-off between the model return and uncertainty in a single objective, we study a bi-objective formulation for model-based offline RL that aims at producing a pool of diverse policies on the Pareto front performing different levels of trade-offs, which provides the flexibility to select the best policy for each realistic environment from the pool. Our method, ''Pareto policy pool (P3)'', does not need to tune the trade-off weight but can produce policies allocated at different regions of the Pareto front. For this purpose, we develop an efficient algorithm that solves multiple bi-objective optimization problems with distinct constraints defined by reference vectors targeting diverse regions of the Pareto front. We theoretically prove that our algorithm can converge to the targeted regions. In order to obtain more Pareto optimal policies without linearly increasing the cost, we leverage the achieved policies as initialization to find more Pareto optimal policies in their neighborhoods. On the D4RL benchmark for offline RL, P3 substantially outperforms several recent baseline methods over multiple tasks, especially when the quality of pre-collected experiences is low.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|pareto_policy_pool_for_modelbased_offline_reinforcement_learning", "pdf": "/pdf/811177d23b2117fa0be0cc22952e7c1e3325bf59.pdf", "one-sentence_summary": "We propose a model-based offline RL method that builds a diverse set of optimal policies on Pareto front providing different levels of model return-uncertainty trade-off and it significantly outperforms single-policy methods.", "data": "", "_bibtex": "@inproceedings{\nyang2022pareto,\ntitle={Pareto Policy Pool for Model-based Offline Reinforcement Learning},\nauthor={Yijun Yang and Jing Jiang and Tianyi Zhou and Jie Ma and Yuhui Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OqcZu8JIIzS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "kOu3-S3wJ7", "original": "7FSfffgY8kJ", "number": 637, "cdate": 1632875465831, "mdate": null, "ddate": null, "tcdate": 1632875465831, "tmdate": 1676330661078, "tddate": null, "forum": "kOu3-S3wJ7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks", "authorids": ["~Andrea_Cini1", "~Ivan_Marisca1", "~Cesare_Alippi1"], "authors": ["Andrea Cini", "Ivan Marisca", "Cesare Alippi"], "keywords": ["graph neural networks", "missing data", "time series analysis", "time series imputation"], "abstract": "Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.", "one-sentence_summary": "We propose a graph neural network architecture for multivariate time series imputation and achieve state-of-the-art results on several benchmarks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cini|filling_the_g_ap_s_multivariate_time_series_imputation_by_graph_neural_networks", "pdf": "/pdf/227091a9cda893265aa46e20fcf42675d01a8158.pdf", "supplementary_material": "/attachment/b37815dd4f2fc6137bdb944c378b7d0d97268813.zip", "_bibtex": "@inproceedings{\ncini2022filling,\ntitle={Filling the G\\_ap\\_s: Multivariate Time Series Imputation by Graph Neural Networks},\nauthor={Andrea Cini and Ivan Marisca and Cesare Alippi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kOu3-S3wJ7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "WZ3yjh8coDg", "original": "Y6OCgKmVLzW", "number": 627, "cdate": 1632875465188, "mdate": null, "ddate": null, "tcdate": 1632875465188, "tmdate": 1676330661391, "tddate": null, "forum": "WZ3yjh8coDg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "An Unconstrained Layer-Peeled Perspective on Neural Collapse", "authorids": ["~Wenlong_Ji1", "~Yiping_Lu1", "~Yiliang_Zhang1", "~Zhun_Deng1", "~Weijie_J_Su1"], "authors": ["Wenlong Ji", "Yiping Lu", "Yiliang Zhang", "Zhun Deng", "Weijie J Su"], "keywords": ["neural collapse", "uncostrained model", "implicit regularization"], "abstract": "Neural collapse is a highly symmetric geometry of neural networks that emerges during the terminal phase of training, with profound implications on the generalization performance and robustness of the trained networks. To understand how the last-layer features and classifiers exhibit this recently discovered implicit bias, in this paper, we introduce a surrogate model called the unconstrained layer-peeled model (ULPM). We prove that gradient flow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Moreover, we show that the ULPM with the cross-entropy loss has a benign global landscape for its loss function, which allows us to prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon. Empirically, we show that our results also hold during the training of neural networks in real-world tasks when explicit regularization or weight decay is not used.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ji|an_unconstrained_layerpeeled_perspective_on_neural_collapse", "pdf": "/pdf/8859a1d1fdfa5baf8756010e0ce23bf5d5ff920c.pdf", "one-sentence_summary": "We investigate how the gradient flow converges to a neural collapse solution in an unconstrained model.", "supplementary_material": "/attachment/bfb832d58bd1c82bce12a9295410d61d104b7f57.zip", "data": "", "_bibtex": "@inproceedings{\nji2022an,\ntitle={An Unconstrained Layer-Peeled Perspective on Neural Collapse},\nauthor={Wenlong Ji and Yiping Lu and Yiliang Zhang and Zhun Deng and Weijie J Su},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WZ3yjh8coDg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "pN1JOdrSY9", "original": "mhx9O3sDLzG", "number": 626, "cdate": 1632875465117, "mdate": null, "ddate": null, "tcdate": 1632875465117, "tmdate": 1676330661488, "tddate": null, "forum": "pN1JOdrSY9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation", "authorids": ["~Xuan-Phi_Nguyen1", "~Hongyu_Gong1", "~Yun_Tang1", "changhan@fb.com", "~Philipp_Koehn2", "~Shafiq_Joty1"], "authors": ["Xuan-Phi Nguyen", "Hongyu Gong", "Yun Tang", "Changhan Wang", "Philipp Koehn", "Shafiq Joty"], "keywords": ["machine translation", "unsupervised machine translation", "pseudo-parallel data", "contrastive clustering", "pretraining"], "abstract": "Modern unsupervised machine translation systems mostly train their models by generating synthetic parallel training data from large unlabeled monolingual corpora of different languages through various means, such as iterative back-translation. However, there may exist small amount of actual parallel data hidden in the sea of unlabeled data, which has not been exploited. We develop a new fine-tuning objective, called Language-Agnostic Constraint for SwAV loss, or LAgSwAV, which enables a pre-trained model to extract such pseudo-parallel data from the monolingual corpora in a fully unsupervised manner. We then propose an effective strategy to utilize the obtained synthetic data to augment unsupervised machine translation. Our method achieves the state of the art in the WMT'14 English-French, WMT'16 German-English and English-Romanian bilingual unsupervised translation tasks, with 40.2, 36.8, 37.0 BLEU, respectively. We also achieve substantial improvements in the FLoRes low-resource English-Nepali and English-Sinhala unsupervised tasks with 5.3 and 5.4 BLEU, respectively.\n", "pdf": "/pdf/eaf9a294ad1f34490a07fd0b42916f9dce419572.pdf", "one-sentence_summary": "We propose a fine-tuning loss that enables pre-trained model's ability to mine pseudo-parallel data for fully unsupervised machine translation.", "supplementary_material": "/attachment/e6d6548dfe8e6cd897f796567ff95bc3b925a931.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nguyen|contrastive_clustering_to_mine_pseudo_parallel_data_for_unsupervised_translation", "data": "", "_bibtex": "@inproceedings{\nnguyen2022contrastive,\ntitle={Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation},\nauthor={Xuan-Phi Nguyen and Hongyu Gong and Yun Tang and Changhan Wang and Philipp Koehn and Shafiq Joty},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pN1JOdrSY9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 6}}, {"id": "QRX0nCX_gk", "original": "I5pZFRNWeK0", "number": 609, "cdate": 1632875463950, "mdate": null, "ddate": null, "tcdate": 1632875463950, "tmdate": 1697934913036, "tddate": null, "forum": "QRX0nCX_gk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multimeasurement Generative Models", "authorids": ["~Saeed_Saremi1", "~Rupesh_Kumar_Srivastava1"], "authors": ["Saeed Saremi", "Rupesh Kumar Srivastava"], "keywords": ["energy based models", "Langevin MCMC", "score matching", "denoising autoencoders", "empirical Bayes"], "abstract": "We formally map the problem of sampling from an unknown distribution with a density in $\\mathbb{R}^d$ to the problem of learning and sampling a smoother density in $\\mathbb{R}^{Md}$ obtained by convolution with a fixed factorial kernel: the new density is referred to as M-density and the kernel as multimeasurement noise model (MNM). The M-density in $\\mathbb{R}^{Md}$ is smoother than the original density in $\\mathbb{R}^d$, easier to learn and sample from, yet for large $M$ the two problems are mathematically equivalent since clean data can be estimated exactly given a multimeasurement noisy observation using the Bayes estimator. To formulate the problem, we derive the Bayes estimator for Poisson and Gaussian MNMs in closed form in terms of the unnormalized M-density. This leads to a simple least-squares objective for learning parametric energy and score functions. We present various parametrization schemes of interest including one in which studying Gaussian M-densities directly leads to multidenoising autoencoders\u2014this is the first theoretical connection made between denoising autoencoders and empirical Bayes in the literature.  Samples in $\\mathbb{R}^d$ are obtained by walk-jump sampling (Saremi & Hyvarinen, 2019) via underdamped Langevin MCMC (walk) to sample from M-density and the multimeasurement Bayes estimation (jump). We study permutation invariant Gaussian M-densities on MNIST, CIFAR-10, and FFHQ-256 datasets, and demonstrate the effectiveness of this framework for realizing fast-mixing stable Markov chains in high dimensions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "saremi|multimeasurement_generative_models", "pdf": "/pdf/10cd3713d7239dab545f68a0ef99b74a479a6ef8.pdf", "supplementary_material": "/attachment/c03ed89627764a0a1ea5a42d2e6a7afb2603e42f.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2112.09822/code)", "_bibtex": "@inproceedings{\nsaremi2022multimeasurement,\ntitle={Multimeasurement Generative Models},\nauthor={Saeed Saremi and Rupesh Kumar Srivastava},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=QRX0nCX_gk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "USC0-nvGPK", "original": "6N2SHQzAaWt", "number": 607, "cdate": 1632875463809, "mdate": null, "ddate": null, "tcdate": 1632875463809, "tmdate": 1676330662534, "tddate": null, "forum": "USC0-nvGPK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Information Gain Propagation: a New Way to Graph Active Learning with Soft Labels", "authorids": ["~Wentao_Zhang1", "~Yexin_Wang2", "~Zhenbang_You1", "~Meng_Cao2", "~Ping_Huang1", "~Jiulong_Shan2", "~Zhi_Yang4", "~Bin_CUI2"], "authors": ["Wentao Zhang", "Yexin Wang", "Zhenbang You", "Meng Cao", "Ping Huang", "Jiulong Shan", "Zhi Yang", "Bin CUI"], "keywords": ["Active Learning", "Graph", "Information Gain"], "abstract": "Graph Neural Networks (GNNs) have achieved great success in various tasks, but their performance highly relies on a large number of labeled nodes, which typically requires considerable human effort. GNN-based Active Learning (AL) methods are proposed to improve the labeling efficiency by selecting the most valuable nodes to label. Existing methods assume an oracle can correctly categorize all the selected nodes and thus just focus on the node selection. However, such an exact labeling task is costly, especially when the categorization is out of the domain of individual expert (oracle). The paper goes further, presenting a soft-label approach to AL on GNNs. Our key innovations are: i) relaxed queries where a domain expert (oracle) only judges the correctness of the predicted labels (a binary question) rather than identifying the exact class (a multi-class question), and ii) new criteria of maximizing information gain propagation for active learner with relaxed queries and soft labels. Empirical studies on public datasets demonstrate that our method significantly outperforms the state-of-the-art GNN-based AL methods in terms of both accuracy and labeling cost. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|information_gain_propagation_a_new_way_to_graph_active_learning_with_soft_labels", "pdf": "/pdf/6f02bdd5a0b35aba61f2cdcb8fc6b599e55addb4.pdf", "one-sentence_summary": "A new way to graph active learning with soft labels", "supplementary_material": "/attachment/bc5526647bcea7f5991ea3c0561946d6f68b8129.zip", "data": "", "_bibtex": "@inproceedings{\nzhang2022information,\ntitle={Information Gain Propagation: a New Way to Graph Active Learning with Soft Labels},\nauthor={Wentao Zhang and Yexin Wang and Zhenbang You and Meng Cao and Ping Huang and Jiulong Shan and Zhi Yang and Bin CUI},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=USC0-nvGPK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "Zr5W2LSRhD", "original": "qLUzbhTcF1u", "number": 603, "cdate": 1632875463524, "mdate": null, "ddate": null, "tcdate": 1632875463524, "tmdate": 1676330662631, "tddate": null, "forum": "Zr5W2LSRhD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Constructing Orthogonal Convolutions in an Explicit Manner", "authorids": ["~Tan_Yu3", "~Jun_Li13", "~YUNFENG_CAI1", "~Ping_Li3"], "authors": ["Tan Yu", "Jun Li", "YUNFENG CAI", "Ping Li"], "keywords": [], "abstract": "Convolutions with orthogonal input-output Jacobian matrix, i.e., orthogonal convolution,  have recently attracted substantial attention.  A convolution layer with an orthogonal Jacobian matrix is 1-Lipschitz  in the  2-norm, making the output robust to the perturbation in input. Meanwhile, an orthogonal Jacobian matrix preserves the gradient norm in back-propagation, which is critical for stable training deep networks. Nevertheless,  existing orthogonal convolutions are burdened by high computational costs for preserving orthogonality.\nIn this work, we exploit the relation between the singular values of the convolution layer's  Jacobian and the structure of the convolution kernel.  To achieve orthogonality, we explicitly construct the convolution kernel for enforcing all singular values of the convolution layer's Jacobian to be $1$s.   After training,  the explicitly constructed orthogonal (ECO) convolution is constructed only once, and their weights are stored. Then,  in evaluation, we only need to load the stored weights of the trained  ECO convolution, and the computational cost of ECO convolution is the same as the standard dilated convolution. It is more efficient than the recent state-of-the-art approach, skew orthogonal convolution (SOC) in evaluation.    Experiments on CIFAR-10 and CIFAR-100  demonstrate that the proposed ECO convolution is faster than SOC in evaluation while leading to competitive standard and certified robust accuracies. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|constructing_orthogonal_convolutions_in_an_explicit_manner", "pdf": "/pdf/e7aa822079e9e6465557895630b7d180c2a2de75.pdf", "supplementary_material": "/attachment/4fb1b2a2a0083de95407a8a6e9cd4f3d9553c365.zip", "_bibtex": "@inproceedings{\nyu2022constructing,\ntitle={Constructing Orthogonal Convolutions in an Explicit Manner},\nauthor={Tan Yu and Jun Li and YUNFENG CAI and Ping Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Zr5W2LSRhD}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 36}}, {"id": "P3Bh01hBYTH", "original": "rCo-32WDSZ", "number": 600, "cdate": 1632875463304, "mdate": null, "ddate": null, "tcdate": 1632875463304, "tmdate": 1697934913845, "tddate": null, "forum": "P3Bh01hBYTH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "X-model: Improving Data Efficiency in Deep Learning with A Minimax Model", "authorids": ["~Ximei_Wang1", "~Xinyang_Chen1", "~Jianmin_Wang1", "~Mingsheng_Long5"], "authors": ["Ximei Wang", "Xinyang Chen", "Jianmin Wang", "Mingsheng Long"], "keywords": ["Data Efficiency", "Deep Learning", "Minimax Model"], "abstract": "To mitigate the burden of data labeling, we aim at improving data efficiency for both classification and regression setups in deep learning. However, the current focus is on classification problems while rare attention has been paid to deep regression, which usually requires more human effort to labeling. Further, due to the intrinsic difference between categorical and continuous label space, the common intuitions for classification, \\textit{e.g.} cluster assumptions or pseudo labeling strategies, cannot be naturally adapted into deep regression. To this end, we first delved into the existing data-efficient methods in deep learning and found that they either encourage invariance to \\textit{data stochasticity} (\\textit{e.g.}, consistency regularization under different augmentations) or \\textit{model stochasticity} (\\textit{e.g.}, difference penalty for predictions of models with different dropout). To take the power of both worlds, we propose a novel \\Chi-model by simultaneously encouraging the invariance to {data stochasticity} and {model stochasticity}. Further, the \\Chi-model plays a minimax game between the feature extractor and task-specific heads to further enhance the invariance to model stochasticity. Extensive experiments verify the superiority of the \\Chi-model among various tasks, from a single-value prediction task of age estimation to a dense-value prediction task of keypoint localization, a 2D synthetic and a 3D realistic dataset, as well as a multi-category object recognition task.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|xmodel_improving_data_efficiency_in_deep_learning_with_a_minimax_model", "pdf": "/pdf/21f01d34ca3af217c072e1f376fb6c73e1709aee.pdf", "one-sentence_summary": "This paper proposes a novel X-Model for improving data efficiency in deep learning with a minimax model.", "supplementary_material": "/attachment/36a9fce1ebd67825186b697a60cc53377f6f7df3.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.04572/code)", "_bibtex": "@inproceedings{\nwang2022xmodel,\ntitle={X-model: Improving Data Efficiency in Deep Learning with A Minimax Model},\nauthor={Ximei Wang and Xinyang Chen and Jianmin Wang and Mingsheng Long},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=P3Bh01hBYTH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "2-mkiUs9Jx7", "original": "g3MJvtH5xa", "number": 594, "cdate": 1632875462878, "mdate": null, "ddate": null, "tcdate": 1632875462878, "tmdate": 1676330663008, "tddate": null, "forum": "2-mkiUs9Jx7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Stein Latent Optimization for Generative Adversarial Networks", "authorids": ["~Uiwon_Hwang1", "~Heeseung_Kim1", "~Dahuin_Jung2", "~Hyemi_Jang1", "~Hyungyu_Lee1", "~Sungroh_Yoon1"], "authors": ["Uiwon Hwang", "Heeseung Kim", "Dahuin Jung", "Hyemi Jang", "Hyungyu Lee", "Sungroh Yoon"], "keywords": ["Generative Adversarial Networks", "Unsupervised Conditional GANs"], "abstract": "Generative adversarial networks (GANs) with clustered latent spaces can perform conditional generation in a completely unsupervised manner. In the real world, the salient attributes of unlabeled data can be imbalanced. However, most of existing unsupervised conditional GANs cannot cluster attributes of these data in their latent spaces properly because they assume uniform distributions of the attributes. To address this problem, we theoretically derive Stein latent optimization that provides reparameterizable gradient estimations of the latent distribution parameters assuming a Gaussian mixture prior in a continuous latent space. Structurally, we introduce an encoder network and novel unsupervised conditional contrastive loss to ensure that data generated from a single mixture component represent a single attribute. We confirm that the proposed method, named Stein Latent Optimization for GANs (SLOGAN), successfully learns balanced or imbalanced attributes and achieves state-of-the-art unsupervised conditional generation performance even in the absence of attribute information (e.g., the imbalance ratio). Moreover, we demonstrate that the attributes to be learned can be manipulated using a small amount of probe data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hwang|stein_latent_optimization_for_generative_adversarial_networks", "pdf": "/pdf/4edcadc882e7eafc4345a5d2a1b17ee37da071db.pdf", "one-sentence_summary": "We propose a novel GAN method that performs unsupervised conditional generation robustly on real-world datasets with balanced or imbalanced attributes even in the absence of attribute information (e.g., the imbalance ratio)", "code": "", "data": "", "_bibtex": "@inproceedings{\nhwang2022stein,\ntitle={Stein Latent Optimization for Generative Adversarial Networks},\nauthor={Uiwon Hwang and Heeseung Kim and Dahuin Jung and Hyemi Jang and Hyungyu Lee and Sungroh Yoon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=2-mkiUs9Jx7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "RRGVCN8kjim", "original": "d22zwYLCegL", "number": 592, "cdate": 1632875462740, "mdate": null, "ddate": null, "tcdate": 1632875462740, "tmdate": 1697934915146, "tddate": null, "forum": "RRGVCN8kjim", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity", "authorids": ["~Byungseok_Roh1", "~JaeWoong_Shin1", "~Wuhyun_Shin2", "~Saehoon_Kim1"], "authors": ["Byungseok Roh", "JaeWoong Shin", "Wuhyun Shin", "Saehoon Kim"], "keywords": ["Transformer Query Sparsification Mechanism", "Efficient End-to-End Object Detection"], "abstract": "DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10x faster convergence and improved performance. Using the multiscale feature to ameliorate performance, however, the number of encoder queries increases by 20x compared to DETR, and the computation cost of the encoder attention remains a bottleneck. We observe that the encoder queries referenced by the decoder account for only 45% of the total, and find out the detection accuracy does not deteriorate significantly even if only the referenced queries are polished in the encoder block. Inspired by this observation, we propose Sparse DETR that selectively updates only the queries expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected queries in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only 10% encoder queries on the COCO dataset. Albeit only the encoder queries are sparsified, the total computation cost decreases by 38% and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code will be released.\n", "one-sentence_summary": "Sparse DETR is an efficient end-to-end object detector that sparsifies encoder queries by using the learnable decoder attention map predictor. It achieves better performance than Deformable DETR even with only 10% encoder queries on the COCO dataset.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "roh|sparse_detr_efficient_endtoend_object_detection_with_learnable_sparsity", "pdf": "/pdf/2bbd874554428242c046934b85aa5bb782b31b60.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.14330/code)", "_bibtex": "@inproceedings{\nroh2022sparse,\ntitle={Sparse {DETR}: Efficient End-to-End Object Detection with Learnable Sparsity},\nauthor={Byungseok Roh and JaeWoong Shin and Wuhyun Shin and Saehoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RRGVCN8kjim}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "HMJdXzbWKH", "original": "-1oftwUUu1U", "number": 587, "cdate": 1632875462381, "mdate": null, "ddate": null, "tcdate": 1632875462381, "tmdate": 1676330663634, "tddate": null, "forum": "HMJdXzbWKH", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear MDPs", "authorids": ["~Naman_Agarwal1", "~Syomantak_Chaudhuri1", "~Prateek_Jain1", "~Dheeraj_Mysore_Nagaraj1", "~Praneeth_Netrapalli1"], "authors": ["Naman Agarwal", "Syomantak Chaudhuri", "Prateek Jain", "Dheeraj Mysore Nagaraj", "Praneeth Netrapalli"], "keywords": ["Q Learning", "RL with Function Approximation", "Experience Replay", "Online Target Learning"], "abstract": "Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely used in practice with function approximation (Mnih et al., 2015). In contrast, existing theoretical results are pessimistic about Q-learning. For example, (Baird, 1995) shows that Q-learning does not converge even with linear function approximation for linear MDPs. Furthermore, even for tabular MDPs with synchronous updates, Q-learning was shown to have sub-optimal sample complexity (Li et al., 2021, Azar et al., 2013). The goal of this work is to bridge the gap between practical success of Q-learning and the relatively pessimistic theoretical results. The starting point of our work is the observation that in practice, Q-learning is used with two important modifications: (i) training with two networks, called online network and target network simultaneously (online target learning, or OTL) , and (ii) experience replay (ER) (Mnih et al., 2015). While they have been observed to play a significant role in the practical success of Q-learning, a thorough theoretical understanding of how these two modifications improve the convergence behavior of Q-learning has been missing in literature. By carefully combining the Q-learning with OTL and reverse experience replay (RER) (a form of experience replay), we present novel methods Q-Rex and Q-RexDaRe (Q-Rex+data reuse). We show that Q-Rex efficiently finds the optimal policy for linear MDPs and provide non-asymptotic bounds on sample complexity -- the first such result for a Q-learning method with linear MDPs. Furthermore, we demonstrate that Q-RexDaRe in fact achieves near optimal sample complexity in the tabular setting, improving upon the existing results for vanilla Q-learning. \n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "agarwal|online_target_qlearning_with_reverse_experience_replay_efficiently_finding_the_optimal_policy_for_linear_mdps", "pdf": "/pdf/6b594a687dec20908b7cad9a357b456275c44067.pdf", "_bibtex": "@inproceedings{\nagarwal2022online,\ntitle={Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear {MDP}s},\nauthor={Naman Agarwal and Syomantak Chaudhuri and Prateek Jain and Dheeraj Mysore Nagaraj and Praneeth Netrapalli},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HMJdXzbWKH}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "7I8LPkcx8V", "original": "ezwkVueDKVW", "number": 579, "cdate": 1632875461952, "mdate": null, "ddate": null, "tcdate": 1632875461952, "tmdate": 1676330663914, "tddate": null, "forum": "7I8LPkcx8V", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Differentially Private Fractional Frequency Moments Estimation with Polylogarithmic Space", "authorids": ["~Lun_Wang1", "~Iosif_Pinelis1", "~Dawn_Song2"], "authors": ["Lun Wang", "Iosif Pinelis", "Dawn Song"], "keywords": ["Differential Privacy", "Fractional Frequency Moments"], "abstract": "We prove that $\\mathbb{F}_p$ sketch, a well-celebrated streaming algorithm for frequency moments estimation, is differentially private as is when $p\\in(0, 1]$. $\\mathbb{F}_p$ sketch uses only polylogarithmic space, exponentially better than existing DP baselines and only worse than the optimal non-private baseline by a logarithmic factor. The evaluation shows that $\\mathbb{F}_p$ sketch can achieve reasonable accuracy with strong privacy guarantees. The code for evaluation is included in the supplementary material.", "pdf": "/pdf/3464581c1cb95e51fe436b0388170760d904c5a4.pdf", "one-sentence_summary": "We prove that $\\mathbb{F}_p$ sketch, a well-celebrated streaming algorithm for frequency moments estimation, is differentially private as is when $p\\in(0, 1]$.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|differentially_private_fractional_frequency_moments_estimation_with_polylogarithmic_space", "supplementary_material": "/attachment/a35eb16208dbfcc38520e52c573e4e5bdf9f05c6.zip", "_bibtex": "@inproceedings{\nwang2022differentially,\ntitle={Differentially Private Fractional Frequency Moments Estimation with Polylogarithmic Space},\nauthor={Lun Wang and Iosif Pinelis and Dawn Song},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7I8LPkcx8V}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "YpSxqy_RE84", "original": "eH9awY2NbGW", "number": 574, "cdate": 1632875461599, "mdate": null, "ddate": null, "tcdate": 1632875461599, "tmdate": 1697934916996, "tddate": null, "forum": "YpSxqy_RE84", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How Low Can We Go: Trading Memory for Error in Low-Precision Training", "authorids": ["~Chengrun_Yang1", "~Ziyang_Wu1", "~Jerry_Chee1", "~Christopher_De_Sa2", "~Madeleine_Udell1"], "authors": ["Chengrun Yang", "Ziyang Wu", "Jerry Chee", "Christopher De Sa", "Madeleine Udell"], "keywords": ["low-precision training", "meta-learning", "Pareto frontier", "error-memory tradeoff", "active learning", "matrix factorization"], "abstract": "Low-precision arithmetic trains deep learning models using less energy, less memory and less time. However, we pay a price for the savings: lower precision may yield larger round-off error and hence larger prediction error. As applications proliferate, users must choose which precision to use to train a new model, and chip manufacturers must decide which precisions to manufacture. We view these precision choices as a hyperparameter tuning problem, and borrow ideas from meta-learning to learn the tradeoff between memory and error. In this paper, we introduce Pareto Estimation to Pick the Perfect Precision (PEPPP). We use matrix factorization to find non-dominated configurations (the Pareto frontier) with a limited number of network evaluations. For any given memory budget, the precision that minimizes error is a point on this frontier. Practitioners can use the frontier to trade memory for error and choose the best precision for their goals.", "one-sentence_summary": "Given a dataset and a memory budget, we use matrix factorization and active learning to efficiently pick the perfect low-precision configuration for a neural network.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|how_low_can_we_go_trading_memory_for_error_in_lowprecision_training", "pdf": "/pdf/2236c2cfad186cc512b5230dbb649d54722a6579.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.09686/code)", "_bibtex": "@inproceedings{\nyang2022how,\ntitle={How Low Can We Go: Trading Memory for Error in Low-Precision Training},\nauthor={Chengrun Yang and Ziyang Wu and Jerry Chee and Christopher De Sa and Madeleine Udell},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YpSxqy_RE84}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "rUwm9wCjURV", "original": "mxhcQfFOndU", "number": 566, "cdate": 1632875461082, "mdate": null, "ddate": null, "tcdate": 1632875461082, "tmdate": 1676330664708, "tddate": null, "forum": "rUwm9wCjURV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications", "authorids": ["~Borja_G._Le\u00f3n1", "~Murray_Shanahan1", "~Francesco_Belardinelli1"], "authors": ["Borja G. Le\u00f3n", "Murray Shanahan", "Francesco Belardinelli"], "keywords": ["Deep Reinforcement Learning", "Out-Of-Distribution Generalisation", "Temporal Logic"], "abstract": "We address the problem of building agents whose goal is to learn to execute out-of distribution (OOD) multi-task instructions expressed in temporal logic (TL) by using deep reinforcement learning (DRL). Recent works provided evidence that the agent's neural architecture is a key feature when DRL agents are learning to solve OOD tasks in TL. Yet, the studies on this topic are still in their infancy. In this work, we propose a new deep learning configuration with inductive biases that lead agents to generate latent representations of their current goal, yielding a stronger generalization performance. We use these latent-goal networks within a neuro-symbolic framework that executes multi-task formally-defined instructions and contrast the performance of the proposed neural networks against employing different state-of-the-art (SOTA) architectures when generalizing to unseen instructions in OOD environments. ", "one-sentence_summary": "Inducing architectures to generate low-dimensional representations of their current goal processing observations and instructions together yields stronger out-of-distribution generalisation", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "le\u00f3n|in_a_nutshell_the_human_asked_for_this_latent_goals_for_following_temporal_specifications", "pdf": "/pdf/29122e59291a932f64f32109f73dba7f83c9bedd.pdf", "_bibtex": "@inproceedings{\nle{\\'o}n2022in,\ntitle={In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications},\nauthor={Borja G. Le{\\'o}n and Murray Shanahan and Francesco Belardinelli},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rUwm9wCjURV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "8hWs60AZcWk", "original": "AlQkJXh4NmF", "number": 563, "cdate": 1632875460861, "mdate": null, "ddate": null, "tcdate": 1632875460861, "tmdate": 1697934918302, "tddate": null, "forum": "8hWs60AZcWk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Discrete Representations Strengthen Vision Transformer Robustness", "authorids": ["~Chengzhi_Mao2", "~Lu_Jiang1", "~Mostafa_Dehghani1", "~Carl_Vondrick2", "~Rahul_Sukthankar1", "~Irfan_Essa1"], "authors": ["Chengzhi Mao", "Lu Jiang", "Mostafa Dehghani", "Carl Vondrick", "Rahul Sukthankar", "Irfan Essa"], "keywords": ["vision transformer", "robustness", "image recognition"], "abstract": "Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs are overly reliant on local features (\\eg, nuisances and texture) and fail to make adequate use of global context (\\eg, shape and structure). As a result, ViTs fail to generalize to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12\\% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mao|discrete_representations_strengthen_vision_transformer_robustness", "pdf": "/pdf/248e8be4fabe9aa0349e4e24290dd07abecaa424.pdf", "one-sentence_summary": "We present a simple and effective architecture modification to ViT's input layer with discrete token representations.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2111.10493/code)", "_bibtex": "@inproceedings{\nmao2022discrete,\ntitle={Discrete Representations Strengthen Vision Transformer Robustness},\nauthor={Chengzhi Mao and Lu Jiang and Mostafa Dehghani and Carl Vondrick and Rahul Sukthankar and Irfan Essa},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8hWs60AZcWk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "JzNB0eA2-M4", "original": "U3vXoUG7dHh", "number": 561, "cdate": 1632875460715, "mdate": null, "ddate": null, "tcdate": 1632875460715, "tmdate": 1676330665088, "tddate": null, "forum": "JzNB0eA2-M4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Convergence of the Monte Carlo Exploring Starts Algorithm for Reinforcement Learning", "authorids": ["~Che_Wang1", "~Shuhan_Yuan3", "~Kai_Shao1", "~Keith_W._Ross1"], "authors": ["Che Wang", "Shuhan Yuan", "Kai Shao", "Keith W. Ross"], "keywords": ["reinforcement learning", "convergence of reinforcement learning algorithm", "monte carlo exploring starts"], "abstract": "A simple and natural algorithm for reinforcement learning (RL) is Monte Carlo Exploring Starts (MCES), where the Q-function is estimated by averaging the Monte Carlo returns, and the policy is improved by choosing actions that maximize the current estimate of the Q-function. Exploration is performed by \"exploring starts\", that is, each episode begins with a randomly chosen state and action, and then follows the current policy to the terminal state. In the classic book on RL by Sutton & Barto (2018), it is stated that establishing convergence for the MCES algorithm is one of the most important remaining open theoretical problems in RL. However, the convergence question for MCES turns out to be quite nuanced. Bertsekas & Tsitsiklis (1996) provide a counter-example showing that the MCES algorithm does not necessarily converge. Tsitsiklis (2002) further shows that if the original MCES algorithm is modified so that the Q-function estimates are updated at the same rate for all state-action pairs, and the discount factor is strictly less than one, then the MCES algorithm converges. \nIn this paper we make headway with the original and more efficient MCES algorithm given in Sutton et al. (1998), establishing almost sure convergence for Optimal Policy Feed-Forward MDPs, which are MDPs whose states are not revisited within any episode when using an optimal policy. Such MDPs include a large class of environments such as all deterministic environments and all episodic environments with a timestep or any monotonically changing values as part of the state. Different from the previous proofs using stochastic approximations, we introduce a novel inductive approach, which is very simple and only makes use of the strong law of large numbers.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|on_the_convergence_of_the_monte_carlo_exploring_starts_algorithm_for_reinforcement_learning", "pdf": "/pdf/8c7929143db896ebc14d388427a6dc8f388a5e43.pdf", "one-sentence_summary": "We prove that the Monte Carlo Exploring Starts algorithm converges for optimal policy feed-forward MDPs. ", "supplementary_material": "/attachment/6dbe6c70da3e2cd90473d64de538b82a51e3e332.zip", "_bibtex": "@inproceedings{\nwang2022on,\ntitle={On the Convergence of the Monte Carlo Exploring Starts Algorithm for Reinforcement Learning},\nauthor={Che Wang and Shuhan Yuan and Kai Shao and Keith W. Ross},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JzNB0eA2-M4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "rw1mZl_ss3L", "original": "B6zI8OxaAco", "number": 554, "cdate": 1632875460277, "mdate": null, "ddate": null, "tcdate": 1632875460277, "tmdate": 1676330665620, "tddate": null, "forum": "rw1mZl_ss3L", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Concurrent Adversarial Learning for Large-Batch Training", "authorids": ["~Yong_Liu13", "~Xiangning_Chen1", "~Minhao_Cheng1", "~Cho-Jui_Hsieh1", "~Yang_You1"], "authors": ["Yong Liu", "Xiangning Chen", "Minhao Cheng", "Cho-Jui Hsieh", "Yang You"], "keywords": ["Distributed Machine Learnig", "Large-Batch Training", "Adversarial Learning"], "abstract": "Large-batch training has become a commonly used technique when training neural networks with a large number of GPU/TPU processors. As batch size increases, stochastic optimizers tend to converge to sharp local minima, leading to degraded test performance. Current methods usually use extensive data augmentation to increase the batch size, but we found the performance gain with data augmentation decreases as batch size increases, and data augmentation will become insufficient after certain point. In this paper, we propose to use adversarial learning to increase the batch size in large-batch training. Despite being a natural choice for smoothing the decision surface and biasing towards a flat region, adversarial learning has not been successfully applied in large-batch training since it requires at least two sequential gradient computations at each step, which will at least double the running time compared with vanilla training even with a large number of processors. To overcome this issue, we propose a novel Concurrent Adversarial Learning (ConAdv) method that decouple the sequential gradient computations in adversarial learning by utilizing staled parameters. Experimental results demonstrate that ConAdv can successfully  increase the batch size on both ResNet-50 and EfficientNet training on ImageNet while maintaining high accuracy. In particular, we show ConAdv along can achieve 75.3\\% top-1 accuracy on ImageNet ResNet-50 training with 96K batch size, and the accuracy can be further improved to 76.2\\% when combining ConAdv with data augmentation. This is the first work successfully scales ResNet-50 training batch size to 96K. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|concurrent_adversarial_learning_for_largebatch_training", "pdf": "/pdf/25ea73927b74e924f44d2eff35ccc805441bcbe9.pdf", "data": "", "code": "", "_bibtex": "@inproceedings{\nliu2022concurrent,\ntitle={Concurrent Adversarial Learning for Large-Batch Training},\nauthor={Yong Liu and Xiangning Chen and Minhao Cheng and Cho-Jui Hsieh and Yang You},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rw1mZl_ss3L}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "5K7RRqZEjoS", "original": "r26S054a5Tu", "number": 545, "cdate": 1632875459631, "mdate": null, "ddate": null, "tcdate": 1632875459631, "tmdate": 1676330666074, "tddate": null, "forum": "5K7RRqZEjoS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation", "authorids": ["~Yan_Zhang1", "~David_W_Zhang1", "~Simon_Lacoste-Julien1", "~Gertjan_J._Burghouts1", "~Cees_G._M._Snoek1"], "authors": ["Yan Zhang", "David W Zhang", "Simon Lacoste-Julien", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "permutation equivariance", "implicit differentiation"], "abstract": "Most set prediction models in deep learning use set-equivariant operations, but they actually operate on multisets. We show that set-equivariant functions cannot represent certain functions on multisets, so we introduce the more appropriate notion of multiset-equivariance. We identify that the existing Deep Set Prediction Network (DSPN) can be multiset-equivariant without being hindered by set-equivariance and improve it with approximate implicit differentiation, allowing for better optimization while being faster and saving memory. In a range of toy experiments, we show that the perspective of multiset-equivariance is beneficial and that our changes to DSPN achieve better results in most cases. On CLEVR object property prediction, we substantially improve over the state-of-the-art Slot Attention from 8% to 77% in one of the strictest evaluation metrics because of the benefits made possible by implicit differentiation.", "pdf": "/pdf/3f0a2b8f6922fd811b23ed91b53451a3e4c8efc1.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|multisetequivariant_set_prediction_with_approximate_implicit_differentiation", "one-sentence_summary": "We propose a better permutation-equivariance property for multisets and improve an existing set predictor that has this property with approximate implicit differentiation", "supplementary_material": "/attachment/fa63763b8da525fc375314aa5c3de0fc4248f164.zip", "_bibtex": "@inproceedings{\nzhang2022multisetequivariant,\ntitle={Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation},\nauthor={Yan Zhang and David W Zhang and Simon Lacoste-Julien and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5K7RRqZEjoS}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "msRBojTz-Nh", "original": "KCVwn-CJQ-O", "number": 541, "cdate": 1632875459407, "mdate": null, "ddate": null, "tcdate": 1632875459407, "tmdate": 1676330666330, "tddate": null, "forum": "msRBojTz-Nh", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learned Simulators for Turbulence", "authorids": ["~Kim_Stachenfeld1", "~Drummond_Buschman_Fielding1", "~Dmitrii_Kochkov1", "~Miles_Cranmer1", "~Tobias_Pfaff1", "~Jonathan_Godwin1", "~Can_Cui2", "~Shirley_Ho2", "~Peter_Battaglia1", "~Alvaro_Sanchez-Gonzalez1"], "authors": ["Kim Stachenfeld", "Drummond Buschman Fielding", "Dmitrii Kochkov", "Miles Cranmer", "Tobias Pfaff", "Jonathan Godwin", "Can Cui", "Shirley Ho", "Peter Battaglia", "Alvaro Sanchez-Gonzalez"], "keywords": ["learned simulation", "turbulence"], "abstract": "Turbulence simulation with classical numerical solvers requires  high-resolution grids to accurately resolve dynamics. Here we train learned simulators at low spatial and temporal resolutions to capture turbulent dynamics generated at high resolution. We show that our proposed model can simulate turbulent dynamics more accurately than classical numerical solvers at the comparably low resolutions across various scientifically relevant metrics. Our model is trained end-to-end from data and is capable of learning a range of challenging chaotic and turbulent dynamics at low resolution, including trajectories generated by the state-of-the-art Athena++ engine. We show that our simpler, general-purpose architecture outperforms various more specialized, turbulence-specific architectures from the learned turbulence simulation literature. In general, we see that learned simulators yield unstable trajectories; however, we show that tuning training noise and temporal downsampling solves this problem. We also find that while generalization beyond the training distribution is a challenge for learned models, training noise, added loss constraints, and dataset augmentation can help. Broadly, we conclude that our learned simulator outperforms traditional solvers run on coarser grids, and emphasize that simple design choices can offer stability and robust generalization.", "one-sentence_summary": "Learned simulators that outperform baselines in capturing turbulent dynamics at low resolution across multiple challenging turbulence domains.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "stachenfeld|learned_simulators_for_turbulence", "pdf": "/pdf/a5285382bfece0e469683d88f7053a50f45acb4e.pdf", "_bibtex": "@inproceedings{\nstachenfeld2022learned,\ntitle={Learned Simulators for Turbulence},\nauthor={Kim Stachenfeld and Drummond Buschman Fielding and Dmitrii Kochkov and Miles Cranmer and Tobias Pfaff and Jonathan Godwin and Can Cui and Shirley Ho and Peter Battaglia and Alvaro Sanchez-Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=msRBojTz-Nh}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "5XmLzdslFNN", "original": "AC77ddt4eAC", "number": 539, "cdate": 1632875459265, "mdate": null, "ddate": null, "tcdate": 1632875459265, "tmdate": 1697934921404, "tddate": null, "forum": "5XmLzdslFNN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Modular Lifelong Reinforcement Learning via Neural Composition", "authorids": ["~Jorge_A_Mendez1", "~Harm_van_Seijen1", "~ERIC_EATON1"], "authors": ["Jorge A Mendez", "Harm van Seijen", "ERIC EATON"], "keywords": ["lifelong learning", "continual learning", "reinforcement learning", "composition", "modularity", "compositionality"], "abstract": "Humans commonly solve complex problems by decomposing them into easier subproblems and then combining the subproblem solutions. This type of compositional reasoning permits reuse of the subproblem solutions when tackling future tasks that share part of the underlying compositional structure. In a continual or lifelong reinforcement learning (RL) setting, this ability to decompose knowledge into reusable components would enable agents to quickly learn new RL tasks by leveraging accumulated compositional structures. We explore a particular form of composition based on neural modules and present a set of RL problems that intuitively admit compositional solutions. Empirically, we demonstrate that neural composition indeed captures the underlying structure of this space of problems. We further propose a compositional lifelong RL method that leverages accumulated neural components to accelerate the learning of future tasks while retaining performance on previous tasks via off-line RL over replayed experiences.", "one-sentence_summary": "We explore the problem of lifelong RL of functionally composable knowledge, and develop an algorithm that demonstrates zero-shot and forward transfer, avoidance of forgetting, and backward transfer in discrete 2-D and robotic manipulation domains.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mendez|modular_lifelong_reinforcement_learning_via_neural_composition", "pdf": "/pdf/a7e9350f0b9a40fc77e2f196fa086d7759fbbbd7.pdf", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2207.00429/code)", "_bibtex": "@inproceedings{\nmendez2022modular,\ntitle={Modular Lifelong Reinforcement Learning via Neural Composition},\nauthor={Jorge A Mendez and Harm van Seijen and ERIC EATON},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5XmLzdslFNN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "7B3IJMM1k_M", "original": "PFzar8A1pxE", "number": 530, "cdate": 1632875458682, "mdate": null, "ddate": null, "tcdate": 1632875458682, "tmdate": 1697934922198, "tddate": null, "forum": "7B3IJMM1k_M", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks", "authorids": ["~Tong_Bu1", "~Wei_Fang2", "~Jianhao_Ding1", "~PENGLIN_DAI1", "~Zhaofei_Yu1", "~Tiejun_Huang1"], "authors": ["Tong Bu", "Wei Fang", "Jianhao Ding", "PENGLIN DAI", "Zhaofei Yu", "Tiejun Huang"], "keywords": ["Spiking Neural Networks", "ANN-SNN Conversion", "Ultra-low Latency", "Quantization Clip-floor-shift Activation"], "abstract": "Spiking Neural Networks (SNNs) have gained great attraction due to their distinctive properties of low power consumption and fast inference on neuromorphic hardware. As the most effective method to get deep SNNs, ANN-SNN conversion has achieved comparable performance as ANNs on large-scale datasets. Despite this, it requires long time-steps to match the firing rates of SNNs to the activation of ANNs. As a result, the converted SNN suffers severe performance degradation problems with short time-steps, which hamper the practical application of SNNs. In this paper, we theoretically analyze ANN-SNN conversion error and derive the estimated activation function of SNNs. Then we propose the quantization clip-floor-shift activation function to replace the ReLU activation function in source ANNs, which can better approximate the activation function of SNNs. We prove that the expected conversion error between SNNs and ANNs is zero, enabling us to achieve high-accuracy and ultra-low-latency SNNs. We evaluate our method on CIFAR-10/100 and ImageNet datasets, and show that it outperforms the state-of-the-art ANN-SNN and directly trained SNNs in both accuracy and time-steps. To the best of our knowledge, this is the first time to explore high-performance ANN-SNN conversion with ultra-low latency (4 time-steps). Code is available at https://github.com/putshua/SNN_conversion_QCFS", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bu|optimal_annsnn_conversion_for_highaccuracy_and_ultralowlatency_spiking_neural_networks", "pdf": "/pdf/b0826b541fa995e04f8d1b450b17143170fe1d8f.pdf", "one-sentence_summary": "An ANN-SNN conversion method enables high-accuracy and ultra-low-latency deep SNNs.", "supplementary_material": "/attachment/8cca47a6a2fc40304aad64e6c22b5fd8bae3f00e.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2303.04347/code)", "_bibtex": "@inproceedings{\nbu2022optimal,\ntitle={Optimal {ANN}-{SNN} Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks},\nauthor={Tong Bu and Wei Fang and Jianhao Ding and PENGLIN DAI and Zhaofei Yu and Tiejun Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7B3IJMM1k_M}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "fvLLcIYmXb", "original": "maldvN1Q0Y", "number": 518, "cdate": 1632875457816, "mdate": null, "ddate": null, "tcdate": 1632875457816, "tmdate": 1697934923266, "tddate": null, "forum": "fvLLcIYmXb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "AS-MLP: An Axial Shifted MLP Architecture for Vision", "authorids": ["~Dongze_Lian1", "~Zehao_Yu2", "~Xing_Sun1", "~Shenghua_Gao1"], "authors": ["Dongze Lian", "Zehao Yu", "Xing Sun", "Shenghua Gao"], "keywords": ["Architecture Design", "MLP", "Classification", "Detection", "Segmentation"], "abstract": "An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper. Different from MLP-Mixer, where the global spatial feature is encoded for information flow through matrix transposition and one token-mixing MLP, we pay more attention to the local features interaction. By axially shifting channels of the feature map, AS-MLP is able to obtain the information flow from different axial directions, which captures the local dependencies. Such an operation enables us to utilize a pure MLP architecture to achieve the same local receptive field as CNN-like architecture. We can also design the receptive field size and dilation of blocks of AS-MLP, \\emph{etc}, in the same spirit of  convolutional neural networks. With the proposed AS-MLP architecture, our model obtains 83.3\\% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the ImageNet-1K dataset. Such a simple yet effective architecture outperforms all MLP-based architectures and achieves competitive performance compared to the transformer-based architectures (\\emph{e.g.}, Swin Transformer) even with slightly lower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be applied to the downstream tasks (\\emph{e.g.}, object detection and semantic segmentation). The experimental results are also impressive. Our proposed AS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the ADE20K dataset, which is competitive compared to the transformer-based architectures. Our AS-MLP establishes a strong baseline of MLP-based architecture. Code is available at \\url{https://github.com/svip-lab/AS-MLP}.", "one-sentence_summary": "We design the first MLP-based architecture for downstream tasks. It achieves competitive performance compared to the transformer-based architecture, which establishes a new strong baseline of MLP-based architecture. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lian|asmlp_an_axial_shifted_mlp_architecture_for_vision", "pdf": "/pdf/4ba030acd877662e2f1238c570aa52552d7f3997.pdf", "supplementary_material": "/attachment/d45563113193a9197eae5e7816797a07946560a6.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.08391/code)", "_bibtex": "@inproceedings{\nlian2022asmlp,\ntitle={{AS}-{MLP}: An Axial Shifted {MLP} Architecture for Vision},\nauthor={Dongze Lian and Zehao Yu and Xing Sun and Shenghua Gao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fvLLcIYmXb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "nrGGfMbY_qK", "original": "mctbLTW0Ij6", "number": 517, "cdate": 1632875457748, "mdate": null, "ddate": null, "tcdate": 1632875457748, "tmdate": 1676330667564, "tddate": null, "forum": "nrGGfMbY_qK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference", "authorids": ["~Hyunseo_Koh1", "~Dahyun_Kim1", "~Jung-Woo_Ha1", "~Jonghyun_Choi1"], "authors": ["Hyunseo Koh", "Dahyun Kim", "Jung-Woo Ha", "Jonghyun Choi"], "keywords": ["online", "continual learning", "task-free continual learning", "any-time inference"], "abstract": "Despite rapid advances in continual learning, a large body of research is devoted to improving performance in the existing setups.\nWhile a handful of work do propose new continual learning setups, they still lack practicality in certain aspects.\nFor better practicality, we first propose a novel continual learning setup that is online, task-free, class-incremental, of blurry task boundaries and subject to inference queries at any moment.\nWe additionally propose a new metric to better measure the performance of the continual learning methods subject to inference queries at any moment.\nTo address the challenging setup and evaluation protocol, we propose an effective method that employs a new memory management scheme and novel learning techniques.\nOur empirical validation demonstrates that the proposed method outperforms prior arts by large margins. Code and data splits are available at https://github.com/naver-ai/i-Blurry.", "one-sentence_summary": "a novel continual learning set-up that is online and task-free, has new class distributions and focuses on any-time inference", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "koh|online_continual_learning_on_class_incremental_blurry_task_configuration_with_anytime_inference", "pdf": "/pdf/ee1c732abb45197cf11bd542a9df4b33a403823f.pdf", "supplementary_material": "/attachment/82e03b6d6fd59d09c337e250793d406fb48d085c.zip", "_bibtex": "@inproceedings{\nkoh2022online,\ntitle={Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference},\nauthor={Hyunseo Koh and Dahyun Kim and Jung-Woo Ha and Jonghyun Choi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nrGGfMbY_qK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 34}}, {"id": "TBWA6PLJZQm", "original": "bFuPVNWw5Jo", "number": 499, "cdate": 1632875456715, "mdate": null, "ddate": null, "tcdate": 1632875456715, "tmdate": 1676330668271, "tddate": null, "forum": "TBWA6PLJZQm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations", "authorids": ["~Jiaheng_Wei1", "~Zhaowei_Zhu1", "~Hao_Cheng5", "~Tongliang_Liu1", "~Gang_Niu1", "~Yang_Liu3"], "authors": ["Jiaheng Wei", "Zhaowei Zhu", "Hao Cheng", "Tongliang Liu", "Gang Niu", "Yang Liu"], "keywords": ["Learning with noisy labels", "benchmark", "real-world label noise", "human annotations"], "abstract": "Existing research on learning with noisy labels mainly focuses on synthetic label noise. The synthetic noise, though has clean structures which greatly enabled statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the existing efforts suffer from two caveats: firstly, the lack of ground-truth verification makes it hard to theoretically study the property and treatment of real-world label noise. Secondly, these efforts are often of large scales, which may result in unfair comparisons of robust methods within reasonable and accessible computation power. To better understand real-world label noise, it is important to establish controllable, easy-to-use, and moderate-sized real-world noisy datasets with both ground-truth and noisy labels. This work presents two new benchmark datasets, which we name as CIFAR-10N, CIFAR-100N (jointly we call them CIFAR-N), equipping the training datasets of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels we collected from Amazon Mechanical Turk. We quantitatively and qualitatively show that real-world noisy labels follow an instance-dependent pattern rather than the classically assumed and adopted ones (e.g.,  class-dependent label noise). We then initiate an effort to benchmarking a subset of the existing solutions using  CIFAR-10N and CIFAR-100N. We further proceed to study the memorization of correct and wrong predictions, which further illustrates the difference between human noise and class-dependent synthetic noise. We show indeed the real-world noise patterns impose new and outstanding challenges as compared to synthetic label noise. These observations require us to rethink the treatment of noisy labels, and we hope the availability of these two datasets would facilitate the development and evaluation of future learning with noisy label solutions. The corresponding datasets and the leaderboard are available at http://noisylabels.com. ", "one-sentence_summary": "In this paper, we revisit the problem of learning from noisy labels using human annotated CIFAR datasets we collected from Amazon Mechanical Turks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wei|learning_with_noisy_labels_revisited_a_study_using_realworld_human_annotations", "pdf": "/pdf/fcda4f135277eb47cf549ba438285dc78fe27977.pdf", "_bibtex": "@inproceedings{\nwei2022learning,\ntitle={Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations},\nauthor={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang Niu and Yang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TBWA6PLJZQm}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "nbC8iTTXIrk", "original": "rnP8VJm6Ft", "number": 493, "cdate": 1632875456283, "mdate": null, "ddate": null, "tcdate": 1632875456283, "tmdate": 1676330668446, "tddate": null, "forum": "nbC8iTTXIrk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Optimization inspired Multi-Branch Equilibrium Models", "authorids": ["~Mingjie_Li1", "~Yisen_Wang1", "~Xingyu_Xie1", "~Zhouchen_Lin1"], "authors": ["Mingjie Li", "Yisen Wang", "Xingyu Xie", "Zhouchen Lin"], "keywords": [], "abstract": "Works have shown the strong connections between some implicit models and optimization problems. However, explorations on such relationships are limited. Most works pay attention to some common mathematical properties, such as sparsity. In this work, we propose a new type of implicit model inspired by the designing of the systems' hidden objective functions, called the Multi-branch Optimization induced Equilibrium networks~(MOptEqs). The model architecture is designed based on modelling the hidden objective function for the multi-resolution recognition task. Furthermore, we also propose a new strategy inspired by our understandings of the hidden objective function. In this manner, the proposed model can better utilize the hierarchical patterns for recognition tasks and retain the abilities for interpreting the whole structure as trying to obtain the minima of the problem's goal. Comparing with the state-of-the-art models, our MOptEqs not only enjoys better explainability but are also superior to MDEQ with less parameter consumption and better performance on practical tasks. Furthermore, we also implement various experiments to demonstrate the effectiveness of our new methods and explore the applicability of the model's hidden objective function.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|optimization_inspired_multibranch_equilibrium_models", "pdf": "/pdf/64ff80fa7f6ea2e747dc3daa4a44b500a5e7888a.pdf", "one-sentence_summary": "A Multi-Branch DEQ model and its training strategy proposed by minimizing an objective function designed as our purpose.", "supplementary_material": "/attachment/5755acb9850c2c7760083a2d75aa69346cde740d.zip", "data": "", "_bibtex": "@inproceedings{\nli2022optimization,\ntitle={Optimization inspired Multi-Branch Equilibrium Models},\nauthor={Mingjie Li and Yisen Wang and Xingyu Xie and Zhouchen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nbC8iTTXIrk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "zNR43c03lRy", "original": "Ni07eEEIARL", "number": 489, "cdate": 1632875455995, "mdate": null, "ddate": null, "tcdate": 1632875455995, "tmdate": 1697934925242, "tddate": null, "forum": "zNR43c03lRy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Annotate Part Segmentation with Gradient Matching", "authorids": ["~Yu_Yang6", "~Xiaotian_Cheng1", "~Hakan_Bilen1", "~Xiangyang_Ji1"], "authors": ["Yu Yang", "Xiaotian Cheng", "Hakan Bilen", "Xiangyang Ji"], "keywords": ["semi-supervised learning", "part segmentation", "semantic segmentation", "generative models", "gradient matching"], "abstract": "The success of state-of-the-art deep neural networks heavily relies on the presence of large-scale labelled datasets, which are extremely expensive and time-consuming to annotate. This paper focuses on tackling semi-supervised part segmentation tasks by generating high-quality images with a pre-trained GAN and labelling the generated images with an automatic annotator. In particular, we formulate the annotator learning as a learning-to-learn problem. Given a pre-trained GAN, the annotator learns to label object parts in a set of randomly generated images such that a part segmentation model trained on these synthetic images with their predicted labels obtains low segmentation error on a small validation set of manually labelled images. We further reduce this nested-loop optimization problem to a simple gradient matching problem and efficiently solve it with an iterative algorithm. We show that our method can learn annotators from a broad range of labelled images including real images, generated images, and even analytically rendered images. Our method is evaluated with semi-supervised part segmentation tasks and significantly outperforms other semi-supervised competitors when the amount of labelled examples is extremely limited.", "pdf": "/pdf/ad3311cb0430eae8c6960121482e58765c803178.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|learning_to_annotate_part_segmentation_with_gradient_matching", "one-sentence_summary": "We propose a gradient-matching-based method to learn annotator which is able to label generated images with part segmentation by decoding the generator features into segmentation masks.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2211.03003/code)", "_bibtex": "@inproceedings{\nyang2022learning,\ntitle={Learning to Annotate Part Segmentation with Gradient Matching},\nauthor={Yu Yang and Xiaotian Cheng and Hakan Bilen and Xiangyang Ji},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zNR43c03lRy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "pfNyExj7z2", "original": "PL2ADDsarP", "number": 488, "cdate": 1632875455921, "mdate": null, "ddate": null, "tcdate": 1632875455921, "tmdate": 1697934925537, "tddate": null, "forum": "pfNyExj7z2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Vector-quantized Image Modeling with Improved VQGAN", "authorids": ["~Jiahui_Yu1", "~Xin_Li41", "~Jing_Yu_Koh2", "~Han_Zhang1", "ruoming@gmail.com", "~James_Qin1", "~Alexander_Ku1", "~Yuanzhong_Xu1", "~Jason_Baldridge1", "~Yonghui_Wu1"], "authors": ["Jiahui Yu", "Xin Li", "Jing Yu Koh", "Han Zhang", "Ruoming Pang", "James Qin", "Alexander Ku", "Yuanzhong Xu", "Jason Baldridge", "Yonghui Wu"], "keywords": ["VQGAN", "Vision Transformers", "Vector-quantized Image Modeling"], "abstract": "Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on ImageNet at 256x256 resolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 73.2% for a similar model size. ViM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.", "one-sentence_summary": "We propose the ViT-VQGAN and further explore a Vector-quantized Image Modeling (VIM) approach on both generative and discriminative tasks on images.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|vectorquantized_image_modeling_with_improved_vqgan", "pdf": "/pdf/63ce2022df1b6352ef17e394bb00cd416cf9497c.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.04627/code)", "_bibtex": "@inproceedings{\nyu2022vectorquantized,\ntitle={Vector-quantized Image Modeling with Improved {VQGAN}},\nauthor={Jiahui Yu and Xin Li and Jing Yu Koh and Han Zhang and Ruoming Pang and James Qin and Alexander Ku and Yuanzhong Xu and Jason Baldridge and Yonghui Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pfNyExj7z2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "R8sQPpGCv0", "original": "e7K6XA6STZ", "number": 486, "cdate": 1632875455774, "mdate": null, "ddate": null, "tcdate": 1632875455774, "tmdate": 1676330668767, "tddate": null, "forum": "R8sQPpGCv0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation", "authorids": ["~Ofir_Press1", "~Noah_Smith1", "~Mike_Lewis1"], "authors": ["Ofir Press", "Noah Smith", "Mike Lewis"], "keywords": [], "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings;  instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.", "pdf": "/pdf/76259151174b85b4bf1f526f9f72da486f40418b.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "press|train_short_test_long_attention_with_linear_biases_enables_input_length_extrapolation", "one-sentence_summary": "We show that our simple position method enables transformer LMs to efficiently and accurately perform inference on longer sequences than they were trained on. ", "supplementary_material": "/attachment/b12fdf10bb64068446f8e39dc39fb169ebbc4ca5.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\npress2022train,\ntitle={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},\nauthor={Ofir Press and Noah Smith and Mike Lewis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=R8sQPpGCv0}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "J1rhANsCY9", "original": "trPrBUo_UCA", "number": 482, "cdate": 1632875455633, "mdate": null, "ddate": null, "tcdate": 1632875455633, "tmdate": 1676330668889, "tddate": null, "forum": "J1rhANsCY9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Representation from Neural Fisher Kernel with Low-rank Approximation", "authorids": ["~Ruixiang_ZHANG1", "~Shuangfei_Zhai3", "~Etai_Littwin2", "~Joshua_M._Susskind1"], "authors": ["Ruixiang ZHANG", "Shuangfei Zhai", "Etai Littwin", "Joshua M. Susskind"], "keywords": [], "abstract": "In this paper, we study the representation of neural networks from the view of kernels. We first define the Neural Fisher Kernel (NFK), which is the Fisher Kernel applied to neural networks. We show that NFK can be computed for both supervised and unsupervised learning models, which can serve as a unified tool for representation extraction. Furthermore, we show that practical NFKs exhibit low-rank structures. We then propose an efficient algorithm that computes a low-rank approximation of NFK, which scales to large datasets and networks. We show that the low-rank approximation of NFKs derived from unsupervised generative models and supervised learning models gives rise to high-quality compact representations of data, achieving competitive results on a variety of machine learning tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|learning_representation_from_neural_fisher_kernel_with_lowrank_approximation", "pdf": "/pdf/decd70c5ae7ce929f66d927827dec0e696609320.pdf", "_bibtex": "@inproceedings{\nzhang2022learning,\ntitle={Learning Representation from Neural Fisher Kernel with Low-rank Approximation},\nauthor={Ruixiang ZHANG and Shuangfei Zhai and Etai Littwin and Joshua M. Susskind},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=J1rhANsCY9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "RDlLMjLJXdq", "original": "j61GZV9zFip", "number": 481, "cdate": 1632875455561, "mdate": null, "ddate": null, "tcdate": 1632875455561, "tmdate": 1697934926227, "tddate": null, "forum": "RDlLMjLJXdq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Temporally Causal Latent Processes from General Temporal Data", "authorids": ["~Weiran_Yao1", "~Yuewen_Sun1", "~Alex_Ho1", "~Changyin_Sun1", "~Kun_Zhang1"], "authors": ["Weiran Yao", "Yuewen Sun", "Alex Ho", "Changyin Sun", "Kun Zhang"], "keywords": [], "abstract": "Our goal is to recover time-delayed latent causal variables and identify their relations from measured temporal data. Estimating causally-related latent variables from observations is particularly challenging as the latent variables are not uniquely recoverable in the most general case. In this work, we consider both a nonparametric, nonstationary setting and a parametric setting for the latent processes and propose two provable conditions under which temporally causal latent processes can be identified from their nonlinear mixtures. We propose LEAP, a theoretically-grounded framework that extends Variational AutoEncoders (VAEs) by enforcing our conditions through proper constraints in causal process prior. Experimental results on various datasets demonstrate that temporally causal latent processes are reliably identified from observed variables under different dependency structures and that our approach considerably outperforms baselines that do not properly leverage history or nonstationarity information. This demonstrates that using temporal information to learn latent processes from their invertible nonlinear mixtures in an unsupervised manner, for which we believe our work is one of the first, seems promising even without sparsity or minimality assumptions. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yao|learning_temporally_causal_latent_processes_from_general_temporal_data", "pdf": "/pdf/917054f5da79e94676388fea7fc2c7b9714dd8d9.pdf", "one-sentence_summary": "Propose two provable conditions and training framework with which temporally latent causal processes are identifiable from observed variables.", "supplementary_material": "/attachment/791e0b61918749998721915927bec7be836034c7.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.05428/code)", "_bibtex": "@inproceedings{\nyao2022learning,\ntitle={Learning Temporally Causal Latent Processes from General Temporal Data},\nauthor={Weiran Yao and Yuewen Sun and Alex Ho and Changyin Sun and Kun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RDlLMjLJXdq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "DXPftn5kjQK", "original": "WbXdyQeVb9Q", "number": 469, "cdate": 1632875454684, "mdate": null, "ddate": null, "tcdate": 1632875454684, "tmdate": 1697934927590, "tddate": null, "forum": "DXPftn5kjQK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Rich Get Richer: Disparate Impact of Semi-Supervised Learning", "authorids": ["~Zhaowei_Zhu1", "~Tianyi_Luo1", "~Yang_Liu3"], "authors": ["Zhaowei Zhu", "Tianyi Luo", "Yang Liu"], "keywords": ["semi-supervised learning", "fairness", "disparate impact", "Matthew effect", "consistency regularization"], "abstract": "Semi-supervised learning (SSL) has demonstrated its potential to improve the model accuracy for a variety of learning tasks when the high-quality supervised data is severely limited. Although it is often established that the average accuracy for the entire population of data is improved, it is unclear how SSL fares with different sub-populations. Understanding the above question has substantial fairness implications when different sub-populations are defined by the demographic groups that we aim to treat fairly. In this paper, we reveal the disparate impacts of deploying SSL: the sub-population who has a higher baseline accuracy without using SSL (the \"rich\" one) tends to benefit more from SSL; while the sub-population who suffers from a low baseline accuracy (the \"poor\" one) might even observe a performance drop after adding the SSL module. We theoretically and empirically establish the above observation for a broad family of SSL algorithms, which either explicitly or implicitly use an auxiliary \"pseudo-label\". Experiments on a set of image and text classification tasks confirm our claims. We introduce a new metric, Benefit Ratio, and promote the evaluation of the fairness of SSL (Equalized Benefit Ratio). We further discuss how the disparate impact can be mitigated. We hope our paper will alarm the potential pitfall of using SSL and encourage a multifaceted evaluation of future SSL algorithms.  ", "one-sentence_summary": "We reveal the disparate impacts of deploying SSL: the \"rich\" sub-population (higher baseline accuracy without SSL) benefits more from SSL; while the \"poor\" sub-population (low baseline accuracy) might even observe a performance drop after SSL.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|the_rich_get_richer_disparate_impact_of_semisupervised_learning", "pdf": "/pdf/512dd2bf00378d8e1ddb8ff29166f99a1339d921.pdf", "supplementary_material": "/attachment/e12dbf1d09d5d2b1202eb9145d191aceec0b1476.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.06282/code)", "_bibtex": "@inproceedings{\nzhu2022the,\ntitle={The Rich Get Richer: Disparate Impact of Semi-Supervised Learning},\nauthor={Zhaowei Zhu and Tianyi Luo and Yang Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DXPftn5kjQK}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "HBsJNesj2S", "original": "mv3iNx4TJDz", "number": 466, "cdate": 1632875454466, "mdate": null, "ddate": null, "tcdate": 1632875454466, "tmdate": 1676330669883, "tddate": null, "forum": "HBsJNesj2S", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Relational Inference with Node-Specific Information ", "authorids": ["~Ershad_Banijamali2"], "authors": ["Ershad Banijamali"], "keywords": ["Graph Neural Networks", "Variational Inference", "Trajectory Prediction"], "abstract": "Inferring interactions among entities is an important problem in studying dynamical systems, which greatly impacts the performance of downstream tasks, such as prediction. In this paper, we tackle the relational inference problem in a setting where each entity can potentially have a set of individualized information that other entities cannot have access to. Specifically, we represent the system using a graph in which the individualized information become node-specific information (NSI). We build our model in the framework of Neural Relation Inference (NRI), where the interaction among entities are uncovered using variational inference. We adopt NRI model to incorporate the individualized information by introducing private nodes in the graph that represent NSI. Such representation enables us to uncover more accurate relations among the agents and therefore leads to better performance on the downstream tasks. Our experiment results over real-world datasets validate the merit of our proposed algorithm. ", "one-sentence_summary": "We use variational inference to uncover relations among agents in a multi-agent system, given that the agents can have access to some private information", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "banijamali|neural_relational_inference_with_nodespecific_information", "pdf": "/pdf/96c13a371051c12e097a483a7c11e21162aa63da.pdf", "data": "", "_bibtex": "@inproceedings{\nbanijamali2022neural,\ntitle={Neural Relational Inference with Node-Specific Information },\nauthor={Ershad Banijamali},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HBsJNesj2S}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "ZU-zFnTum1N", "original": "ynQ_cEgn8l", "number": 457, "cdate": 1632875453721, "mdate": null, "ddate": null, "tcdate": 1632875453721, "tmdate": 1697934929058, "tddate": null, "forum": "ZU-zFnTum1N", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Bregman Gradient Policy Optimization", "authorids": ["~Feihu_Huang1", "~Shangqian_Gao1", "~Heng_Huang1"], "authors": ["Feihu Huang", "Shangqian Gao", "Heng Huang"], "keywords": [], "abstract": "In the paper, we design a novel Bregman gradient policy optimization framework for reinforcement learning based on Bregman divergences and momentum techniques. Specifically, we propose a Bregman gradient policy optimization (BGPO) algorithm based on the basic momentum technique and mirror descent iteration. Meanwhile, we further propose an accelerated Bregman gradient policy optimization (VR-BGPO) algorithm based on the variance reduced technique. Moreover, we provide a convergence analysis framework for our Bregman gradient policy optimization under the nonconvex setting. We prove that our BGPO achieves a  sample complexity of $O(\\epsilon^{-4})$ for finding $\\epsilon$-stationary policy only requiring one trajectory at each iteration, and our VR-BGPO reaches the best known sample complexity of $O(\\epsilon^{-3})$, which also only requires one trajectory at each iteration. In particular, by using different Bregman divergences, our BGPO framework unifies many existing policy optimization algorithms such as the existing (variance reduced) policy gradient algorithms such as natural policy gradient algorithm. Extensive experimental results on multiple reinforcement learning tasks demonstrate the efficiency of our new algorithms. ", "pdf": "/pdf/e120a3566088f53c36d56bd58eba97a8e2c4ffc6.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|bregman_gradient_policy_optimization", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.12112/code)", "_bibtex": "@inproceedings{\nhuang2022bregman,\ntitle={Bregman Gradient Policy Optimization},\nauthor={Feihu Huang and Shangqian Gao and Heng Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZU-zFnTum1N}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "hgKtwSb4S2", "original": "ucQq9C4Gs5y", "number": 450, "cdate": 1632875453221, "mdate": null, "ddate": null, "tcdate": 1632875453221, "tmdate": 1676330670887, "tddate": null, "forum": "hgKtwSb4S2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A generalization of the randomized singular value decomposition", "authorids": ["~Nicolas_Boulle1", "~Alex_Townsend1"], "authors": ["Nicolas Boulle", "Alex Townsend"], "keywords": ["Low rank approximation", "Randomized SVD", "Hilbert--Schmidt operators", "Gaussian processes"], "abstract": "The randomized singular value decomposition (SVD) is a popular and effective algorithm for computing a near-best rank $k$ approximation of a matrix $A$ using matrix-vector products with standard Gaussian vectors. Here, we generalize the theory of randomized SVD to multivariate Gaussian vectors, allowing one to incorporate prior knowledge of $A$ into the algorithm. This enables us to explore the continuous analogue of the randomized SVD for Hilbert--Schmidt (HS) operators using operator-function products with functions drawn from a Gaussian process (GP). We then construct a new covariance kernel for GPs, based on weighted Jacobi polynomials, which allows us to rapidly sample the GP and control the smoothness of the randomly generated functions. Numerical examples on matrices and HS operators demonstrate the applicability of the algorithm.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "boulle|a_generalization_of_the_randomized_singular_value_decomposition", "pdf": "/pdf/fa63296901b49d968d4f9bcdde4847cb73c0b1a3.pdf", "one-sentence_summary": "The randomized SVD is generalized to multivariate Gaussian input vectors and Hilbert-Schmidt operators.", "supplementary_material": "/attachment/22307c6009ec85164c46736b5486f1003eeed7a3.zip", "_bibtex": "@inproceedings{\nboulle2022a,\ntitle={A generalization of the randomized singular value decomposition},\nauthor={Nicolas Boulle and Alex Townsend},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hgKtwSb4S2}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "xCVJMsPv3RT", "original": "Q7cfPbRmjJN", "number": 449, "cdate": 1632875453150, "mdate": null, "ddate": null, "tcdate": 1632875453150, "tmdate": 1697934930324, "tddate": null, "forum": "xCVJMsPv3RT", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Dropout Q-Functions for Doubly Efficient Reinforcement Learning", "authorids": ["~Takuya_Hiraoka1", "~Takahisa_Imagawa1", "~Taisei_Hashimoto1", "~Takashi_Onishi1", "~Yoshimasa_Tsuruoka1"], "authors": ["Takuya Hiraoka", "Takahisa Imagawa", "Taisei Hashimoto", "Takashi Onishi", "Yoshimasa Tsuruoka"], "keywords": ["Reinforcement learning"], "abstract": "Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has recently achieved state-of-the-art sample efficiency on continuous-action reinforcement learning benchmarks. This superior sample efficiency is made possible by using a large Q-function ensemble. However, REDQ is much less computationally efficient than non-ensemble counterparts such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018a). To make REDQ more computationally efficient, we propose a method of improving computational efficiency called DroQ, which is a variant of REDQ that uses a small ensemble of dropout Q-functions. Our dropout Q-functions are simple Q-functions equipped with dropout connection and layer normalization. Despite its simplicity of implementation, our experimental results indicate that DroQ is doubly (sample and computationally) efficient. It achieved comparable sample efficiency with REDQ, much better computational efficiency than REDQ, and comparable computational efficiency with that of SAC.", "pdf": "/pdf/b31fb60261f2746e9fc9ccca341e609e0b152e43.pdf", "one-sentence_summary": "We propose a doubly (sample and computationally) efficient RL method (Dr.Q) in which a small ensemble of dropout Q-functions is used. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hiraoka|dropout_qfunctions_for_doubly_efficient_reinforcement_learning", "supplementary_material": "/attachment/f092f9beff8d577d4e346d244650718a5959f424.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.02034/code)", "_bibtex": "@inproceedings{\nhiraoka2022dropout,\ntitle={Dropout Q-Functions for Doubly Efficient Reinforcement Learning},\nauthor={Takuya Hiraoka and Takahisa Imagawa and Taisei Hashimoto and Takashi Onishi and Yoshimasa Tsuruoka},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xCVJMsPv3RT}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "ySQH0oDyp7", "original": "ahxny2oaiMm", "number": 443, "cdate": 1632875452775, "mdate": null, "ddate": null, "tcdate": 1632875452775, "tmdate": 1697934930527, "tddate": null, "forum": "ySQH0oDyp7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization", "authorids": ["~Xiuying_Wei1", "~Ruihao_Gong1", "~Yuhang_Li1", "~Xianglong_Liu3", "~Fengwei_Yu1"], "authors": ["Xiuying Wei", "Ruihao Gong", "Yuhang Li", "Xianglong Liu", "Fengwei Yu"], "keywords": [], "abstract": "Recently, post-training quantization (PTQ) has driven much attention to produce efficient neural networks without long-time retraining. Despite the low cost, current PTQ works always fail under the extremely low-bit setting. In this study, we pioneeringly confirm that properly incorporating activation quantization into the PTQ reconstruction benefits the final accuracy. To deeply understand the inherent reason, a theoretical framework is established, which inspires us that the flatness of the optimized low-bit model on calibration and test data is crucial. Based on the conclusion, a simple yet effective approach dubbed as \\textsc{QDrop} is proposed, which randomly drops the quantization of activations during reconstruction. Extensive experiments on various tasks including computer vision (image classification, object detection) and natural language processing (text classification and question answering) prove its superiority. With \\textsc{QDrop}, the limit of PTQ is pushed to the 2-bit activation for the first time and the accuracy boost can be up to 51.49\\%. Without bells and whistles, \\textsc{QDrop} establishes a new state of the art for PTQ.", "pdf": "/pdf/d4b98e0b2a7f155f01c30723878a99c839a19f05.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wei|qdrop_randomly_dropping_quantization_for_extremely_lowbit_posttraining_quantization", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.05740/code)", "_bibtex": "@inproceedings{\nwei2022qdrop,\ntitle={{QD}rop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization},\nauthor={Xiuying Wei and Ruihao Gong and Yuhang Li and Xianglong Liu and Fengwei Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ySQH0oDyp7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "POxF-LEqnF", "original": "ulCl_y5ZaQ", "number": 437, "cdate": 1632875452341, "mdate": null, "ddate": null, "tcdate": 1632875452341, "tmdate": 1676330671388, "tddate": null, "forum": "POxF-LEqnF", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction", "authorids": ["~Osama_Makansi1", "~Julius_Von_K\u00fcgelgen1", "~Francesco_Locatello1", "~Peter_Vincent_Gehler1", "~Dominik_Janzing1", "~Thomas_Brox1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Osama Makansi", "Julius Von K\u00fcgelgen", "Francesco Locatello", "Peter Vincent Gehler", "Dominik Janzing", "Thomas Brox", "Bernhard Sch\u00f6lkopf"], "keywords": ["Feature Attribution", "Shapley values", "Trajectory Prediction", "Causality"], "abstract": "Predicting the future trajectory of a moving agent can be easy when the past trajectory continues smoothly but is challenging when complex interactions with other agents are involved. Recent deep learning approaches for trajectory prediction show promising performance and partially attribute this to successful reasoning about agent-agent interactions.  However, it remains unclear which features such black-box models actually learn to use for making predictions. This paper proposes a procedure that quantifies the contributions of different cues to model performance based on a variant of Shapley values. Applying this procedure to state-of-the-art trajectory prediction methods on standard benchmark datasets shows that they are, in fact, unable to reason about interactions. Instead, the past trajectory of the target is the only feature used for predicting its future. For a task with richer social interaction patterns, on the other hand, the tested models do pick up such interactions to a certain extent, as quantified by our feature attribution method. We discuss the limits of the proposed method and its links to causality.", "pdf": "/pdf/c2255d7b6f3e4ca52c2e54c74469ac86211d02ca.pdf", "one-sentence_summary": "We propose a Shapley value-based method for attributing trajectory prediction performance to different input features and show on common benchmark datasets that existing models do not use interaction information, contrary to their claims.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "makansi|you_mostly_walk_alone_analyzing_feature_attribution_in_trajectory_prediction", "data": "", "_bibtex": "@inproceedings{\nmakansi2022you,\ntitle={You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction},\nauthor={Osama Makansi and Julius Von K{\\\"u}gelgen and Francesco Locatello and Peter Vincent Gehler and Dominik Janzing and Thomas Brox and Bernhard Sch{\\\"o}lkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=POxF-LEqnF}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "aYAA-XHKyk", "original": "AG2yuD4LdIB", "number": 435, "cdate": 1632875452193, "mdate": null, "ddate": null, "tcdate": 1632875452193, "tmdate": 1676330671480, "tddate": null, "forum": "aYAA-XHKyk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Rethinking Class-Prior Estimation for Positive-Unlabeled Learning", "authorids": ["~Yu_Yao3", "~Tongliang_Liu1", "~Bo_Han1", "~Mingming_Gong1", "~Gang_Niu1", "~Masashi_Sugiyama1", "~Dacheng_Tao1"], "authors": ["Yu Yao", "Tongliang Liu", "Bo Han", "Mingming Gong", "Gang Niu", "Masashi Sugiyama", "Dacheng Tao"], "keywords": ["Positive-Unlabeled Learning", "Class-Prior Estimation"], "abstract": "Given only positive (P) and unlabeled (U) data, PU learning can train a binary classifier without any negative data. It has two building blocks: PU class-prior estimation (CPE) and PU classification; the latter has been well studied while the former has received less attention. Hitherto, the distributional-assumption-free CPE methods rely on a critical assumption that the support of the positive data distribution cannot be contained in the support of the negative data distribution. If this is violated, those CPE methods will systematically overestimate the class prior; it is even worse that we cannot verify the assumption based on the data. In this paper, we rethink CPE for PU learning\u2014can we remove the assumption to make CPE always valid? We show an affirmative answer by proposing Regrouping CPE (ReCPE) that builds an auxiliary probability distribution such that the support of the positive data distribution is never contained in the support of the negative data distribution. ReCPE can work with any CPE method by treating it as the base method. Theoretically, ReCPE does not affect its base if the assumption already holds for the original probability distribution; otherwise, it reduces the positive bias of its base. Empirically, ReCPE improves all state-of-the-art CPE methods on various datasets, implying that the assumption has indeed been violated here.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yao|rethinking_classprior_estimation_for_positiveunlabeled_learning", "pdf": "/pdf/edf5d059e7a69954e63ffc928b52490879c12cdb.pdf", "one-sentence_summary": "Class-Prior Estimation for Positive-Unlabeled Learning", "supplementary_material": "/attachment/f6da1273d3affae318189d1e6f7dbc0b737e2070.zip", "_bibtex": "@inproceedings{\nyao2022rethinking,\ntitle={Rethinking Class-Prior Estimation for Positive-Unlabeled Learning},\nauthor={Yu Yao and Tongliang Liu and Bo Han and Mingming Gong and Gang Niu and Masashi Sugiyama and Dacheng Tao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=aYAA-XHKyk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "bfuGjlCwAq", "original": "fpaYIYMYlZ", "number": 416, "cdate": 1632875450852, "mdate": null, "ddate": null, "tcdate": 1632875450852, "tmdate": 1676330672241, "tddate": null, "forum": "bfuGjlCwAq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Efficient Online 3D Bin Packing on Packing Configuration Trees", "authorids": ["~Hang_Zhao3", "~Yang_Yu5", "~Kai_Xu5"], "authors": ["Hang Zhao", "Yang Yu", "Kai Xu"], "keywords": ["Bin Packing Problem", "Online 3D-BPP", "Reinforcement Learning"], "abstract": "Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation and has aroused enthusiastic research interest recently. Existing methods usually solve the problem with limited resolution of spatial discretization, and/or cannot deal with complex practical constraints well. We propose to enhance the practical applicability of online 3D-BPP via learning on a novel hierarchical representation \u2013 packing configuration tree (PCT). PCT is a full-fledged description of the state and action space of bin packing which can support packing policy learning based on deep reinforcement learning (DRL). The size of the packing action space is proportional to the number of leaf nodes, making the DRL model easy to train and well-performing even with continuous solution space. During training, PCT expands based on heuristic rules, however, the DRL model learns a much more effective and robust packing policy than heuristic methods. Through extensive evaluation, we demonstrate that our method outperforms all existing online BPP methods and is versatile in terms of incorporating various practical constraints.", "pdf": "/pdf/e5e3ec7c1a72d597a2bb22fa50e8e20915ce740c.pdf", "one-sentence_summary": "We propose to enhance the practical applicability of online 3D-BPP via learning on a hierarchical packing configuration tree which makes the DRL model easy to deal with practical constraints and well-performing even with continuous solution space.", "supplementary_material": "/attachment/58c94c6f61b3dafa283a2fb41693fe9b5045a884.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhao|learning_efficient_online_3d_bin_packing_on_packing_configuration_trees", "_bibtex": "@inproceedings{\nzhao2022learning,\ntitle={Learning Efficient Online 3D Bin Packing on Packing Configuration Trees},\nauthor={Hang Zhao and Kai Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bfuGjlCwAq}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "6HN7LHyzGgC", "original": "exvzJfmj9uY", "number": 410, "cdate": 1632875450486, "mdate": null, "ddate": null, "tcdate": 1632875450486, "tmdate": 1697934933756, "tddate": null, "forum": "6HN7LHyzGgC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Uncertainty Modeling for Out-of-Distribution Generalization", "authorids": ["~Xiaotong_Li2", "~Yongxing_Dai1", "~Yixiao_Ge2", "~Jun_Liu8", "~Ying_Shan2", "~LINGYU_DUAN1"], "authors": ["Xiaotong Li", "Yongxing Dai", "Yixiao Ge", "Jun Liu", "Ying Shan", "LINGYU DUAN"], "keywords": ["domain generalization", "uncertainty modeling"], "abstract": "Though remarkable progress has been achieved in various vision tasks, deep neural networks still suffer obvious performance degradation when tested in out-of-distribution scenarios. We argue that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization ability of deep learning models. Common methods often consider the feature statistics as deterministic values measured from the learned features and do not explicitly consider the uncertain statistics discrepancy caused by potential domain shifts during testing. In this paper, we improve the network generalization ability by modeling the uncertainty of domain shifts with synthesized feature statistics during training. Specifically, we hypothesize that the feature statistic, after considering the potential uncertainties, follows a multivariate Gaussian distribution. Hence, each feature statistic is no longer a deterministic value, but a probabilistic point with diverse distribution possibilities. With the uncertain feature statistics, the models can be trained to alleviate the domain perturbations and achieve better robustness against potential domain shifts. Our method can be readily integrated into networks without additional parameters. Extensive experiments demonstrate that our proposed method consistently improves the network generalization ability on multiple vision tasks, including image classification, semantic segmentation, and instance retrieval.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|uncertainty_modeling_for_outofdistribution_generalization", "pdf": "/pdf/8b458e772747ce913aafc8c12549f3d6efa75dd9.pdf", "one-sentence_summary": "We for the first time treat feature statistics as uncertain distributions to improve the model generalization ability.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.03958/code)", "_bibtex": "@inproceedings{\nli2022uncertainty,\ntitle={Uncertainty Modeling for Out-of-Distribution Generalization},\nauthor={Xiaotong Li and Yongxing Dai and Yixiao Ge and Jun Liu and Ying Shan and LINGYU DUAN},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6HN7LHyzGgC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "bYGSzbCM_i", "original": "smybaD3HKr8", "number": 409, "cdate": 1632875450412, "mdate": null, "ddate": null, "tcdate": 1632875450412, "tmdate": 1697934934032, "tddate": null, "forum": "bYGSzbCM_i", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Online Adversarial Attacks", "authorids": ["~Andjela_Mladenovic1", "~Joey_Bose1", "~Hugo_Berard2", "~William_L._Hamilton1", "~Simon_Lacoste-Julien1", "~Pascal_Vincent1", "~Gauthier_Gidel1"], "authors": ["Andjela Mladenovic", "Joey Bose", "Hugo Berard", "William L. Hamilton", "Simon Lacoste-Julien", "Pascal Vincent", "Gauthier Gidel"], "keywords": ["Online Algorithms", "Adversarial Attacks"], "abstract": "Adversarial attacks expose important vulnerabilities of deep learning models, yet little attention has been paid to settings where data arrives as a stream. In this paper, we formalize the online adversarial attack problem, emphasizing two key elements found in real-world use-cases: attackers must operate under partial knowledge of the target model, and the decisions made by the attacker are irrevocable since they operate on a transient data stream. We first rigorously analyze a deterministic variant of the online threat model by drawing parallels to the well-studied $k$-secretary problem in theoretical computer science and propose Virtual+, a simple yet practical online algorithm. Our main theoretical result shows Virtual+ yields provably the best competitive ratio over all single-threshold algorithms for $k<5$---extending the previous analysis of the $k$-secretary problem. We also introduce the \\textit{stochastic $k$-secretary}---effectively reducing online blackbox transfer attacks to a $k$-secretary problem under noise---and prove theoretical bounds on the performance of Virtual+ adapted to this setting. Finally, we complement our theoretical results by conducting experiments on MNIST, CIFAR-10, and Imagenet classifiers, revealing the necessity of online algorithms in achieving near-optimal performance and also the rich interplay between attack strategies and online attack selection, enabling simple strategies like FGSM to outperform stronger adversaries.", "one-sentence_summary": "We consider a new adversarial attack setting in which the data arrives as a stream and an adversary must pick top-k items to craft blackbox transfer attacks against an unkown target model.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mladenovic|online_adversarial_attacks", "pdf": "/pdf/24f13e61662f3513c6032f30cf8bf30a47bdd2d1.pdf", "supplementary_material": "/attachment/6c0a2e919f8a5680a3c89a2eff430d7dfa586422.zip", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2103.02014/code)", "_bibtex": "@inproceedings{\nmladenovic2022online,\ntitle={Online Adversarial Attacks},\nauthor={Andjela Mladenovic and Joey Bose and Hugo Berard and William L. Hamilton and Simon Lacoste-Julien and Pascal Vincent and Gauthier Gidel},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bYGSzbCM_i}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "kNKFOXleuC", "original": "4q1Cke0SeyD", "number": 407, "cdate": 1632875450262, "mdate": null, "ddate": null, "tcdate": 1632875450262, "tmdate": 1676330672749, "tddate": null, "forum": "kNKFOXleuC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Anytime Dense Prediction with Confidence Adaptivity", "authorids": ["~Zhuang_Liu1", "~Zhiqiu_Xu1", "~Hung-Ju_Wang1", "~Trevor_Darrell2", "~Evan_Shelhamer2"], "authors": ["Zhuang Liu", "Zhiqiu Xu", "Hung-Ju Wang", "Trevor Darrell", "Evan Shelhamer"], "keywords": ["Efficient Inference", "Anytime Inference", "Semantic Segmentation", "Dense Prediction", "Computer Vision"], "abstract": "Anytime inference requires a model to make a progression of predictions which might be halted at any time. Prior research on anytime visual recognition has mostly focused on image classification.We propose the first unified and end-to-end approach for anytime dense prediction. A cascade of \"exits\" is attached to the model to make multiple predictions. We redesign the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, and make full use of prior predictions, we develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. Our full method, named anytime dense prediction with confidence (ADP-C), achieves the same level of final accuracy, and meanwhile significantly reduces total computation. We evaluate our method on Cityscapes semantic segmentation and MPII human pose estimation: ADP-C enables anytime inference without sacrificing accuracy while also reducing the total FLOPs of its base models by 44.4% and 59.1%. We compare with anytime inference by deep equilibrium networks and feature-based stochastic sampling, showing that ADP-C dominates both across the accuracy-computation curve. Our code is available at https://github.com/liuzhuang13/anytime.", "pdf": "/pdf/32a205bf9187cb9f1f299230fb996fb4ca7c920a.pdf", "one-sentence_summary": "First single-model anytime approach for pixel-level visual recognition; our model with redesigned exits and confidence adaptivity enables anytime inference, achieves the same level of final accuracy, and significantly reduces total computation. ", "supplementary_material": "/attachment/be17d20312589464e5ee7f5d3779aab2282cf1f4.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|anytime_dense_prediction_with_confidence_adaptivity", "code": "", "data": "", "_bibtex": "@inproceedings{\nliu2022anytime,\ntitle={Anytime Dense Prediction with Confidence Adaptivity},\nauthor={Zhuang Liu and Zhiqiu Xu and Hung-Ju Wang and Trevor Darrell and Evan Shelhamer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kNKFOXleuC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "q4HaTeMO--y", "original": "J0TDLxzLbJZ", "number": 406, "cdate": 1632875450190, "mdate": null, "ddate": null, "tcdate": 1632875450190, "tmdate": 1676330672859, "tddate": null, "forum": "q4HaTeMO--y", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Declarative nets that are equilibrium models", "authorids": ["~Russell_Tsuchida1", "~Suk_Yee_Yong1", "~Mohammad_Ali_Armin1", "~Lars_Petersson2", "~Cheng_Soon_Ong1"], "authors": ["Russell Tsuchida", "Suk Yee Yong", "Mohammad Ali Armin", "Lars Petersson", "Cheng Soon Ong"], "keywords": ["deep equilibrium models", "deep declarative networks", "implicit layers", "kernel methods", "generalised linear models"], "abstract": "Implicit layers are computational modules that output the solution to some problem depending on the input and the layer parameters. Deep equilibrium models (DEQs) output a solution to a fixed point equation. Deep declarative networks (DDNs) solve an optimisation problem in their forward pass, an arguably more intuitive, interpretable problem than finding a fixed point. We show that solving a kernelised regularised maximum likelihood estimate as an inner problem in a DDN yields a large class of DEQ architectures. Our proof uses the exponential family in canonical form, and provides a closed-form expression for the DEQ parameters in terms of the kernel. The activation functions have interpretations in terms of the derivative of the log partition function. Building on existing literature, we interpret DEQs as fine-tuned, unrolled classical algorithms, giving an intuitive justification for why DEQ models are sensible. We use our theoretical result to devise an initialisation scheme for DEQs that allows them to solve kGLMs in their forward pass at initialisation. We empirically show that this initialisation scheme improves training stability and performance over random initialisation.", "one-sentence_summary": "Choosing a kernelised generalised linear model as the inner problem of a DDN yields a DEQ with specific fixed weights.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tsuchida|declarative_nets_that_are_equilibrium_models", "pdf": "/pdf/691b2e9a113f764639503e6882404ede8e06e75c.pdf", "supplementary_material": "/attachment/72066ec7a2392d6d58e86a17668039ad1f5c0639.zip", "_bibtex": "@inproceedings{\ntsuchida2022declarative,\ntitle={Declarative nets that are equilibrium models},\nauthor={Russell Tsuchida and Suk Yee Yong and Mohammad Ali Armin and Lars Petersson and Cheng Soon Ong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=q4HaTeMO--y}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "AcrlgZ9BKed", "original": "1aRxOuPCmba", "number": 402, "cdate": 1632875449973, "mdate": null, "ddate": null, "tcdate": 1632875449973, "tmdate": 1676330673102, "tddate": null, "forum": "AcrlgZ9BKed", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning", "authorids": ["~Yunchang_Yang2", "~Tianhao_Wu1", "~Han_Zhong1", "~Evrard_Garcelon1", "~Matteo_Pirotta1", "~Alessandro_Lazaric2", "~Liwei_Wang1", "~Simon_Shaolei_Du1"], "authors": ["Yunchang Yang", "Tianhao Wu", "Han Zhong", "Evrard Garcelon", "Matteo Pirotta", "Alessandro Lazaric", "Liwei Wang", "Simon Shaolei Du"], "keywords": ["bandits", "lower bound", "reinforcement learning theory"], "abstract": "We study bandits and reinforcement learning (RL) subject to a conservative constraint where the agent is asked to perform at least as well as a given baseline policy. This setting is particular relevant in real-world domains including digital marketing, healthcare, production, finance, etc. In this paper, we present a reduction-based framework for conservative bandits and RL, in which our core technique is to calculate the necessary and sufficient budget obtained from running the baseline policy. For lower bounds, we improve the existing lower bound for conservative multi-armed bandits and obtain new lower bounds for conservative linear bandits, tabular RL and low-rank MDP, through a black-box reduction that turns a certain lower bound in the nonconservative setting into a new lower bound in the conservative setting.  For upper bounds, in multi-armed bandits, linear bandits and tabular RL, our new upper bounds tighten or match existing ones with significantly simpler analyses. We also obtain a new upper bound for conservative low-rank MDP.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|a_reductionbased_framework_for_conservative_bandits_and_reinforcement_learning", "pdf": "/pdf/559947894a1c34a3506ac8af2c0fe052117babd0.pdf", "one-sentence_summary": "We give general framework that turns upper and lower bounds in non-conservative settings to bounds in conservative settings.", "supplementary_material": "/attachment/45ba5a4be1d2fc184a6934b192f04e6bb712b358.zip", "_bibtex": "@inproceedings{\nyang2022a,\ntitle={A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning},\nauthor={Yunchang Yang and Tianhao Wu and Han Zhong and Evrard Garcelon and Matteo Pirotta and Alessandro Lazaric and Liwei Wang and Simon Shaolei Du},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AcrlgZ9BKed}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "MvO2t0vbs4-", "original": "TfF-FBxF6P", "number": 398, "cdate": 1632875449668, "mdate": null, "ddate": null, "tcdate": 1632875449668, "tmdate": 1676330673251, "tddate": null, "forum": "MvO2t0vbs4-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models", "authorids": ["~Xiaofang_Wang1", "dankondratyuk@google.com", "~Eric_Christiansen1", "~Kris_M._Kitani1", "~Yair_Movshovitz-Attias1", "~Elad_Eban1"], "authors": ["Xiaofang Wang", "Dan Kondratyuk", "Eric Christiansen", "Kris M. Kitani", "Yair Movshovitz-Attias", "Elad Eban"], "keywords": ["Ensemble", "Cascade", "Efficiency"], "abstract": "Committee-based models (ensembles or cascades) construct models by combining existing pre-trained ones. While ensembles and cascades are well-known techniques that were proposed before deep learning, they are not considered a core building block of deep model architectures and are rarely compared to in recent literature on developing efficient models. In this work, we go back to basics and conduct a comprehensive analysis of the efficiency of committee-based models. We find that even the most simplistic method for building committees from existing, independently pre-trained models can match or exceed the accuracy of state-of-the-art models while being drastically more efficient. These simple committee-based models also outperform sophisticated neural architecture search methods (e.g., BigNAS). These findings hold true for several tasks, including image classification, video classification, and semantic segmentation, and various architecture families, such as ViT, EfficientNet, ResNet, MobileNetV2, and X3D. Our results show that an EfficientNet cascade can achieve a 5.4x speedup over B7 and a ViT cascade can achieve a 2.3x speedup over ViT-L-384 while being equally accurate.", "pdf": "/pdf/30614ee406abf0fbe954e884f90eb52e58cb5336.pdf", "one-sentence_summary": "A simple ensemble or cascade of off-the-shelf pre-trained models can match or exceed the accuracy of SOTA models while being drastically more efficient.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|wisdom_of_committees_an_overlooked_approach_to_faster_and_more_accurate_models", "data": "", "code": "", "_bibtex": "@inproceedings{\nwang2022wisdom,\ntitle={Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models},\nauthor={Xiaofang Wang and Dan Kondratyuk and Eric Christiansen and Kris M. Kitani and Yair Movshovitz-Attias and Elad Eban},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MvO2t0vbs4-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "rwE8SshAlxw", "original": "QxTa8cajyiG", "number": 396, "cdate": 1632875449521, "mdate": null, "ddate": null, "tcdate": 1632875449521, "tmdate": 1697934935125, "tddate": null, "forum": "rwE8SshAlxw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Discovery of Object Radiance Fields", "authorids": ["~Hong-Xing_Yu1", "~Leonidas_Guibas1", "~Jiajun_Wu1"], "authors": ["Hong-Xing Yu", "Leonidas Guibas", "Jiajun Wu"], "keywords": ["object discovery", "scene decomposition", "3D scene representations", "object-centric learning"], "abstract": "We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF enables novel tasks, such as scene segmentation and editing in 3D, and it performs well on these tasks and on novel view synthesis on three datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|unsupervised_discovery_of_object_radiance_fields", "pdf": "/pdf/cfb2b52583e94904f05e5386618edd96d4521272.pdf", "one-sentence_summary": "Inferring object-centric factorized 3D scene representations from a single image, learned without 3D geometry or segmentation supervision.", "supplementary_material": "/attachment/8b2c25b6813698abdb1d7f7aed2cd5857aefe293.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2107.07905/code)", "_bibtex": "@inproceedings{\nyu2022unsupervised,\ntitle={Unsupervised Discovery of Object Radiance Fields},\nauthor={Hong-Xing Yu and Leonidas Guibas and Jiajun Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rwE8SshAlxw}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "fPhKeld3Okz", "original": "ic3CiH_H6Rs", "number": 392, "cdate": 1632875449221, "mdate": null, "ddate": null, "tcdate": 1632875449221, "tmdate": 1676330673611, "tddate": null, "forum": "fPhKeld3Okz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Gradient Step Denoiser for convergent Plug-and-Play", "authorids": ["~Samuel_Hurault1", "arthur.leclaire@math.u-bordeaux.fr", "~Nicolas_Papadakis3"], "authors": ["Samuel Hurault", "Arthur Leclaire", "Nicolas Papadakis"], "keywords": ["Plug-and-Play", "Inverse Problem", "Image Restoration", "Denoising"], "abstract": "Plug-and-Play methods constitute a class of iterative algorithms for imaging problems where regularization is performed by an off-the-shelf denoiser. Although Plug-and-Play methods can lead to tremendous visual performance for various image problems, the few existing convergence guarantees are based on unrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly convex data terms. In this work, we propose a new type of Plug-and-Play methods, based on half-quadratic splitting, for which the denoiser is realized as a gradient descent step on a functional parameterized by a deep neural network. Exploiting convergence results for proximal gradient descent algorithms in the non-convex setting, we show that the proposed Plug-and-Play algorithm is a convergent iterative scheme that targets stationary points of an explicit global functional. Besides, experiments show that it is possible to learn such a deep denoiser while not compromising the performance in comparison to other state-of-the-art deep denoisers used in Plug-and-Play schemes. We apply our proximal gradient algorithm to various ill-posed inverse problems, e.g. deblurring, super-resolution and inpainting. For all these applications, numerical results empirically confirm the convergence results. Experiments also show that this new algorithm reaches state-of-the-art performance, both quantitatively and qualitatively.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hurault|gradient_step_denoiser_for_convergent_plugandplay", "pdf": "/pdf/ebee4c9eada2179f7cee9f61783b96bb27df1819.pdf", "one-sentence_summary": "We propose a performant Plug-and-Play image restoration algorithm that theoretically converges with an exact gradient step deep denoiser.", "supplementary_material": "/attachment/1e5855977beb186097e47f716e0ca0fb79449511.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nhurault2022gradient,\ntitle={Gradient Step Denoiser for convergent Plug-and-Play},\nauthor={Samuel Hurault and Arthur Leclaire and Nicolas Papadakis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fPhKeld3Okz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "edONMAnhLu-", "original": "7ogurVsxkRD", "number": 389, "cdate": 1632875449067, "mdate": null, "ddate": null, "tcdate": 1632875449067, "tmdate": 1676330673783, "tddate": null, "forum": "edONMAnhLu-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Surrogate Gap Minimization Improves Sharpness-Aware Training", "authorids": ["~Juntang_Zhuang1", "~Boqing_Gong1", "~Liangzhe_Yuan2", "~Yin_Cui1", "~Hartwig_Adam1", "~Nicha_C_Dvornek1", "~sekhar_tatikonda1", "~James_s_Duncan1", "~Ting_Liu4"], "authors": ["Juntang Zhuang", "Boqing Gong", "Liangzhe Yuan", "Yin Cui", "Hartwig Adam", "Nicha C Dvornek", "sekhar tatikonda", "James s Duncan", "Ting Liu"], "keywords": ["generalization", "sharpness-aware minimization", "surrogate gap", "deep learning"], "abstract": "The recently proposed  Sharpness-Aware  Minimization  (SAM)  improves generalization by minimizing a perturbed loss defined as the maximum loss within a neighborhood in the parameter space. However, we show that both sharp and flat minima can have a low perturbed loss, implying that SAM does not always prefer flat minima. Instead, we define a surrogate gap, a measure equivalent to the dominant eigenvalue of Hessian at a local minimum when the radius of neighborhood (to derive the perturbed loss) is small.  The surrogate gap is easy to compute and feasible for direct minimization during training. Based on the above observations, we propose Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), a novel improvement over SAM with negligible computation overhead.  Conceptually, GSAM consists of two steps:  1) a gradient descent like SAM to minimize the perturbed loss, and 2) an ascent step in the orthogonal direction (after gradient decomposition) to minimize the surrogate gap and yet not affect the perturbed loss. GSAM seeks a region with both small loss (by step 1) and low sharpness (by step 2), giving rise to a model with high generalization capabilities. Theoretically, we show the convergence of GSAM and provably better generalization than SAM.Empirically, GSAM consistently improves generalization (e.g., +3.2% over SAM and +5.4% over AdamW on ImageNet top-1 accuracy for ViT-B/32). Code is released at https://sites.google.com/view/gsam-iclr22/home", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhuang|surrogate_gap_minimization_improves_sharpnessaware_training", "pdf": "/pdf/a8445fdd027dcffe865628c3024581cae637dca1.pdf", "one-sentence_summary": "We propose GSAM which seeks a region with both small loss and low sharpness, and improves generalization over SAM with negligible computation overhead.", "supplementary_material": "/attachment/210c14e034871d4f8f5976c1586aaa7425b7d225.zip", "data": "", "_bibtex": "@inproceedings{\nzhuang2022surrogate,\ntitle={Surrogate Gap Minimization Improves Sharpness-Aware Training},\nauthor={Juntang Zhuang and Boqing Gong and Liangzhe Yuan and Yin Cui and Hartwig Adam and Nicha C Dvornek and sekhar tatikonda and James s Duncan and Ting Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=edONMAnhLu-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "MQ2sAGunyBP", "original": "TLO_jMxkNot", "number": 388, "cdate": 1632875448991, "mdate": null, "ddate": null, "tcdate": 1632875448991, "tmdate": 1697934936291, "tddate": null, "forum": "MQ2sAGunyBP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "R4D: Utilizing Reference Objects for Long-Range Distance Estimation", "authorids": ["~Yingwei_Li4", "yuhanc@waymo.com", "~Maya_Kabkab1", "~Ruichi_Yu2", "~Longlong_Jing1", "~Yurong_You1", "~Hang_Zhao1"], "authors": ["Yingwei Li", "Tiffany Chen", "Maya Kabkab", "Ruichi Yu", "Longlong Jing", "Yurong You", "Hang Zhao"], "keywords": ["Self-driving", "distance estimation", "long-range objects"], "abstract": "Estimating the distance of objects is a safety-critical task for autonomous driving. Focusing on short-range objects, existing methods and datasets neglect the equally important long-range objects. In this paper, we introduce a challenging and under-explored task, which we refer to as Long-Range Distance Estimation, as well as two datasets to validate new methods developed for this task. We then proposeR4D, the first framework to accurately estimate the distance of long-range objects by using references with known distances in the scene. Drawing inspiration from human perception, R4D builds a graph by connecting a target object to all references. An edge in the graph encodes the relative distance information between a pair of target and reference objects. An attention module is then used to weigh the importance of reference objects and combine them into one target object distance prediction. Experiments on the two proposed datasets demonstrate the effectiveness and robustness of R4D by showing significant improvements compared to existing baselines. We\u2019re looking to make the proposed dataset, Waymo OpenDataset - Long-Range Labels, available publicly at waymo.com/open/download.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|r4d_utilizing_reference_objects_for_longrange_distance_estimation", "pdf": "/pdf/1f644c94f6909e0b3a0ceb6f97174f34a0130d65.pdf", "supplementary_material": "/attachment/0d9bd59a6464cc2bb95916bf42279df004dea28a.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2206.04831/code)", "_bibtex": "@inproceedings{\nli2022rd,\ntitle={R4D: Utilizing Reference Objects for Long-Range Distance Estimation},\nauthor={Yingwei Li and Tiffany Chen and Maya Kabkab and Ruichi Yu and Longlong Jing and Yurong You and Hang Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MQ2sAGunyBP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "YevsQ05DEN7", "original": "x6akXaqpkJP", "number": 387, "cdate": 1632875448913, "mdate": null, "ddate": null, "tcdate": 1632875448913, "tmdate": 1676330674014, "tddate": null, "forum": "YevsQ05DEN7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning", "authorids": ["~Li_Jing1", "~Pascal_Vincent1", "~Yann_LeCun1", "~Yuandong_Tian1"], "authors": ["Li Jing", "Pascal Vincent", "Yann LeCun", "Yuandong Tian"], "keywords": ["self-supervised learning", "contrastive learning", "implicit regularization", "dimensional collapse"], "abstract": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory,  we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on a trainable projector. Experiments show that DirectCLR  outperforms SimCLR with a trainable linear projector on ImageNet. ", "pdf": "/pdf/29c47b5da8f299ffc2b636c45a4854bc2a62e102.pdf", "one-sentence_summary": "We observe and theoretically explain the dimensional collapse in contrastive self-supervised learning. We also propose a novel SSL method that does not rely on a projector.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jing|understanding_dimensional_collapse_in_contrastive_selfsupervised_learning", "supplementary_material": "/attachment/c38a47fdd124927be812b8c10aabaf900a38363b.zip", "_bibtex": "@inproceedings{\njing2022understanding,\ntitle={Understanding Dimensional Collapse in Contrastive Self-supervised Learning},\nauthor={Li Jing and Pascal Vincent and Yann LeCun and Yuandong Tian},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YevsQ05DEN7}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "d71n4ftoCBy", "original": "SlL9HcUwxy1", "number": 379, "cdate": 1632875448477, "mdate": null, "ddate": null, "tcdate": 1632875448477, "tmdate": 1676330674281, "tddate": null, "forum": "d71n4ftoCBy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "FedPara: Low-rank Hadamard Product for Communication-Efficient Federated Learning", "authorids": ["~Nam_Hyeon-Woo1", "~Moon_Ye-Bin1", "~Tae-Hyun_Oh3"], "authors": ["Nam Hyeon-Woo", "Moon Ye-Bin", "Tae-Hyun Oh"], "keywords": ["Federated learning", "Parameterization", "Communication efficiency"], "abstract": "In this work, we propose a communication-efficient parameterization, $\\texttt{FedPara}$, for federated learning (FL) to overcome the burdens on frequent model uploads and downloads. Our method re-parameterizes weight parameters of layers using low-rank weights followed by the Hadamard product. Compared to the conventional low-rank parameterization, our $\\texttt{FedPara}$ method is not restricted to low-rank constraints, and thereby it has a far larger capacity. This property enables to achieve comparable performance while requiring 3 to 10 times lower communication costs than the model with the original layers, which is not achievable by the traditional low-rank methods. The efficiency of our method can be further improved by combining with other efficient FL optimizers. In addition, we extend our method to a personalized FL application, $\\texttt{pFedPara}$, which separates parameters into global and local ones. We show that $\\texttt{pFedPara}$ outperforms competing personalized FL methods with more than three times fewer parameters.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hyeonwoo|fedpara_lowrank_hadamard_product_for_communicationefficient_federated_learning", "pdf": "/pdf/4dc185d690323acd6419fa90d304ed213b53a7e6.pdf", "one-sentence_summary": "New communication-efficient neural network parameterization for federated learning.", "data": "", "_bibtex": "@inproceedings{\nhyeon-woo2022fedpara,\ntitle={FedPara: Low-rank Hadamard Product for Communication-Efficient Federated Learning},\nauthor={Nam Hyeon-Woo and Moon Ye-Bin and Tae-Hyun Oh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=d71n4ftoCBy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "T__V3uLix7V", "original": "CVxEWUZADEm", "number": 375, "cdate": 1632875448248, "mdate": null, "ddate": null, "tcdate": 1632875448248, "tmdate": 1697934937509, "tddate": null, "forum": "T__V3uLix7V", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "RegionViT: Regional-to-Local Attention for Vision Transformers", "authorids": ["~Chun-Fu_Chen1", "~Rameswar_Panda1", "~Quanfu_Fan1"], "authors": ["Chun-Fu Chen", "Rameswar Panda", "Quanfu Fan"], "keywords": ["vision transformer", "image recognition", "multi-scale feature"], "abstract": "Vision transformer (ViT) has recently shown its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a new architecture that adopts the pyramid structure and employ novel regional-to-local attention rather than global self-attention in vision transformers. More specifically, our model first generates regional tokens and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on the spatial location. The regional-to-local attention includes two steps: first, the regional self-attention extracts global information among all regional tokens and then the local self-attention exchanges the information among one regional token and the associated local tokens via self-attention. Therefore, even though local self-attention confines the scope in a local region but it can still receive global information.\nExtensive experiments on four vision tasks, including image classification, object and keypoint detection, semantics segmentation and action recognition, show that our approach outperforms or is on par with state-of-the-art ViT variants including many concurrent works. Our source codes and models are available at \\url{https://github.com/IBM/RegionViT}.", "one-sentence_summary": "A new architecture for vision transformer", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|regionvit_regionaltolocal_attention_for_vision_transformers", "pdf": "/pdf/a70d9722581424639326f684a4184d8a5af3dcb3.pdf", "supplementary_material": "/attachment/af4a70e3fc07b53c39dea613593d6634e7c9c517.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.02689/code)", "_bibtex": "@inproceedings{\nchen2022regionvit,\ntitle={RegionViT: Regional-to-Local Attention for Vision Transformers},\nauthor={Chun-Fu Chen and Rameswar Panda and Quanfu Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=T__V3uLix7V}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "fR-EnKWL_Zb", "original": "2SFN_B0rpyE", "number": 372, "cdate": 1632875448020, "mdate": null, "ddate": null, "tcdate": 1632875448020, "tmdate": 1697934937515, "tddate": null, "forum": "fR-EnKWL_Zb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Quadtree Attention for Vision Transformers", "authorids": ["~Shitao_Tang1", "~Jiahui_Zhang3", "~Siyu_Zhu1", "~Ping_Tan2"], "authors": ["Shitao Tang", "Jiahui Zhang", "Siyu Zhu", "Ping Tan"], "keywords": ["Vision Transformer", "Efficient Transformer", "Feature matching", "Stereo", "image classification", "detection", "3D Vision"], "abstract": "Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the computational complexity from quadratic to linear. Our quadtree transformer builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches. We demonstrate that quadtree attention achieves state-of-the-art performance in various vision tasks, e.g. with 4.0% improvement in feature matching on ScanNet, about 50% flops reduction in stereo matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification, 1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on semantic segmentation over previous state-of-the-art transformers. The codes are available at https://github.com/Tangshitao/QuadtreeAttention.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tang|quadtree_attention_for_vision_transformers", "pdf": "/pdf/b7f143e3e44e4a03ec6c8a7bb34ff7bf86d510f6.pdf", "one-sentence_summary": "Reduce the computation of transformer by coarse-to-fine attention computation for feature matching/stereo/classification/detection tasks.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.02767/code)", "_bibtex": "@inproceedings{\ntang2022quadtree,\ntitle={Quadtree Attention for Vision Transformers},\nauthor={Shitao Tang and Jiahui Zhang and Siyu Zhu and Ping Tan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fR-EnKWL_Zb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "jaLDP8Hp_gc", "original": "kuA_AiLpz9", "number": 370, "cdate": 1632875447878, "mdate": null, "ddate": null, "tcdate": 1632875447878, "tmdate": 1676330674584, "tddate": null, "forum": "jaLDP8Hp_gc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Visual Correspondence Hallucination", "authorids": ["~Hugo_Germain1", "~Vincent_Lepetit1", "~Guillaume_Bourmaud1"], "authors": ["Hugo Germain", "Vincent Lepetit", "Guillaume Bourmaud"], "keywords": ["visual correspondence hallucination", "camera pose estimation"], "abstract": "Given a pair of partially overlapping source and target images and a keypoint in the source image, the keypoint's correspondent in the target image can be either visible, occluded or outside the field of view. Local feature matching methods are only able to identify the correspondent's location when it is visible, while humans can also hallucinate its location when it is occluded or outside the field of view through geometric reasoning.  In this paper, we bridge this gap by training a network to output a peaked probability distribution over the correspondent's location, regardless of this correspondent being visible, occluded, or outside the field of view.  We experimentally demonstrate that this network is indeed able to hallucinate correspondences on pairs of images captured in scenes that were not seen at training-time.  We also apply this network to an absolute camera pose estimation problem and find it is significantly more robust than state-of-the-art local feature matching-based competitors.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "germain|visual_correspondence_hallucination", "pdf": "/pdf/6374bf841920e8781ce8a4dbbf3877649df2ae13.pdf", "one-sentence_summary": "We learn to hallucinate visual correspondences.", "supplementary_material": "/attachment/35d511b626b0b65a11b283affbbeaae3edaa0ef7.zip", "data": "", "code": "", "_bibtex": "@inproceedings{\ngermain2022visual,\ntitle={Visual Correspondence Hallucination},\nauthor={Hugo Germain and Vincent Lepetit and Guillaume Bourmaud},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=jaLDP8Hp_gc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "mk0HzdqY7i1", "original": "4Ge5rAVuUAY", "number": 368, "cdate": 1632875447727, "mdate": null, "ddate": null, "tcdate": 1632875447727, "tmdate": 1676330674827, "tddate": null, "forum": "mk0HzdqY7i1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "What\u2019s Wrong with Deep Learning in Tree Search for Combinatorial Optimization", "authorids": ["~Maximilian_B\u00f6ther2", "~Otto_Ki\u00dfig1", "~Martin_Taraz1", "~Sarel_Cohen1", "~Karen_Seidel1", "~Tobias_Friedrich1"], "authors": ["Maximilian B\u00f6ther", "Otto Ki\u00dfig", "Martin Taraz", "Sarel Cohen", "Karen Seidel", "Tobias Friedrich"], "keywords": ["deep learning", "combinatorial optimization", "maximum independent set"], "abstract": "Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks (GNNs), the deep learning community has been developing solvers that derive solutions to NP-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make three contributions. First, we present an open-source benchmark suite for the NP-hard Maximum Independent Set problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. Second, using our benchmark suite, we conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs. By re-implementing their algorithm with a focus on code quality and extensibility, we show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Instead, the tree search relies on algorithmic techniques like graph kernelization to find good solutions. Thus, the results from the original publication are not reproducible. Third, we extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, we analyze a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality.", "one-sentence_summary": "Using our open-source Maximum Independent Set benchmarking suite, we show that in tree search for combinatorial optimization, the GNN can be replaced by random values without performance decrease.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "b\u00f6ther|whats_wrong_with_deep_learning_in_tree_search_for_combinatorial_optimization", "pdf": "/pdf/e5f8071b2a7b4ea5b72a544d5351737d28998d92.pdf", "supplementary_material": "/attachment/954db6d49975c87f0bfc6818f7e3a797c02a0cb0.zip", "data": "", "_bibtex": "@inproceedings{\nb{\\\"o}ther2022whats,\ntitle={What{\\textquoteright}s Wrong with Deep Learning in Tree Search for Combinatorial Optimization},\nauthor={Maximilian B{\\\"o}ther and Otto Ki{\\ss}ig and Martin Taraz and Sarel Cohen and Karen Seidel and Tobias Friedrich},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mk0HzdqY7i1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "T4-65DNlDij", "original": "p3fD6nlfUY1", "number": 367, "cdate": 1632875447654, "mdate": null, "ddate": null, "tcdate": 1632875447654, "tmdate": 1676330674973, "tddate": null, "forum": "T4-65DNlDij", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Deep Attentive Variational Inference", "authorids": ["~Ifigeneia_Apostolopoulou1", "~Ian_Char1", "~Elan_Rosenfeld1", "~Artur_Dubrawski2"], "authors": ["Ifigeneia Apostolopoulou", "Ian Char", "Elan Rosenfeld", "Artur Dubrawski"], "keywords": ["variational inference", "approximate inference", "deep probabilistic models", "deep probabilistic learning", "variational autoencoder", "probabilistic methods for deep learning", "attention"], "abstract": "Stochastic Variational Inference is a powerful framework for learning large-scale probabilistic latent variable models. However, typical assumptions on the factorization or independence  of the latent variables can substantially restrict its capacity for inference and generative modeling. A major line of active research aims at building more expressive variational models by designing deep hierarchies of interdependent latent variables. Although these models exhibit superior performance and enable richer latent representations, we show that they incur diminishing returns: adding more stochastic layers to an already very deep model yields small predictive improvement while substantially increasing the inference and training time. Moreover, the architecture for this class of models favors local interactions among the latent variables between neighboring layers when designing the conditioning factors of the involved distributions. This is the first work that proposes attention mechanisms to build more expressive variational distributions in deep probabilistic models by explicitly modeling both local and global interactions in the latent space. Specifically, we propose deep attentive variational autoencoder and test it on a variety of established datasets. We show it achieves state-of-the-art log-likelihoods while using fewer latent layers and requiring less  training time than existing models. The proposed non-local inference reduces computational footprint by alleviating the need for deep hierarchies. Project code:\nhttps://github.com/ifiaposto/Deep_Attentive_VI", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "apostolopoulou|deep_attentive_variational_inference", "pdf": "/pdf/e146a17aaa803dbcfae93f2869963af182eb5007.pdf", "supplementary_material": "/attachment/bfd63b63242e785e2bd4ad29b7e58dcd5d6e6ef1.zip", "data": "", "_bibtex": "@inproceedings{\napostolopoulou2022deep,\ntitle={Deep Attentive Variational Inference},\nauthor={Ifigeneia Apostolopoulou and Ian Char and Elan Rosenfeld and Artur Dubrawski},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=T4-65DNlDij}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "CVfLvQq9gLo", "original": "s6QqDcNAkE9", "number": 366, "cdate": 1632875447581, "mdate": null, "ddate": null, "tcdate": 1632875447581, "tmdate": 1676330674978, "tddate": null, "forum": "CVfLvQq9gLo", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity", "authorids": ["ginger.delmas@naverlabs.com", "~Rafael_S._Rezende1", "~Gabriela_Csurka2", "~Diane_Larlus1"], "authors": ["Ginger Delmas", "Rafael S. Rezende", "Gabriela Csurka", "Diane Larlus"], "keywords": [], "abstract": "An intuitive way to search for images is to use queries composed of an example image and a complementary text. While the first provides rich and implicit context for the search, the latter explicitly calls for new traits, or specifies how some elements of the example image should be changed to retrieve the desired target image. Current approaches typically combine the features of each of the two elements of the query into a single representation, which can then be compared to the ones of the potential target images. Our work aims at shedding new light on the task by looking at it through the prism of two familiar and related frameworks: text-to-image and image-to-image retrieval. Taking inspiration from them, we exploit the specific relation of each query element with the targeted image and derive light-weight attention mechanisms which enable to mediate between the two complementary modalities. We validate our approach on several retrieval benchmarks, querying with images and their associated free-form text modifiers. Our method obtains state-of-the-art results without resorting to side information, multi-level features, heavy pre-training nor large architectures as in previous works.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "delmas|artemis_attentionbased_retrieval_with_textexplicit_matching_and_implicit_similarity", "pdf": "/pdf/7417abca3de0104a752284ee1cbdc467dd175c20.pdf", "data": "", "_bibtex": "@inproceedings{\ndelmas2022artemis,\ntitle={{ARTEMIS}: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity},\nauthor={Ginger Delmas and Rafael S. Rezende and Gabriela Csurka and Diane Larlus},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CVfLvQq9gLo}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "C_vsGwEIjAr", "original": "CBmXV3rPaCQ", "number": 357, "cdate": 1632875446911, "mdate": null, "ddate": null, "tcdate": 1632875446911, "tmdate": 1676330675461, "tddate": null, "forum": "C_vsGwEIjAr", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Trivial or Impossible --- dichotomous data difficulty masks model differences (on ImageNet and beyond)", "authorids": ["~Kristof_Meding1", "~Luca_M._Schulze_Buschoff1", "~Robert_Geirhos1", "~Felix_A._Wichmann1"], "authors": ["Kristof Meding", "Luca M. Schulze Buschoff", "Robert Geirhos", "Felix A. Wichmann"], "keywords": ["CNNs", "Cognitive Science", "Vision Science", "Psychophysics", "Neuroscience", "Visual perception", "Inductive bias", "ImageNet", "CIFAR", "RSA", "Representation similarity analysis", "Error consistency", "Datasets"], "abstract": "\"The power of a generalization system follows directly from its biases\" (Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems---but to what degree have we understood how their inductive bias influences model decisions? We here attempt to disentangle the various aspects that determine how a model decides. In particular, we ask: what makes one model decide differently from another? In a meticulously controlled setting, we find that (1.) irrespective of the network architecture or objective (e.g. self-supervised, semi-supervised, vision transformers, recurrent models) all models end up with a similar decision boundary. (2.) To understand these findings, we analysed model decisions on the ImageNet validation set from epoch to epoch and image by image. We find that the ImageNet validation set, among others, suffers from dichotomous data difficulty (DDD): For the range of investigated models and their accuracies, it is dominated by 46.0% \"trivial\" and 11.5% \"impossible\" images (beyond label errors). Only 42.5%  of the images could possibly be responsible for the differences between two models' decision boundaries. (3.) Only removing the \"impossible\" and \"trivial\" images allows us to see pronounced differences between models. (4.) Humans are highly accurate at predicting which images are \"trivial\" and \"impossible\" for CNNs (81.4%). This implies that in future comparisons of brains, machines and behaviour, much may be gained from investigating the decisive role of images and the distribution of their difficulties.", "one-sentence_summary": "All CNNs make similar decisions since data difficulty is dichotomous. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "meding|trivial_or_impossible_dichotomous_data_difficulty_masks_model_differences_on_imagenet_and_beyond", "pdf": "/pdf/d76062ba60abbde95bd2a493d0d7749efdfd41c0.pdf", "supplementary_material": "/attachment/644316362c11915e9e9592575bd35d82d8635092.zip", "data": "", "_bibtex": "@inproceedings{\nmeding2022trivial,\ntitle={Trivial or Impossible --- dichotomous data difficulty masks model differences (on ImageNet and beyond)},\nauthor={Kristof Meding and Luca M. Schulze Buschoff and Robert Geirhos and Felix A. Wichmann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=C_vsGwEIjAr}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 30}}, {"id": "u6s8dSporO8", "original": "T0x9S80R00R", "number": 353, "cdate": 1632875446615, "mdate": null, "ddate": null, "tcdate": 1632875446615, "tmdate": 1697934939992, "tddate": null, "forum": "u6s8dSporO8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Group equivariant neural posterior estimation", "authorids": ["~Maximilian_Dax1", "~Stephen_R_Green1", "~Jonathan_Gair1", "~Michael_Deistler1", "~Bernhard_Sch\u00f6lkopf1", "~Jakob_H._Macke1"], "authors": ["Maximilian Dax", "Stephen R Green", "Jonathan Gair", "Michael Deistler", "Bernhard Sch\u00f6lkopf", "Jakob H. Macke"], "keywords": ["simulation-based inference", "likelihood-free inference", "machine learning for science", "equivariances", "group transformations"], "abstract": "Simulation-based inference with conditional neural density estimators is a powerful approach to solving inverse problems in science. However, these methods typically treat the underlying forward model as a black box, with no way to exploit geometric properties such as equivariances. Equivariances are common in scientific models, however integrating them directly into expressive inference networks (such as normalizing flows) is not straightforward. We here describe an alternative method to incorporate equivariances under joint transformations of parameters and data. Our method---called group equivariant neural posterior estimation (GNPE)---is based on self-consistently standardizing the \"pose\" of the data while estimating the posterior over parameters. It is architecture-independent, and applies both to exact and approximate equivariances. As a real-world application, we use GNPE for amortized inference of astrophysical binary black hole systems from gravitational-wave observations. We show that GNPE achieves state-of-the-art accuracy while reducing inference times by three orders of magnitude.", "one-sentence_summary": "We describe a method to incorporate group equivariances into neural posterior estimation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dax|group_equivariant_neural_posterior_estimation", "pdf": "/pdf/57dd5a689e8ee692e40ff4ca7c15b889e41e29c2.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.13139/code)", "_bibtex": "@inproceedings{\ndax2022group,\ntitle={Group equivariant neural posterior estimation},\nauthor={Maximilian Dax and Stephen R Green and Jonathan Gair and Michael Deistler and Bernhard Sch{\\\"o}lkopf and Jakob H. Macke},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=u6s8dSporO8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 35}}, {"id": "-AOEi-5VTU8", "original": "LlSo7eolG-n", "number": 352, "cdate": 1632875446540, "mdate": null, "ddate": null, "tcdate": 1632875446540, "tmdate": 1697934939989, "tddate": null, "forum": "-AOEi-5VTU8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fast Differentiable Matrix Square Root", "authorids": ["~Yue_Song1", "~Nicu_Sebe1", "~Wei_Wang43"], "authors": ["Yue Song", "Nicu Sebe", "Wei Wang"], "keywords": ["Differentiabl Matrix Square Root", "Differentiable Matrix Decomposition", "Vision Transformers"], "abstract": "Computing the matrix square root or its inverse in a differentiable manner is important in a variety of computer vision tasks. Previous methods either adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix or use the Newton-Schulz iteration (NS iteration) to derive the approximate solution. However, both methods are not computationally efficient enough in either the forward pass or in the backward pass. In this paper, we propose two more efficient variants to compute the differentiable matrix square root. For the forward propagation, one method is to use Matrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'e Approximants (MPA). The backward gradient is computed by iteratively solving the continuous-time Lyapunov equation using the matrix sign function. Both methods yield considerable speed-up compared with the SVD or the Newton-Schulz iteration. Experimental results on the de-correlated batch normalization and second-order vision transformer demonstrate that our methods can also achieve competitive and even slightly better performances. The code is available at \\href{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}.", "pdf": "/pdf/641879a2e12839331fe8fe974fdc1ef6494cbb9c.pdf", "one-sentence_summary": "We develop two fast methods to compute the differentiable matrix square root.", "supplementary_material": "/attachment/fc3bf977f32fbcf2d828f8a8411abeaf42b59d1b.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "song|fast_differentiable_matrix_square_root", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2201.08663/code)", "_bibtex": "@inproceedings{\nsong2022fast,\ntitle={Fast Differentiable Matrix Square Root},\nauthor={Yue Song and Nicu Sebe and Wei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-AOEi-5VTU8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "JXhROKNZzOc", "original": "zRyb9tkTQmL", "number": 351, "cdate": 1632875446464, "mdate": null, "ddate": null, "tcdate": 1632875446464, "tmdate": 1697934940164, "tddate": null, "forum": "JXhROKNZzOc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation", "authorids": ["~Cong_Guo1", "~Yuxian_Qiu1", "~Jingwen_Leng1", "~Xiaotian_Gao1", "~Chen_Zhang11", "~Yunxin_Liu2", "~Fan_Yang28", "~Yuhao_Zhu1", "~Minyi_Guo1"], "authors": ["Cong Guo", "Yuxian Qiu", "Jingwen Leng", "Xiaotian Gao", "Chen Zhang", "Yunxin Liu", "Fan Yang", "Yuhao Zhu", "Minyi Guo"], "keywords": ["Data-Free Quantization", "Hessian Matrix", "Approximation"], "abstract": "Quantization of deep neural networks (DNN) has been proven effective for compressing and accelerating DNN models. Data-free quantization (DFQ) is a promising approach without the original datasets under privacy-sensitive and confidential scenarios. However, current DFQ solutions degrade accuracy, need synthetic data to calibrate networks, and are time-consuming and costly. This paper proposes an on-the-fly DFQ framework with sub-second quantization time, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. With the theoretical analysis of the second-order information of DNN task loss, we decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise, and output channel-wise. Then, we progressively compose sub-items and propose a novel data-free optimization objective in the discrete domain,  minimizing Constrained Absolute Sum of Error (or CASE in short), which surprisingly does not need any dataset and is even not aware of network architecture. We also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver. Finally, without fine-tuning and synthetic datasets, SQuant accelerates the data-free quantization process to a sub-second level with >30% accuracy improvement over the existing data-free post-training quantization works, with the evaluated models under 4-bit quantization. We have open-sourced the SQuant framework at https://github.com/clevercool/SQuant.", "pdf": "/pdf/2bb26dab9a7db75000cffe21e5229acfdca132af.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guo|squant_onthefly_datafree_quantization_via_diagonal_hessian_approximation", "one-sentence_summary": "A fast and accurate data-free quantization framework named SQuant.", "supplementary_material": "/attachment/9246edccc6a08a3022f2eb56d883f8b3641dc453.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.07471/code)", "_bibtex": "@inproceedings{\nguo2022squant,\ntitle={{SQ}uant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation},\nauthor={Cong Guo and Yuxian Qiu and Jingwen Leng and Xiaotian Gao and Chen Zhang and Yunxin Liu and Fan Yang and Yuhao Zhu and Minyi Guo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JXhROKNZzOc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "lyLVzukXi08", "original": "TEhl2GIr5nz", "number": 350, "cdate": 1632875446391, "mdate": null, "ddate": null, "tcdate": 1632875446391, "tmdate": 1676330675968, "tddate": null, "forum": "lyLVzukXi08", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Variational Dropout Processes", "authorids": ["~Insu_Jeon2", "youngjin.create@gmail.com", "~Gunhee_Kim1"], "authors": ["Insu Jeon", "Youngjin Park", "Gunhee Kim"], "keywords": ["Meta Learning", "Few-shot Learning", "Bayesian Neural Networks", "Variatinoal Dropout"], "abstract": "Learning to infer the conditional posterior model is a key step for robust meta-learning. This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional posterior distribution based on a task-specific dropout; a low-rank product of Bernoulli experts meta-model is utilized for a memory-efficient mapping of dropout rates from a few observed contexts. It allows for a quick reconfiguration of a globally learned and shared neural network for new tasks in multi-task few-shot learning. In addition, NVDPs utilize a novel prior conditioned on the whole task data to optimize the conditional dropout posterior in the amortized variational inference. Surprisingly, this enables the robust approximation of task-specific dropout rates that can deal with a wide range of functional ambiguities and uncertainties. We compared the proposed method with other meta-learning approaches in the few-shot learning tasks such as 1D stochastic regression, image inpainting, and classification. The results show the excellent performance of NVDPs.", "one-sentence_summary": "This paper presents a new model-based Bayesian meta-learning approach called NVDPs that combines a novel conditional dropout posterior with a new variational prior for the data-efficient learning and adaptation of deep neural networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jeon|neural_variational_dropout_processes", "pdf": "/pdf/8553b02c69dba637b2b54e4d035a7c5c7277d5ed.pdf", "data": "", "_bibtex": "@inproceedings{\njeon2022neural,\ntitle={Neural Variational Dropout Processes},\nauthor={Insu Jeon and Youngjin Park and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lyLVzukXi08}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "kQ2SOflIOVC", "original": "rPvuCeAc25", "number": 349, "cdate": 1632875446311, "mdate": null, "ddate": null, "tcdate": 1632875446311, "tmdate": 1676330676048, "tddate": null, "forum": "kQ2SOflIOVC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Better Understanding and Better Generalization of Low-shot Classification in Histology Images with Contrastive Learning", "authorids": ["~Jiawei_Yang1", "hanbochen@tencent.com", "~Jiangpeng_Yan1", "xiaoyuchen@stu.xmu.edu.cn", "jianhuayao@tencent.com"], "authors": ["Jiawei Yang", "Hanbo Chen", "Jiangpeng Yan", "Xiaoyu Chen", "Jianhua Yao"], "keywords": ["Few shot learning", "Histology Image", "Knowledge Transferring"], "abstract": "Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures. Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available.", "pdf": "/pdf/e31401d33fda8d1009bf242636e6aa638632482a.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|towards_better_understanding_and_better_generalization_of_lowshot_classification_in_histology_images_with_contrastive_learning", "_bibtex": "@inproceedings{\nyang2022towards,\ntitle={Towards Better Understanding and Better Generalization of Low-shot Classification in Histology Images with Contrastive Learning},\nauthor={Jiawei Yang and Hanbo Chen and Jiangpeng Yan and Xiaoyu Chen and Jianhua Yao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kQ2SOflIOVC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 37}}, {"id": "QjOQkpzKbNk", "original": "PtmUjEl-F0c", "number": 348, "cdate": 1632875446239, "mdate": null, "ddate": null, "tcdate": 1632875446239, "tmdate": 1676330676250, "tddate": null, "forum": "QjOQkpzKbNk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Distilling GANs with Style-Mixed Triplets for X2I Translation with Limited Data", "authorids": ["~Yaxing_Wang2", "~Joost_van_de_weijer3", "~Lu_Yu3", "~SHANGLING_JUI1"], "authors": ["Yaxing Wang", "Joost van de weijer", "Lu Yu", "SHANGLING JUI"], "keywords": ["Transfer learning", "image synthesis", "limited data."], "abstract": "Conditional image synthesis is an integral part of many X2I translation systems, including image-to-image, text-to-image and audio-to-image translation systems. Training these large systems generally requires huge amounts of training data. \nTherefore, we investigate knowledge distillation to transfer knowledge from a high-quality unconditioned generative model (e.g., StyleGAN) to a conditioned synthetic image generation modules in a variety of systems. To initialize the conditional and reference branch (from a unconditional GAN)  we exploit the style mixing characteristics of high-quality GANs to generate an infinite supply of style-mixed triplets to perform the knowledge distillation. Extensive experimental results in a number of image generation tasks (i.e., image-to-image, semantic segmentation-to-image, text-to-image and audio-to-image) demonstrate qualitatively and quantitatively that our method successfully transfers knowledge to the synthetic image generation modules, resulting in more realistic images than previous methods as confirmed by a significant drop in the FID. ", "one-sentence_summary": "One transfer learning method generalizes varying kinds of  conditional image synthesization tasks. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|distilling_gans_with_stylemixed_triplets_for_x2i_translation_with_limited_data", "pdf": "/pdf/5a2c69f76a578d6db9205be58713ad872914604e.pdf", "supplementary_material": "/attachment/551707bfa58546c14c9e6c0d13a79cfbb6c6cff6.zip", "_bibtex": "@inproceedings{\nwang2022distilling,\ntitle={Distilling {GAN}s with Style-Mixed Triplets for X2I Translation with Limited Data},\nauthor={Yaxing Wang and Joost van de weijer and Lu Yu and SHANGLING JUI},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=QjOQkpzKbNk}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "FQOC5u-1egI", "original": "bajTd5JSa14", "number": 346, "cdate": 1632875446093, "mdate": null, "ddate": null, "tcdate": 1632875446093, "tmdate": 1697934941250, "tddate": null, "forum": "FQOC5u-1egI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Handling Distribution Shifts on Graphs: An Invariance Perspective", "authorids": ["~Qitian_Wu1", "~Hengrui_Zhang1", "~Junchi_Yan2", "~David_Wipf1"], "authors": ["Qitian Wu", "Hengrui Zhang", "Junchi Yan", "David Wipf"], "keywords": ["Representation Learning on Graphs", "Out-of-Distribution Generalization", "Domain Shift", "Graph Structure Learning", "Invariant Models"], "abstract": "There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new invariant learning approach, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates graph neural networks to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|handling_distribution_shifts_on_graphs_an_invariance_perspective", "pdf": "/pdf/f55776307b1b806dc62cd1e642289d70df555fd2.pdf", "one-sentence_summary": "We formulate out-of-distribution generalization problem for node-level prediction on graphs and propose a new learning approach based on invariant models", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2202.02466/code)", "_bibtex": "@inproceedings{\nwu2022handling,\ntitle={Handling Distribution Shifts on Graphs: An Invariance Perspective},\nauthor={Qitian Wu and Hengrui Zhang and Junchi Yan and David Wipf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FQOC5u-1egI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "hSktDu-h94", "original": "ck_1N52rsEZ", "number": 345, "cdate": 1632875446018, "mdate": null, "ddate": null, "tcdate": 1632875446018, "tmdate": 1676330676516, "tddate": null, "forum": "hSktDu-h94", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Automatic Loss Function Search for Predict-Then-Optimize Problems with Strong Ranking Property", "authorids": ["wang.13930@buckeyemail.osu.edu", "j.yi8@lse.ac.uk", "~Hang_Dong3", "~Bo_Qiao1", "~Chuan_Luo1", "~Qingwei_Lin1"], "authors": ["Boshi Wang", "Jialin Yi", "Hang Dong", "Bo Qiao", "Chuan Luo", "Qingwei Lin"], "keywords": [], "abstract": "Combinatorial optimization problems with parameters to be predicted from side information are commonly seen in a variety of problems during the paradigm shift from reactive decision making to proactive decision making. Due to the misalignment between the continuous prediction results and the discrete decisions in optimization problems, it is hard to achieve a satisfactory prediction result with the ordinary $l_2$ loss in the prediction phase. To properly connect the prediction loss with the optimization goal, in this paper we propose a total group preorder (TGP) loss and its differential version called approximated total group preorder (ATGP) loss for predict-then-optimize (PTO) problems with strong ranking property. These new losses are provably more robust than the usual $l_2$ loss in a linear regression setting and have great potential to extend to other settings. We also propose an automatic searching algorithm that adapts the ATGP loss to PTO problems with different combinatorial structures. Extensive experiments on the ranking problem, the knapsack problem, and the shortest path problem have demonstrated that our proposed method can achieve a significant performance compared to the other methods designed for PTO problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|automatic_loss_function_search_for_predictthenoptimize_problems_with_strong_ranking_property", "pdf": "/pdf/98fea8b922621c46bbee3b4b58290769005b0897.pdf", "_bibtex": "@inproceedings{\nwang2022automatic,\ntitle={Automatic Loss Function Search for Predict-Then-Optimize Problems with Strong Ranking Property},\nauthor={Boshi Wang and Jialin Yi and Hang Dong and Bo Qiao and Chuan Luo and Qingwei Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hSktDu-h94}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "YigKlMJwjye", "original": "3x3g-P5qgq9", "number": 334, "cdate": 1632875445338, "mdate": null, "ddate": null, "tcdate": 1632875445338, "tmdate": 1676330676823, "tddate": null, "forum": "YigKlMJwjye", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalized Demographic Parity for Group Fairness", "authorids": ["~Zhimeng_Jiang1", "~Xiaotian_Han1", "~Chao_Fan2", "~Fan_Yang27", "~Ali_Mostafavi2", "~Xia_Hu4"], "authors": ["Zhimeng Jiang", "Xiaotian Han", "Chao Fan", "Fan Yang", "Ali Mostafavi", "Xia Hu"], "keywords": ["Generalized demographic parity", "estimation error analysis"], "abstract": "This work aims to generalize demographic parity to continuous sensitive attributes while preserving tractable computation. Current fairness metrics for continuous sensitive attributes largely rely on intractable statistical independence between variables, such as Hirschfeld-Gebelein-Renyi (HGR) and mutual information. Statistical fairness metrics estimation relying on either tractable bounds or neural network approximation, however, are not sufficiently trustful to rank algorithms prediction bias due to lack of estimation accuracy guarantee. \nTo make fairness metrics trustable, we propose \\textit{\\underline{G}eneralized \\underline{D}emographic \\underline{P}arity} (GDP), a group fairness metric for continuous and discrete attributes. We show the understanding of GDP from the probability perspective and theoretically reveal the connection between GDP regularizer and adversarial debiasing. To estimate GDP, we adopt hard and soft group strategies via the one-hot or the soft group indicator, representing the membership of each sample in different groups of the sensitive attribute. We provably and numerically show that the soft group strategy achieves a faster estimation error convergence rate. Experiments show the better bias mitigation performance of GDP regularizer, compared with adversarial debiasing, for regression and classification tasks in tabular and graph benchmarks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|generalized_demographic_parity_for_group_fairness", "pdf": "/pdf/3f9ffe7eafbd44f0205f3629edbcfb60ec738e7c.pdf", "one-sentence_summary": "This work aims to generalize demographic parity to continuous sensitive attributes while preserving tractable computation.", "supplementary_material": "/attachment/842b40ecf91b9a8b70942e78295167d95058bbec.zip", "_bibtex": "@inproceedings{\njiang2022generalized,\ntitle={Generalized Demographic Parity for Group Fairness},\nauthor={Zhimeng Jiang and Xiaotian Han and Chao Fan and Fan Yang and Ali Mostafavi and Xia Hu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YigKlMJwjye}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "ljxWpdBl4V", "original": "uhjNBjFN-pO", "number": 333, "cdate": 1632875445264, "mdate": null, "ddate": null, "tcdate": 1632875445264, "tmdate": 1676330676820, "tddate": null, "forum": "ljxWpdBl4V", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Closed-form Sample Probing for Learning Generative Models in Zero-shot Learning", "authorids": ["~Samet_Cetin1", "~Orhun_Bu\u011fra_Baran1", "~Ramazan_Gokberk_Cinbis1"], "authors": ["Samet Cetin", "Orhun Bu\u011fra Baran", "Ramazan Gokberk Cinbis"], "keywords": ["zero-shot learning", "generative zero-shot learning", "generative models"], "abstract": "Generative model based approaches have led to significant advances in zero-shot learning (ZSL) over the past few years. These approaches typically aim to learn a conditional generator that synthesizes training samples of classes conditioned on class definitions. The final zero-shot learning model is then obtained by training a supervised classification model over the real and/or synthesized training samples of seen and unseen classes, combined. Therefore, naturally, the generative model needs to produce not only relevant samples, but also those that are sufficiently rich for classifier training purposes, which is handled by various heuristics in existing works. In this paper, we introduce a principled approach for training generative models {\\em directly} for training data generation purposes. Our main observation is that the use of closed-form models opens doors to end-to-end training thanks to the differentiability of the solvers. In our approach, at each generative model update step, we fit a task-specific closed-form ZSL model from generated samples, and measure its loss on novel samples all within the compute graph, a procedure that we refer to as {\\em sample probing}. In this manner, the generator receives feedback directly based on the value of its samples for model training purposes. Our experimental results show that the proposed sample probing approach improves the ZSL results even when integrated into state-of-the-art generative models.\n", "one-sentence_summary": "We show how to train a conditional generative model in a way that directly maximizes the value of its samples for zero-shot model training purposes.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cetin|closedform_sample_probing_for_learning_generative_models_in_zeroshot_learning", "pdf": "/pdf/991863550ac668773c180067c71b28b180a005d0.pdf", "supplementary_material": "/attachment/fe526ff8e2dc83ef303fadc220bed5fbcdc8a592.zip", "data": "", "_bibtex": "@inproceedings{\ncetin2022closedform,\ntitle={Closed-form Sample Probing for Learning Generative Models in Zero-shot Learning},\nauthor={Samet Cetin and Orhun Bu{\\u{g}}ra Baran and Ramazan Gokberk Cinbis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ljxWpdBl4V}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "J_F_qqCE3Z5", "original": "GgkC_WhGGPW", "number": 332, "cdate": 1632875445192, "mdate": null, "ddate": null, "tcdate": 1632875445192, "tmdate": 1676330676821, "tddate": null, "forum": "J_F_qqCE3Z5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DKM: Differentiable k-Means Clustering Layer for Neural Network Compression", "authorids": ["~Minsik_Cho1", "~Keivan_Alizadeh-Vahid1", "sadya@apple.com", "~Mohammad_Rastegari2"], "authors": ["Minsik Cho", "Keivan Alizadeh-Vahid", "Saurabh Adya", "Mohammad Rastegari"], "keywords": ["Deep learning", "neural network", "compression"], "abstract": "Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering-based DNN model compression. DKM casts k-means clustering as an attention problem and enables joint optimization of the DNN parameters and clustering centroids. Unlike prior works that rely on additional regularizers and parameters, DKM-based compression keeps the original loss function and model architecture fixed. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results demonstrate that DKM delivers superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB model size (29.4x model compression factor). For MobileNet-v1, which is a challenging DNN to compress, DKM delivers 63.9% top-1 ImageNet1k accuracy with 0.72 MB model size (22.4x model compression factor). This result is 6.8% higher top-1accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Additionally, DKM enables compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks.", "pdf": "/pdf/32e93d22db59833cad75b3a81ec8d15aa537a574.pdf", "supplementary_material": "", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cho|dkm_differentiable_kmeans_clustering_layer_for_neural_network_compression", "one-sentence_summary": "We propose a novel model compression scheme based on differentiable K-means layer, and it delivers the state-of-the-art results.", "data": "", "_bibtex": "@inproceedings{\ncho2022dkm,\ntitle={{DKM}: Differentiable k-Means Clustering Layer for Neural Network Compression},\nauthor={Minsik Cho and Keivan Alizadeh-Vahid and Saurabh Adya and Mohammad Rastegari},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=J_F_qqCE3Z5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "hcMvApxGSzZ", "original": "Uhq8A1p2MsY", "number": 326, "cdate": 1632875444743, "mdate": null, "ddate": null, "tcdate": 1632875444743, "tmdate": 1676330677280, "tddate": null, "forum": "hcMvApxGSzZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fixed Neural Network Steganography: Train the images, not the network", "authorids": ["~Varsha_Kishore1", "~Xiangyu_Chen1", "~Yan_Wang10", "~Boyi_Li1", "~Kilian_Q_Weinberger1"], "authors": ["Varsha Kishore", "Xiangyu Chen", "Yan Wang", "Boyi Li", "Kilian Q Weinberger"], "keywords": [], "abstract": "Recent attempts at image steganography make use of advances in deep learning to train an encoder-decoder network pair to hide and retrieve secret messages in images. These methods are able to hide large amounts of data, but they also incur high decoding error rates (around 20%). In this paper, we propose a novel algorithm for steganography that takes advantage of the fact that neural networks are sensitive to tiny perturbations. Our method, Fixed Neural Network Steganography (FNNS), yields significantly lower error rates when compared to prior state-of-the-art methods and achieves 0% error reliably for hiding up to 3 bits per pixel (bpp) of secret information in images. FNNS also successfully evades existing statistical steganalysis systems and can be modified to evade neural steganalysis systems as well. Recovering every bit correctly, up to 3 bpp, enables novel applications that requires encryption. We introduce one specific use case for facilitating anonymized and safe image sharing.  Our code is available at https://github.com/varshakishore/FNNS.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kishore|fixed_neural_network_steganography_train_the_images_not_the_network", "pdf": "/pdf/c905cc7ba9e62713ac3a302451e12d3a75523a2c.pdf", "one-sentence_summary": "A novel method for steganography based on adversarial perturbations.", "data": "", "_bibtex": "@inproceedings{\nkishore2022fixed,\ntitle={Fixed Neural Network Steganography: Train the images, not the network},\nauthor={Varsha Kishore and Xiangyu Chen and Yan Wang and Boyi Li and Kilian Q Weinberger},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hcMvApxGSzZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "N9W24a4zU", "original": "-BO2oyGJXy", "number": 317, "cdate": 1632875444059, "mdate": null, "ddate": null, "tcdate": 1632875444059, "tmdate": 1697934943241, "tddate": null, "forum": "N9W24a4zU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Steerable Partial Differential Operators for Equivariant Neural Networks", "authorids": ["~Erik_Jenner1", "~Maurice_Weiler1"], "authors": ["Erik Jenner", "Maurice Weiler"], "keywords": ["partial differential operators", "equivariance", "deep learning", "steerability"], "abstract": "Recent work in equivariant deep learning bears strong similarities to physics. Fields over a base space are fundamental entities in both subjects, as are equivariant maps between these fields. In deep learning, however, these maps are usually defined by convolutions with a kernel, whereas they are partial differential operators (PDOs) in physics. Developing the theory of equivariant PDOs in the context of deep learning could bring these subjects even closer together and lead to a stronger flow of ideas. In this work, we derive a $G$-steerability constraint that completely characterizes when a PDO between feature vector fields is equivariant, for arbitrary symmetry groups $G$. We then fully solve this constraint for several important groups. We use our solutions as equivariant drop-in replacements for convolutional layers and benchmark them in that role. Finally, we develop a framework for equivariant maps based on Schwartz distributions that unifies classical convolutions and differential operators and gives insight about the relation between the two.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jenner|steerable_partial_differential_operators_for_equivariant_neural_networks", "pdf": "/pdf/b60b61cfed84b22e5393025d981343e0ed46ddcf.pdf", "one-sentence_summary": "We present a framework for equivariant partial differential operators, generalizing existing approaches and narrowing the gap between PDOs and convolutions.", "supplementary_material": "/attachment/a29e25c4cd8981fdf8b7e773d00d5bd6c7ab7f6a.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2106.10163/code)", "_bibtex": "@inproceedings{\njenner2022steerable,\ntitle={Steerable Partial Differential Operators for Equivariant Neural Networks},\nauthor={Erik Jenner and Maurice Weiler},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=N9W24a4zU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "oVE1z8NlNe", "original": "21PGohKr_6P", "number": 310, "cdate": 1632875443605, "mdate": null, "ddate": null, "tcdate": 1632875443605, "tmdate": 1676330677726, "tddate": null, "forum": "oVE1z8NlNe", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Divergence-aware Federated Self-Supervised Learning", "authorids": ["~Weiming_Zhuang1", "~Yonggang_Wen1", "zhangshuai@sensetime.com"], "authors": ["Weiming Zhuang", "Yonggang Wen", "Shuai Zhang"], "keywords": ["Federated Learning", "Self-supervised Learning", "Unsupervised representation learning"], "abstract": "Self-supervised learning (SSL) is capable of learning remarkable representations from centrally available data. Recent works further implement federated learning with SSL to learn from rapidly growing decentralized unlabeled images (e.g., from cameras and phones), often resulted from privacy constraints. Extensive attention has been paid to SSL approaches based on Siamese networks. However, such an effort has not yet revealed deep insights into various fundamental building blocks for the federated self-supervised learning (FedSSL) architecture. We aim to fill in this gap via in-depth empirical study and propose a new method to tackle the non-independently and identically distributed (non-IID) data problem of decentralized data. Firstly, we introduce a generalized FedSSL framework that embraces existing SSL methods based on Siamese networks and presents flexibility catering to future methods. In this framework, a server coordinates multiple clients to conduct SSL training and periodically updates local models of clients with the aggregated global model. Using the framework, our study uncovers unique insights of FedSSL: 1) stop-gradient operation, previously reported to be essential, is not always necessary in FedSSL; 2) retaining local knowledge of clients in FedSSL is particularly beneficial for non-IID data. Inspired by the insights, we then propose a new approach for model update, Federated Divergence-aware Exponential Moving Average update (FedEMA). FedEMA updates local models of clients adaptively using EMA of the global model, where the decay rate is dynamically measured by model divergence. Extensive experiments demonstrate that FedEMA outperforms existing methods by 3-4% on linear evaluation. We hope that this work will provide useful insights for future research.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhuang|divergenceaware_federated_selfsupervised_learning", "pdf": "/pdf/0f42d73623138cacd0d8e8d263d5d4fcaa2c5a65.pdf", "one-sentence_summary": "We propose a new approach, FedEMA, to address the non-IID data challenge in federated self-supervised learning (FedSSL), inspired by deep insights uncovered from in-depth empirical studies using a newly introduced FedSSL framework.", "data": "", "_bibtex": "@inproceedings{\nzhuang2022divergenceaware,\ntitle={Divergence-aware Federated Self-Supervised Learning},\nauthor={Weiming Zhuang and Yonggang Wen and Shuai Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oVE1z8NlNe}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "0rcbOaoBXbg", "original": "95l8lc_roO", "number": 282, "cdate": 1632875441510, "mdate": null, "ddate": null, "tcdate": 1632875441510, "tmdate": 1697934946418, "tddate": null, "forum": "0rcbOaoBXbg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Spectral Marked Point Processes", "authorids": ["~Shixiang_Zhu1", "~Haoyun_Wang1", "~Zheng_Dong3", "~Xiuyuan_Cheng1", "~Yao_Xie2"], "authors": ["Shixiang Zhu", "Haoyun Wang", "Zheng Dong", "Xiuyuan Cheng", "Yao Xie"], "keywords": [], "abstract": "Self- and mutually-exciting point processes are popular models in machine learning and statistics for dependent discrete event data. To date, most existing models assume stationary kernels (including the classical Hawkes processes) and simple parametric models. Modern applications with complex event data require more general point process models that can incorporate contextual information of the events, called marks, besides the temporal and location information. Moreover, such applications often require non-stationary models to capture more complex spatio-temporal dependence. To tackle these challenges, a key question is to devise a versatile influence kernel in the point process model. In this paper, we introduce a novel and general neural network-based non-stationary influence kernel with high expressiveness for handling complex discrete events data while providing theoretical performance guarantees. We demonstrate the superior performance of our proposed method compared with the state-of-the-art on synthetic and real data.", "pdf": "/pdf/999254e545c44db20626baabcfcd415ccd206c6a.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|neural_spectral_marked_point_processes", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.10773/code)", "_bibtex": "@inproceedings{\nzhu2022neural,\ntitle={Neural Spectral Marked Point Processes},\nauthor={Shixiang Zhu and Haoyun Wang and Xiuyuan Cheng and Yao Xie},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0rcbOaoBXbg}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "Bn09TnDngN", "original": "jTunBvhnKOp", "number": 281, "cdate": 1632875441438, "mdate": null, "ddate": null, "tcdate": 1632875441438, "tmdate": 1676330679081, "tddate": null, "forum": "Bn09TnDngN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data", "authorids": ["~Zhiyuan_Zhang1", "~Lingjuan_Lyu1", "~Weiqiang_Wang4", "~Lichao_Sun1", "~Xu_Sun1"], "authors": ["Zhiyuan Zhang", "Lingjuan Lyu", "Weiqiang Wang", "Lichao Sun", "Xu Sun"], "keywords": ["backdoor learning", "weight perturbation", "consistency"], "abstract": "Since training a large-scale backdoored model from scratch requires a large training dataset, several recent attacks have considered to inject backdoors into a trained clean model without altering model behaviors on the clean data. Previous work finds that backdoors can be injected into a trained clean model with Adversarial Weight Perturbation (AWP), which means the variation of parameters are small in backdoor learning. In this work, we observe an interesting phenomenon that the variations of parameters are always AWPs when tuning the trained clean model to inject backdoors. We further provide theoretical analysis to explain this phenomenon. We are the first to formulate the behavior of maintaining accuracy on clean data as the consistency of backdoored models, which includes both global consistency and instance-wise consistency. We extensively analyze the effects of AWPs on the consistency of backdoored models. In order to achieve better consistency, we propose a novel anchoring loss to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee. ", "pdf": "/pdf/84f1213ba4770c743a5a5d48d699687b230f8c18.pdf", "one-sentence_summary": "We propose a novel logit anchoring approach for better global and instance-wise consistency in backdoor learning.", "supplementary_material": "", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|how_to_inject_backdoors_with_better_consistency_logit_anchoring_on_clean_data", "data": "", "_bibtex": "@inproceedings{\nzhang2022how,\ntitle={How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data},\nauthor={Zhiyuan Zhang and Lingjuan Lyu and Weiqiang Wang and Lichao Sun and Xu Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Bn09TnDngN}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "Lwr8We4MIxn", "original": "eviofpii-Ar", "number": 274, "cdate": 1632875440914, "mdate": null, "ddate": null, "tcdate": 1632875440914, "tmdate": 1676330679469, "tddate": null, "forum": "Lwr8We4MIxn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease ", "authorids": ["~Sayan_Ghosal1", "~Qiang_Chen6", "~Giulio_Pergola1", "~Aaron_L_Goldman1", "~William_Ulrich1", "~Daniel_R_Weinberger1", "~Archana_Venkataraman1"], "authors": ["Sayan Ghosal", "Qiang Chen", "Giulio Pergola", "Aaron L Goldman", "William Ulrich", "Daniel R Weinberger", "Archana Venkataraman"], "keywords": ["Imaging-genetics", "Hierarchical Graph Convolution", "Gene Ontology", "Bayesian Feature Selection", "Schizophrenia"], "abstract": "We propose a novel end-to-end framework for whole-brain and whole-genome imaging-genetics. Our genetics network uses hierarchical graph convolution and pooling operations to embed subject-level data onto a low-dimensional latent space. The hierarchical network implicitly tracks the convergence of genetic risk across well-established biological pathways, while an attention mechanism automatically identifies the salient edges of this network at the subject level. In parallel, our imaging network projects multimodal data onto a set of latent embeddings. For interpretability, we implement a Bayesian feature selection strategy to extract the discriminative imaging biomarkers; these feature weights are optimized alongside the other model parameters. We couple the imaging and genetic embeddings with a predictor network, to ensure that the learned representations are linked to phenotype. We evaluate our framework on a schizophrenia dataset that includes two functional MRI paradigms and gene scores derived from Single Nucleotide Polymorphism data. Using repeated 10-fold cross-validation, we show that our imaging-genetics fusion achieves the better classification performance than state-of-the-art baselines. In an exploratory analysis, we further show that the biomarkers identified by our model are reproducible and closely associated with deficits in schizophrenia. ", "one-sentence_summary": "Biologically Informed Imaging-Genetics", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ghosal|a_biologically_interpretable_graph_convolutional_network_to_link_genetic_risk_pathways_and_imaging_phenotypes_of_disease", "pdf": "/pdf/e88ef5a6ee79b4188e9a4d9cb222409009729992.pdf", "supplementary_material": "/attachment/b4a7a1f16cabb63013ae6ad8bc7b899c840eba27.zip", "_bibtex": "@inproceedings{\nghosal2022a,\ntitle={A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease },\nauthor={Sayan Ghosal and Qiang Chen and Giulio Pergola and Aaron L Goldman and William Ulrich and Daniel R Weinberger and Archana Venkataraman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Lwr8We4MIxn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "ek9a0qIafW", "original": "tbviZL4iC7z", "number": 273, "cdate": 1632875440839, "mdate": null, "ddate": null, "tcdate": 1632875440839, "tmdate": 1676330679580, "tddate": null, "forum": "ek9a0qIafW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners", "authorids": ["~Ningyu_Zhang1", "~Luoqiu_Li1", "~Xiang_Chen5", "~Shumin_Deng1", "~Zhen_Bi1", "~Chuanqi_Tan3", "~Fei_Huang2", "~Huajun_Chen1"], "authors": ["Ningyu Zhang", "Luoqiu Li", "Xiang Chen", "Shumin Deng", "Zhen Bi", "Chuanqi Tan", "Fei Huang", "Huajun Chen"], "keywords": ["prompt-tuning", "pre-trained language model", "few-shot learning"], "abstract": "Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|differentiable_prompt_makes_pretrained_language_models_better_fewshot_learners", "pdf": "/pdf/d0850eb512cbbd2b27f6ff0ab0edf71861cd4829.pdf", "one-sentence_summary": "A differentiable prompt learning method for few-shot NLP with optimized prompt templates as well as labels. ", "supplementary_material": "/attachment/925f8944c1fc31182868c4e6175b784943f5d78a.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nzhang2022differentiable,\ntitle={Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners},\nauthor={Ningyu Zhang and Luoqiu Li and Xiang Chen and Shumin Deng and Zhen Bi and Chuanqi Tan and Fei Huang and Huajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ek9a0qIafW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "yfe1VMYAXa4", "original": "miwX-S7vwh9", "number": 272, "cdate": 1632875440765, "mdate": null, "ddate": null, "tcdate": 1632875440765, "tmdate": 1697934947710, "tddate": null, "forum": "yfe1VMYAXa4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding", "authorids": ["~Ningyu_Zhang1", "~Zhen_Bi1", "~Xiaozhuan_Liang1", "~Siyuan_Cheng2", "~Haosen_Hong1", "~Shumin_Deng1", "~Qiang_Zhang6", "~Jiazhang_Lian1", "~Huajun_Chen1"], "authors": ["Ningyu Zhang", "Zhen Bi", "Xiaozhuan Liang", "Siyuan Cheng", "Haosen Hong", "Shumin Deng", "Qiang Zhang", "Jiazhang Lian", "Huajun Chen"], "keywords": ["pre-trained language model", "knowledge graph", "protein representation"], "abstract": "Self-supervised protein language models have proved their effectiveness in learning the proteins representations. With the increasing computational power, current protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve remarkable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training.  Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction.", "pdf": "/pdf/45d0f729743dae28d50e8c7e62050d0d95afbf43.pdf", "one-sentence_summary": "A general framework to integrate knowledge graph (gene ontology) into protein pre-training. ", "supplementary_material": "/attachment/1e384ee55d65d84e735e381af28923b6721171dc.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|ontoprotein_protein_pretraining_with_gene_ontology_embedding", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2201.11147/code)", "_bibtex": "@inproceedings{\nzhang2022ontoprotein,\ntitle={OntoProtein: Protein Pretraining With Gene Ontology Embedding},\nauthor={Ningyu Zhang and Zhen Bi and Xiaozhuan Liang and Siyuan Cheng and Haosen Hong and Shumin Deng and Qiang Zhang and Jiazhang Lian and Huajun Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=yfe1VMYAXa4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "GugZ5DzzAu", "original": "Z0PK6OAUD_2", "number": 265, "cdate": 1632875440382, "mdate": null, "ddate": null, "tcdate": 1632875440382, "tmdate": 1676330680080, "tddate": null, "forum": "GugZ5DzzAu", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Permutation Compressors for Provably Faster Distributed Nonconvex Optimization", "authorids": ["~Rafa\u0142_Szlendak1", "~Alexander_Tyurin1", "~Peter_Richt\u00e1rik1"], "authors": ["Rafa\u0142 Szlendak", "Alexander Tyurin", "Peter Richt\u00e1rik"], "keywords": ["MARINA", "distributed training", "permutation compressor", "correlated compressor", "Hessian variance", "communication complexity", "nonconvex optimization"], "abstract": "In this work we study the MARINA method of Gorbunov et al (ICML, 2021) -- the current state-of-the-art distributed non-convex optimization method in terms of theoretical communication complexity. Theoretical superiority of this method can be largely attributed to two sources: a carefully engineered biased stochastic gradient estimator, which leads to a reduction in the number of communication rounds, and  the reliance on\n {\\em independent} stochastic communication compression, which leads to a reduction in the number of  transmitted bits within each communication round. In this paper we  i) extend the theory of MARINA to support a much wider class of potentially {\\em correlated} compressors, extending the reach of the method beyond the classical independent compressors setting,  ii) show that a new quantity, for which we coin the name {\\em Hessian variance}, allows us to significantly refine the original analysis of MARINA without any additional assumptions, and iii) identify a special class of correlated compressors based on the idea of {\\em random  permutations}, for which we coin the term Perm$K$, the use of which leads to up to $O(\\sqrt{n})$ (resp. $O(1 + d/\\sqrt{n})$) improvement in the theoretical communication complexity of MARINA in the low Hessian variance regime when $d\\geq n$ (resp. $d \\leq n$), where $n$ is the number of workers and $d$ is the number of parameters describing the model we are learning. We corroborate our theoretical results with carefully engineered synthetic experiments with minimizing the average of nonconvex quadratics, and on autoencoder training with the MNIST dataset.", "one-sentence_summary": "In this paper, we present the novel compression scheme for distributed non-convex optimization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "szlendak|permutation_compressors_for_provably_faster_distributed_nonconvex_optimization", "pdf": "/pdf/0e0632efe3d09e539cb046531a252270ab830deb.pdf", "_bibtex": "@inproceedings{\nszlendak2022permutation,\ntitle={Permutation Compressors for Provably Faster Distributed Nonconvex Optimization},\nauthor={Rafa{\\l} Szlendak and Alexander Tyurin and Peter Richt{\\'a}rik},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=GugZ5DzzAu}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 40}}, {"id": "6kCiVaoQdx9", "original": "Zt33LhgLK0d", "number": 260, "cdate": 1632875440010, "mdate": null, "ddate": null, "tcdate": 1632875440010, "tmdate": 1676330680620, "tddate": null, "forum": "6kCiVaoQdx9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Few-shot Learning via Dirichlet Tessellation Ensemble", "authorids": ["~Chunwei_Ma1", "~Ziyun_Huang1", "~Mingchen_Gao1", "~Jinhui_Xu1"], "authors": ["Chunwei Ma", "Ziyun Huang", "Mingchen Gao", "Jinhui Xu"], "keywords": ["Few-shot Learning", "Computational Geometry", "Dirichlet Tessellation", "Voronoi Diagram", "Ensemble Learning"], "abstract": "Few-shot learning (FSL) is the process of rapid generalization from abundant base samples to inadequate novel samples. Despite extensive research in recent years, FSL is still not yet able to generate satisfactory solutions for a wide range of real-world applications. To confront this challenge, we study the FSL problem from a geometric point of view in this paper. One observation is that the widely embraced ProtoNet model is essentially a Voronoi Diagram (VD) in the feature space. We retrofit it by making use of a recent advance in computational geometry called Cluster-induced Voronoi Diagram (CIVD). Starting from the simplest nearest neighbor model, CIVD gradually incorporates cluster-to-point and then cluster-to-cluster relationships for space subdivision, which is used to improve the accuracy and robustness at multiple stages of FSL. Specifically, we use CIVD (1) to integrate parametric and nonparametric few-shot classifiers; (2) to combine feature representation and surrogate representation; (3) and to leverage feature-level, transformation-level, and geometry-level heterogeneities for a better ensemble. Our CIVD-based workflow enables us to achieve new state-of-the-art results on mini-ImageNet, CUB, and tiered-ImagenNet datasets, with ${\\sim}2\\%{-}5\\%$ improvements upon the next best. To summarize, CIVD provides a mathematically elegant and geometrically interpretable framework that compensates for extreme data insufficiency, prevents overfitting, and allows for fast geometric ensemble for thousands of individual VD. These together make FSL stronger.", "one-sentence_summary": "We developed a novel geometric framework that greatly improves few-shot classification, based on Cluster-induced Voronoi Diagram (CIVD).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ma|fewshot_learning_via_dirichlet_tessellation_ensemble", "pdf": "/pdf/f02d54e514e3af112f1ef58d64a80b6533b4d63f.pdf", "data": "", "_bibtex": "@inproceedings{\nma2022fewshot,\ntitle={Few-shot Learning via Dirichlet Tessellation Ensemble},\nauthor={Chunwei Ma and Ziyun Huang and Mingchen Gao and Jinhui Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6kCiVaoQdx9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "mKDtUtxIGJ", "original": "Vq-VTD_9s_C", "number": 257, "cdate": 1632875439782, "mdate": null, "ddate": null, "tcdate": 1632875439782, "tmdate": 1676330680724, "tddate": null, "forum": "mKDtUtxIGJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Deep Point Cloud Reconstruction", "authorids": ["~Jaesung_Choe1", "~ByeongIn_Joung1", "~Francois_Rameau1", "~Jaesik_Park3", "~In_So_Kweon2"], "authors": ["Jaesung Choe", "ByeongIn Joung", "Francois Rameau", "Jaesik Park", "In So Kweon"], "keywords": ["Computer Vision", "3D Geometry", "Deep Learning based Point Cloud Understanding", "Point Cloud Denoising", "Point Cloud Upsampling"], "abstract": "Point cloud obtained from 3D scanning is often sparse, noisy, and irregular. To cope with these issues, recent studies have been separately conducted to densify, denoise, and complete inaccurate point cloud. In this paper, we advocate that jointly solving these tasks leads to significant improvement for point cloud reconstruction. To this end, we propose a deep point cloud reconstruction network consisting of two stages: 1) a 3D sparse stacked-hourglass network as for the initial densification and denoising, 2) a refinement via transformers converting the discrete voxels into continuous 3D points. In particular, we further improve the performance of the transformers by a newly proposed module called amplified positional encoding. This module has been designed to differently amplify the magnitude of positional encoding vectors based on the points' distances for adaptive refinements. Extensive experiments demonstrate that our network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNet datasets. Moreover, we underline the ability of our network to generalize toward real-world and unmet scenes.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "choe|deep_point_cloud_reconstruction", "pdf": "/pdf/f30ff432e93adac626108cbcf0a36c3601b912d2.pdf", "one-sentence_summary": "We propose deep learning-based point cloud reconstruction algorithm", "_bibtex": "@inproceedings{\nchoe2022deep,\ntitle={Deep Point Cloud Reconstruction},\nauthor={Jaesung Choe and ByeongIn Joung and Francois Rameau and Jaesik Park and In So Kweon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mKDtUtxIGJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 38}}, {"id": "q7n2RngwOM", "original": "_bLoe3KTXtn", "number": 255, "cdate": 1632875439632, "mdate": null, "ddate": null, "tcdate": 1632875439632, "tmdate": 1676330680775, "tddate": null, "forum": "q7n2RngwOM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "$\\beta$-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap", "authorids": ["~Pengzhou_Abel_Wu1", "~Kenji_Fukumizu1"], "authors": ["Pengzhou Abel Wu", "Kenji Fukumizu"], "keywords": ["VAE", "variational autoencoder", "balanced representation Learning", "treatment effects", "causal inference", "identifiability", "identification", "CATE", "ATE", "weak overlap", "limited overlap", "Prognostic Model", "Prognostic score"], "abstract": "As an important problem in causal inference, we discuss the identification and estimation of treatment effects (TEs) under limited overlap; that is, when subjects with certain features belong to a single treatment group. We use a latent variable to model a prognostic score which is widely used in biostatistics and sufficient for TEs; i.e., we build a generative prognostic model. We prove that the latent variable recovers a prognostic score, and the model identifies individualized treatment effects. The model is then learned as $\\beta$-Intact-VAE\u2013\u2013a new type of variational autoencoder (VAE). We derive the TE error bounds that enable representations balanced for treatment groups conditioned on individualized features. The proposed method is compared with recent methods using (semi-)synthetic datasets. ", "one-sentence_summary": "See all these naturally in one: limited overlap, prognostic score, identifiable VAE, balanced representation Learning, CATE error bounds.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|\\betaintactvae_identifying_and_estimating_causal_effects_under_limited_overlap", "pdf": "/pdf/478449496537cfe4195dfc3df5666eac40c4aaeb.pdf", "supplementary_material": "/attachment/b62f5e57992d1cdabff0277e0af771194c7d3006.zip", "_bibtex": "@inproceedings{\nwu2022betaintactvae,\ntitle={\\${\\textbackslash}beta\\$-Intact-{VAE}: Identifying and Estimating Causal Effects under Limited Overlap},\nauthor={Pengzhou Abel Wu and Kenji Fukumizu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=q7n2RngwOM}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "BZnnMbt0pW", "original": "0u8soOaFDt-", "number": 251, "cdate": 1632875439330, "mdate": null, "ddate": null, "tcdate": 1632875439330, "tmdate": 1676330680951, "tddate": null, "forum": "BZnnMbt0pW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection", "authorids": ["~Wei_Ji2", "~Jingjing_Li5", "~Qi_Bi1", "~chuan_guo2", "~Jie_Liu8", "~Li_Cheng1"], "authors": ["Wei Ji", "Jingjing Li", "Qi Bi", "chuan guo", "Jie Liu", "Li Cheng"], "keywords": ["RGB-D saliency detection", "salient object detection", "deep learning", "unsupervised learning"], "abstract": "Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models.", "one-sentence_summary": "We propose the first deep unsupervised RGB-D saliency detection method, which achieves appealing performance and does not require any human efforts compared to fully-supervised learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ji|promoting_saliency_from_depth_deep_unsupervised_rgbd_saliency_detection", "pdf": "/pdf/a7126368cdf1f32c8b60e48936a7acf1b8e7599e.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nji2022promoting,\ntitle={Promoting Saliency From Depth: Deep Unsupervised {RGB}-D Saliency Detection},\nauthor={Wei Ji and Jingjing Li and Qi Bi and chuan guo and Jie Liu and Li Cheng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BZnnMbt0pW}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "AXWygMvuT6Q", "original": "10c2V5ckAqm", "number": 246, "cdate": 1632875439094, "mdate": null, "ddate": null, "tcdate": 1632875439094, "tmdate": 1697934950397, "tddate": null, "forum": "AXWygMvuT6Q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph", "authorids": ["~Dacheng_Yin1", "~Xuanchi_Ren1", "~Chong_Luo1", "~Yuwang_Wang3", "~Zhiwei_Xiong1", "~Wenjun_Zeng3"], "authors": ["Dacheng Yin", "Xuanchi Ren", "Chong Luo", "Yuwang Wang", "Zhiwei Xiong", "Wenjun Zeng"], "keywords": ["Content-style decomposed representation", "Zero-shot voice conversion", "Style transfer", "Transformer", "Unsupervised learning"], "abstract": "This paper addresses the unsupervised learning of content-style decomposed representation. We first give a definition of style and then model the content-style representation as a token-level bipartite graph. An unsupervised framework, named Retriever, is proposed to learn such representations. First, a cross-attention module is employed to retrieve permutation invariant (P.I.) information, defined as style, from the input data. Second, a vector quantization (VQ) module is used, together with man-induced constraints, to produce interpretable content tokens. Last, an innovative link attention module serves as the decoder to reconstruct data from the decomposed content and style, with the help of the linking keys. Being modal-agnostic, the proposed Retriever is evaluated in both speech and image domains. The state-of-the-art zero-shot voice conversion performance confirms the disentangling ability of our framework. Top performance is also achieved in the part discovery task for images, verifying the interpretability of our representation. In addition, the vivid part-based style transfer quality demonstrates the potential of Retriever to support various fascinating generative tasks. Project page at https://ydcustc.github.io/retriever-demo/.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yin|retriever_learning_contentstyle_representation_as_a_tokenlevel_bipartite_graph", "pdf": "/pdf/40312e47e819cfbbcfde577838af08ac592b9013.pdf", "one-sentence_summary": "We propose a model-agnostic and unsupervised framework to learn a novel token-level bipartite graph representation of content and style from structured input.", "supplementary_material": "/attachment/24e2b696432ca001c85230322a935ce35db1d5dc.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.12307/code)", "_bibtex": "@inproceedings{\nyin2022retriever,\ntitle={Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph},\nauthor={Dacheng Yin and Xuanchi Ren and Chong Luo and Yuwang Wang and Zhiwei Xiong and Wenjun Zeng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AXWygMvuT6Q}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "7DI6op61AY", "original": "gYCB22KAksM", "number": 241, "cdate": 1632875439017, "mdate": null, "ddate": null, "tcdate": 1632875439017, "tmdate": 1676330681246, "tddate": null, "forum": "7DI6op61AY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural Markov Controlled SDE: Stochastic Optimization for Continuous-Time Data", "authorids": ["~Sung_Woo_Park2", "~Kyungjae_Lee1", "~Junseok_Kwon5"], "authors": ["Sung Woo Park", "Kyungjae Lee", "Junseok Kwon"], "keywords": ["controlled stochastic differential equation", "time-series prediction"], "abstract": "We propose a novel probabilistic framework for modeling stochastic dynamics with the rigorous use of stochastic optimal control theory. The proposed model called the neural Markov controlled stochastic differential equation (CSDE) overcomes the fundamental and structural limitations of conventional dynamical models by introducing the following two components: (1) Markov dynamic programming to efficiently train the proposed CSDE and (2) multi-conditional forward-backward losses to provide rich information for accurate inference and to assure theoretical optimality. We demonstrate that our dynamical model efficiently generates a complex time series in the data space without extra networks while showing comparable performance against existing model-based methods on several datasets.", "one-sentence_summary": "We propose a novel probabilistic framework for modelling stochastic dynamics with the rigorous use of optimal control theory. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "park|neural_markov_controlled_sde_stochastic_optimization_for_continuoustime_data", "pdf": "/pdf/d7e9dca5dfd0f6600c830cea3a5efca6ae4b8ae0.pdf", "data": "", "_bibtex": "@inproceedings{\npark2022neural,\ntitle={Neural Markov Controlled {SDE}: Stochastic Optimization for Continuous-Time Data},\nauthor={Sung Woo Park and Kyungjae Lee and Junseok Kwon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7DI6op61AY}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "_PHymLIxuI", "original": "8CLhJncsMB3", "number": 237, "cdate": 1632875438707, "mdate": null, "ddate": null, "tcdate": 1632875438707, "tmdate": 1676330681472, "tddate": null, "forum": "_PHymLIxuI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention", "authorids": ["~Wenxiao_Wang2", "~Lu_Yao2", "~Long_Chen8", "~Binbin_Lin3", "~Deng_Cai4", "~Xiaofei_He2", "~Wei_Liu3"], "authors": ["Wenxiao Wang", "Lu Yao", "Long Chen", "Binbin Lin", "Deng Cai", "Xiaofei He", "Wei Liu"], "keywords": ["vision transformers", "architecture"], "abstract": "Transformers have made great progress in dealing with computer vision tasks. However, existing vision transformers have not yet possessed the ability of building the interactions among features of different scales, which is perceptually important to visual inputs. The reasons are two-fold: (1) Input embeddings of each layer are equal-scale, so no cross-scale feature can be extracted; (2) to lower the computational cost, some vision transformers merge adjacent embeddings inside the self-attention module, thus sacrificing small-scale (fine-grained) features of the embeddings and also disabling the cross-scale interactions. To this end, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). On the one hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features. On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings. Through the above two designs, we achieve cross-scale attention. Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs. Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|crossformer_a_versatile_vision_transformer_hinging_on_crossscale_attention", "pdf": "/pdf/6d2cbac2997d9b594cd4e0076cfceef1cdfc3319.pdf", "supplementary_material": "/attachment/4f3263d9cf91a43a9bc6b4900188fa10f25bbb04.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nwang2022crossformer,\ntitle={CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention},\nauthor={Wenxiao Wang and Lu Yao and Long Chen and Binbin Lin and Deng Cai and Xiaofei He and Wei Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_PHymLIxuI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "9L1BsI4wP1H", "original": "jNZRo9gaBoq", "number": 232, "cdate": 1632875438341, "mdate": null, "ddate": null, "tcdate": 1632875438341, "tmdate": 1676330681914, "tddate": null, "forum": "9L1BsI4wP1H", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Adversarially Robust Conformal Prediction", "authorids": ["~Asaf_Gendler1", "~Tsui-Wei_Weng1", "~Luca_Daniel1", "~Yaniv_Romano1"], "authors": ["Asaf Gendler", "Tsui-Wei Weng", "Luca Daniel", "Yaniv Romano"], "keywords": ["Conformal Prediction", "Adversarial Robustness", "Randomized Smoothing", "Uncertainty Estimation", "Calibration"], "abstract": "Conformal prediction is a model-agnostic tool for constructing prediction sets that are valid under the common i.i.d. assumption, which has been applied to quantify the prediction uncertainty of deep net classifiers. In this paper, we generalize this framework to the case where adversaries exist during inference time, under which the i.i.d. assumption is grossly violated. By combining conformal prediction with randomized smoothing, our proposed method forms a prediction set with finite-sample coverage guarantee that holds for any data distribution with $\\ell_2$-norm bounded adversarial noise, generated by any adversarial attack algorithm. The core idea is to bound the Lipschitz constant of the non-conformity score by smoothing it with Gaussian noise and leverage this knowledge to account for the effect of the unknown adversarial perturbation. We demonstrate the necessity of our method in the adversarial setting and the validity of our theoretical guarantee on three widely used benchmark data sets: CIFAR10, CIFAR100, and ImageNet.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gendler|adversarially_robust_conformal_prediction", "pdf": "/pdf/8c5857f5c0f9cb907a44054df718409de9263370.pdf", "one-sentence_summary": "Multi-class calibration procedure that is provably robust to adversarial attacks", "supplementary_material": "/attachment/dd1d8b8cd038c5f5af21205164d7a36773f76009.zip", "data": "", "_bibtex": "@inproceedings{\ngendler2022adversarially,\ntitle={Adversarially Robust Conformal Prediction},\nauthor={Asaf Gendler and Tsui-Wei Weng and Luca Daniel and Yaniv Romano},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9L1BsI4wP1H}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "HTp-6yLGGX", "original": "Jt346Nqm12", "number": 231, "cdate": 1632875438268, "mdate": null, "ddate": null, "tcdate": 1632875438268, "tmdate": 1676330682045, "tddate": null, "forum": "HTp-6yLGGX", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hot-Refresh Model Upgrades with Regression-Free Compatible Training in Image Retrieval", "authorids": ["~Binjie_Zhang1", "~Yixiao_Ge2", "~Yantao_Shen2", "~Yu_Li4", "~Chun_Yuan1", "~XUYUAN_XU1", "~Yexin_Wang3", "~Ying_Shan2"], "authors": ["Binjie Zhang", "Yixiao Ge", "Yantao Shen", "Yu Li", "Chun Yuan", "XUYUAN XU", "Yexin Wang", "Ying Shan"], "keywords": ["Compatible Representation Learning", "Image Retrieval", "Model Regression"], "abstract": "The task of hot-refresh model upgrades of image retrieval systems plays an essential role in the industry but has never been investigated in academia before. Conventional cold-refresh model upgrades can only deploy new models after the gallery is overall backfilled, taking weeks or even months for massive data. In contrast, hot-refresh model upgrades deploy the new model immediately and then gradually improve the retrieval accuracy by backfilling the gallery on-the-fly. Compatible training has made it possible, however, the problem of model regression with negative flips poses a great challenge to the stable improvement of user experience. We argue that it is mainly due to the fact that new-to-old positive query-gallery pairs may show less similarity than new-to-new negative pairs. To solve the problem, we introduce a Regression-Alleviating Compatible Training (RACT) method to properly constrain the feature compatibility while reducing negative flips. The core is to encourage the new-to-old positive pairs to be more similar than both the new-to-old negative pairs and the new-to-new negative pairs. An efficient uncertainty-based backfilling strategy is further introduced to fasten accuracy improvements. Extensive experiments on large-scale retrieval benchmarks (e.g., Google Landmark) demonstrate that our RACT effectively alleviates the model regression for one more step towards seamless model upgrades.", "one-sentence_summary": "We for the first time study the model regression problem in hot-refresh model upgrades of image retrieval systems with compatible representation learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|hotrefresh_model_upgrades_with_regressionfree_compatible_training_in_image_retrieval", "pdf": "/pdf/19b014f7d493937c96b4232e3959811fd2b47ce5.pdf", "supplementary_material": "/attachment/8074b69255ff3099ae4fd679a3163eadb0581df7.zip", "data": "", "_bibtex": "@inproceedings{\nzhang2022hotrefresh,\ntitle={Hot-Refresh Model Upgrades with Regression-Free Compatible Training in Image Retrieval},\nauthor={Binjie Zhang and Yixiao Ge and Yantao Shen and Yu Li and Chun Yuan and XUYUAN XU and Yexin Wang and Ying Shan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HTp-6yLGGX}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "kG0AtPi6JI1", "original": "CirzCe_e4rY", "number": 228, "cdate": 1632875438047, "mdate": null, "ddate": null, "tcdate": 1632875438047, "tmdate": 1676330682208, "tddate": null, "forum": "kG0AtPi6JI1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Visual Representation Learning over Latent Domains", "authorids": ["~Lucas_Deecke1", "~Timothy_Hospedales1", "~Hakan_Bilen1"], "authors": ["Lucas Deecke", "Timothy Hospedales", "Hakan Bilen"], "keywords": ["transfer learning", "latent domains", "computer vision"], "abstract": "A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While multi-domain learning enables the learning of compact models that span multiple visual domains, these rely on the presence of domain labels, in turn requiring laborious curation of datasets. This paper proposes a less explored, but highly realistic new setting called latent domain learning: learning over data from different domains, without access to domain annotations. Experiments show that this setting is challenging for standard models and existing multi-domain approaches, calling for new customized solutions: a sparse adaptation strategy is formulated which enhances performance by accounting for latent domains in data. Our method can be paired seamlessly with existing models, and benefits conceptually related tasks, e.g. empirical fairness problems and long-tailed recognition.", "one-sentence_summary": "A new setting for learning over data with multiple latent domains is introduced, alongside new methods for it.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "deecke|visual_representation_learning_over_latent_domains", "pdf": "/pdf/c43020f29d1ac17488c3b808196c9b0243182218.pdf", "data": "", "_bibtex": "@inproceedings{\ndeecke2022visual,\ntitle={Visual Representation Learning over Latent Domains},\nauthor={Lucas Deecke and Timothy Hospedales and Hakan Bilen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kG0AtPi6JI1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "6sh3pIzKS-", "original": "XzmX0DtSQ2G", "number": 225, "cdate": 1632875437816, "mdate": null, "ddate": null, "tcdate": 1632875437816, "tmdate": 1697934953389, "tddate": null, "forum": "6sh3pIzKS-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Chemical-Reaction-Aware Molecule Representation Learning", "authorids": ["~Hongwei_Wang1", "~Weijiang_Li1", "xjin17@illinois.edu", "~Kyunghyun_Cho1", "~Heng_Ji3", "~Jiawei_Han1", "~Martin_D._Burke1"], "authors": ["Hongwei Wang", "Weijiang Li", "Xiaomeng Jin", "Kyunghyun Cho", "Heng Ji", "Jiawei Han", "Martin D. Burke"], "keywords": ["molecule representation learning", "graph neural networks", "chemical reaction"], "abstract": "Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, existing SMILES-based (Simplified Molecular-Input Line-Entry System) or GNN-based (Graph Neural Networks) MRL methods either take SMILES strings as input that have difficulty in encoding molecule structure information, or over-emphasize the importance of GNN architectures but neglect their generalization ability. Here we propose using chemical reactions to assist learning molecule representation. The key idea of our approach is to preserve the equivalence of molecules with respect to chemical reactions in the embedding space, i.e., forcing the sum of reactant embeddings and the sum of product embeddings to be equal for each chemical equation. This constraint is proven effective to 1) keep the embedding space well-organized and 2) improve the generalization ability of molecule embeddings. Moreover, our model can use any GNN as the molecule encoder and is thus agnostic to GNN architectures. Experimental results demonstrate that our method achieves state-of-the-art performance in a variety of downstream tasks, e.g., reaction product prediction, molecule property prediction, reaction classification, and graph-edit-distance prediction. The code is available at https://github.com/hwwang55/MolR.", "pdf": "/pdf/90448ed39b2846f07097bae6944cbb6a3e2bdaf5.pdf", "one-sentence_summary": "We make use of chemical reactions to improve the generalization ability of learned molecule embeddings", "supplementary_material": "/attachment/a26121fc79d0df8f5fafc3f2477fb793fae69ff7.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|chemicalreactionaware_molecule_representation_learning", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.09888/code)", "_bibtex": "@inproceedings{\nwang2022chemicalreactionaware,\ntitle={Chemical-Reaction-Aware Molecule Representation Learning},\nauthor={Hongwei Wang and Weijiang Li and Xiaomeng Jin and Kyunghyun Cho and Heng Ji and Jiawei Han and Martin D. Burke},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6sh3pIzKS-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "jeLW-Fh9bV", "original": "iGBWvFtrAEk", "number": 223, "cdate": 1632875437662, "mdate": null, "ddate": null, "tcdate": 1632875437662, "tmdate": 1697934953522, "tddate": null, "forum": "jeLW-Fh9bV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Skill-based Meta-Reinforcement Learning", "authorids": ["~Taewook_Nam1", "~Shao-Hua_Sun1", "~Karl_Pertsch1", "~Sung_Ju_Hwang1", "~Joseph_J_Lim1"], "authors": ["Taewook Nam", "Shao-Hua Sun", "Karl Pertsch", "Sung Ju Hwang", "Joseph J Lim"], "keywords": ["meta-RL", "meta-reinforcement learning", "skill-based meta-reinforcement learning", "meta-learning", "skill-based RL"], "abstract": "While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible. To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been limited to short-horizon tasks with dense rewards. To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form of offline datasets without reward or task annotations. While these approaches yield improved sample efficiency, millions of interactions with environments are still required to solve complex tasks. In this work, we devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions. Our core idea is to leverage prior experience extracted from offline datasets during meta-learning. Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task. Experimental results on continuous control tasks in navigation and manipulation demonstrate that the proposed method can efficiently solve long-horizon novel target tasks by combining the strengths of meta-learning and the usage of offline datasets, while prior approaches in RL, meta-RL, and multi-task RL require substantially more environment interactions to solve the tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nam|skillbased_metareinforcement_learning", "pdf": "/pdf/8c5b0157e0e15aedce63445fa2114ab02797c487.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.11828/code)", "_bibtex": "@inproceedings{\nnam2022skillbased,\ntitle={Skill-based Meta-Reinforcement Learning},\nauthor={Taewook Nam and Shao-Hua Sun and Karl Pertsch and Sung Ju Hwang and Joseph J Lim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=jeLW-Fh9bV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "ufGMqIM0a4b", "original": "0xAPc4L9F7z", "number": 218, "cdate": 1632875437284, "mdate": null, "ddate": null, "tcdate": 1632875437284, "tmdate": 1697934954321, "tddate": null, "forum": "ufGMqIM0a4b", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "InfinityGAN: Towards Infinite-Pixel Image Synthesis", "authorids": ["~Chieh_Hubert_Lin1", "~Hsin-Ying_Lee2", "~Yen-Chi_Cheng1", "~Sergey_Tulyakov1", "~Ming-Hsuan_Yang1"], "authors": ["Chieh Hubert Lin", "Hsin-Ying Lee", "Yen-Chi Cheng", "Sergey Tulyakov", "Ming-Hsuan Yang"], "keywords": ["generative modeling", "image synthesis", "generative adversarial networks", "infinite-pixel synthesis", "GANs"], "abstract": "We present InfinityGAN, a method to generate arbitrary-sized images. The problem is associated with several key challenges. First, scaling existing models to an arbitrarily large image size is resource-constrained, both in terms of computation and availability of large-field-of-view training data. InfinityGAN trains and infers patch-by-patch seamlessly with low computational resources. Second, large images should be locally and globally consistent, avoid repetitive patterns, and look realistic. To address these, InfinityGAN takes global appearance, local structure and texture into account. With this formulation, we can generate images with spatial size and level of detail not attainable before. Experimental evaluation supports that InfinityGAN generates images with superior global structure compared to baselines and features parallelizable inference. Finally, we show several applications unlocked by our approach, such as fusing styles spatially, multi-modal outpainting and image inbetweening at arbitrary input and output sizes.", "one-sentence_summary": "InfinityGAN learns to synthesize arbitrary-sized images with limited resources and enables multiple new applications.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lin|infinitygan_towards_infinitepixel_image_synthesis", "pdf": "/pdf/0907e14b44a58aca8f7577b9e7f45870865cbfc4.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2104.03963/code)", "_bibtex": "@inproceedings{\nlin2022infinitygan,\ntitle={Infinity{GAN}: Towards Infinite-Pixel Image Synthesis},\nauthor={Chieh Hubert Lin and Hsin-Ying Lee and Yen-Chi Cheng and Sergey Tulyakov and Ming-Hsuan Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ufGMqIM0a4b}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "DrZXuTGg2A-", "original": "P_tpZlxlovh", "number": 209, "cdate": 1632875436682, "mdate": null, "ddate": null, "tcdate": 1632875436682, "tmdate": 1676330683131, "tddate": null, "forum": "DrZXuTGg2A-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Shuffle Private Stochastic Convex Optimization", "authorids": ["~Albert_Cheu1", "~Matthew_Joseph1", "~Jieming_Mao1", "~Binghui_Peng1"], "authors": ["Albert Cheu", "Matthew Joseph", "Jieming Mao", "Binghui Peng"], "keywords": ["shuffle privacy", "stochastic convex optimization", "differential privacy"], "abstract": "In shuffle privacy, each user sends a collection of randomized messages to a trusted shuffler, the shuffler randomly permutes these messages, and the resulting shuffled collection of messages must satisfy differential privacy. Prior work in this model has largely focused on protocols that use a single round of communication to compute algorithmic primitives like means, histograms, and counts. In this work, we present interactive shuffle protocols for stochastic convex optimization. Our optimization protocols rely on a new noninteractive protocol for summing vectors of bounded $\\ell_2$ norm. By combining this sum subroutine with techniques including mini-batch stochastic gradient descent, accelerated gradient descent, and Nesterov's smoothing method, we obtain loss guarantees for a variety of convex loss functions that significantly improve on those of the local model and sometimes match those of the central model.", "one-sentence_summary": "The first analysis of shuffle private stochastic convex optimization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cheu|shuffle_private_stochastic_convex_optimization", "pdf": "/pdf/7c8c9c62ecd536de1d742c826d9a85c22f8e430b.pdf", "_bibtex": "@inproceedings{\ncheu2022shuffle,\ntitle={Shuffle Private Stochastic Convex Optimization},\nauthor={Albert Cheu and Matthew Joseph and Jieming Mao and Binghui Peng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DrZXuTGg2A-}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "MljXVdp4A3N", "original": "dUdQCclS9yE", "number": 204, "cdate": 1632875436372, "mdate": null, "ddate": null, "tcdate": 1632875436372, "tmdate": 1676330683494, "tddate": null, "forum": "MljXVdp4A3N", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Know Your Action Set: Learning Action Relations for Reinforcement Learning", "authorids": ["~Ayush_Jain2", "~Norio_Kosaka1", "~Kyung-Min_Kim1", "~Joseph_J_Lim1"], "authors": ["Ayush Jain", "Norio Kosaka", "Kyung-Min Kim", "Joseph J Lim"], "keywords": ["reinforcement learning", "varying action space", "relational reasoning"], "abstract": "Intelligent agents can solve tasks in various ways depending on their available set of actions. However, conventional reinforcement learning (RL) assumes a fixed action set. This work asserts that tasks with varying action sets require reasoning of the relations between the available actions. For instance, taking a nail-action in a repair task is meaningful only if a hammer-action is also available. To learn and utilize such action relations, we propose a novel policy architecture consisting of a graph attention network over the available actions. We show that our model makes informed action decisions by correctly attending to other related actions in both value-based and policy-based RL. Consequently, it outperforms non-relational architectures on applications where the action space often varies, such as recommender systems and physical reasoning with tools and skills. Results and code at https://sites.google.com/view/varyingaction .", "pdf": "/pdf/e227e96a6b5847d80aeaa7803fa8a32a8b03f28c.pdf", "one-sentence_summary": "Learning action interdependence for reinforcement learning under a varying action space.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jain|know_your_action_set_learning_action_relations_for_reinforcement_learning", "supplementary_material": "/attachment/bb5d263ecab9babc9df0f10e53a8fcc609a8f305.zip", "_bibtex": "@inproceedings{\njain2022know,\ntitle={Know Your Action Set: Learning Action Relations for Reinforcement Learning},\nauthor={Ayush Jain and Norio Kosaka and Kyung-Min Kim and Joseph J Lim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MljXVdp4A3N}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "3eIrli0TwQ", "original": "M9hpUr_Ywb6", "number": 191, "cdate": 1632875435376, "mdate": null, "ddate": null, "tcdate": 1632875435376, "tmdate": 1676330684137, "tddate": null, "forum": "3eIrli0TwQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Importance of Difficulty Calibration in Membership Inference Attacks", "authorids": ["~Lauren_Watson1", "~Chuan_Guo1", "~Graham_Cormode1", "~Alexandre_Sablayrolles1"], "authors": ["Lauren Watson", "Chuan Guo", "Graham Cormode", "Alexandre Sablayrolles"], "keywords": ["membership inference attack", "privacy"], "abstract": "The vulnerability of machine learning models to membership inference attacks has received much attention in recent years. However, existing attacks mostly remain impractical due to having high false positive rates, where non-member samples are often erroneously predicted as members. This type of error makes the predicted membership signal unreliable, especially since most samples are non-members in real world applications. In this work, we argue that membership inference attacks can benefit drastically from difficulty calibration, where an attack's predicted membership score is adjusted to the difficulty of correctly classifying the target sample. We show that difficulty calibration can significantly reduce the false positive rate of a variety of existing attacks without a loss in accuracy.", "one-sentence_summary": "Membership inference attacks can greatly benefit from a technique called difficulty calibration, significantly improving their reliability.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "watson|on_the_importance_of_difficulty_calibration_in_membership_inference_attacks", "pdf": "/pdf/245838afd6ed33e19d36ead3f797f80899b36b37.pdf", "_bibtex": "@inproceedings{\nwatson2022on,\ntitle={On the Importance of Difficulty Calibration in Membership Inference Attacks},\nauthor={Lauren Watson and Chuan Guo and Graham Cormode and Alexandre Sablayrolles},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3eIrli0TwQ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "VrjOFfcnSV8", "original": "w0VI5nDfxwq", "number": 190, "cdate": 1632875435301, "mdate": null, "ddate": null, "tcdate": 1632875435301, "tmdate": 1676330684137, "tddate": null, "forum": "VrjOFfcnSV8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Entroformer: A Transformer-based Entropy Model for Learned Image Compression", "authorids": ["~Yichen_Qian1", "~Xiuyu_Sun1", "~Ming_Lin4", "~Zhiyu_Tan2", "~Rong_Jin1"], "authors": ["Yichen Qian", "Xiuyu Sun", "Ming Lin", "Zhiyu Tan", "Rong Jin"], "keywords": ["Image compression", "Entropy Model", "Global Dependencies"], "abstract": "One critical component in lossy deep image compression is the entropy model, which predicts the probability distribution of the quantized latent representation in the encoding and decoding modules. Previous works build entropy models upon convolutional neural networks which are inefficient in capturing global dependencies. In this work, we propose a novel transformer-based entropy model, termed Entroformer, to capture long-range dependencies in probability distribution estimation effectively and efficiently. Different from vision transformers in image classification, the Entroformer is highly optimized for image compression, including a top-k self-attention and a diamond relative position encoding. Meanwhile, we further expand this architecture with a parallel bidirectional context model to speed up the decoding process. The experiments show that the Entroformer achieves state-of-the-art performance on image compression while being time-efficient.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "qian|entroformer_a_transformerbased_entropy_model_for_learned_image_compression", "pdf": "/pdf/375073115f9e0608eacb1f009688d213ac8f1d11.pdf", "one-sentence_summary": "In this work, we propose a novel transformer-based entropy model, termed Entroformer, to capture long-range dependencies in probability distribution estimation effectively and efficiently.", "_bibtex": "@inproceedings{\nqian2022entroformer,\ntitle={Entroformer: A Transformer-based Entropy Model for Learned Image Compression},\nauthor={Yichen Qian and Xiuyu Sun and Ming Lin and Zhiyu Tan and Rong Jin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=VrjOFfcnSV8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "fOsN52jn25l", "original": "itJU6LNZA_-", "number": 181, "cdate": 1632875434679, "mdate": null, "ddate": null, "tcdate": 1632875434679, "tmdate": 1697934957259, "tddate": null, "forum": "fOsN52jn25l", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Dual Lottery Ticket Hypothesis", "authorids": ["~Yue_Bai1", "~Huan_Wang3", "~ZHIQIANG_TAO2", "~Kunpeng_Li1", "~Yun_Fu1"], "authors": ["Yue Bai", "Huan Wang", "ZHIQIANG TAO", "Kunpeng Li", "Yun Fu"], "keywords": ["Dual Lottery Ticket Hypothesis", "Sparse Network Training"], "abstract": "Fully exploiting the learning capacity of neural networks requires overparameterized dense networks. On the other side, directly training sparse neural networks typically results in unsatisfactory performance. Lottery Ticket Hypothesis (LTH) provides a novel view to investigate sparse network training and maintain its capacity. Concretely, it claims there exist winning tickets from a randomly initialized network found by iterative magnitude pruning and preserving promising trainability (or we say being in trainable condition). In this work, we regard the winning ticket from LTH as the subnetwork which is in trainable condition and its performance as our benchmark, then go from a complementary direction to articulate the Dual Lottery Ticket Hypothesis (DLTH): Randomly selected subnetworks from a randomly initialized dense network can be transformed into a trainable condition and achieve admirable performance compared with LTH --- random tickets in a given lottery pool can be transformed into winning tickets. Specifically, by using uniform-randomly selected subnetworks to represent the general cases, we propose a simple sparse network training strategy, Random Sparse Network Transformation (RST), to substantiate our DLTH. Concretely, we introduce a regularization term to borrow learning capacity and realize information extrusion from the weights which will be masked. After finishing the transformation for the randomly selected subnetworks, we conduct the regular finetuning to evaluate the model using fair comparisons with LTH and other strong baselines. Extensive experiments on several public datasets and comparisons with competitive approaches validate our DLTH as well as the effectiveness of the proposed model RST. Our work is expected to pave a way for inspiring new research directions of sparse network training in the future. Our code is available at https://github.com/yueb17/DLTH.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bai|dual_lottery_ticket_hypothesis", "pdf": "/pdf/839164c6325efc6887b07ade7198a9c83757669d.pdf", "one-sentence_summary": "We articulate a Dual Lottery Ticket Hypothesis (DLTH) with a proposed training strategy Random Sparse Network to validate DLTH.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.04248/code)", "_bibtex": "@inproceedings{\nbai2022dual,\ntitle={Dual Lottery Ticket Hypothesis},\nauthor={Yue Bai and Huan Wang and ZHIQIANG TAO and Kunpeng Li and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fOsN52jn25l}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "hzmQ4wOnSb", "original": "DVYgWPEamiW", "number": 165, "cdate": 1632875433553, "mdate": null, "ddate": null, "tcdate": 1632875433553, "tmdate": 1697934958638, "tddate": null, "forum": "hzmQ4wOnSb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GNN is a Counter? Revisiting GNN for Question Answering", "authorids": ["~Kuan_Wang1", "~Yuyu_Zhang1", "~Diyi_Yang2", "~Le_Song1", "~Tao_Qin1"], "authors": ["Kuan Wang", "Yuyu Zhang", "Diyi Yang", "Le Song", "Tao Qin"], "keywords": ["GNN", "Question Answering", "QA", "Reasoning", "ML"], "abstract": "Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies has been conducted to attempt to equip QA systems with human-level reasoning capability. To approximate the complicated human reasoning process, state-of-the-art QA systems commonly use pre-trained language models (LMs) to access knowledge encoded in LMs together with elaborately designed modules based on Graph Neural Networks (GNNs) to perform reasoning over knowledge graphs (KGs). However, many problems remain open regarding the reasoning functionality of these GNN-based modules. Can these GNN-based modules really perform a complex reasoning process? Are they under- or over-complicated for QA? To open the black box of GNN and investigate these problems, we dissect state-of-the-art GNN modules for QA and analyze their reasoning capability. We discover that even a very simple graph neural counter can outperform all the existing GNN modules on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge-aware reasoning. Our work reveals that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting. It remains a challenging open problem to build comprehensive reasoning modules for knowledge-powered QA.", "one-sentence_summary": "Counting is essential for reasoning and our simplistic graph neural counter is efficient and effective for QA tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|gnn_is_a_counter_revisiting_gnn_for_question_answering", "pdf": "/pdf/b4b3e479c999df701d9e74120833e9a3c1edb8f3.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.03192/code)", "_bibtex": "@inproceedings{\nwang2022gnn,\ntitle={{GNN} is a Counter? Revisiting {GNN} for Question Answering},\nauthor={Kuan Wang and Yuyu Zhang and Diyi Yang and Le Song and Tao Qin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hzmQ4wOnSb}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "OT3mLgR8Wg8", "original": "KL6QFLVUP6", "number": 163, "cdate": 1632875433474, "mdate": null, "ddate": null, "tcdate": 1632875433474, "tmdate": 1676330685424, "tddate": null, "forum": "OT3mLgR8Wg8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes", "authorids": ["~QI_LI9", "~Kaichun_Mo1", "~Yanchao_Yang1", "~Hang_Zhao1", "~Leonidas_Guibas1"], "authors": ["QI LI", "Kaichun Mo", "Yanchao Yang", "Hang Zhao", "Leonidas Guibas"], "keywords": ["Inter-object Functional Relationship", "Learning Interactive Policy for Exploration", "Interactive Perception", "3D Scene Understanding"], "abstract": "Building embodied intelligent agents that can interact with 3D indoor environments has received increasing research attention in recent years. While most works focus on single-object or agent-object visual functionality and affordances, our work proposes to study a novel, underexplored, kind of visual relations that is also important to perceive and model -- inter-object functional relationships (e.g., a switch on the wall turns on or off the light, a remote control operates the TV). Humans often spend no effort or only a little to infer these relationships, even when entering a new room, by using our strong prior knowledge (e.g., we know that buttons control electrical devices) or using only a few exploratory interactions in cases of uncertainty (e.g., multiple switches and lights in the same room). In this paper, we take the first step in building AI system learning inter-object functional relationships in 3D indoor environments with key technical contributions of modeling prior knowledge by training over large-scale scenes and designing interactive policies for effectively exploring the training scenes and quickly adapting to novel test scenes. We create a new dataset based on the AI2Thor and PartNet datasets and perform extensive experiments that prove the effectiveness of our proposed method.", "one-sentence_summary": "We formulate a novel problem of learning inter-object functional relationships in 3D indoor environments and propose a novel method that combines prior knowledge modeling and interactive policy learning to solve the task.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|ifrexplore_learning_interobject_functional_relationships_in_3d_indoor_scenes", "pdf": "/pdf/683c0979083ae6f6094f9293b8fdc7408ff7b1c6.pdf", "_bibtex": "@inproceedings{\nli2022ifrexplore,\ntitle={{IFR}-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes},\nauthor={QI LI and Kaichun Mo and Yanchao Yang and Hang Zhao and Leonidas Guibas},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OT3mLgR8Wg8}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "iEx3PiooLy", "original": "9xxkIjP37HT", "number": 162, "cdate": 1632875433398, "mdate": null, "ddate": null, "tcdate": 1632875433398, "tmdate": 1697934959190, "tddate": null, "forum": "iEx3PiooLy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects", "authorids": ["~Ruihai_Wu1", "~Yan_Zhao5", "~Kaichun_Mo1", "~Zizheng_Guo1", "~Yian_Wang1", "~Tianhao_Wu2", "~Qingnan_Fan2", "~Xuelin_Chen1", "~Leonidas_Guibas1", "~Hao_Dong3"], "authors": ["Ruihai Wu", "Yan Zhao", "Kaichun Mo", "Zizheng Guo", "Yian Wang", "Tianhao Wu", "Qingnan Fan", "Xuelin Chen", "Leonidas Guibas", "Hao Dong"], "keywords": ["Visual Representation Learning for Robotics", "Robotic Affordance and Trajectories", "3D Shape Understanding"], "abstract": "Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots. The space of 3D articulated objects is exceptionally rich in their myriad semantic categories, diverse shape geometry, and complicated part functionality. Previous works mostly abstract kinematic structure with estimated joint parameters and part poses as the visual representations for manipulating 3D articulated objects. In this paper, we propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals. We design an interaction-for-perception framework VAT-Mart to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes. Experiments prove the effectiveness of the proposed approach using the large-scale PartNet-Mobility dataset in SAPIEN environment and show promising generalization capabilities to novel test shapes, unseen object categories, and real-world data.", "one-sentence_summary": "We propose a novel interaction-for-perception framework to learn visual actionable representations (i.e. affordance and action trajectory proposals) for robotic manipulation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|vatmart_learning_visual_action_trajectory_proposals_for_manipulating_3d_articulated_objects", "pdf": "/pdf/5a365ba429a2d4c37365ec69749ea712c8ea6d5c.pdf", "supplementary_material": "/attachment/6e2db40294e936e65e4708c1fa1f985553ff0ab0.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.14440/code)", "_bibtex": "@inproceedings{\nwu2022vatmart,\ntitle={{VAT}-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D {ART}iculated Objects},\nauthor={Ruihai Wu and Yan Zhao and Kaichun Mo and Zizheng Guo and Yian Wang and Tianhao Wu and Qingnan Fan and Xuelin Chen and Leonidas Guibas and Hao Dong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iEx3PiooLy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "SsHBkfeRF9L", "original": "0gtTHCND40k", "number": 161, "cdate": 1632875433323, "mdate": null, "ddate": null, "tcdate": 1632875433323, "tmdate": 1676330685579, "tddate": null, "forum": "SsHBkfeRF9L", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Neural graphical modelling in continuous-time: consistency guarantees and algorithms", "authorids": ["~Alexis_Bellot1", "~Kim_Branson1", "~Mihaela_van_der_Schaar2"], "authors": ["Alexis Bellot", "Kim Branson", "Mihaela van der Schaar"], "keywords": ["Dynamical systems", "graphical modelling", "structure learning"], "abstract": "The discovery of structure from time series data is a key problem in fields of study working with complex systems. Most identifiability results and learning algorithms assume the underlying dynamics to be discrete in time. Comparatively few, in contrast, explicitly define dependencies in infinitesimal intervals of time, independently of the scale of observation and of the regularity of sampling. In this paper, we consider score-based structure learning for the study of dynamical systems. We prove that for vector fields parameterized in a large class of neural networks, least squares optimization with adaptive regularization schemes consistently recovers directed graphs of local independencies in systems of stochastic differential equations. Using this insight, we propose a score-based learning algorithm based on penalized Neural Ordinary Differential Equations (modelling the mean process) that we show to be applicable to the general setting of irregularly-sampled multivariate time series and to outperform the state of the art across a range of dynamical systems.", "pdf": "/pdf/53828fbbd7e3c6be425428ad3c70cdf5514e080a.pdf", "one-sentence_summary": "We present algorithms and consistency guarantees for graphical modelling in dynamical systems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bellot|neural_graphical_modelling_in_continuoustime_consistency_guarantees_and_algorithms", "_bibtex": "@inproceedings{\nbellot2022neural,\ntitle={Neural graphical modelling in continuous-time: consistency guarantees and algorithms},\nauthor={Alexis Bellot and Kim Branson and Mihaela van der Schaar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SsHBkfeRF9L}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 4}}, {"id": "K2JfSnLBD9", "original": "4eVVtxtaYbq", "number": 156, "cdate": 1632875432938, "mdate": null, "ddate": null, "tcdate": 1632875432938, "tmdate": 1697934960328, "tddate": null, "forum": "K2JfSnLBD9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks", "authorids": ["~Tianjun_Zhang1", "~Benjamin_Eysenbach1", "~Ruslan_Salakhutdinov1", "~Sergey_Levine1", "~Joseph_E._Gonzalez1"], "authors": ["Tianjun Zhang", "Benjamin Eysenbach", "Ruslan Salakhutdinov", "Sergey Levine", "Joseph E. Gonzalez"], "keywords": ["reinforcement learning", "planning", "variational inference", "curriculum learning", "waypoints", "subgoals"], "abstract": "Goal-conditioned reinforcement learning (RL) has shown great success recently at solving a wide range of tasks(e.g., navigation, robotic manipulation). However, learning to reach distant goals remains a central challenge to the field, and the task is particularly hard without any offline data, expert demonstrations, and reward shaping. In this paper, we propose to solve the distant goal-reaching task by using search at training time to generate a curriculum of intermediate states.  Specifically, we introduce the algorithm Classifier-Planning (C-Planning) by framing the learning of the goal-conditioned policies as variational inference. C-Planning naturally follows expectation maximization (EM): the E-step corresponds to planning an optimal sequence of waypoints using graph search, while the M-step aims to learn a goal-conditioned policy to reach those waypoints. One essential difficulty of designing such an algorithm is accurately modeling the distribution over way-points to sample from. In C-Planning, we propose to sample the waypoints using contrastive methods to learn a value function. Unlike prior methods that combine goal-conditioned RL with graph search, ours performs search only during training and not testing, significantly decreasing the compute costs of deploying the learned policy.  Empirically,  we demonstrate that our method not only improves the sample efficiency of prior methods but also successfully solves temporally extended navigation and manipulation tasks,  where prior goal-conditioned RL methods (including those based on graph search) fail to solve.", "one-sentence_summary": "An algorithm for goal-conditioned RL that uses an automatic curriculum of waypoints during exploration, derived from variational inference.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|cplanning_an_automatic_curriculum_for_learning_goalreaching_tasks", "pdf": "/pdf/94d8bdac5cba49cee62dd6963b40612d903f60af.pdf", "supplementary_material": "/attachment/c6c692d89e98ec00612f1634a297549444611a86.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.12080/code)", "_bibtex": "@inproceedings{\nzhang2022cplanning,\ntitle={C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks},\nauthor={Tianjun Zhang and Benjamin Eysenbach and Ruslan Salakhutdinov and Sergey Levine and Joseph E. Gonzalez},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=K2JfSnLBD9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "0DLwqQLmqV", "original": "Chk1r0lTpAm", "number": 150, "cdate": 1632875432473, "mdate": null, "ddate": null, "tcdate": 1632875432473, "tmdate": 1697934961290, "tddate": null, "forum": "0DLwqQLmqV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy", "authorids": ["~Yash_Mehta1", "~Colin_White1", "~Arber_Zela1", "~Arjun_Krishnakumar1", "~Guri_Zabergja1", "~Shakiba_Moradian1", "~Mahmoud_Safari1", "~Kaicheng_Yu1", "~Frank_Hutter1"], "authors": ["Yash Mehta", "Colin White", "Arber Zela", "Arjun Krishnakumar", "Guri Zabergja", "Shakiba Moradian", "Mahmoud Safari", "Kaicheng Yu", "Frank Hutter"], "keywords": ["neural architecture search", "AutoML"], "abstract": "The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search (NAS). Although they have been widely adopted and used to tune real-world NAS algorithms, these benchmarks are limited to small search spaces and focus solely on image classification. Recently, several new NAS benchmarks have been introduced that cover significantly larger search spaces over a wide range of tasks, including object detection, speech recognition, and natural language processing. However, substantial differences among these NAS benchmarks have so far prevented their widespread adoption, limiting researchers to using just a few benchmarks. In this work, we present an in-depth analysis of popular NAS algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, finding that many conclusions drawn from a few NAS benchmarks do \\emph{not} generalize to other benchmarks. To help remedy this problem, we introduce \\nasbs, a comprehensive and extensible collection of NAS benchmarks, accessible through a unified interface, created with the aim to facilitate reproducible, generalizable, and rapid NAS research. Our code is available at https://github.com/automl/naslib.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mehta|nasbenchsuite_nas_evaluation_is_now_surprisingly_easy", "pdf": "/pdf/3e2df1273dd62fef9940be12e0bdd7e3fd0a5b78.pdf", "one-sentence_summary": "We show that you cannot get away only with NAS-Bench-101 and -201; to fix this, we release a unified NAS benchmark suite with 25 benchmarks.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2201.13396/code)", "_bibtex": "@inproceedings{\nmehta2022nasbenchsuite,\ntitle={{NAS}-Bench-Suite: {NAS} Evaluation is (Now) Surprisingly Easy},\nauthor={Yash Mehta and Colin White and Arber Zela and Arjun Krishnakumar and Guri Zabergja and Shakiba Moradian and Mahmoud Safari and Kaicheng Yu and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0DLwqQLmqV}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "mhYUBYNoGz", "original": "Hok0aRT8xgM", "number": 146, "cdate": 1632875432168, "mdate": null, "ddate": null, "tcdate": 1632875432168, "tmdate": 1676330686535, "tddate": null, "forum": "mhYUBYNoGz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality", "authorids": ["~Yiping_Lu1", "~Haoxuan_Chen1", "~Jianfeng_Lu1", "~Lexing_Ying1", "~Jose_Blanchet1"], "authors": ["Yiping Lu", "Haoxuan Chen", "Jianfeng Lu", "Lexing Ying", "Jose Blanchet"], "keywords": ["Numerical PDE", "non-parametric statistics", "computational physics"], "abstract": "In this paper, we study the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify the problem, we focus on a prototype elliptic PDE: the Schr\\\"odinger equation on a hypercube with zero Dirichlet boundary condition, which has wide application in the quantum-mechanical systems. We establish upper and lower bounds for both methods, which improves upon concurrently developed upper bounds for this problem via a fast rate generalization bound. We discover that the current Deep Ritz Methods is sub-optimal and propose a modified version of it. We also prove that PINN and the modified version of DRM can achieve minimax optimal bounds over Sobolev spaces. Empirically, following recent work which has shown that the deep model accuracy will improve with growing training sets according to a power law, we supply computational experiments to show a similar behavior of dimension dependent power law for deep PDE solvers.", "pdf": "/pdf/f419ff83079cd27aae97a07c87ba18f22a355d19.pdf", "one-sentence_summary": "We provided min-max optimal convergence bound for machine learning based PDE solvers and numerically verified the scaling law.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lu|machine_learning_for_elliptic_pdes_fast_rate_generalization_bound_neural_scaling_law_and_minimax_optimality", "_bibtex": "@inproceedings{\nlu2022machine,\ntitle={Machine Learning For Elliptic {PDE}s: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality},\nauthor={Yiping Lu and Haoxuan Chen and Jianfeng Lu and Lexing Ying and Jose Blanchet},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mhYUBYNoGz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "pjqqxepwoMy", "original": "RpOGKzNPWzP", "number": 135, "cdate": 1632875431278, "mdate": null, "ddate": null, "tcdate": 1632875431278, "tmdate": 1676330686854, "tddate": null, "forum": "pjqqxepwoMy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Variational oracle guiding for reinforcement learning", "authorids": ["~Dongqi_Han1", "~Tadashi_Kozuno1", "~Xufang_Luo1", "~Zhao-Yun_Chen1", "~Kenji_Doya1", "~Yuqing_Yang1", "~Dongsheng_Li2"], "authors": ["Dongqi Han", "Tadashi Kozuno", "Xufang Luo", "Zhao-Yun Chen", "Kenji Doya", "Yuqing Yang", "Dongsheng Li"], "keywords": ["variational Bayes", "oracle guiding", "reinforcement learning", "decision making", "probabilistic modeling", "game", "Mahjong"], "abstract": "How to make intelligent decisions is a central problem in machine learning and artificial intelligence. Despite recent successes of deep reinforcement learning (RL) in various decision making problems, an important but under-explored aspect is how to leverage oracle observation (the information that is invisible during online decision making, but is available during offline training) to facilitate learning. For example, human experts will look at the replay after a Poker game, in which they can check the opponents' hands to improve their estimation of the opponents' hands from the visible information during playing. In this work, we study such problems based on Bayesian theory and derive an objective to leverage oracle observation in RL using variational methods. Our key contribution is to propose a general learning framework referred to as variational latent oracle guiding (VLOG) for DRL. VLOG is featured with preferable properties such as its robust and promising performance and its versatility to incorporate with any value-based DRL algorithm. We empirically demonstrate the effectiveness of VLOG in online and offline RL domains with tasks ranging from video games to a challenging tile-based game Mahjong. Furthermore, we publish the Mahjong environment and an offline RL dataset as a benchmark to facilitate future research on oracle guiding (https://github.com/Agony5757/mahjong).", "one-sentence_summary": "We propose a variational Bayes framework leveraging oracle (hindsight) information available in training to improve deep reinforcement learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "han|variational_oracle_guiding_for_reinforcement_learning", "pdf": "/pdf/88d5d80331a1704592f9a66d052c33f976661945.pdf", "supplementary_material": "/attachment/db5a510d63aee06fc1fadc9d11c429537c48581c.zip", "_bibtex": "@inproceedings{\nhan2022variational,\ntitle={Variational oracle guiding for reinforcement learning},\nauthor={Dongqi Han and Tadashi Kozuno and Xufang Luo and Zhao-Yun Chen and Kenji Doya and Yuqing Yang and Dongsheng Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pjqqxepwoMy}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 30}}, {"id": "XGzk5OKWFFc", "original": "r4FIMVX3_ti", "number": 129, "cdate": 1632875430798, "mdate": null, "ddate": null, "tcdate": 1632875430798, "tmdate": 1697934963194, "tddate": null, "forum": "XGzk5OKWFFc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation", "authorids": ["~Tongkun_Xu1", "~Weihua_Chen1", "~Pichao_WANG3", "~Fan_Wang6", "~Hao_Li16", "~Rong_Jin1"], "authors": ["Tongkun Xu", "Weihua Chen", "Pichao WANG", "Fan Wang", "Hao Li", "Rong Jin"], "keywords": [], "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the category level based UDA is the production of pseudo labels for samples in target domain, which are usually too noisy for accurate domain alignment, inevitably compromising the UDA performance.  With the success of Transformer in various tasks, we find that the cross-attention in Transformer is robust to the noisy input pairs for better feature alignment, thus in this paper Transformer is adopted for the challenging UDA task. Specifically, to generate accurate input pairs, we design a two-way center-aware labeling algorithm to produce pseudo labels for target samples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply self-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively. \nSuch design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously. The proposed method is dubbed CDTrans (cross-domain transformer), and it provides one of the first attempts to solve UDA tasks with a pure transformer solution. Experiments show that our proposed method achieves the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet. Code and models are available at https://github.com/CDTrans/CDTrans.", "pdf": "/pdf/5af495124dcff5b772396db65ab98725f7b036b7.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|cdtrans_crossdomain_transformer_for_unsupervised_domain_adaptation", "supplementary_material": "/attachment/62f785dd36c2c426747623fbb98295fc4f21bcff.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.06165/code)", "_bibtex": "@inproceedings{\nxu2022cdtrans,\ntitle={{CDT}rans: Cross-domain Transformer for Unsupervised Domain Adaptation},\nauthor={Tongkun Xu and Weihua Chen and Pichao WANG and Fan Wang and Hao Li and Rong Jin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XGzk5OKWFFc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "QkRV50TZyP", "original": "OUzIMIuXmjW", "number": 127, "cdate": 1632875430713, "mdate": null, "ddate": null, "tcdate": 1632875430713, "tmdate": 1697934963200, "tddate": null, "forum": "QkRV50TZyP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains", "authorids": ["~Qilong_Zhang2", "~Xiaodan_Li1", "~YueFeng_Chen1", "~Jingkuan_Song3", "~Lianli_Gao1", "~Yuan_He2", "~Hui_Xue'1"], "authors": ["Qilong Zhang", "Xiaodan Li", "YueFeng Chen", "Jingkuan Song", "Lianli Gao", "Yuan He", "Hui Xue'"], "keywords": ["practice black-box attack", "cross-domain transferability"], "abstract": "Adversarial examples have posed a severe threat to deep neural networks due to their transferable nature. Currently, various works have paid great efforts to enhance the cross-model transferability, which mostly assume the substitute model is trained in the same domain as the target model.\nHowever, in reality, the relevant information of the deployed model is unlikely to leak.\nHence, it is vital to build a more practical black-box threat model to overcome this limitation and evaluate the vulnerability of deployed models.\nIn this paper, with only the knowledge of the ImageNet domain, we propose a Beyond ImageNet Attack (BIA) to investigate the transferability towards black-box domains (unknown classification tasks). Specifically, we leverage a generative model to learn the adversarial function for disrupting low-level features of input images. \nBased on this framework, we further propose two variants to narrow the gap between the source and target domains from the data and model perspectives, respectively. Extensive experiments on coarse-grained and fine-grained domains demonstrate the effectiveness of our proposed methods. Notably,\nour methods outperform state-of-the-art approaches by up to 7.71\\% (towards coarse-grained domains) and 25.91\\% (towards fine-grained domains) on average. Our code is available at \\url{https://github.com/Alibaba-AAIG/Beyond-ImageNet-Attack}.", "pdf": "/pdf/e3edfd55ba8b10084cbe821c5e89dd92bb887262.pdf", "one-sentence_summary": "We propose an effective method that can craft adversarial examples for black-box domain.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|beyond_imagenet_attack_towards_crafting_adversarial_examples_for_blackbox_domains", "supplementary_material": "/attachment/3cbae3878dc7337b05d588f87e92aceacf18622a.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.11528/code)", "_bibtex": "@inproceedings{\nzhang2022beyond,\ntitle={Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains},\nauthor={Qilong Zhang and Xiaodan Li and YueFeng Chen and Jingkuan Song and Lianli Gao and Yuan He and Hui Xue'},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=QkRV50TZyP}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "k7efTb0un9z", "original": "NIOrD-KqSiB", "number": 124, "cdate": 1632875430459, "mdate": null, "ddate": null, "tcdate": 1632875430459, "tmdate": 1676330687266, "tddate": null, "forum": "k7efTb0un9z", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning to Schedule Learning rate with Graph Neural Networks", "authorids": ["~Yuanhao_Xiong1", "~Li-Cheng_Lan1", "~Xiangning_Chen1", "~Ruochen_Wang2", "~Cho-Jui_Hsieh1"], "authors": ["Yuanhao Xiong", "Li-Cheng Lan", "Xiangning Chen", "Ruochen Wang", "Cho-Jui Hsieh"], "keywords": ["learning rate scheduling", "graph neural networks"], "abstract": "Recent decades have witnessed great development of stochastic optimization in training deep neural networks. Learning rate scheduling is one of the most important factors that influence the performance of stochastic optimizers like Adam. Traditional methods seek to find a relatively proper scheduling among a limited number of pre-defined rules and might not accommodate a particular target problem. Instead, we propose a novel Graph-Network-based Scheduler (GNS), aiming at learning a specific scheduling mechanism without restrictions to existing principles. By constructing a directed graph for the underlying neural network of the target problem, GNS encodes current dynamics with a graph message passing network and trains an agent to control the learning rate accordingly via reinforcement learning. The proposed scheduler can capture the intermediate layer information while being able to generalize to problems of varying scales. Besides, an efficient reward collection procedure is leveraged to speed up training. We evaluate our framework on benchmarking datasets, Fashion-MNIST and CIFAR10 for image classification, and GLUE for language understanding. GNS shows consistent improvement over popular baselines when training CNN and Transformer models. Moreover, GNS demonstrates great generalization to different datasets and network structures.", "one-sentence_summary": "We propose a novel Graph-Network-based Scheduler (GNS), which is both informative to to encode rich information and generalizable to different architectures.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xiong|learning_to_schedule_learning_rate_with_graph_neural_networks", "pdf": "/pdf/70093080d93ffa224e3b7bb6cfa481cad374f005.pdf", "supplementary_material": "/attachment/c7578d2e34717acfb6760abd1890f01ec87201a2.zip", "data": "", "_bibtex": "@inproceedings{\nxiong2022learning,\ntitle={Learning to Schedule Learning rate with Graph Neural Networks},\nauthor={Yuanhao Xiong and Li-Cheng Lan and Xiangning Chen and Ruochen Wang and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=k7efTb0un9z}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "c-4HSDAWua5", "original": "65vafkeoCUW", "number": 119, "cdate": 1632875430052, "mdate": null, "ddate": null, "tcdate": 1632875430052, "tmdate": 1676330687741, "tddate": null, "forum": "c-4HSDAWua5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SketchODE: Learning neural sketch representation in continuous time", "authorids": ["~Ayan_Das1", "~Yongxin_Yang1", "~Timothy_Hospedales1", "~Tao_Xiang1", "~Yi-Zhe_Song2"], "authors": ["Ayan Das", "Yongxin Yang", "Timothy Hospedales", "Tao Xiang", "Yi-Zhe Song"], "keywords": ["Chirography", "Sketch", "Free-form", "Neural ODE"], "abstract": "Learning meaningful representations for chirographic drawing data such as sketches, handwriting, and flowcharts is a gateway for understanding and emulating human creative expression. Despite being inherently continuous-time data, existing works have treated these as discrete-time sequences, disregarding their true nature. In this work, we model such data as continuous-time functions and learn compact representations by virtue of Neural Ordinary Differential Equations. To this end, we introduce the first continuous-time Seq2Seq model and demonstrate some remarkable properties that set it apart from traditional discrete-time analogues. We also provide solutions for some practical challenges for such models, including introducing a family of parameterized ODE dynamics & continuous-time data augmentation particularly suitable for the task. Our models are validated on several datasets including VectorMNIST, DiDi and Quick, Draw!.", "one-sentence_summary": "Modelling continuous time chirographic structures like handwriting, diagrams, sketches etc with Neural Ordinary Differential Equations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "das|sketchode_learning_neural_sketch_representation_in_continuous_time", "pdf": "/pdf/9c21c733257184e97d7da6ff51910cd4e914fbb4.pdf", "_bibtex": "@inproceedings{\ndas2022sketchode,\ntitle={Sketch{ODE}: Learning neural sketch representation in continuous time},\nauthor={Ayan Das and Yongxin Yang and Timothy Hospedales and Tao Xiang and Yi-Zhe Song},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=c-4HSDAWua5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "HFPTzdwN39", "original": "L3HSk5mkzcc", "number": 118, "cdate": 1632875429975, "mdate": null, "ddate": null, "tcdate": 1632875429975, "tmdate": 1676330687880, "tddate": null, "forum": "HFPTzdwN39", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing", "authorids": ["~Iro_Laina1", "~Yuki_M_Asano1", "~Andrea_Vedaldi1"], "authors": ["Iro Laina", "Yuki M Asano", "Andrea Vedaldi"], "keywords": ["Representation learning", "Computer vision", "Interpretability"], "abstract": "Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i.e. understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters in representation space. This approach, which we call reverse linear probing, provides a single number sensitive to the semanticity of the representation. This measure is also able to detect when the representation contains combinations of concepts (e.g., \"red apple'') instead of just individual attributes (\"red'' and \"apple'' independently). Finally, we propose to use supervised classifiers to automatically label large datasets in order to enrich the space of concepts used for probing. We use our method to evaluate a large number of self-supervised representations, ranking them by interpretability, highlight the differences that emerge compared to the standard evaluation with linear probes and discuss several qualitative insights. Code at: https://github.com/iro-cp/ssl-qrp.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "laina|measuring_the_interpretability_of_unsupervised_representations_via_quantized_reversed_probing", "pdf": "/pdf/24d5777c7ce2fb03b77a283be66a63b1b952d589.pdf", "one-sentence_summary": "We propose quantized reverse probing as a information-theoretic measure to assess the degree to which self-supervised visual representations align with human-interpretable concepts. ", "data": "", "_bibtex": "@inproceedings{\nlaina2022measuring,\ntitle={Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing},\nauthor={Iro Laina and Yuki M Asano and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HFPTzdwN39}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "qjN4h_wwUO", "original": "8anOiO38Wwb", "number": 114, "cdate": 1632875429822, "mdate": null, "ddate": null, "tcdate": 1632875429822, "tmdate": 1697934964798, "tddate": null, "forum": "qjN4h_wwUO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GradMax: Growing Neural Networks using Gradient Information", "authorids": ["~Utku_Evci1", "~Bart_van_Merrienboer1", "~Thomas_Unterthiner1", "~Fabian_Pedregosa1", "~Max_Vladymyrov1"], "authors": ["Utku Evci", "Bart van Merrienboer", "Thomas Unterthiner", "Fabian Pedregosa", "Max Vladymyrov"], "keywords": ["efficient training", "efficient", "computer vision", "architecture search"], "abstract": "The architecture and the parameters of neural networks are often optimized independently, which requires costly retraining of the parameters whenever the architecture is modified. In this work we instead focus on growing the architecture without requiring costly retraining. We present a method that adds new neurons during training without impacting what is already learned, while improving the training dynamics. We achieve the latter by maximizing the gradients of the new weights  and  efficiently  find  the  optimal  initialization  by  means  of  the  singular value decomposition (SVD). We call this technique Gradient Maximizing Growth (GradMax) and demonstrate its effectiveness in variety of vision tasks and architectures. We open sourced our code at https://github.com/google-research/growneuron", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "evci|gradmax_growing_neural_networks_using_gradient_information", "pdf": "/pdf/217711e1fabb5f74caa7140f98b25bb2d847e75b.pdf", "one-sentence_summary": "We present a method that adds new neurons during training without impacting what is already learned, while improving the training dynamics.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.05125/code)", "_bibtex": "@inproceedings{\nevci2022gradmax,\ntitle={GradMax: Growing Neural Networks using Gradient Information},\nauthor={Utku Evci and Bart van Merrienboer and Thomas Unterthiner and Fabian Pedregosa and Max Vladymyrov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qjN4h_wwUO}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "f9D-5WNG4Nv", "original": "3I05Zb_AfLk", "number": 113, "cdate": 1632875429749, "mdate": null, "ddate": null, "tcdate": 1632875429749, "tmdate": 1676330687997, "tddate": null, "forum": "f9D-5WNG4Nv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Online Coreset Selection for Rehearsal-based Continual Learning", "authorids": ["~Jaehong_Yoon1", "~Divyam_Madaan1", "~Eunho_Yang1", "~Sung_Ju_Hwang1"], "authors": ["Jaehong Yoon", "Divyam Madaan", "Eunho Yang", "Sung Ju Hwang"], "keywords": ["Continual Learning"], "abstract": "A dataset is a shred of crucial evidence to describe a task. However, each data point in the dataset does not have the same potential, as some of the data points can be more representative or informative than others. This unequal importance among the data points may have a large impact in rehearsal-based continual learning, where we store a subset of the training examples (coreset) to be replayed later to alleviate catastrophic forgetting. In continual learning, the quality of the samples stored in the coreset directly affects the model's effectiveness and efficiency. The coreset selection problem becomes even more important under realistic settings, such as imbalanced continual learning or noisy data scenarios. To tackle this problem, we propose Online Coreset Selection (OCS), a simple yet effective method that selects the most representative and informative coreset at each iteration and trains them in an online manner. Our proposed method maximizes the model's adaptation to a target dataset while selecting high-affinity samples to past tasks, which directly inhibits catastrophic forgetting. We validate the effectiveness of our coreset selection mechanism over various standard, imbalanced, and noisy datasets against strong continual learning baselines, demonstrating that it improves task adaptation and prevents catastrophic forgetting in a sample-efficient manner. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yoon|online_coreset_selection_for_rehearsalbased_continual_learning", "pdf": "/pdf/2e384d1d48f24a63d604505e79eb70f45ecbfe91.pdf", "one-sentence_summary": "We propose Online Coreset Selection (OCS), a simple yet effective method that selects the most representative and informative coreset at each iteration and trains them in an online manner.", "supplementary_material": "/attachment/886ff124cff5783349d6831b7eee770158a15bef.zip", "data": "", "code": "", "_bibtex": "@inproceedings{\nyoon2022online,\ntitle={Online Coreset Selection for Rehearsal-based Continual Learning},\nauthor={Jaehong Yoon and Divyam Madaan and Eunho Yang and Sung Ju Hwang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=f9D-5WNG4Nv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 33}}, {"id": "H-iABMvzIc", "original": "9jD5YaWQIm", "number": 112, "cdate": 1632875429675, "mdate": null, "ddate": null, "tcdate": 1632875429675, "tmdate": 1676330688046, "tddate": null, "forum": "H-iABMvzIc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification", "authorids": ["~Zhengdong_Hu1", "~Yifan_Sun2", "~Yi_Yang22"], "authors": ["Zhengdong Hu", "Yifan Sun", "Yi Yang"], "keywords": [], "abstract": "This paper considers few-shot learning under the cross-domain scenario. The cross-domain setting imposes a critical challenge, i.e., using very few (support) samples to generalize the already-learned model to a novel domain. We hold a hypothesis, i.e., if a deep model is capable to fast generalize itself to different domains (using very few samples) during training, it will maintain such domain generalization capacity for testing. It motivates us to propose a novel Domain-Switch Learning (DSL) framework. DSL embeds the cross-domain scenario into the training stage in a ``fast switching'' manner. Specifically, DSL uses a single domain for a training iteration and switches into another domain for the following iteration. During the switching, DSL enforces two constraints: 1) the deep model should not over-fit the domain in the current iteration and 2) the deep model should not forget the already-learned knowledge of other domains. These two constraints jointly promote fast generalization across different domains. Experimental results confirm that the cross-domain generalization capacity can be inherited from the training stage to the testing stage, validating our key hypothesis. Consequentially, DSL significantly improves cross-domain few-shot classification and sets up new state of the art.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hu|switch_to_generalize_domainswitch_learning_for_crossdomain_fewshot_classification", "pdf": "/pdf/d34f743c2c676ac34213466790c222f7aa5a1b28.pdf", "data": "", "_bibtex": "@inproceedings{\nhu2022switch,\ntitle={Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification},\nauthor={Zhengdong Hu and Yifan Sun and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=H-iABMvzIc}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "RAW9tCdVxLj", "original": "j2WxIdmmd0", "number": 109, "cdate": 1632875429439, "mdate": null, "ddate": null, "tcdate": 1632875429439, "tmdate": 1676330688096, "tddate": null, "forum": "RAW9tCdVxLj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Zero-CL: Instance and Feature decorrelation for negative-free symmetric contrastive learning", "authorids": ["~Shaofeng_Zhang1", "~Feng_Zhu1", "~Junchi_Yan2", "~Rui_Zhao6", "~Xiaokang_Yang1"], "authors": ["Shaofeng Zhang", "Feng Zhu", "Junchi Yan", "Rui Zhao", "Xiaokang Yang"], "keywords": ["Self supervised learning", "representation learning"], "abstract": "For self-supervised contrastive learning, models can easily collapse and generate trivial constant solutions. The issue has been mitigated by recent improvement on objective design, which however often requires square complexity either for the size of instances ($\\mathcal{O}(N^{2})$) or feature dimensions ($\\mathcal{O}(d)^2$). To prevent such collapse, we develop two novel methods by decorrelating on different dimensions on the instance embedding stacking matrix, i.e., \\textbf{I}nstance-wise (ICL) and \\textbf{F}eature-wise (FCL) \\textbf{C}ontrastive \\textbf{L}earning. The proposed two methods (FCL, ICL) can be combined synthetically, called Zero-CL, where ``Zero'' means negative samples are \\textbf{zero} relevant, which allows Zero-CL to completely discard negative pairs i.e., with \\textbf{zero} negative samples. Compared with previous methods, Zero-CL mainly enjoys three advantages: 1) Negative free in symmetric architecture. 2) By whitening transformation, the correlation of the different features is equal to zero, alleviating information redundancy. 3) Zero-CL remains original information to a great extent after transformation, which improves the accuracy against other whitening transformation techniques. Extensive experimental results on CIFAR-10/100 and ImageNet show that Zero-CL outperforms or is on par with state-of-the-art symmetric contrastive learning methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|zerocl_instance_and_feature_decorrelation_for_negativefree_symmetric_contrastive_learning", "pdf": "/pdf/545b532dc138dbfc098e9384dbc805e96fceed40.pdf", "one-sentence_summary": "We develop two contrastive learning methods to prevent collapses in symmetric architecture without negative pairs.", "data": "", "_bibtex": "@inproceedings{\nzhang2022zerocl,\ntitle={Zero-{CL}: Instance and Feature decorrelation for negative-free symmetric contrastive learning},\nauthor={Shaofeng Zhang and Feng Zhu and Junchi Yan and Rui Zhao and Xiaokang Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RAW9tCdVxLj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "qwULHx9zld", "original": "KKDBZrJSBL6", "number": 104, "cdate": 1632875428929, "mdate": null, "ddate": null, "tcdate": 1632875428929, "tmdate": 1676330688470, "tddate": null, "forum": "qwULHx9zld", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Random matrices in service of ML footprint: ternary random features with no performance loss", "authorids": ["~Hafiz_Tiomoko_Ali1", "~Zhenyu_Liao1", "~Romain_Couillet1"], "authors": ["Hafiz Tiomoko Ali", "Zhenyu Liao", "Romain Couillet"], "keywords": ["Computationally efficient methods", "kernel methods", "random features", "random matrix theory"], "abstract": "In this article, we investigate the spectral behavior of random features kernel matrices of the type ${\\bf K} = \\mathbb{E}_{{\\bf w}} \\left[\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_i\\right)\\sigma\\left({\\bf w}^{\\sf T}{\\bf x}_j\\right)\\right]_{i,j=1}^n$, with nonlinear function $\\sigma(\\cdot)$, data ${\\bf x}_1, \\ldots, {\\bf x}_n \\in \\mathbb{R}^p$, and random projection vector ${\\bf w} \\in \\mathbb{R}^p$ having i.i.d. entries. In a high-dimensional setting where the number of data $n$ and their dimension $p$ are both large and comparable, we show, under a Gaussian mixture model for the data, that the eigenspectrum of ${\\bf K}$ is independent of the distribution of the i.i.d.(zero-mean and unit-variance) entries of ${\\bf w}$, and only depends on $\\sigma(\\cdot)$ via its (generalized) Gaussian moments $\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma'(z)]$ and $\\mathbb{E}_{z\\sim \\mathcal N(0,1)}[\\sigma''(z)]$. As a result, for any kernel matrix ${\\bf K}$ of the form above, we propose a novel random features technique, called Ternary Random Features (TRFs), that (i) asymptotically yields the same limiting kernel as the original ${\\bf K}$ in a spectral sense and (ii) can be computed and stored much more efficiently, by wisely tuning (in a data-dependent manner) the function $\\sigma$ and the random vector ${\\bf w}$, both taking values in $\\{-1,0,1\\}$. The computation of the proposed random features requires no multiplication, and a factor of $b$ times less bits for storage compared to classical random features such as random Fourier features, with $b$ the number of bits to store full precision values. Besides, it appears in our experiments on real data that the substantial gains in computation and storage are accompanied with somewhat improved performances compared to state-of-the-art random features methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ali|random_matrices_in_service_of_ml_footprint_ternary_random_features_with_no_performance_loss", "pdf": "/pdf/a1f6abc6863f0c0b0a7a4a0bedd62b6b5b103968.pdf", "one-sentence_summary": "A novel computational and storage efficient random features technique with no performance loss", "supplementary_material": "/attachment/4a8c158efe92889493fd4b09b921b0d957d7dcb2.zip", "data": "", "_bibtex": "@inproceedings{\nali2022random,\ntitle={Random matrices in service of {ML} footprint: ternary random features with no performance loss},\nauthor={Hafiz Tiomoko Ali and Zhenyu Liao and Romain Couillet},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qwULHx9zld}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "gfwON7rAm4", "original": "cUJBH7epw1V", "number": 102, "cdate": 1632875428777, "mdate": null, "ddate": null, "tcdate": 1632875428777, "tmdate": 1676330688806, "tddate": null, "forum": "gfwON7rAm4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games", "authorids": ["~Stefanos_Leonardos1", "~Will_Overman1", "~Ioannis_Panageas1", "~Georgios_Piliouras1"], "authors": ["Stefanos Leonardos", "Will Overman", "Ioannis Panageas", "Georgios Piliouras"], "keywords": ["Multi-agent Reinforcement Learning", "Markov Potential Games", "Policy Gradient"], "abstract": "Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination in which all agents utilities are perfectly aligned via a common potential function. Can this intuitive framework be transplanted in the setting of Markov games? What are the similarities and differences between multi-agent coordination with and without state dependence? To answer these questions, we study a natural class of Markov Potential Games (MPGs) that generalize prior attempts at capturing complex stateful multi-agent coordination. Counter-intuitively, insights from normal-form potential games do not carry over as MPGs involve settings where state-games can be zero-sum games. In the opposite direction, Markov games where every state-game is a potential game are not necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties such as the existence of deterministic Nash policies. In our main technical result, we prove convergence of independent policy gradient and its stochastic counterpart to Nash policies (polynomially fast in the approximation error) by adapting recent gradient dominance property arguments developed for single-agent Markov decision processes to multi-agent learning settings. \n", "one-sentence_summary": "Convergence of policy gradient in a class of MDPs called Markov Potential Games in which cooperation is desired.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "leonardos|global_convergence_of_multiagent_policy_gradient_in_markov_potential_games", "pdf": "/pdf/91e8010784f5d9c14cf26f0a2e86f8b46808d3ae.pdf", "supplementary_material": "/attachment/fd730c765d9a2bc5db1c20e681f030ddcbd8784d.zip", "_bibtex": "@inproceedings{\nleonardos2022global,\ntitle={Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games},\nauthor={Stefanos Leonardos and Will Overman and Ioannis Panageas and Georgios Piliouras},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gfwON7rAm4}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "gVRhIEajG1k", "original": "kigUG8pPrAf", "number": 101, "cdate": 1632875428699, "mdate": null, "ddate": null, "tcdate": 1632875428699, "tmdate": 1676330688899, "tddate": null, "forum": "gVRhIEajG1k", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Rethinking Adversarial Transferability from a Data Distribution Perspective", "authorids": ["~Yao_Zhu2", "~Jiacheng_Sun1", "~Zhenguo_Li1"], "authors": ["Yao Zhu", "Jiacheng Sun", "Zhenguo Li"], "keywords": ["Adversarial Attack", "Adversarial Transferability", "Black-box Attack"], "abstract": "Adversarial transferability enables attackers to generate adversarial examples from the source model to attack the target model, which has raised security concerns about the deployment of DNNs in practice. In this paper, we rethink adversarial transferability from a data distribution perspective and further enhance transferability by score matching based optimization. We identify that some samples with injecting small Gaussian noise can fool different target models, and their adversarial examples under different source models have much stronger transferability. We hypothesize that these samples are in the low-density region of the ground truth distribution where models are not well trained. To improve the attack success rate of adversarial examples, we match the adversarial attacks with the directions which effectively decrease the ground truth density. We propose Intrinsic Adversarial Attack (IAA), which smooths the activation function and decreases the impact of the later layers of a given normal model, to increase the alignment of adversarial attack and the gradient of joint data distribution. We conduct comprehensive transferable attacks against multiple DNNs and show that our IAA can boost the transferability of the crafted attacks in all cases and go beyond state-of-the-art methods.", "one-sentence_summary": "In this paper, we rethink adversarial transferability from a data distribution perspective and further enhance transferability by score matching based optimization. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhu|rethinking_adversarial_transferability_from_a_data_distribution_perspective", "pdf": "/pdf/d835f2b3333622a19adc0a569eaa27153c426c28.pdf", "_bibtex": "@inproceedings{\nzhu2022rethinking,\ntitle={Rethinking Adversarial Transferability from a Data Distribution Perspective},\nauthor={Yao Zhu and Jiacheng Sun and Zhenguo Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gVRhIEajG1k}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "KSugKcbNf9", "original": "bXzVwL4LWEy", "number": 96, "cdate": 1632875428316, "mdate": null, "ddate": null, "tcdate": 1632875428316, "tmdate": 1697934967520, "tddate": null, "forum": "KSugKcbNf9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Transformers Can Do Bayesian Inference", "authorids": ["~Samuel_M\u00fcller1", "noah.hollmann@charite.de", "~Sebastian_Pineda_Arango1", "~Josif_Grabocka1", "~Frank_Hutter1"], "authors": ["Samuel M\u00fcller", "Noah Hollmann", "Sebastian Pineda Arango", "Josif Grabocka", "Frank Hutter"], "keywords": [], "abstract": "Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classification problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efficient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classification for small tabular data sets, and few-shot image classification, demonstrating the generality of PFNs. Code and trained PFNs are released at https://github.com/automl/TransformersCanDoBayesianInference.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "m\u00fcller|transformers_can_do_bayesian_inference", "pdf": "/pdf/0eee14e64526e89aa03be830dcf8d0b5024fd405.pdf", "supplementary_material": "/attachment/35d6fda07da707d1db00c7217f6a9db24c6bb59b.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.10510/code)", "_bibtex": "@inproceedings{\nm{\\\"u}ller2022transformers,\ntitle={Transformers Can Do Bayesian Inference},\nauthor={Samuel M{\\\"u}ller and Noah Hollmann and Sebastian Pineda Arango and Josif Grabocka and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KSugKcbNf9}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "JJCjv4dAbyL", "original": "dwfkHAKEisO", "number": 90, "cdate": 1632875427930, "mdate": null, "ddate": null, "tcdate": 1632875427930, "tmdate": 1676330689403, "tddate": null, "forum": "JJCjv4dAbyL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Discrete Structured Variational Auto-Encoder using Natural Evolution Strategies", "authorids": ["~Alon_Berliner1", "~Guy_Rotman1", "~Yossi_Adi1", "~Roi_Reichart1", "~Tamir_Hazan1"], "authors": ["Alon Berliner", "Guy Rotman", "Yossi Adi", "Roi Reichart", "Tamir Hazan"], "keywords": ["structured prediction", "derivative-free optimization", "variational autoencoder"], "abstract": "Discrete variational auto-encoders (VAEs) are able to represent semantic latent spaces in generative learning. In many real-life settings, the discrete latent space consists of high-dimensional structures, and propagating gradients through the relevant structures often requires enumerating over an exponentially large latent space. Recently, various approaches were devised to propagate approximated gradients without enumerating over the space of possible structures. In this work, we use Natural Evolution Strategies (NES), a class of gradient-free black-box optimization algorithms, to learn discrete structured VAEs. The NES algorithms are computationally appealing as they estimate gradients with forward pass evaluations only, thus they do not require to propagate gradients through their discrete structures. We demonstrate empirically that optimizing discrete structured VAEs using NES is as effective as gradient-based approximations. Lastly, we prove NES converges for non-Lipschitz functions as appear in discrete structured VAEs.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "berliner|learning_discrete_structured_variational_autoencoder_using_natural_evolution_strategies", "pdf": "/pdf/31efa202599273e261068a5a41dffffc4fe9a922.pdf", "supplementary_material": "/attachment/bced5440302053278159398e732d35a70a01ed77.zip", "data": "", "_bibtex": "@inproceedings{\nberliner2022learning,\ntitle={Learning Discrete Structured Variational Auto-Encoder using Natural Evolution Strategies},\nauthor={Alon Berliner and Guy Rotman and Yossi Adi and Roi Reichart and Tamir Hazan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JJCjv4dAbyL}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "bCrdi4iVvv", "original": "u8EbFzs1Iw1", "number": 89, "cdate": 1632875427852, "mdate": null, "ddate": null, "tcdate": 1632875427852, "tmdate": 1697934968062, "tddate": null, "forum": "bCrdi4iVvv", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Features with Parameter-Free Layers", "authorids": ["~Dongyoon_Han1", "~YoungJoon_Yoo1", "~Beomyoung_Kim1", "~Byeongho_Heo1"], "authors": ["Dongyoon Han", "YoungJoon Yoo", "Beomyoung Kim", "Byeongho Heo"], "keywords": ["ImageNet", "efficient network architecture", "network design", "image classification"], "abstract": "Trainable layers such as convolutional building blocks are the standard network design choices by learning parameters to capture the global context through successive spatial operations. When designing an efficient network, trainable layers such as the depthwise convolution is the source of efficiency in the number of parameters and FLOPs, but there was little improvement to the model speed in practice. This paper argues that simple built-in parameter-free operations can be a favorable alternative to the efficient trainable layers replacing spatial operations in a network architecture. We aim to break the stereotype of organizing the spatial operations of building blocks into trainable layers. Extensive experimental analyses based on layer-level studies with fully-trained models and neural architecture searches are provided to investigate whether parameter-free operations such as the max-pool are functional. The studies eventually give us a simple yet effective idea for redesigning network architectures, where the parameter-free operations are heavily used as the main building block without sacrificing the model accuracy as much. Experimental results on the ImageNet dataset demonstrate that the network architectures with parameter-free operations could enjoy the advantages of further efficiency in terms of model speed, the number of the parameters, and FLOPs. Code and ImageNet pretrained models are available at https://github.com/naver-ai/PfLayer.\n\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "han|learning_features_with_parameterfree_layers", "pdf": "/pdf/45b77b67f9fd2463226093e385789c3f8167284d.pdf", "one-sentence_summary": "This paper introduces a new design paradigm rethinking a parameter-free operation as the main building block of network architecture.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2202.02777/code)", "_bibtex": "@inproceedings{\nhan2022learning,\ntitle={Learning Features with Parameter-Free Layers},\nauthor={Dongyoon Han and YoungJoon Yoo and Beomyoung Kim and Byeongho Heo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bCrdi4iVvv}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "LcF-EEt8cCC", "original": "D8nlIxsh7Fy", "number": 88, "cdate": 1632875427773, "mdate": null, "ddate": null, "tcdate": 1632875427773, "tmdate": 1676330689509, "tddate": null, "forum": "LcF-EEt8cCC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Denoising Likelihood Score Matching for Conditional Score-based Data Generation", "authorids": ["~Chen-Hao_Chao2", "~Wei-Fang_Sun1", "~Bo-Wun_Cheng1", "~Yi-Chen_Lo1", "~Chia-Che_Chang1", "~Yu-Lun_Liu2", "~Yu-Lin_Chang1", "~Chia-Ping_Chen2", "~Chun-Yi_Lee1"], "authors": ["Chen-Hao Chao", "Wei-Fang Sun", "Bo-Wun Cheng", "Yi-Chen Lo", "Chia-Che Chang", "Yu-Lun Liu", "Yu-Lin Chang", "Chia-Ping Chen", "Chun-Yi Lee"], "keywords": ["score-based generative model", "conditional sampling"], "abstract": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we theoretically formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidences show that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chao|denoising_likelihood_score_matching_for_conditional_scorebased_data_generation", "pdf": "/pdf/ccc930d6cefd7ea8811e858a55d6318d0ea95abb.pdf", "one-sentence_summary": "In this paper, we theoretically formulate a new training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. ", "data": "", "_bibtex": "@inproceedings{\nchao2022denoising,\ntitle={Denoising Likelihood Score Matching for Conditional Score-based Data Generation},\nauthor={Chen-Hao Chao and Wei-Fang Sun and Bo-Wun Cheng and Yi-Chen Lo and Chia-Che Chang and Yu-Lun Liu and Yu-Lin Chang and Chia-Ping Chen and Chun-Yi Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LcF-EEt8cCC}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "a7H7OucbWaU", "original": "0M784gsuBSs", "number": 80, "cdate": 1632875427161, "mdate": null, "ddate": null, "tcdate": 1632875427161, "tmdate": 1676330689837, "tddate": null, "forum": "a7H7OucbWaU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Memory Replay with Data Compression for Continual Learning", "authorids": ["~Liyuan_Wang1", "~Xingxing_Zhang3", "~Kuo_Yang1", "~Longhui_Yu1", "~Chongxuan_Li1", "~Lanqing_HONG1", "~Shifeng_Zhang5", "~Zhenguo_Li1", "~Yi_Zhong1", "~Jun_Zhu2"], "authors": ["Liyuan Wang", "Xingxing Zhang", "Kuo Yang", "Longhui Yu", "Chongxuan Li", "Lanqing HONG", "Shifeng Zhang", "Zhenguo Li", "Yi Zhong", "Jun Zhu"], "keywords": ["Continual Learning", "Memory Replay", "Data Compression"], "abstract": "Continual learning needs to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing work is mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression to reduce the storage cost of old training samples and thus increase their amount that can be stored in the memory buffer. Observing that the trade-off between the quality and quantity of compressed data is highly nontrivial for the efficacy of memory replay, we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate compression quality for currently-arrived training samples. In this way, using a naive data compression algorithm with a properly selected quality can largely boost recent strong baselines by saving more compressed data in a limited storage space. We extensively validate this across several benchmarks of class-incremental learning and in a realistic scenario of object detection for autonomous driving.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|memory_replay_with_data_compression_for_continual_learning", "pdf": "/pdf/f05469fd5cb6748c9852f8b46453a984b34fe0f6.pdf", "one-sentence_summary": "We propose memory replay with data compression, which is an important yet neglected baseline and a promising direction for continual learning.", "supplementary_material": "/attachment/6bc530a049dae5f81022889b174599d9ccc7c1cb.zip", "data": "", "_bibtex": "@inproceedings{\nwang2022memory,\ntitle={Memory Replay with Data Compression for Continual Learning},\nauthor={Liyuan Wang and Xingxing Zhang and Kuo Yang and Longhui Yu and Chongxuan Li and Lanqing HONG and Shifeng Zhang and Zhenguo Li and Yi Zhong and Jun Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=a7H7OucbWaU}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "LDAwu17QaJz", "original": "UkfNJfYEYSi", "number": 73, "cdate": 1632875426772, "mdate": null, "ddate": null, "tcdate": 1632875426772, "tmdate": 1676330689957, "tddate": null, "forum": "LDAwu17QaJz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MAML is a Noisy Contrastive Learner in Classification", "authorids": ["~Chia_Hsiang_Kao1", "~Wei-Chen_Chiu3", "~Pin-Yu_Chen1"], "authors": ["Chia Hsiang Kao", "Wei-Chen Chiu", "Pin-Yu Chen"], "keywords": ["Meta learning", "contrastive learning", "few shot learning"], "abstract": "Model-agnostic meta-learning (MAML) is one of the most popular and widely adopted meta-learning algorithms, achieving remarkable success in various learning problems. Yet, with the unique design of nested inner-loop and outer-loop updates, which govern the task-specific and meta-model-centric learning, respectively, the underlying learning objective of MAML remains implicit, impeding a more straightforward understanding of it. In this paper, we provide a new perspective of the working mechanism of MAML. We discover that MAML is analogous to a meta-learner using a supervised contrastive objective in classification. The query features are pulled towards the support features of the same class and against those of different classes. Such contrastiveness is experimentally verified via an analysis based on the cosine similarity. Moreover, we reveal that vanilla MAML has an undesirable interference term originating from the random initialization and the cross-task interaction. We thus propose a simple but effective technique, the zeroing trick, to alleviate the interference. Extensive experiments are conducted on both mini-ImageNet and Omniglot datasets to validate the consistent improvement brought by our proposed method.", "pdf": "/pdf/0139eec2530d6f2ebd6cb972ba3c98769ed552df.pdf", "one-sentence_summary": "The Model-agnostic meta learning (MAML) algorithm is a noisy supervised contrastive learner where the noise comes from random initialization and cross-task interference.", "supplementary_material": "/attachment/73980c12aa41f6303f739d0cd1e7b33ad13e5c8f.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kao|maml_is_a_noisy_contrastive_learner_in_classification", "_bibtex": "@inproceedings{\nkao2022maml,\ntitle={{MAML} is a Noisy Contrastive Learner in Classification},\nauthor={Chia Hsiang Kao and Wei-Chen Chiu and Pin-Yu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LDAwu17QaJz}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "afoV8W3-IYp", "original": "KWSqSyzQtuH", "number": 68, "cdate": 1632875426396, "mdate": null, "ddate": null, "tcdate": 1632875426396, "tmdate": 1676330690295, "tddate": null, "forum": "afoV8W3-IYp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning", "authorids": ["~Xiaojian_Ma1", "~Weili_Nie1", "~Zhiding_Yu1", "~Huaizu_Jiang1", "~Chaowei_Xiao2", "~Yuke_Zhu1", "~Song-Chun_Zhu1", "~Anima_Anandkumar1"], "authors": ["Xiaojian Ma", "Weili Nie", "Zhiding Yu", "Huaizu Jiang", "Chaowei Xiao", "Yuke Zhu", "Song-Chun Zhu", "Anima Anandkumar"], "keywords": ["visual relational reasoning", "representation learning", "systematic generalization"], "abstract": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ma|relvit_conceptguided_vision_transformer_for_visual_relational_reasoning", "pdf": "/pdf/9ae93c86cdada9dfdeef3cf2c7ee77363fe51c7b.pdf", "one-sentence_summary": "We propose a novel concept-feature dictionary to enable two new concept-guided auxiliary tasks, which largely improve the model performances on visual relational reasoning, especially for systematic generalization.", "data": "", "_bibtex": "@inproceedings{\nma2022relvit,\ntitle={RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning},\nauthor={Xiaojian Ma and Weili Nie and Zhiding Yu and Huaizu Jiang and Chaowei Xiao and Yuke Zhu and Song-Chun Zhu and Anima Anandkumar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=afoV8W3-IYp}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 4}}, {"id": "anbBFlX1tJ1", "original": "apX2fJX8cg", "number": 56, "cdate": 1632875425543, "mdate": null, "ddate": null, "tcdate": 1632875425543, "tmdate": 1676330690888, "tddate": null, "forum": "anbBFlX1tJ1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Boosted Curriculum Reinforcement Learning", "authorids": ["~Pascal_Klink2", "~Carlo_D'Eramo2", "~Jan_Peters3", "~Joni_Pajarinen2"], "authors": ["Pascal Klink", "Carlo D'Eramo", "Jan Peters", "Joni Pajarinen"], "keywords": ["reinforcement learning", "curriculum learning", "boosting", "residual learning"], "abstract": "Curriculum value-based reinforcement learning (RL) solves a complex target task by reusing action-values across a tailored sequence of related tasks of increasing difficulty. However, finding an exact way of reusing action-values in this setting is still a poorly understood problem. In this paper, we introduce the concept of boosting to curriculum value-based RL, by approximating the action-value function as a sum of residuals trained on each task. This approach, which we refer to as boosted curriculum reinforcement learning (BCRL), has the benefit of naturally increasing the representativeness of the functional space by adding a new residual each time a new task is presented. This procedure allows reusing previous action-values while promoting expressiveness of the action-value function. We theoretically study BCRL as an approximate value iteration algorithm, discussing advantages over regular curriculum RL in terms of approximation accuracy and convergence to the optimal action-value function. Finally, we provide detailed empirical evidence of the benefits of BCRL in problems requiring curricula for accurate action-value estimation and targeted exploration.", "one-sentence_summary": "A novel approach for curriculum RL that increases the representativeness of the functional space as new, increasingly complex, tasks from the curriculum are presented to the agent.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "klink|boosted_curriculum_reinforcement_learning", "pdf": "/pdf/066eed802f996fdef355f574e65bcaba5794933a.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nklink2022boosted,\ntitle={Boosted Curriculum Reinforcement Learning},\nauthor={Pascal Klink and Carlo D'Eramo and Jan Peters and Joni Pajarinen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=anbBFlX1tJ1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "w4cXZDDib1H", "original": "vegJXZ8jkj-", "number": 45, "cdate": 1632875424686, "mdate": null, "ddate": null, "tcdate": 1632875424686, "tmdate": 1697934971656, "tddate": null, "forum": "w4cXZDDib1H", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ViDT: An Efficient and Effective Fully Transformer-based Object Detector", "authorids": ["~Hwanjun_Song2", "~Deqing_Sun2", "~Sanghyuk_Chun1", "~Varun_Jampani2", "~Dongyoon_Han1", "~Byeongho_Heo1", "~Wonjae_Kim1", "~Ming-Hsuan_Yang1"], "authors": ["Hwanjun Song", "Deqing Sun", "Sanghyuk Chun", "Varun Jampani", "Dongyoon Han", "Byeongho Heo", "Wonjae Kim", "Ming-Hsuan Yang"], "keywords": ["object detection", "vision transformer", "detection transformer"], "abstract": "Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We release the code and trained models at https://github.com/naver-ai/vidt.", "one-sentence_summary": "We integrate vision and detection transformers to build an efficient  and effective fully transformer-based object detector. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "song|vidt_an_efficient_and_effective_fully_transformerbased_object_detector", "pdf": "/pdf/264491cc60f75b87764768f41d002c6f4bee883e.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.03921/code)", "_bibtex": "@inproceedings{\nsong2022vidt,\ntitle={Vi{DT}: An Efficient and Effective Fully Transformer-based Object Detector},\nauthor={Hwanjun Song and Deqing Sun and Sanghyuk Chun and Varun Jampani and Dongyoon Han and Byeongho Heo and Wonjae Kim and Ming-Hsuan Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=w4cXZDDib1H}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "5xEgrl_5FAJ", "original": "p8-m-AeCCB3", "number": 42, "cdate": 1632875424455, "mdate": null, "ddate": null, "tcdate": 1632875424455, "tmdate": 1697934971854, "tddate": null, "forum": "5xEgrl_5FAJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "BiBERT: Accurate Fully Binarized BERT", "authorids": ["~Haotong_Qin1", "~Yifu_Ding2", "~Mingyuan_Zhang1", "~Qinghua_YAN1", "~Aishan_Liu1", "dangqingqing@baidu.com", "~Ziwei_Liu1", "~Xianglong_Liu2"], "authors": ["Haotong Qin", "Yifu Ding", "Mingyuan Zhang", "Qinghua YAN", "Aishan Liu", "Qingqing Dang", "Ziwei Liu", "Xianglong Liu"], "keywords": ["Network Binarization", "Model Compression", "BERT", "NLP"], "abstract": "The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but is also computation and memory expensive. As one of the powerful compression approaches, binarization extremely reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit weight, embedding, and activation) usually suffer a significant performance drop, and there is rare study addressing this problem. In this paper, with the theoretical justification and empirical analysis, we identify that the severe performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks. Specifically, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a Direction-Matching Distillation (DMD) scheme to optimize the full binarized BERT accurately. Extensive experiments show that BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. As the first fully binarized BERT, our method yields impressive 56.3 times and 31.2 times saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios.", "one-sentence_summary": "Our BiBERT, for the first time, presents a promising route towards the accurate fully binarized BERT (with 1-bit weight, embedding, and activation) and gives impressive 56.3 times and 31.2 times saving on FLOPs and model size, respectively.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "qin|bibert_accurate_fully_binarized_bert", "pdf": "/pdf/09aef2ecce1fcaf41eaa870ad5afc7e4d3222dad.pdf", "supplementary_material": "/attachment/251338709ffa2ac9f911a7383a2772d80a7b5b6d.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.06390/code)", "_bibtex": "@inproceedings{\nqin2022bibert,\ntitle={Bi{BERT}: Accurate Fully Binarized {BERT}},\nauthor={Haotong Qin and Yifu Ding and Mingyuan Zhang and Qinghua YAN and Aishan Liu and Qingqing Dang and Ziwei Liu and Xianglong Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5xEgrl_5FAJ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 30}}, {"id": "tBIQEvApZK5", "original": "0Ulu1K1feNL", "number": 40, "cdate": 1632875424296, "mdate": null, "ddate": null, "tcdate": 1632875424296, "tmdate": 1676330691315, "tddate": null, "forum": "tBIQEvApZK5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Feature Kernel Distillation", "authorids": ["~Bobby_He1", "~Mete_Ozay1"], "authors": ["Bobby He", "Mete Ozay"], "keywords": ["Knowledge distillation", "Neural Network (NN) Feature learning", "ensembling NNs", "Deep learning fundamentals", "Image classification"], "abstract": "Trained Neural Networks (NNs) can be viewed as data-dependent kernel machines, with predictions determined by the inner product of last-layer representations across inputs, referred to as the feature kernel. We explore the relevance of the feature kernel for Knowledge Distillation (KD), using a mechanistic understanding of an NN\u2019s optimisation process. We extend the theoretical analysis of Allen-Zhu & Li (2020) to show that a trained NN\u2019s feature kernel is highly dependent on its parameter initialisation, which biases different initialisations of the same architecture to learn different data attributes in a multi-view data setting. This enables us to prove that KD using only pairwise feature kernel comparisons can improve NN test accuracy in such settings, with both single & ensemble teacher models, whereas standard training without KD fails to generalise. We further use our theory to motivate practical considerations for improving student generalisation when using distillation with feature kernels, which allows us to propose a novel approach: Feature Kernel Distillation (FKD). Finally, we experimentally corroborate our theory in the image classification setting, showing that FKD is amenable to ensemble distillation, can transfer knowledge across datasets, and outperforms both vanilla KD & other feature kernel based KD baselines across a range of standard architectures & datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "he|feature_kernel_distillation", "pdf": "/pdf/60c40aa9a9f7d00701fc1025d8f2d4245ad5ced2.pdf", "one-sentence_summary": "A feature-learning perspective of (ensemble) Knowledge Distillation (KD) in Neural Networks to propose a new method (FKD), with both theoretical & experimental results demonstrating FKD's advantages over standard KD baselines.", "_bibtex": "@inproceedings{\nhe2022feature,\ntitle={Feature Kernel Distillation},\nauthor={Bobby He and Mete Ozay},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tBIQEvApZK5}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "-ngwPqanCEZ", "original": "TM7YjUV4NBE", "number": 39, "cdate": 1632875424220, "mdate": null, "ddate": null, "tcdate": 1632875424220, "tmdate": 1697934972454, "tddate": null, "forum": "-ngwPqanCEZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Representation-Agnostic Shape Fields", "authorids": ["~Xiaoyang_Huang1", "~Jiancheng_Yang3", "~Yanjun_Wang1", "~Ziyu_Chen2", "~Linguo_Li1", "~Teng_Li2", "~Bingbing_Ni3", "~Wenjun_Zhang3"], "authors": ["Xiaoyang Huang", "Jiancheng Yang", "Yanjun Wang", "Ziyu Chen", "Linguo Li", "Teng Li", "Bingbing Ni", "Wenjun Zhang"], "keywords": ["shape embedding", "3D deep learning", "shape classification and segmentation"], "abstract": "3D shape analysis has been widely explored in the era of deep learning. Numerous models have been developed for various 3D data representation formats, e.g., MeshCNN for meshes, PointNet for point clouds and VoxNet for voxels. In this study, we present Representation-Agnostic Shape Fields (RASF), a generalizable and computation-efficient shape embedding module for 3D deep learning. RASF is implemented with a learnable 3D grid with multiple channels to store local geometry. Based on RASF, shape embeddings for various 3D shape representations (point clouds, meshes and voxels) are retrieved by coordinate indexing. While there are multiple ways to optimize the learnable parameters of RASF, we provide two effective schemes among all in this paper for RASF pre-training: shape reconstruction and normal estimation. Once trained, RASF becomes a plug-and-play performance booster with negligible cost. Extensive experiments on diverse 3D representation formats, networks and applications, validate the universal effectiveness of the proposed RASF. Code and pre-trained models are publicly available\\footnote{\\url{https://github.com/seanywang0408/RASF}}.", "pdf": "/pdf/3dd6d2a26afbbd24597f42af0a520b3b487fc536.pdf", "one-sentence_summary": "We propose a  generalizable and computation-efficient shape embedding layer for 3D deep learning, named Representation-Agnostic Shape Fields (RASF), to improve performance across different representations, backbones and down-stream tasks", "supplementary_material": "/attachment/77e2838b3b479e8a98a2b8f6f40ba11760b5e4dc.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|representationagnostic_shape_fields", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.10259/code)", "_bibtex": "@inproceedings{\nhuang2022representationagnostic,\ntitle={Representation-Agnostic Shape Fields},\nauthor={Xiaoyang Huang and Bingbing Ni and Jiancheng Yang and Yanjun Wang and Ziyu Chen and Linguo Li and Teng Li and Wenjun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-ngwPqanCEZ}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "C1_esHN6AVn", "original": "50eEK9BvVt9", "number": 31, "cdate": 1632875423600, "mdate": null, "ddate": null, "tcdate": 1632875423600, "tmdate": 1676330692045, "tddate": null, "forum": "C1_esHN6AVn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Synthetic Environments and Reward Networks for Reinforcement Learning", "authorids": ["~Fabio_Ferreira1", "~Thomas_Nierhoff1", "~Andreas_S\u00e4linger1", "~Frank_Hutter1"], "authors": ["Fabio Ferreira", "Thomas Nierhoff", "Andreas S\u00e4linger", "Frank Hutter"], "keywords": ["Synthetic Environments", "Synthetic Data", "Meta-Learning", "Reinforcement Learning", "Evolution Strategies", "Reward Shaping"], "abstract": "We introduce Synthetic Environments (SEs) and Reward Networks (RNs), represented by neural networks, as proxy environment models for training Reinforcement Learning (RL) agents. We show that an agent, after being trained exclusively on the SE, is able to solve the corresponding real environment. While an SE acts as a full proxy to a real environment by learning about its state dynamics and rewards, an RN is a partial proxy that learns to augment or replace rewards. We use bi-level optimization to evolve SEs and RNs: the inner loop trains the RL agent, and the outer loop trains the parameters of the SE / RN via an evolution strategy. We evaluate our proposed new concept on a broad range of RL algorithms and classic control environments. In a one-to-one comparison, learning an SE proxy requires more interactions with the real environment than training agents only on the real environment. However, once such an SE has been learned, we do not need any interactions with the real environment to train new agents. Moreover, the learned SE proxies allow us to train agents with fewer interactions while maintaining the original task performance. Our empirical results suggest that SEs achieve this result by learning informed representations that bias the agents towards relevant states. Moreover, we find that these proxies are robust against hyperparameter variation and can also transfer to unseen agents.", "one-sentence_summary": "We propose an evolution-based approach to meta-learn synthetic neural environments and reward neural networks for reinforcement learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ferreira|learning_synthetic_environments_and_reward_networks_for_reinforcement_learning", "pdf": "/pdf/706b0f26d6790fba3e56e1ae94751cce8f4a0789.pdf", "_bibtex": "@inproceedings{\nferreira2022learning,\ntitle={Learning Synthetic Environments and Reward Networks for Reinforcement Learning},\nauthor={Fabio Ferreira and Thomas Nierhoff and Andreas S{\\\"a}linger and Frank Hutter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=C1_esHN6AVn}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "NH29920YEmj", "original": "UIeA47iqbCK", "number": 30, "cdate": 1632875423524, "mdate": null, "ddate": null, "tcdate": 1632875423524, "tmdate": 1676330692230, "tddate": null, "forum": "NH29920YEmj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Who Is Your Right Mixup Partner in Positive and Unlabeled Learning", "authorids": ["~Changchun_Li1", "~Ximing_Li1", "~Lei_Feng1", "~Jihong_Ouyang2"], "authors": ["Changchun Li", "Ximing Li", "Lei Feng", "Jihong Ouyang"], "keywords": ["Positive and Unlabeled Learning", "Mixup", "Heuristic"], "abstract": "Positive and Unlabeled (PU) learning targets inducing a binary classifier from weak training datasets of positive and unlabeled instances, which arise in many real-world applications. In this paper, we propose a novel PU learning method, namely Positive and unlabeled learning with Partially Positive Mixup (P3Mix), which simultaneously benefits from data augmentation and supervision correction with a heuristic mixup technique. To be specific, we take inspiration from the directional boundary deviation phenomenon observed in our preliminary experiments, where the learned PU boundary tends to deviate from the fully supervised boundary towards the positive side. For the unlabeled instances with ambiguous predictive results, we select their mixup partners from the positive instances around the learned PU boundary, so as to transform them into augmented instances near to the boundary yet with more precise supervision. Accordingly, those augmented instances may push the learned PU boundary towards the fully supervised boundary, thereby improving the classification performance. Comprehensive experimental results demonstrate the effectiveness of the heuristic mixup technique in PU learning and show that P3Mix can consistently outperform the state-of-the-art PU learning methods.", "pdf": "/pdf/d687929225415551919e783b0c52f61382054101.pdf", "one-sentence_summary": "We propose a novel PU learning method named P3Mix which simultaneously benefits from instance augmentation and supervision correction with a heuristic mixup technique.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|who_is_your_right_mixup_partner_in_positive_and_unlabeled_learning", "supplementary_material": "/attachment/cd51230305ce5cc0e120580e58977ba7f4a7b903.zip", "data": "", "_bibtex": "@inproceedings{\nli2022who,\ntitle={Who Is Your Right Mixup Partner in Positive and Unlabeled Learning},\nauthor={Changchun Li and Ximing Li and Lei Feng and Jihong Ouyang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NH29920YEmj}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "dDjSKKA5TP1", "original": "U-NczbO38_J", "number": 25, "cdate": 1632875423289, "mdate": null, "ddate": null, "tcdate": 1632875423289, "tmdate": 1697934973310, "tddate": null, "forum": "dDjSKKA5TP1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Incremental False Negative Detection for Contrastive Learning", "authorids": ["~Tsai-Shien_Chen1", "~Wei-Chih_Hung1", "~Hung-Yu_Tseng2", "~Shao-Yi_Chien1", "~Ming-Hsuan_Yang1"], "authors": ["Tsai-Shien Chen", "Wei-Chih Hung", "Hung-Yu Tseng", "Shao-Yi Chien", "Ming-Hsuan Yang"], "keywords": ["Self-supervised learning", "Contrastive learning", "Representation learning", "Clustering-based learning"], "abstract": "Self-supervised learning has recently shown great potential in vision tasks through contrastive learning, which aims to discriminate each image, or instance, in the dataset. However, such instance-level learning ignores the semantic relationship among instances and sometimes undesirably repels the anchor from the semantically similar samples, termed as \"false negatives\". In this work, we show that the unfavorable effect from false negatives is more significant for the large-scale datasets with more semantic concepts. To address the issue, we propose a novel self-supervised contrastive learning framework that incrementally detects and explicitly removes the false negative samples. Specifically, following the training process, our method dynamically detects increasing high-quality false negatives considering that the encoder gradually improves and the embedding space becomes more semantically structural. Next, we discuss two strategies to explicitly remove the detected false negatives during contrastive learning. Extensive experiments show that our framework outperforms other self-supervised contrastive learning methods on multiple benchmarks in a limited resource setup.", "pdf": "/pdf/bb320db0548af9f9dda16d602425fa7632befbb0.pdf", "one-sentence_summary": "This paper explores the effect of false negative samples in self-supervised contrastive learning and introduce a framework to incrementally detect and explicitly remove the false negatives.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|incremental_false_negative_detection_for_contrastive_learning", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2106.03719/code)", "_bibtex": "@inproceedings{\nchen2022incremental,\ntitle={Incremental False Negative Detection for Contrastive Learning},\nauthor={Tsai-Shien Chen and Wei-Chih Hung and Hung-Yu Tseng and Shao-Yi Chien and Ming-Hsuan Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dDjSKKA5TP1}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "rJvY_5OzoI", "original": "t-UCz7Z7ZvB", "number": 14, "cdate": 1632875422492, "mdate": null, "ddate": null, "tcdate": 1632875422492, "tmdate": 1676330692502, "tddate": null, "forum": "rJvY_5OzoI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multi-Critic Actor Learning: Teaching RL Policies to Act with Style", "authorids": ["~Siddharth_Mysore1", "gecheng@ea.com", "yuzhao@ea.com", "~Kate_Saenko1", "febmeng@gmail.com"], "authors": ["Siddharth Mysore", "George Cheng", "Yunqi Zhao", "Kate Saenko", "Meng Wu"], "keywords": ["Reinforcement Learning", "Multi-Style Learning", "Multi-Task Learning", "Actor-Critic"], "abstract": "Using a single value function (critic) shared over multiple tasks in Actor-Critic multi-task reinforcement learning (MTRL) can result in negative interference between tasks, which can compromise learning performance. Multi-Critic Actor Learning (MultiCriticAL) proposes instead maintaining separate critics for each task being trained while training a single multi-task actor. Explicitly distinguishing between tasks also eliminates the need for critics to learn to do so and mitigates interference between task-value estimates. MultiCriticAL is tested in the context of multi-style learning, a special case of MTRL where agents are trained to behave with different distinct behavior styles, and yields up to 56% performance gains over the single-critic baselines and even successfully learns behavior styles in cases where single-critic approaches may simply fail to learn. In a simulated real-world use case, MultiCriticAL enables learning policies that smoothly transition between multiple fighting styles on an experimental build of EA\u2019s UFC game.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mysore|multicritic_actor_learning_teaching_rl_policies_to_act_with_style", "pdf": "/pdf/9ba47239c3de56ba46f2cac821cf17c1791a8444.pdf", "one-sentence_summary": "MultiCriticAL is a single-actor, multi-critic framework for multi-task reinforcement learning, where task-based critic separation provides explicit per-task value-function approximation and enables improved performance over single-critic frameworks.", "supplementary_material": "/attachment/d860f607da2c6fec97bfa5a71e0350d3ae203047.zip", "_bibtex": "@inproceedings{\nmysore2022multicritic,\ntitle={Multi-Critic Actor Learning: Teaching {RL} Policies to Act with Style},\nauthor={Siddharth Mysore and George Cheng and Yunqi Zhao and Kate Saenko and Meng Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=rJvY_5OzoI}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "kezNJydWvE", "original": "ssOM_oeFhux", "number": 12, "cdate": 1632875422323, "mdate": null, "ddate": null, "tcdate": 1632875422323, "tmdate": 1676330692550, "tddate": null, "forum": "kezNJydWvE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring", "authorids": ["~Seungjun_Nah1", "~Sanghyun_Son1", "~Jaerin_Lee1", "~Kyoung_Mu_Lee2"], "authors": ["Seungjun Nah", "Sanghyun Son", "Jaerin Lee", "Kyoung Mu Lee"], "keywords": ["Deblur", "Reblur", "Loss", "Test-time adaptation", "Self-supervised"], "abstract": "The goal of dynamic scene deblurring is to remove the motion blur in a given image. Typical learning-based approaches implement their solutions by minimizing the L1 or L2 distance between the output and the reference sharp image. Recent attempts adopt visual recognition features in training to improve the perceptual quality. However, those features are primarily designed to capture high-level contexts rather than low-level structures such as blurriness. Instead, we propose a more direct way to make images sharper by exploiting the inverse task of deblurring, namely, reblurring. Reblurring amplifies the remaining blur to rebuild the original blur, however, a well-deblurred clean image with zero-magnitude blur is hard to reblur. Thus, we design two types of reblurring loss functions for better deblurring. The supervised reblurring loss at training stage compares the amplified blur between the deblurred and the sharp images. The self-supervised reblurring loss at inference stage inspects if noticeable blur remains in the deblurred. Our experimental results on large-scale benchmarks and real images demonstrate the effectiveness of the reblurring losses in improving the perceptual quality of the deblurred images in terms of NIQE and LPIPS scores as well as visual sharpness.", "one-sentence_summary": "Reblurring, the inverse task of deblurring, is used for supervised/self-supervised learning of deblurring and improves the image sharpness.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nah|clean_images_are_hard_to_reblur_exploiting_the_illposed_inverse_task_for_dynamic_scene_deblurring", "pdf": "/pdf/1636f146bd4ebc888881dafbf13129a2072ba805.pdf", "supplementary_material": "/attachment/a879a11996948496162b67206db66b6fc0c6357f.zip", "data": "", "_bibtex": "@inproceedings{\nnah2022clean,\ntitle={Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring},\nauthor={Seungjun Nah and Sanghyun Son and Jaerin Lee and Kyoung Mu Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kezNJydWvE}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "j-63FSNcO5a", "original": "sK7HkhtXbj9", "number": 11, "cdate": 1632875422247, "mdate": null, "ddate": null, "tcdate": 1632875422247, "tmdate": 1676330692599, "tddate": null, "forum": "j-63FSNcO5a", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Disentangled Representation by Exploiting Pretrained Generative Models:  A Contrastive Learning View", "authorids": ["~Xuanchi_Ren1", "~Tao_Yang9", "~Yuwang_Wang3", "~Wenjun_Zeng3"], "authors": ["Xuanchi Ren", "Tao Yang", "Yuwang Wang", "Wenjun Zeng"], "keywords": ["Latent space discovery", "Disentangled representation learning", "Generative models", "Contrastive learning"], "abstract": "From the intuitive notion of disentanglement, the image variations corresponding to different generative factors should be distinct from each other, and the disentangled representation should reflect those variations with separate dimensions. To discover the generative factors and learn disentangled representation, previous methods typically leverage an extra regularization term when learning to generate realistic images. However, the term usually results in a trade-off between disentanglement and generation quality. For the generative models pretrained without any disentanglement term, the generated images show semantically meaningful variations when traversing along different directions in the latent space. Based on this observation, we argue that it is possible to mitigate the trade-off by (i) leveraging the pretrained generative models with high generation quality, (ii) focusing on discovering the traversal directions as generative factors for disentangled representation learning. To achieve this, we propose Disentaglement via Contrast (DisCo) as a framework to model the variations based on the target disentangled representations, and contrast the variations to jointly discover disentangled directions and learn disentangled representations. DisCo achieves the state-of-the-art disentangled representation learning and distinct direction discovering, given pretrained non-disentangled generative models including GAN, VAE, and Flow. Source code is at https://github.com/xrenaa/DisCo.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ren|learning_disentangled_representation_by_exploiting_pretrained_generative_models_a_contrastive_learning_view", "pdf": "/pdf/e69c1d9f39564cc023d156cc675d4675ad2da583.pdf", "one-sentence_summary": "DisCo is a new contrastive learning framework to leverage pretrained generative models to jointly learn disentangled representation and discover disentangled directions in the latent space.", "data": "", "_bibtex": "@inproceedings{\nren2022learning,\ntitle={Learning Disentangled Representation by Exploiting Pretrained Generative Models:  A Contrastive Learning View},\nauthor={Xuanchi Ren and Tao Yang and Yuwang Wang and Wenjun Zeng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=j-63FSNcO5a}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "YgPqNctmyd", "original": "uE-HOE56ErH", "number": 10, "cdate": 1632875422167, "mdate": null, "ddate": null, "tcdate": 1632875422167, "tmdate": 1676330692668, "tddate": null, "forum": "YgPqNctmyd", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Building A Group-based Unsupervised Representation Disentanglement Framework", "authorids": ["~Tao_Yang9", "~Xuanchi_Ren1", "~Yuwang_Wang3", "~Wenjun_Zeng3", "~Nanning_Zheng1"], "authors": ["Tao Yang", "Xuanchi Ren", "Yuwang Wang", "Wenjun Zeng", "Nanning Zheng"], "keywords": ["Disentangled representation learning", "Group theory", "VAE"], "abstract": "Disentangled representation learning is one of the major goals of deep learning, and is a key step for achieving explainable and generalizable models. The key idea of the state-of-the-art VAE-based unsupervised representation disentanglement methods is to minimize the total correlation of the joint distribution of the latent variables. However, it has been proved that their goal can not be achieved without introducing other inductive biases. The Group Theory based definition of representation disentanglement mathematically connects the data transformations to the representations using the formalism of group. In this paper, built on the group-based definition and inspired by the \\emph{n-th dihedral group}, we first propose a theoretical framework towards achieving unsupervised representation disentanglement. We then propose a model based on existing VAE-based methods to tackle the unsupervised learning problem of the framework. In the theoretical framework, we prove three sufficient conditions on model, group structure, and data respectively in an effort to achieve, in an unsupervised way, disentangled representation per group-based definition. With these conditions, we offer an option, from the perspective of the group-based definition, for the inductive bias that existing VAE-based models lack. Experimentally, we train 1800 models covering the most prominent VAE-based methods on five datasets to verify the effectiveness of our theoretical framework. Compared to the original VAE-based methods, these Groupified VAEs consistently achieve better mean performance with smaller variances.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|towards_building_a_groupbased_unsupervised_representation_disentanglement_framework", "pdf": "/pdf/5394844d0eb50251cdb0af3ec98822f39236eb03.pdf", "one-sentence_summary": "In this paper, built on the group-based definition and inspired by the n-th dihedral group, we first propose a theoretical framework towards achieving unsupervised representation disentanglement.", "data": "", "_bibtex": "@inproceedings{\nyang2022towards,\ntitle={Towards Building A Group-based Unsupervised Representation Disentanglement Framework},\nauthor={Tao Yang and Xuanchi Ren and Yuwang Wang and Wenjun Zeng and Nanning Zheng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YgPqNctmyd}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 32}}, {"id": "AjGC97Aofee", "original": "3bK5ftlgbD9", "number": 9, "cdate": 1632875422076, "mdate": null, "ddate": null, "tcdate": 1632875422076, "tmdate": 1676330692782, "tddate": null, "forum": "AjGC97Aofee", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning", "authorids": ["~Yulun_Zhang1", "~Huan_Wang3", "~Can_Qin1", "~Yun_Fu1"], "authors": ["Yulun Zhang", "Huan Wang", "Can Qin", "Yun Fu"], "keywords": ["image super-resolution"], "abstract": "Several image super-resolution (SR) networks have been proposed of late for efficient SR, achieving promising results. However, they are still not lightweight enough and neglect to be extended to larger networks. At the same time, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly because filter pruning for residual blocks is well-known tricky. To address the above issues, we propose structure-regularized pruning (SRP), which imposes regularization on the pruned structure to ensure the locations of pruned filters are aligned across different layers. Specifically, for the layers connected by the same residual, we select the filters of the same indices as unimportant filters. To transfer the expressive power in the unimportant filters to the rest of the network, we employ $L_2$ regularization to drive the weights towards zero so that eventually, their absence will cause minimal performance degradation. We apply SRP to train efficient image SR networks, resulting in a lightweight network SRPN-Lite and a very deep one SRPN. We conduct extensive comparisons with both lightweight and larger networks. SRPN-Lite and SRPN perform favorably against other recent efficient SR approaches quantitatively and visually.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|learning_efficient_image_superresolution_networks_via_structureregularized_pruning", "pdf": "/pdf/538af917ae93b620773d2ce7ce65f827922d0e3f.pdf", "one-sentence_summary": "Learning efficient compressed models for bother lightweight and large image super-resolution networks", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nzhang2022learning,\ntitle={Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning},\nauthor={Yulun Zhang and Huan Wang and Can Qin and Yun Fu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AjGC97Aofee}\n}", "venue": "ICLR 2022 Poster", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}], "count": 864}