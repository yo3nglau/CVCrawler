{"notes": [{"id": "H21qm4xyk9", "number": 6792, "cdate": 1674763202738, "tcdate": 1674763202738, "mdate": 1687337081702, "tmdate": 1687337081702, "signatures": ["ICML.cc/2023/Conference/Submission6792/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6792/Authors"], "forum": "H21qm4xyk9", "content": {"title": {"value": "Taming graph kernels with random features"}, "authors": {"value": ["Krzysztof Marcin Choromanski"]}, "authorids": {"value": ["~Krzysztof_Marcin_Choromanski1"]}, "abstract": {"value": "We introduce in this paper the mechanism of graph random features (GRFs). GRFs can be used to construct unbiased randomized estimators of several important kernels defined on graphs' nodes, in particular the regularized Laplacian kernel. As regular RFs for non-graph kernels, they provide means to scale up kernel methods defined on graphs to larger networks. Importantly, they give substantial computational gains also for smaller graphs, while applied in downstream applications. Consequently, GRFs address the notoriously difficult problem of cubic (in the number of the nodes of the graph) time complexity of graph kernels algorithms. We provide a detailed theoretical analysis of GRFs and an extensive empirical evaluation: from speed tests, through Frobenius relative error analysis to kmeans graph-clustering with graph kernels. We show that the computation of GRFs admits an embarrassingly simple distributed algorithm that can be applied if the graph under consideration needs to be split across several machines. We also introduce a (still unbiased) quasi Monte Carlo variant of GRFs, q-GRFs, relying on the so-called reinforced random walks that might be used to optimize the variance of GRFs. As a byproduct, we obtain a novel approach to solve certain classes of linear equations with positive and symmetric matrices."}, "pdf": {"value": "/pdf/881495f7b1a15d6cc4cb1f4790f2aa9340408ed3.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "choromanski|taming_graph_kernels_with_random_features"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6792/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373365047, "odate": 1686841490751, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "tRhQsHnoFw", "number": 6716, "cdate": 1674763008023, "tcdate": 1674763008023, "mdate": 1686841490604, "tmdate": 1686841490604, "signatures": ["ICML.cc/2023/Conference/Submission6716/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6716/Authors"], "forum": "tRhQsHnoFw", "content": {"title": {"value": "Bayesian Design Principles for Frequentist Sequential Learning"}, "authors": {"value": ["Yunbei Xu", "assaf zeevi"]}, "authorids": {"value": ["~Yunbei_Xu1", "~assaf_zeevi2"]}, "abstract": {"value": "We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to create \"algorithmic beliefs\" at each round, and use Bayesian posteriors to make decisions. This is the first approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the \"best-of-all-worlds\" empirical performance in the stochastic, adversarial, and non-stationary environments. And we illustrate how these principles can be used in linear bandits, convex bandits, and reinforcement learning."}, "pdf": {"value": "/pdf/aa28b9794b325c53d3f08117f3cfa370e25dca67.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "xu|bayesian_design_principles_for_frequentist_sequential_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6716/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373363596, "odate": 1686841490592, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "bF1LVbP493", "number": 6684, "cdate": 1674762922062, "tcdate": 1674762922062, "mdate": 1686841490509, "tmdate": 1686841490509, "signatures": ["ICML.cc/2023/Conference/Submission6684/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6684/Authors"], "forum": "bF1LVbP493", "content": {"title": {"value": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"}, "authors": {"value": ["Kenton Lee", "Mandar Joshi", "Iulia Raluca Turc", "Hexiang Hu", "Fangyu Liu", "Julian Martin Eisenschlos", "Urvashi Khandelwal", "Peter Shaw", "Ming-Wei Chang", "Kristina Toutanova"]}, "authorids": {"value": ["~Kenton_Lee1", "~Mandar_Joshi1", "~Iulia_Raluca_Turc1", "~Hexiang_Hu1", "~Fangyu_Liu1", "~Julian_Martin_Eisenschlos1", "~Urvashi_Khandelwal1", "~Peter_Shaw1", "~Ming-Wei_Chang3", "~Kristina_Toutanova1"]}, "abstract": {"value": "Visually-situated language is ubiquitous---sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, and image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images."}, "pdf": {"value": "/pdf/954f1a958bdb959f36f79264cd7fc0769f70efbd.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "lee|pix2struct_screenshot_parsing_as_pretraining_for_visual_language_understanding"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6684/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373362703, "odate": 1686841490498, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "ORxBEWMPAJ", "number": 6673, "cdate": 1674762889583, "tcdate": 1674762889583, "mdate": 1686841490435, "tmdate": 1686841490435, "signatures": ["ICML.cc/2023/Conference/Submission6673/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6673/Authors"], "forum": "ORxBEWMPAJ", "content": {"title": {"value": "JAWS-X: Addressing Efficiency Bottlenecks of Conformal Prediction Under Standard and Feedback Covariate Shift"}, "authors": {"value": ["Drew Prinster", "Suchi Saria", "Anqi Liu"]}, "authorids": {"value": ["~Drew_Prinster1", "~Suchi_Saria1", "~Anqi_Liu2"]}, "abstract": {"value": "We study the efficient estimation of predictive confidence intervals for black-box predictors when the common data exchangeability (e.g., i.i.d.) assumption is violated due to potentially feedback-induced shifts in the input data distribution. That is, we focus on standard and feedback covariate shift (FCS), where the latter allows for feedback dependencies between train and test data that occur in many decision-making scenarios like experimental design. Whereas prior conformal prediction methods for this problem are in general either extremely computationally demanding or make inefficient use of labeled data, we propose a collection of methods based on the jackknife+ that achieve a practical balance of computational and statistical efficiency. Theoretically, our proposed JAW-FCS method extends the rigorous, finite-sample coverage guarantee of the jackknife+ to FCS. We moreover propose two tunable relaxations to JAW-FCS's computation that maintain finite-sample guarantees: one using only $K$ leave-one-out models (JAW-$K$LOO) and a second building on $K$-fold cross validation+ (WCV+). Practically, we demonstrate that JAW-FCS and its computational relaxations outperform state-of-the-art baselines on a variety of real-world datasets under standard and feedback covariate shift, including for biomolecular design and active learning tasks."}, "pdf": {"value": "/pdf/48a96d71cfe77ae505c5f68037dd0fb7e3986e2e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "prinster|jawsx_addressing_efficiency_bottlenecks_of_conformal_prediction_under_standard_and_feedback_covariate_shift"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6673/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373362498, "odate": 1686841490421, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "mGUJMqjDwE", "number": 6664, "cdate": 1674762868581, "tcdate": 1674762868581, "mdate": 1686841490329, "tmdate": 1686841490329, "signatures": ["ICML.cc/2023/Conference/Submission6664/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6664/Authors"], "forum": "mGUJMqjDwE", "content": {"title": {"value": "Provably Learning Object-Centric Representations"}, "authors": {"value": ["Jack Brady", "Roland S. Zimmermann", "Yash Sharma", "Bernhard Sch\u00f6lkopf", "Julius von K\u00fcgelgen", "Wieland Brendel"]}, "authorids": {"value": ["~Jack_Brady1", "~Roland_S._Zimmermann1", "~Yash_Sharma1", "~Bernhard_Sch\u00f6lkopf1", "~Julius_von_K\u00fcgelgen2", "~Wieland_Brendel1"]}, "abstract": {"value": "Learning structured representations of the visual world in terms of objects promises to significantly improve the generalization abilities of current machine learning models. While recent efforts to this end have shown promising empirical progress, a theoretical account of when unsupervised object-centric representation learning is possible is still lacking. Consequently, understanding the reasons for the success of existing object-centric methods as well as designing new theoretically grounded methods remains challenging. In the present work, we analyze when object-centric representations can provably be learned without supervision. To this end, we first introduce two assumptions on the generative process for scenes comprised of several objects, which we call compositionality and irreducibility. Under this generative process, we prove that the ground-truth object representations can be identified by an invertible and compositional inference model, even in the presence of dependencies between objects. We empirically validate our results through experiments on synthetic data. Finally, we provide evidence that our theory holds predictive power for existing object-centric models by showing a close correspondence between models' compositionality and invertibility and their empirical identifiability."}, "pdf": {"value": "/pdf/efa54b0c0d41f86b280d7fd2e2363774cde24952.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "brady|provably_learning_objectcentric_representations"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6664/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373362249, "odate": 1686841490316, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "MtopPVk3Ll", "number": 6548, "cdate": 1674762598588, "tcdate": 1674762598588, "mdate": 1687337071480, "tmdate": 1687337071480, "signatures": ["ICML.cc/2023/Conference/Submission6548/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6548/Authors"], "forum": "MtopPVk3Ll", "content": {"title": {"value": "H-Likelihood Approach to Deep Neural Networks  with Temporal-Spatial Random Effects for High-Cardinality Categorical Features"}, "authors": {"value": ["Hangbin Lee", "Youngjo Lee"]}, "authorids": {"value": ["~Hangbin_Lee1", "~Youngjo_Lee2"]}, "abstract": {"value": "Deep Neural Networks (DNNs) are one of the most powerful tools for prediction, but many of them implicitly assume that the data are statistically independent. However, in the real world, it is common for large-scale data to be clustered with temporal-spatial correlation structures. Variational approaches and integrated likelihood approaches have been proposed to obtain approximate maximum likelihood estimators (MLEs) for correlated data. However, due to the large size of data, they cannot provide exact MLEs. In this study, we propose a new hierarchical likelihood approach to DNNs with correlated random effects for clustered data. By jointly optimizing the the negative h-likelihood loss, we can provide exact MLEs for both mean and dispersion parameters, as well as the best linear unbiased predictors for the random effects. Moreover, the hierarchical likelihood allows a computable procedure for restricted maximum likelihood estimators of dispersion parameters. The proposed two-step algorithm enables online learning for the neural networks, whereas the integrated likelihood cannot decompose like a widely-used loss function in DNNs. The proposed h-likelihood approach offers several advantages, which we demonstrate through numerical studies and real data analyses."}, "pdf": {"value": "/pdf/c5dceefa2fcd49f4eede19417d8ab3be5553190a.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "lee|hlikelihood_approach_to_deep_neural_networks_with_temporalspatial_random_effects_for_highcardinality_categorical_features"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6548/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373359385, "odate": 1686841489778, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "eYlLlvzngu", "number": 6534, "cdate": 1674762544281, "tcdate": 1674762544281, "mdate": 1686841489703, "tmdate": 1686841489703, "signatures": ["ICML.cc/2023/Conference/Submission6534/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6534/Authors"], "forum": "eYlLlvzngu", "content": {"title": {"value": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies"}, "authors": {"value": ["Gati Aher", "Rosa I. Arriaga", "Adam Tauman Kalai"]}, "authorids": {"value": ["gaher@olin.edu", "~Rosa_I._Arriaga1", "~Adam_Tauman_Kalai1"]}, "abstract": {"value": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model\u2019s simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a \u201chyper-accuracy distortion\u201d present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."}, "pdf": {"value": "/pdf/ed44ef1affc19ca6a95371bc9432c79af85a2a92.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "aher|using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject_studies"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6534/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373359073, "odate": 1686841489692, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "ET6qkbzeOx", "number": 6436, "cdate": 1674762183184, "tcdate": 1674762183184, "mdate": 1686841489159, "tmdate": 1686841489159, "signatures": ["ICML.cc/2023/Conference/Submission6436/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6436/Authors"], "forum": "ET6qkbzeOx", "content": {"title": {"value": "Tractable Control for Autoregressive Language Generation"}, "authors": {"value": ["Honghua Zhang", "Meihua Dang", "Nanyun Peng", "Guy Van den Broeck"]}, "authorids": {"value": ["~Honghua_Zhang1", "~Meihua_Dang1", "~Nanyun_Peng1", "~Guy_Van_den_Broeck1"]}, "abstract": {"value": "Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution ${\\Pr}(\\text{text} | \\alpha)$ is intractable for even the simplest lexical constraints $\\alpha$. To overcome this challenge, we propose to use tractable probabilistic models (TPMs) to impose lexical constraints in autoregressive text generation models, which we refer to as GeLaTo (Generating Language with Tractable Constraints). To demonstrate the effectiveness of this framework, we use distilled hidden Markov models, where we can efficiently compute ${\\Pr}(\\text{text} | \\alpha)$, to guide autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on challenging benchmarks for constrained text generation (e.g., CommonGen), beating various strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive TPMs."}, "pdf": {"value": "/pdf/436ac6708601b93cdfd858b3cae04efaaeaf3db6.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "zhang|tractable_control_for_autoregressive_language_generation"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6436/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373356577, "odate": 1686841489146, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "450iImFM4U", "number": 6337, "cdate": 1674761692019, "tcdate": 1674761692019, "mdate": 1687337058553, "tmdate": 1687337058553, "signatures": ["ICML.cc/2023/Conference/Submission6337/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6337/Authors"], "forum": "450iImFM4U", "content": {"title": {"value": "Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains"}, "authors": {"value": ["Vishwaraj Doshi", "Jie Hu", "Do Young Eun"]}, "authorids": {"value": ["vishwaraj.doshi@iqvia.com", "~Jie_Hu7", "~Do_Young_Eun1"]}, "abstract": {"value": "We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a *self-repellent random walk* (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real $\\alpha$, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the SRRW with a stronger repellence (larger $\\alpha$) always achieves a smaller asymptotic covariance, in the sense of Loewner ordering of co-variance matrices. Especially for SRRW-driven MCMC algorithms, we show that the decrease in the asymptotic sampling variance is of the order $O(1/\\alpha)$, eventually going down to zero. Finally, we provide numerical simulations complimentary to our theoretical results, also empirically testing a version of SRRW with $\\alpha$ increasing in time to combine the benefits of smaller asymptotic variance due to large $\\alpha$, with empirically observed faster mixing properties of SRRW with smaller $\\alpha$."}, "pdf": {"value": "/pdf/0d9b8d444cff00c3f3457a035e33c881dccc12e7.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "doshi|selfrepellent_random_walks_on_general_graphs_achieving_minimal_sampling_variance_via_nonlinear_markov_chains"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6337/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373353999, "odate": 1686841488543, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "fmLW8Eq3VQ", "number": 6282, "cdate": 1674761501177, "tcdate": 1674761501177, "mdate": 1687337051809, "tmdate": 1687337051809, "signatures": ["ICML.cc/2023/Conference/Submission6282/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6282/Authors"], "forum": "fmLW8Eq3VQ", "content": {"title": {"value": "Active Ranking of Experts Based on their Performances in Many Tasks"}, "authors": {"value": ["El Mehdi Saad", "Nicolas Verzelen", "Alexandra Carpentier"]}, "authorids": {"value": ["~El_Mehdi_Saad1", "nicolas.verzelen@inrae.fr", "~Alexandra_Carpentier1"]}, "abstract": {"value": "We consider the problem of ranking n experts based on their performances on d tasks. We make a monotonicity assumption stating that for each pair of experts, one outperforms the other on all tasks. We consider the sequential setting where in each round the learner has access to noisy evaluations of actively chosen pair of expert-task, given the information available up to the actual round. Given a confidence parameter $\\delta \\in (0, 1)$, we provide strategies allowing to recover the correct ranking of experts and develop a bound on the total number of queries made by our algorithm that hold with probability at least $1-\\delta$. We show that our strategy is adaptive to the complexity of the problem (our bounds are instance dependent), and develop matching lower bounds up to a ploy-logarithmic factor. Finally, we adapt our strategy to the relaxed problem of best expert identification and provide numerical simulation consistent with our theoretical results"}, "pdf": {"value": "/pdf/2217589c81cfa59d81eac89c8128e9455a5b2f3b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "saad|active_ranking_of_experts_based_on_their_performances_in_many_tasks"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6282/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373352619, "odate": 1686841488115, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "0BS36re3Cx", "number": 6238, "cdate": 1674761282477, "tcdate": 1674761282477, "mdate": 1687337049356, "tmdate": 1687337049356, "signatures": ["ICML.cc/2023/Conference/Submission6238/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6238/Authors"], "forum": "0BS36re3Cx", "content": {"title": {"value": "Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression"}, "authors": {"value": ["Yihao Xue", "Siddharth Joshi", "Eric Gan", "Pin-Yu Chen", "Baharan Mirzasoleiman"]}, "authorids": {"value": ["~Yihao_Xue1", "~Siddharth_Joshi1", "egan8@g.ucla.edu", "~Pin-Yu_Chen1", "~Baharan_Mirzasoleiman1"]}, "abstract": {"value": "Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of *class collapse* or *feature suppression* at *test* time. We provide the first unified theoretically rigorous framework to determine *which* features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations as two theoretically motivated solutions to feature suppression. We also provide the first theoretical explanation for why employing supervised and unsupervised CL together yields higher-quality representations, even when using commonly-used stochastic gradient methods."}, "pdf": {"value": "/pdf/4e616f770aeecdec6a25ce54e386c62ba615ba12.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "xue|which_features_are_learnt_by_contrastive_learning_on_the_role_of_simplicity_bias_in_class_collapse_and_feature_suppression"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6238/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373351673, "odate": 1686841487884, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "M3Yd3QyRG4", "number": 6159, "cdate": 1674760979414, "tcdate": 1674760979414, "mdate": 1686841487555, "tmdate": 1686841487555, "signatures": ["ICML.cc/2023/Conference/Submission6159/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6159/Authors"], "forum": "M3Yd3QyRG4", "content": {"title": {"value": "Resurrecting Recurrent Neural Networks for Long Sequences"}, "authors": {"value": ["Antonio Orvieto", "Samuel L Smith", "Albert Gu", "Anushan Fernando", "Caglar Gulcehre", "Razvan Pascanu", "Soham De"]}, "authorids": {"value": ["~Antonio_Orvieto3", "~Samuel_L_Smith1", "~Albert_Gu1", "anushanf@google.com", "~Caglar_Gulcehre1", "~Razvan_Pascanu1", "~Soham_De2"]}, "abstract": {"value": "Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. We show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring careful normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, and introduce an RNN block called the Linear Recurrent Unit (or LRU) that matches both their performance on the Long Range Arena benchmark and their computational efficiency."}, "pdf": {"value": "/pdf/ec83f0e515cdebd8425fcc23bd1976f032eff172.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "orvieto|resurrecting_recurrent_neural_networks_for_long_sequences"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6159/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373349663, "odate": 1686841487543, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "p5ZMcFXKvm", "number": 6102, "cdate": 1674760717890, "tcdate": 1674760717890, "mdate": 1687337039983, "tmdate": 1687337039983, "signatures": ["ICML.cc/2023/Conference/Submission6102/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6102/Authors"], "forum": "p5ZMcFXKvm", "content": {"title": {"value": "Warm-Start Actor-Critic: From Approximation Error to Sub-optimality Gap"}, "authors": {"value": ["Hang Wang", "Sen Lin", "Junshan Zhang"]}, "authorids": {"value": ["~Hang_Wang2", "~Sen_Lin1", "~Junshan_Zhang1"]}, "abstract": {"value": "Warm-Start reinforcement learning (RL), aided by a prior policy obtained from offline training, is emerging as a promising RL approach for practical applications. Recent empirical studies have demonstrated that the performance of Warm-Start RL can be improved *quickly* in some cases but become *stagnant* in other cases, especially when the function approximation is used. To this end, the primary objective of this work is to build a fundamental understanding on ''whether and when online learning can be significantly accelerated by a warm-start policy from offline RL?''. Specifically, we consider the widely used Actor-Critic (A-C) method with a prior policy. We first quantify the approximation errors in the Actor update and the Critic update, respectively. Next, we cast the Warm-Start A-C algorithm as Newton's method with perturbation, and study the impact of the approximation errors on the finite-time learning performance with inaccurate Actor/Critic updates. Under some general technical conditions, we derive the upper bounds, which shed light on achieving the desired finite-learning performance in the Warm-Start A-C algorithm. In particular, our findings reveal that it is essential to reduce the algorithm bias in online learning. We also obtain lower bounds on the sub-optimality gap of the Warm-Start A-C algorithm to quantify the impact of the bias and error propagation."}, "pdf": {"value": "/pdf/635b8158069c23cef9d585a7baef62e698a656ce.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wang|warmstart_actorcritic_from_approximation_error_to_suboptimality_gap"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6102/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373348223, "odate": 1686841487149, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "yPUc796tVF", "number": 6033, "cdate": 1674760257691, "tcdate": 1674760257691, "mdate": 1687337036995, "tmdate": 1687337036995, "signatures": ["ICML.cc/2023/Conference/Submission6033/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6033/Authors"], "forum": "yPUc796tVF", "content": {"title": {"value": "The Price of Differential Privacy under Continual Observation"}, "authors": {"value": ["Palak Jain", "Sofya Raskhodnikova", "Satchit Sivakumar", "Adam Smith"]}, "authorids": {"value": ["~Palak_Jain2", "~Sofya_Raskhodnikova1", "~Satchit_Sivakumar1", "~Adam_Smith1"]}, "abstract": {"value": "We study the accuracy of differentially private mechanisms in the continual release model. A continual release mechanism receives a sensitive dataset as a stream of $T$ inputs and produces, after receiving each input, an output that is accurate for all the inputs received so far. We provide the first strong lower bounds on the error of continual release mechanisms. In particular, for two fundamental problems that are closely related to empirical risk minimization and widely studied and used in the standard (batch) model, we prove that the worst case error of every continual release algorithm is $\\tilde \\Omega(T^{1/3})$ times larger than that of the best batch algorithm. Previous work shows only a $\\Omega(\\log T)$ gap between the worst case error achievable in these two models. We also formulate a model that allows for adaptively selected inputs, thus capturing dependencies that arise in many applications of continual release. Even though, in general, both privacy and accuracy are harder to attain in this model, we show that our lower bounds are matched by the error of simple algorithms that work even for adaptively selected inputs."}, "pdf": {"value": "/pdf/6c12c9be1b0d09f6524412c63073843c8e22cdc5.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "jain|the_price_of_differential_privacy_under_continual_observation"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6033/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373346530, "odate": 1686841486752, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "TwsJ9IOZDx", "number": 6009, "cdate": 1674760118696, "tcdate": 1674760118696, "mdate": 1686841486652, "tmdate": 1686841486652, "signatures": ["ICML.cc/2023/Conference/Submission6009/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission6009/Authors"], "forum": "TwsJ9IOZDx", "content": {"title": {"value": "Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere"}, "authors": {"value": ["Boris Bonev", "Thorsten Kurth", "Christian Hundt", "Jaideep Pathak", "Maximilian Baust", "Karthik Kashinath", "Anima Anandkumar"]}, "authorids": {"value": ["~Boris_Bonev1", "tkurth@nvidia.com", "chundt@nvidia.com", "jpathak@nvidia.com", "~Maximilian_Baust1", "~Karthik_Kashinath1", "~Anima_Anandkumar1"]}, "abstract": {"value": "Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolution-independent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates by incorrectly assuming flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries. We apply SFNOs to forecasting atmo- spheric dynamics, and demonstrate stable autoregressive rollouts for a year of simulated time (1,460 steps), while retaining physically plausible dynamics. The SFNO has important implications for machine learning-based simulation of climate dynamics that could eventually help accelerate our response to climate change."}, "pdf": {"value": "/pdf/72ae52d0e825be2bf521476aa5e4b447d4a55630.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "bonev|spherical_fourier_neural_operators_learning_stable_dynamics_on_the_sphere"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission6009/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373345977, "odate": 1686841486640, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "wbs1fKLfOe", "number": 5999, "cdate": 1674760054535, "tcdate": 1674760054535, "mdate": 1687337035444, "tmdate": 1687337035444, "signatures": ["ICML.cc/2023/Conference/Submission5999/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5999/Authors"], "forum": "wbs1fKLfOe", "content": {"title": {"value": "Analysis of Error Feedback in Federated Non-Convex Optimization with Biased Compression: Fast Convergence and Partial Participation"}, "authors": {"value": ["Xiaoyun Li", "Ping Li"]}, "authorids": {"value": ["~Xiaoyun_Li1", "~Ping_Li3"]}, "abstract": {"value": "In practical federated learning (FL) systems, the communication cost between the clients and the central server can often be a bottleneck. In this paper, we focus on biased gradient compression in non-convex FL problems. In the classical distributed learning, the method of error feedback (EF) is a common technique to remedy the downsides of biased gradient compression, but the performance of EF in FL still lacks systematic investigation. In this work, we study a compressed FL scheme with error feedback, named Fed-EF, with two variants depending on the global model optimizer. While directly applying biased compression in FL leads to poor convergence, we show that Fed-EF is able to match the convergence rate of the full-precision FL counterpart with a linear speedup w.r.t. the number of clients. Experiments verify that Fed-EF achieves the same performance as the full-precision FL approach, at the substantially reduced communication cost. Moreover, we develop a new analysis of the EF under partial participation (PP), an important scenario in FL. Under PP, the convergence rate of Fed-EF exhibits an extra slow-down factor due to a so-called ``stale error compensation'' effect, which is also justified in our experiments. Our results provide insights on a theoretical limitation of EF, and possible directions for improvements."}, "pdf": {"value": "/pdf/fca63b7f849f958e2125c9e6ed5bb17fb33e2349.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "li|analysis_of_error_feedback_in_federated_nonconvex_optimization_with_biased_compression_fast_convergence_and_partial_participation"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5999/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373345719, "odate": 1686841486600, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "jawDXfCldp", "number": 5918, "cdate": 1674759537441, "tcdate": 1674759537441, "mdate": 1686841486103, "tmdate": 1686841486103, "signatures": ["ICML.cc/2023/Conference/Submission5918/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5918/Authors"], "forum": "jawDXfCldp", "content": {"title": {"value": "Task-specific experimental design for treatment effect estimation"}, "authors": {"value": ["Bethany Connolly", "Kim Moore", "Tobias Schwedes", "Alexander Adam", "Gary Willis", "Ilya Feige", "Christopher Frye"]}, "authorids": {"value": ["~Bethany_Connolly1", "~Kim_Moore1", "~Tobias_Schwedes1", "alexander.g.adam@gmail.com", "gary.willis@faculty.ai", "~Ilya_Feige1", "~Christopher_Frye1"]}, "abstract": {"value": "Understanding causality should be a core requirement of any attempt to build real impact through AI. Due to the inherent unobservability of counterfactuals, large randomised trials (RCTs) are the standard for causal inference. But large experiments are generically expensive, and randomisation carries its own costs, e.g. when suboptimal decisions are trialed. Recent work has proposed more sample-efficient alternatives to RCTs, but these are not adaptable to the downstream application for which the causal effect is sought. In this work, we develop a task-specific approach to experimental design and derive sampling strategies customised to particular downstream applications. Across a range of important tasks, real-world datasets, and sample sizes, our method outperforms other benchmarks, e.g. requiring an order-of-magnitude less data to match RCT performance on targeted marketing tasks."}, "pdf": {"value": "/pdf/ba84faa1ce47f7e97c2d3c949121290b921ddfe0.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "connolly|taskspecific_experimental_design_for_treatment_effect_estimation"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5918/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373343412, "odate": 1686841486090, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Lhyy8H75KA", "number": 5915, "cdate": 1674759524688, "tcdate": 1674759524688, "mdate": 1687337027665, "tmdate": 1687337027665, "signatures": ["ICML.cc/2023/Conference/Submission5915/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5915/Authors"], "forum": "Lhyy8H75KA", "content": {"title": {"value": "Scaling Vision Transformers to 22 Billion Parameters"}, "authors": {"value": ["Mostafa Dehghani", "Josip Djolonga", "Basil Mustafa", "Piotr Padlewski", "Jonathan Heek", "Justin Gilmer", "Andreas Peter Steiner", "Mathilde Caron", "Robert Geirhos", "Ibrahim Alabdulmohsin", "Rodolphe Jenatton", "Lucas Beyer", "Michael Tschannen", "Anurag Arnab", "Xiao Wang", "Carlos Riquelme Ruiz", "Matthias Minderer", "Joan Puigcerver", "Utku Evci", "Manoj Kumar", "Sjoerd van Steenkiste", "Gamaleldin Fathy Elsayed", "Aravindh Mahendran", "Fisher Yu", "Avital Oliver", "Fantine Huot", "Jasmijn Bastings", "Mark Collier", "Alexey A. Gritsenko", "Vighnesh Birodkar", "Cristina Nader Vasconcelos", "Yi Tay", "Thomas Mensink", "Alexander Kolesnikov", "Filip Pavetic", "Dustin Tran", "Thomas Kipf", "Mario Lucic", "Xiaohua Zhai", "Daniel Keysers", "Jeremiah J. Harmsen", "Neil Houlsby"]}, "authorids": {"value": ["~Mostafa_Dehghani1", "~Josip_Djolonga2", "~Basil_Mustafa1", "~Piotr_Padlewski1", "~Jonathan_Heek1", "~Justin_Gilmer1", "~Andreas_Peter_Steiner1", "~Mathilde_Caron1", "~Robert_Geirhos1", "~Ibrahim_Alabdulmohsin1", "~Rodolphe_Jenatton3", "~Lucas_Beyer1", "~Michael_Tschannen1", "~Anurag_Arnab1", "~Xiao_Wang5", "~Carlos_Riquelme_Ruiz1", "~Matthias_Minderer1", "~Joan_Puigcerver1", "~Utku_Evci1", "~Manoj_Kumar1", "~Sjoerd_van_Steenkiste1", "~Gamaleldin_Fathy_Elsayed1", "~Aravindh_Mahendran2", "~Fisher_Yu2", "~Avital_Oliver2", "~Fantine_Huot1", "~Jasmijn_Bastings1", "~Mark_Collier1", "~Alexey_A._Gritsenko1", "~Vighnesh_Birodkar1", "~Cristina_Nader_Vasconcelos1", "~Yi_Tay1", "~Thomas_Mensink1", "~Alexander_Kolesnikov2", "~Filip_Pavetic1", "~Dustin_Tran1", "~Thomas_Kipf2", "~Mario_Lucic1", "~Xiaohua_Zhai2", "~Daniel_Keysers2", "~Jeremiah_J._Harmsen1", "~Neil_Houlsby1"]}, "abstract": {"value": "The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for \"LLM-like\" scaling in vision, and provides key steps towards getting there."}, "pdf": {"value": "/pdf/6ab77dfe106d9f14b4b75a7de2228acab72d8f30.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "dehghani|scaling_vision_transformers_to_22_billion_parameters"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5915/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373343321, "odate": 1686841486035, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "nS2x7LOKZk", "number": 5878, "cdate": 1674759238189, "tcdate": 1674759238189, "mdate": 1687337024033, "tmdate": 1687337024033, "signatures": ["ICML.cc/2023/Conference/Submission5878/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5878/Authors"], "forum": "nS2x7LOKZk", "content": {"title": {"value": "Are labels informative in semi-supervised learning? Estimating and leveraging the missing-data mechanism."}, "authors": {"value": ["Aude Sportisse", "Hugo Schmutz", "Olivier HUMBERT", "Charles Bouveyron", "Pierre-Alexandre Mattei"]}, "authorids": {"value": ["aude.sportisse@inria.fr", "~Hugo_Schmutz1", "~Olivier_HUMBERT1", "~Charles_Bouveyron2", "~Pierre-Alexandre_Mattei3"]}, "abstract": {"value": "Semi-supervised learning is a powerful technique for leveraging unlabeled data to improve machine learning models, but it can be affected by the presence of ``informative\" labels, which occur when some classes are more likely to be labeled than others. In the missing data literature, such labels are called missing not at random. In this paper, we propose a novel approach to address this issue by estimating the missing-data mechanism and using inverse propensity weighting to debias any SSL algorithm, including those using data augmentation. We also propose a likelihood ratio test to assess whether or not labels are indeed informative. Finally, we demonstrate the performance of the proposed methods on different datasets, in particular on two medical datasets for which we design pseudo-realistic missing data scenarios."}, "pdf": {"value": "/pdf/d0fd6e11bff43caff3bc309642ac8b46ed5f86f9.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "sportisse|are_labels_informative_in_semisupervised_learning_estimating_and_leveraging_the_missingdata_mechanism"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5878/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373342293, "odate": 1686841485753, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "24wzmwrldX", "number": 5871, "cdate": 1674759182911, "tcdate": 1674759182911, "mdate": 1687337023725, "tmdate": 1687337023725, "signatures": ["ICML.cc/2023/Conference/Submission5871/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5871/Authors"], "forum": "24wzmwrldX", "content": {"title": {"value": "Graphically Structured Diffusion Models"}, "authors": {"value": ["Christian Dietrich Weilbach", "William Harvey", "Frank Wood"]}, "authorids": {"value": ["~Christian_Dietrich_Weilbach1", "~William_Harvey1", "~Frank_Wood2"]}, "abstract": {"value": "We introduce a framework for automatically defining and learning deep generative models with problem-specific structure. We tackle problem domains that are more traditionally solved by algorithms such as sorting, constraint satisfaction for Sudoku, and matrix factorization. Concretely, we train diffusion models with an architecture tailored to the problem specification. This problem specification should contain a graphical model describing relationships between variables, and often benefits from explicit representation of subcomputations. Permutation invariances can also be exploited. Across a diverse set of experiments we improve the scaling relationship between problem dimension and our model's performance, in terms of both training time and final accuracy. Our code can be found at https://github.com/plai-group/gsdm."}, "pdf": {"value": "/pdf/64891bc495c61c440c47c2e5f01cf908fefcedd2.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "weilbach|graphically_structured_diffusion_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5871/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373342074, "odate": 1686841485698, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "oke1MUPK2l", "number": 5777, "cdate": 1674758511580, "tcdate": 1674758511580, "mdate": 1686841485202, "tmdate": 1686841485202, "signatures": ["ICML.cc/2023/Conference/Submission5777/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5777/Authors"], "forum": "oke1MUPK2l", "content": {"title": {"value": "Learning Control-Oriented Dynamical Structure from Data"}, "authors": {"value": ["Spencer M. Richards", "Jean-Jacques Slotine", "Navid Azizan", "Marco Pavone"]}, "authorids": {"value": ["~Spencer_M._Richards1", "~Jean-Jacques_Slotine1", "~Navid_Azizan1", "~Marco_Pavone1"]}, "abstract": {"value": "Even for known nonlinear dynamical systems, feedback controller synthesis is a difficult problem that often requires leveraging the particular structure of the dynamics to induce a stable closed-loop system. For general nonlinear models, including those fit to data, there may not be enough known structure to reliably synthesize a stabilizing feedback controller. In this paper, we discuss a state-dependent nonlinear tracking controller formulation based on a state-dependent Riccati equation for general nonlinear control-affine systems. This formulation depends on a nonlinear factorization of the system of vector fields defining the control-affine dynamics, which always exists under mild smoothness assumptions. We propose a method for learning this factorization from a finite set of data. On a variety of simulated nonlinear dynamical systems, we empirically demonstrate the efficacy of learned versions of this controller in stable trajectory tracking. Alongside our learning method, we evaluate recent ideas in jointly learning a controller and stabilizability certificate for known dynamical systems; we show experimentally that such methods can be frail in comparison."}, "pdf": {"value": "/pdf/1636b554cf381f8f5b9e6ef96da0c9f988c0edae.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "richards|learning_controloriented_dynamical_structure_from_data"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5777/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373339691, "odate": 1686841485189, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "cjWHQpEqaZ", "number": 5767, "cdate": 1674758456109, "tcdate": 1674758456109, "mdate": 1687336768748, "tmdate": 1687336768748, "signatures": ["ICML.cc/2023/Conference/Submission5767/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5767/Authors"], "forum": "cjWHQpEqaZ", "content": {"title": {"value": "Robustly Learning a Single Neuron via Sharpness"}, "authors": {"value": ["Puqian Wang", "Nikos Zarifis", "Ilias Diakonikolas", "Jelena Diakonikolas"]}, "authorids": {"value": ["~Puqian_Wang1", "~Nikos_Zarifis1", "~Ilias_Diakonikolas1", "~Jelena_Diakonikolas2"]}, "abstract": {"value": "We study the problem of learning a single neuron with respect to the $L_2^2$-loss in the presence of adversarial label noise. We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal $L_2^2$-error within a constant factor. Notably, our algorithm succeeds under much milder distributional assumptions compared to prior work. The key ingredient enabling our results is a novel connection to local error bounds from optimization theory."}, "pdf": {"value": "/pdf/283985d7d29c1e7bc1ce7d22508b916931b97447.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wang|robustly_learning_a_single_neuron_via_sharpness"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5767/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373339450, "odate": 1686841485073, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "1CqtvwHTKQ", "number": 5760, "cdate": 1674758385840, "tcdate": 1674758385840, "mdate": 1687336767872, "tmdate": 1687336767872, "signatures": ["ICML.cc/2023/Conference/Submission5760/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5760/Authors"], "forum": "1CqtvwHTKQ", "content": {"title": {"value": "A Study of Global and Episodic Bonuses for Exploration in Contextual MDPs"}, "authors": {"value": ["Mikael Henaff", "Minqi Jiang", "Roberta Raileanu"]}, "authorids": {"value": ["~Mikael_Henaff1", "~Minqi_Jiang1", "~Roberta_Raileanu2"]}, "abstract": {"value": "Exploration in environments which differ across episodes has received increasing attention in recent years. Current methods use some combination of global novelty bonuses, computed using the agent's entire training experience, and episodic novelty bonuses, computed using only experience from the current episode. However, the use of these two types of bonuses has been ad-hoc and poorly understood. In this work, we shed light on the behavior of these two types of bonuses through controlled experiments on easily interpretable tasks as well as challenging pixel-based settings. We find that the two types of bonuses succeed in different settings, with episodic bonuses being most effective when there is little shared structure across episodes and global bonuses being effective when more structure is shared. We develop a conceptual framework which makes this notion of shared structure precise by considering the variance of the value function across contexts, and which provides a unifying explanation of our empirical results. We furthermore find that combining the two bonuses can lead to more robust performance across different degrees of shared structure, and investigate different algorithmic choices for defining and combining global and episodic bonuses based on function approximation. This results in an algorithm which sets a new state of the art across 16 tasks from the MiniHack suite used in prior work, and also performs robustly on Habitat and Montezuma's Revenge."}, "pdf": {"value": "/pdf/d8f2288b7b6e2095646a7a6b0b62ba9cb690ecfc.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "henaff|a_study_of_global_and_episodic_bonuses_for_exploration_in_contextual_mdps"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5760/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373339242, "odate": 1686841484997, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "wLAMOoL0KD", "number": 5746, "cdate": 1674758296672, "tcdate": 1674758296672, "mdate": 1686841484899, "tmdate": 1686841484899, "signatures": ["ICML.cc/2023/Conference/Submission5746/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5746/Authors"], "forum": "wLAMOoL0KD", "content": {"title": {"value": "Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch"}, "authors": {"value": ["Xunyi Zhao", "Th\u00e9otime Le Hellard", "Lionel Eyraud-Dubois", "Julia Gusak", "Olivier Beaumont"]}, "authorids": {"value": ["~Xunyi_Zhao1", "~Th\u00e9otime_Le_Hellard1", "~Lionel_Eyraud-Dubois1", "~Julia_Gusak1", "~Olivier_Beaumont1"]}, "abstract": {"value": "We propose Rockmate to control the memory requirements when training PyTorch DNN models. Rockmate is an automatic tool that starts from the model code and generates an equivalent model, using a predefined amount of memory for activations, at the cost of a few re-computations. Rockmate automatically detects the structure of computational and data dependencies and rewrites the initial model as a sequence of complex blocks. We show that such a structure is widespread and can be found in many models in the literature (Transformer based models, ResNet, RegNets,...). This structure allows us to solve the problem in a fast and efficient way, using an adaptation of Checkmate (too slow on the whole model but general) at the level of individual blocks and an adaptation of Rotor (fast but limited to sequential models) at the level of the sequence itself. We show through experiments on many models that Rockmate is as fast as Rotor and as efficient as Checkmate, and that it allows in many cases to obtain a significantly lower memory consumption for activations (by a factor of 2 to 5) for a rather negligible overhead (of the order of 10% to 20%). Rockmate is open source and available at https://github.com/topal-team/rockmate."}, "pdf": {"value": "/pdf/06f1921b85fc4681e38c22e2521bd2b432e8d80e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "zhao|rockmate_an_efficient_fast_automatic_and_generic_tool_for_rematerialization_in_pytorch"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5746/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373338962, "odate": 1686841484885, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "9PJ2V6qvQL", "number": 5728, "cdate": 1674758158946, "tcdate": 1674758158946, "mdate": 1687336765755, "tmdate": 1687336765755, "signatures": ["ICML.cc/2023/Conference/Submission5728/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5728/Authors"], "forum": "9PJ2V6qvQL", "content": {"title": {"value": "Inflow, Outflow, and Reciprocity in Machine Learning"}, "authors": {"value": ["Mukund Sundararajan", "Walid Krichene"]}, "authorids": {"value": ["~Mukund_Sundararajan1", "~Walid_Krichene1"]}, "abstract": {"value": "Data is pooled across entities (individuals or enterprises) to create machine learning models, and sometimes, the entities that contribute the data also benefit from the models. Consider for instance a recommender system (e.g. Spotify, Instagram or YouTube), a health care app that predicts the risk for some disease, or a service built by pooling data across enterprises. In this work we propose a framework to study this value exchange, i.e., we model and measure contributions (outflows), benefits (inflows) and the balance between contributions and benefits (the degree of reciprocity). We show theoretically, and via experiments that under certain distributional assumptions, some classes of models are approximately reciprocal. These results only scratch the surface; we conclude with several open directions."}, "pdf": {"value": "/pdf/70b700e38694a5683f09929ea6f1fa1587739f7c.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "sundararajan|inflow_outflow_and_reciprocity_in_machine_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5728/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373338523, "odate": 1686841484748, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "wIPIhHd00i", "number": 5722, "cdate": 1674758091659, "tcdate": 1674758091659, "mdate": 1686841484725, "tmdate": 1686841484725, "signatures": ["ICML.cc/2023/Conference/Submission5722/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5722/Authors"], "forum": "wIPIhHd00i", "content": {"title": {"value": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"}, "authors": {"value": ["Zichang Liu", "Jue WANG", "Tri Dao", "Tianyi Zhou", "Binhang Yuan", "Zhao Song", "Anshumali Shrivastava", "Ce Zhang", "Yuandong Tian", "Christopher Re", "Beidi Chen"]}, "authorids": {"value": ["~Zichang_Liu1", "~Jue_WANG1", "~Tri_Dao1", "~Tianyi_Zhou4", "~Binhang_Yuan1", "~Zhao_Song6", "~Anshumali_Shrivastava1", "~Ce_Zhang1", "~Yuandong_Tian1", "~Christopher_Re1", "~Beidi_Chen1"]}, "abstract": {"value": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2$\\times$ compared to the state-of-the-art FasterTransformer, and over 6$\\times$ compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu."}, "pdf": {"value": "/pdf/8a0626de2c7e3b5d4509a29f92630a7ae8fd8231.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "liu|deja_vu_contextual_sparsity_for_efficient_llms_at_inference_time"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5722/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373338362, "odate": 1686841484712, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Ovu1horBiZ", "number": 5702, "cdate": 1674757948281, "tcdate": 1674757948281, "mdate": 1687336763949, "tmdate": 1687336763949, "signatures": ["ICML.cc/2023/Conference/Submission5702/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5702/Authors"], "forum": "Ovu1horBiZ", "content": {"title": {"value": "Reinforcement Learning from Passive Data via Latent Intentions"}, "authors": {"value": ["Dibya Ghosh", "Chethan Anand Bhateja", "Sergey Levine"]}, "authorids": {"value": ["~Dibya_Ghosh1", "~Chethan_Anand_Bhateja1", "~Sergey_Levine1"]}, "abstract": {"value": "Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from many forms of passive data, including cross-embodiment video data and YouTube videos."}, "pdf": {"value": "/pdf/088af6b3dbc11e9c5912e081bf7dde20cc3028a0.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ghosh|reinforcement_learning_from_passive_data_via_latent_intentions"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5702/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373337830, "odate": 1686841484547, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "bUFUaawOTk", "number": 5669, "cdate": 1674757643016, "tcdate": 1674757643016, "mdate": 1687336761006, "tmdate": 1687336761006, "signatures": ["ICML.cc/2023/Conference/Submission5669/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5669/Authors"], "forum": "bUFUaawOTk", "content": {"title": {"value": "Best of Both Worlds Policy Optimization"}, "authors": {"value": ["Christoph Dann", "Chen-Yu Wei", "Julian Zimmert"]}, "authorids": {"value": ["~Christoph_Dann1", "~Chen-Yu_Wei1", "~Julian_Zimmert1"]}, "abstract": {"value": "Policy optimization methods are popular reinforcement learning algorithms in practice and recent works have build theoretical foundation for them by proving $\\sqrt{T}$ regret bounds even when the losses are adversarial. Such bounds are tight in the worst case but often overly pessimistic. In this work, we show that by carefully designing the regularizer, bonus terms, and learning rates, one can achieve a more favorable $\\text{polylog}(T)$ regret bound when the losses are stochastic, without sacrificing the worst-case guarantee in the adversarial regime. Specifically, we show the first best of both worlds guarantee for policy optimization in tabular MDPs by leveraging either a Tsallis entropy or a Shannon entropy regularizer. Then we show that under known transitions, we can further obtain a first-order regret bound in the adversarial regime by leveraging the log barrier regularizer."}, "pdf": {"value": "/pdf/297528456a1eef7abd0ac08b01bb3b2f29e430b9.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "dann|best_of_both_worlds_policy_optimization"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5669/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373337073, "odate": 1686841484354, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Iwt7oI9cNb", "number": 5600, "cdate": 1674757121974, "tcdate": 1674757121974, "mdate": 1687336755777, "tmdate": 1687336755777, "signatures": ["ICML.cc/2023/Conference/Submission5600/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5600/Authors"], "forum": "Iwt7oI9cNb", "content": {"title": {"value": "Inferring Relational Potentials in Interacting Systems"}, "authors": {"value": ["Armand Comas", "Yilun Du", "Christian Fernandez Lopez", "Sandesh Ghimire", "Mario Sznaier", "Joshua B. Tenenbaum", "Octavia Camps"]}, "authorids": {"value": ["~Armand_Comas1", "~Yilun_Du1", "~Christian_Fernandez_Lopez1", "~Sandesh_Ghimire2", "~Mario_Sznaier1", "~Joshua_B._Tenenbaum1", "~Octavia_Camps1"]}, "abstract": {"value": "Systems consisting of interacting agents are prevalent in the world, ranging from dynamical systems in physics to complex biological networks. To build systems which can interact robustly in the real world, it is thus important to be able to infer the precise interactions governing such systems. Existing approaches typically discover such interactions by explicitly modeling the feed-forward dynamics of the trajectories. In this work, we propose Neural Interaction Inference with Potentials (NIIP) as an alternative approach to discover such interactions that enables greater flexibility in trajectory modeling: it discovers a set of relational potentials, represented as energy functions, which when minimized reconstruct the original trajectory. NIIP assigns low energy to the subset of trajectories which respect the relational constraints observed. We illustrate that with these representations NIIP displays unique capabilities in test-time. First, it allows trajectory manipulation, such as interchanging interaction types across separately trained models, as well as trajectory forecasting. Additionally, it allows adding external hand-crafted potentials at test-time. Finally, NIIP enables the detection of out-of-distribution samples and anomalies without explicit training."}, "pdf": {"value": "/pdf/cc369e2dfdc541d6798be36a358ec26488f7352c.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "comas|inferring_relational_potentials_in_interacting_systems"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5600/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373335338, "odate": 1686841483946, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "mSKJS7YbwU", "number": 5595, "cdate": 1674757077087, "tcdate": 1674757077087, "mdate": 1686841483938, "tmdate": 1686841483938, "signatures": ["ICML.cc/2023/Conference/Submission5595/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5595/Authors"], "forum": "mSKJS7YbwU", "content": {"title": {"value": "Raising the Cost of Malicious AI-Powered Image Editing"}, "authors": {"value": ["Hadi Salman", "Alaa Khaddaj", "Guillaume Leclerc", "Andrew Ilyas", "Aleksander Madry"]}, "authorids": {"value": ["~Hadi_Salman1", "~Alaa_Khaddaj1", "~Guillaume_Leclerc1", "~Andrew_Ilyas1", "~Aleksander_Madry1"]}, "abstract": {"value": "We present an approach to mitigating the risks of malicious image editing posed by large diffusion models. The key idea is to immunize images so as to make them resistant to manipulation by these models. This immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. We provide two methods for crafting such perturbations, and then demonstrate their efficacy. Finally, we discuss a policy component necessary to make our approach fully effective and practical---one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process."}, "pdf": {"value": "/pdf/2eac04bb7ed7bdb692d932b9171f3738df0fb093.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "salman|raising_the_cost_of_malicious_aipowered_image_editing"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5595/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373335235, "odate": 1686841483924, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "6rlGbYv4bT", "number": 5582, "cdate": 1674756895303, "tcdate": 1674756895303, "mdate": 1686841483813, "tmdate": 1686841483813, "signatures": ["ICML.cc/2023/Conference/Submission5582/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5582/Authors"], "forum": "6rlGbYv4bT", "content": {"title": {"value": "Weighted Flow Diffusion for Local Graph Clustering with Node Attributes: an Algorithm and Statistical Guarantees"}, "authors": {"value": ["Shenghao Yang", "Kimon Fountoulakis"]}, "authorids": {"value": ["~Shenghao_Yang1", "~Kimon_Fountoulakis1"]}, "abstract": {"value": "Local graph clustering methods aim to detect small clusters in very large graphs without the need to process the whole graph. They are fundamental and scalable tools for a wide range of tasks such as local community detection, node ranking and node embedding. While prior work on local graph clustering mainly focuses on graphs without node attributes, modern real-world graph datasets typically come with node attributes that provide valuable additional information. We present a simple local graph clustering algorithm for graphs with node attributes, based on the idea of diffusing mass locally in the graph while accounting for both structural and attribute proximities. Using high-dimensional concentration results, we provide statistical guarantees on the performance of the algorithm for the recovery of a target cluster with a single seed node. We give conditions under which a target cluster generated from a fairly general contextual random graph model, which includes both the stochastic block model and the planted cluster model as special cases, can be fully recovered with bounded false positives. Empirically, we validate all theoretical claims using synthetic data, and we show that incorporating node attributes leads to superior local clustering performances using real-world graph datasets."}, "pdf": {"value": "/pdf/d6407e5af5b22fd94410bfec07818799d4b1f6f1.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "yang|weighted_flow_diffusion_for_local_graph_clustering_with_node_attributes_an_algorithm_and_statistical_guarantees"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5582/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373334862, "odate": 1686841483802, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "rVtdWHPFxX", "number": 5570, "cdate": 1674756790421, "tcdate": 1674756790421, "mdate": 1686841483737, "tmdate": 1686841483737, "signatures": ["ICML.cc/2023/Conference/Submission5570/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5570/Authors"], "forum": "rVtdWHPFxX", "content": {"title": {"value": "Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL"}, "authors": {"value": ["Zakaria Mhammedi", "Dylan J Foster", "Alexander Rakhlin"]}, "authorids": {"value": ["~Zakaria_Mhammedi1", "~Dylan_J_Foster1", "~Alexander_Rakhlin1"]}, "abstract": {"value": "We study the design of sample-efficient algorithms for reinforcement learning in the presence of rich, high-dimensional observations, formalized via the Block MDP problem. Existing algorithms suffer from either 1) computational intractability, 2) strong statistical assumptions that are not necessarily satisfied in practice, or 3) suboptimal sample complexity. We address these issues by providing the first computationally efficient algorithm that attains rate-optimal sample complexity with respect to the desired accuracy level, with minimal statistical assumptions. Our algorithm, MusIK, combines exploration with representation learning based on multi-step inverse kinematics, a learning objective in which the aim is to predict the current action from the current observation and observations in the (potentially distant) future. MusIK is simple and flexible, and can efficiently take advantage of general-purpose function approximation. Our analysis of MusIK leverages several new techniques tailored to non-optimistic algorithms for reward-free exploration, which we anticipate will find broader use."}, "pdf": {"value": "/pdf/21e1113a4591f9d5dc00461008c5bbfedd98fa89.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "mhammedi|representation_learning_with_multistep_inverse_kinematics_an_efficient_and_optimal_approach_to_richobservation_rl"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/-/PC_Revision", "ICML.cc/2023/Conference/Submission5570/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373334554, "odate": 1686841483722, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "y6gg68aGiq", "number": 5565, "cdate": 1674756756011, "tcdate": 1674756756011, "mdate": 1686841483738, "tmdate": 1686841483738, "signatures": ["ICML.cc/2023/Conference/Submission5565/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5565/Authors"], "forum": "y6gg68aGiq", "content": {"title": {"value": "Tighter Information-Theoretic Generalization Bounds from Supersamples"}, "authors": {"value": ["Ziqiao Wang", "Yongyi Mao"]}, "authorids": {"value": ["~Ziqiao_Wang1", "~Yongyi_Mao2"]}, "abstract": {"value": "In this work, we present a variety of novel information-theoretic generalization bounds for learning algorithms, from the supersample setting of Steinke & Zakynthinou (2020)\u2014the setting of the ``conditional mutual information'' framework. Our development exploits projecting the loss pair (obtained from a training instance and a testing instance) down to a single number and correlating loss values with a Rademacher sequence (and its shifted variants). The presented bounds include square-root bounds, fast-rate bounds, including those based on variance and sharpness, and bounds for interpolating algorithms etc. We show theoretically or empirically that these bounds are tighter than all information-theoretic bounds known to date on the same supersample setting."}, "pdf": {"value": "/pdf/8edde748718ca60bbddb2b56393fa711208dbe4c.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wang|tighter_informationtheoretic_generalization_bounds_from_supersamples"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5565/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373334420, "odate": 1686841483720, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "nkals4A4Vs", "number": 5330, "cdate": 1674754777922, "tcdate": 1674754777922, "mdate": 1687336737230, "tmdate": 1687336737230, "signatures": ["ICML.cc/2023/Conference/Submission5330/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5330/Authors"], "forum": "nkals4A4Vs", "content": {"title": {"value": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark"}, "authors": {"value": ["Alexander Pan", "Jun Shern Chan", "Andy Zou", "Nathaniel Li", "Steven Basart", "Thomas Woodside", "Hanlin Zhang", "Scott Emmons", "Dan Hendrycks"]}, "authorids": {"value": ["~Alexander_Pan1", "~Jun_Shern_Chan1", "~Andy_Zou1", "~Nathaniel_Li1", "~Steven_Basart1", "~Thomas_Woodside1", "~Hanlin_Zhang1", "~Scott_Emmons1", "~Dan_Hendrycks1"]}, "abstract": {"value": "Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce Machiavelli, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities."}, "pdf": {"value": "/pdf/7e916e777f0d9e449371f6d9baa5c000ee41911b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "pan|do_the_rewards_justify_the_means_measuring_tradeoffs_between_rewards_and_ethical_behavior_in_the_machiavelli_benchmark"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5330/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373328227, "odate": 1686841482137, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "tHvXrFQma5", "number": 5270, "cdate": 1674754089531, "tcdate": 1674754089531, "mdate": 1687336734681, "tmdate": 1687336734681, "signatures": ["ICML.cc/2023/Conference/Submission5270/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5270/Authors"], "forum": "tHvXrFQma5", "content": {"title": {"value": "Transformers Learn In-Context by Gradient Descent"}, "authors": {"value": ["Johannes Von Oswald", "Eyvind Niklasson", "Ettore Randazzo", "Joao Sacramento", "Alexander Mordvintsev", "Andrey Zhmoginov", "Max Vladymyrov"]}, "authorids": {"value": ["~Johannes_Von_Oswald1", "~Eyvind_Niklasson1", "~Ettore_Randazzo1", "~Joao_Sacramento1", "~Alexander_Mordvintsev1", "~Andrey_Zhmoginov1", "~Max_Vladymyrov1"]}, "abstract": {"value": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers."}, "pdf": {"value": "/pdf/662ad8e7578da6064f9f6b125472c223490eacb9.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "oswald|transformers_learn_incontext_by_gradient_descent"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5270/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373326836, "odate": 1686841481872, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "fZFNPf1QiF", "number": 5181, "cdate": 1674752995097, "tcdate": 1674752995097, "mdate": 1687336732518, "tmdate": 1687336732518, "signatures": ["ICML.cc/2023/Conference/Submission5181/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5181/Authors"], "forum": "fZFNPf1QiF", "content": {"title": {"value": "Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels"}, "authors": {"value": ["Simone Bombari", "Shayan Kiyani", "Marco Mondelli"]}, "authorids": {"value": ["~Simone_Bombari1", "~Shayan_Kiyani2", "~Marco_Mondelli1"]}, "abstract": {"value": "Machine learning models are vulnerable to adversarial perturbations, and a thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon through the lens of over-parameterization: interpolating smoothly the data requires significantly more parameters than simply memorizing it. However, this \"universal\" law provides only a necessary condition for robustness, and it is unable to discriminate between models. In this paper, we address these gaps by focusing on empirical risk minimization in two prototypical settings, namely, random features and the neural tangent kernel (NTK). We prove that, for random features, the model is not robust for any degree of over-parameterization, even when the necessary condition coming from the universal law of robustness is satisfied. In contrast, for even activations, the NTK model meets the universal lower bound, and it is robust as soon as the necessary condition on over-parameterization is fulfilled. This also addresses a conjecture in prior work by Bubeck, Li and Nagaraj. Our analysis decouples the effect of the kernel of the model from an \"interaction matrix\", which describes the interaction with the test data and captures the effect of the activation. Our theoretical results are corroborated by numerical evidence on both synthetic and standard datasets (MNIST, CIFAR-10)."}, "pdf": {"value": "/pdf/79a662cf119d0c70b63ab4437bc73d78cc0d2fb4.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "bombari|beyond_the_universal_law_of_robustness_sharper_laws_for_random_features_and_neural_tangent_kernels"}}, "pdate": 1682373324650, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5181/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "odate": 1686841481370, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "3bkRh3ggAE", "number": 5169, "cdate": 1674752866372, "tcdate": 1674752866372, "mdate": 1686841481264, "tmdate": 1686841481264, "signatures": ["ICML.cc/2023/Conference/Submission5169/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5169/Authors"], "forum": "3bkRh3ggAE", "content": {"title": {"value": "Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond"}, "authors": {"value": ["Jaeyoung Cha", "Jaewook Lee", "Chulhee Yun"]}, "authorids": {"value": ["~Jaeyoung_Cha1", "~Jaewook_Lee6", "~Chulhee_Yun1"]}, "abstract": {"value": "We study convergence lower bounds of without-replacement stochastic gradient descent (SGD) for solving smooth (strongly-)convex finite-sum minimization problems. Unlike most existing results focusing on final iterate lower bounds in terms of the number of components $n$ and the number of epochs $K$, we seek bounds for arbitrary weighted average iterates that are tight in all factors including the condition number $\\kappa$. For SGD with Random Reshuffling, we present lower bounds that have tighter $\\kappa$ dependencies than existing bounds. Our results are the first to perfectly close the gap between lower and upper bounds for weighted average iterates in both strongly-convex and convex cases. We also prove weighted average iterate lower bounds for arbitrary permutation-based SGD, which apply to all variants that carefully choose the best permutation. Our bounds improve the existing bounds in factors of $n$ and $\\kappa$ and thereby match the upper bounds shown for a recently proposed algorithm called GraB."}, "pdf": {"value": "/pdf/55c3e11e92fc874ddb8f5721c895c97ce405216f.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "cha|tighter_lower_bounds_for_shuffling_sgd_random_permutations_and_beyond"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5169/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373324348, "odate": 1686841481252, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "MXuLl38AEm", "number": 5157, "cdate": 1674752760547, "tcdate": 1674752760547, "mdate": 1686841481167, "tmdate": 1686841481167, "signatures": ["ICML.cc/2023/Conference/Submission5157/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5157/Authors"], "forum": "MXuLl38AEm", "content": {"title": {"value": "Specializing Smaller Language Models towards Multi-Step Reasoning"}, "authors": {"value": ["Yao Fu", "Hao Peng", "Litu Ou", "Ashish Sabharwal", "Tushar Khot"]}, "authorids": {"value": ["~Yao_Fu3", "~Hao_Peng4", "~Litu_Ou1", "~Ashish_Sabharwal1", "~Tushar_Khot1"]}, "abstract": {"value": "The surprising ability of Large Language Models (LLMs) to perform well on complex reasoning with only few-shot chain-of-thought prompts is believed to emerge only in very large-scale models. We show that such abilities can, in fact, be distilled down from GPT-3.5 (\u2265 175B) to T5 variants (\u2264 11B). We propose model specialization, to specialize the model\u2019s ability towards a target task. The hypothesis is that large models (commonly viewed as larger than 100B) have strong modeling power such that they can perform a large spectrum of tasks. Small models (commonly viewed as smaller than 10B) have limited model capacity, but if we specialize their capacity towards a target task, the model can achieve decent performance improvements. We use multi-step math reasoning as our testbed because it is a very typical emergent ability. We show two important aspects of model abilities: (1) balancing language model\u2019s performance on multiple tasks is a delicate matter, as improvements on one task may compromise other tasks; (2) yet by intentionally paying the price of decreased generic ability, we can clearly improve across different model scales smaller than 10B towards a specialized multi-step math reasoning ability. We further give comprehensive discussions about important design choices for better generalization, including the data format mixture and the start model checkpoint. We hope our practice and discoveries can serve as an important attempt towards specialized smaller models in the new research paradigm set by LLMs."}, "pdf": {"value": "/pdf/8965650205f4e4b1d55036feea2aadc8e6ecf631.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "fu|specializing_smaller_language_models_towards_multistep_reasoning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5157/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373324043, "odate": 1686841481155, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "gZXFNUcnHd", "number": 5032, "cdate": 1674751095872, "tcdate": 1674751095872, "mdate": 1686841480298, "tmdate": 1686841480298, "signatures": ["ICML.cc/2023/Conference/Submission5032/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission5032/Authors"], "forum": "gZXFNUcnHd", "content": {"title": {"value": "Towards Reliable Neural Specifications"}, "authors": {"value": ["Chuqin Geng", "Nham Le", "Xiaojie Xu", "Zhaoyue Wang", "Arie Gurfinkel", "Xujie Si"]}, "authorids": {"value": ["~Chuqin_Geng1", "~Nham_Le1", "~Xiaojie_Xu2", "~Zhaoyue_Wang1", "~Arie_Gurfinkel1", "~Xujie_Si1"]}, "abstract": {"value": "Having reliable specifications is an unavoidable challenge in achieving verifiable correctness, robustness, and interpretability of AI systems. Existing specifications for neural networks are in the paradigm of data as specification. That is, the local neighborhood centering around a reference input is considered to be correct (or robust). While existing specifications contribute to verifying adversarial robustness, a significant problem in many research domains, our empirical study shows that those verified regions are somewhat tight, and thus fail to allow verification of test set inputs, making them impractical for some real-world applications. To this end, we propose a new family of specifications called neural representation as specification. This form of specifications uses the intrinsic information of neural networks, specifically neural activation patterns (NAPs), rather than input data to specify the correctness and/or robustness of neural network predictions. We present a simple statistical approach to mining neural activation patterns. To show the effectiveness of discovered NAPs, we formally verify several important properties, such as various types of misclassifications will never happen for a given NAP, and there is no ambiguity between different NAPs. We show that by using NAP, we can verify a significant region of the input space, while still recalling 84% of the data on MNIST. Moreover, we can push the verifiable bound to 10 times larger on the CIFAR10 benchmark. Thus, we argue that NAPs can potentially be used as a more reliable and extensible specification for neural network verification."}, "pdf": {"value": "/pdf/9ac10bde7e766f1600cdfad2f98a2367bbae1e0b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "geng|towards_reliable_neural_specifications"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission5032/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373320807, "odate": 1686841480287, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "LJ9iKElXpl", "number": 4897, "cdate": 1674749381156, "tcdate": 1674749381156, "mdate": 1686841479420, "tmdate": 1686841479420, "signatures": ["ICML.cc/2023/Conference/Submission4897/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4897/Authors"], "forum": "LJ9iKElXpl", "content": {"title": {"value": "Exponential Smoothing for Off-Policy Learning"}, "authors": {"value": ["Imad AOUALI", "Victor-Emmanuel Brunel", "David Rohde", "Anna Korba"]}, "authorids": {"value": ["~Imad_AOUALI1", "~Victor-Emmanuel_Brunel1", "~David_Rohde1", "~Anna_Korba2"]}, "abstract": {"value": "Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL."}, "pdf": {"value": "/pdf/2dddf73e2d2a47cb45edafd9e381d75b08e3d135.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "aouali|exponential_smoothing_for_offpolicy_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4897/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373317510, "odate": 1686841479392, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "K1OvMEYEI4", "number": 4836, "cdate": 1674748713215, "tcdate": 1674748713215, "mdate": 1687336710389, "tmdate": 1687336710389, "signatures": ["ICML.cc/2023/Conference/Submission4836/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4836/Authors"], "forum": "K1OvMEYEI4", "content": {"title": {"value": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models"}, "authors": {"value": ["Dongjun Kim", "Yeongmin Kim", "Se Jung Kwon", "Wanmo Kang", "Il-chul Moon"]}, "authorids": {"value": ["~Dongjun_Kim1", "~Yeongmin_Kim1", "~Se_Jung_Kwon1", "~Wanmo_Kang1", "~Il-chul_Moon1"]}, "abstract": {"value": "The proposed method, **Discriminator Guidance**, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG."}, "pdf": {"value": "/pdf/9ecf583c0f2b8ce1f91a8c1851f65266862b6250.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kim|refining_generative_process_with_discriminator_guidance_in_scorebased_diffusion_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4836/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373315926, "odate": 1686841479087, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Wbquvk97t4", "number": 4828, "cdate": 1674748532503, "tcdate": 1674748532503, "mdate": 1686841479038, "tmdate": 1686841479038, "signatures": ["ICML.cc/2023/Conference/Submission4828/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4828/Authors"], "forum": "Wbquvk97t4", "content": {"title": {"value": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples"}, "authors": {"value": ["Chumeng Liang", "Xiaoyu Wu", "Yang Hua", "Jiaru Zhang", "Yiming Xue", "Tao Song", "Zhengui XUE", "Ruhui Ma", "Haibing Guan"]}, "authorids": {"value": ["~Chumeng_Liang2", "~Xiaoyu_Wu1", "~Yang_Hua2", "~Jiaru_Zhang1", "~Yiming_Xue1", "~Tao_Song2", "~Zhengui_XUE1", "~Ruhui_Ma1", "~Haibing_Guan1"]}, "abstract": {"value": "Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs and generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm to generate these adversarial examples, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-Art applications. The code of our method is available on GitHub: https://github.com/mist-project/mist.git."}, "pdf": {"value": "/pdf/936778ef337b29ecd6b1b0ab1f780a53f8e87755.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "liang|adversarial_example_does_good_preventing_painting_imitation_from_diffusion_models_via_adversarial_examples"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4828/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373315744, "odate": 1686841479023, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "JPMT9kjeJi", "number": 4757, "cdate": 1674747552482, "tcdate": 1674747552482, "mdate": 1687336698512, "tmdate": 1687336698512, "signatures": ["ICML.cc/2023/Conference/Submission4757/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4757/Authors"], "forum": "JPMT9kjeJi", "content": {"title": {"value": "Self-Interpretable Time Series Prediction with Counterfactual Explanations"}, "authors": {"value": ["Jingquan Yan", "Hao Wang"]}, "authorids": {"value": ["~Jingquan_Yan2", "~Hao_Wang3"]}, "abstract": {"value": "Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy."}, "pdf": {"value": "/pdf/d72a76f53a86d3b7002e0962f1c4cf9cc435d228.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "yan|selfinterpretable_time_series_prediction_with_counterfactual_explanations"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4757/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373313965, "odate": 1686841478591, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "BAQWKRdL1D", "number": 4754, "cdate": 1674747538095, "tcdate": 1674747538095, "mdate": 1687336697765, "tmdate": 1687336697765, "signatures": ["ICML.cc/2023/Conference/Submission4754/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4754/Authors"], "forum": "BAQWKRdL1D", "content": {"title": {"value": "Sketch-Flip-Merge: Mergeable Sketches for Private Distinct Counting"}, "authors": {"value": ["Jonathan Hehir", "Daniel Ting", "Graham Cormode"]}, "authorids": {"value": ["~Jonathan_Hehir1", "~Daniel_Ting1", "~Graham_Cormode1"]}, "abstract": {"value": "Data sketching is a critical tool for distinct counting, enabling multisets to be represented by compact summaries that admit fast cardinality estimates. Because sketches may be merged to summarize multiset unions, they are a basic building block in data warehouses. Although many practical sketches for cardinality estimation exist, none provide privacy when merging. We propose the first practical cardinality sketches that are simultaneously mergeable, differentially private (DP), and have low empirical errors. These introduce a novel randomized algorithm for performing logical operations on noisy bits, a tight privacy analysis, and provably optimal estimation. Our sketches dramatically outperform existing theoretical solutions in simulations and on real-world data."}, "pdf": {"value": "/pdf/71e47db5ce8c6e26faa1aea152d27808aa558eb0.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "hehir|sketchflipmerge_mergeable_sketches_for_private_distinct_counting"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4754/-/Camera_Ready_Revision", "ICML.cc/2023/Conference/-/PC_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373313893, "odate": 1686841478566, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2, "hidden": true}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Hk2fFm7W8c", "number": 4738, "cdate": 1674747222207, "tcdate": 1674747222207, "mdate": 1686841478396, "tmdate": 1686841478396, "signatures": ["ICML.cc/2023/Conference/Submission4738/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4738/Authors"], "forum": "Hk2fFm7W8c", "content": {"title": {"value": "Second-Order Optimization with Lazy Hessians"}, "authors": {"value": ["Nikita Doikov", "El Mahdi Chayti", "Martin Jaggi"]}, "authorids": {"value": ["~Nikita_Doikov1", "~El_Mahdi_Chayti2", "~Martin_Jaggi1"]}, "abstract": {"value": "We analyze Newton's method with lazy Hessian updates for solving general possibly non-convex optimization problems. We propose to reuse a previously seen Hessian for several iterations while computing new gradients at each step of the method. This significantly reduces the overall arithmetic complexity of second-order optimization schemes. By using the cubic regularization technique, we establish fast global convergence of our method to a second-order stationary point, while the Hessian does not need to be updated each iteration. For convex problems, we justify global and local superlinear rates for lazy Newton steps with quadratic regularization, which is easier to compute. The optimal frequency for updating the Hessian is once every $d$ iterations, where $d$ is the dimension of the problem. This provably improves the total arithmetic complexity of second-order algorithms by a factor $\\sqrt{d}$."}, "pdf": {"value": "/pdf/5308f44d726e9cceeb070993ba93aee6fd61ca7e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "doikov|secondorder_optimization_with_lazy_hessians"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4738/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373313499, "odate": 1686841478382, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "4yoLVter71", "number": 4724, "cdate": 1674747088425, "tcdate": 1674747088425, "mdate": 1687336694153, "tmdate": 1687336694153, "signatures": ["ICML.cc/2023/Conference/Submission4724/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4724/Authors"], "forum": "4yoLVter71", "content": {"title": {"value": "Quantile Credit Assignment"}, "authors": {"value": ["Thomas Mesnard", "Wenqi Chen", "Alaa Saade", "Yunhao Tang", "Mark Rowland", "Theophane Weber", "Clare Lyle", "Audrunas Gruslys", "Michal Valko", "Will Dabney", "Georg Ostrovski", "Eric Moulines", "Remi Munos"]}, "authorids": {"value": ["~Thomas_Mesnard2", "~Wenqi_Chen2", "~Alaa_Saade1", "~Yunhao_Tang1", "~Mark_Rowland1", "~Theophane_Weber1", "~Clare_Lyle1", "~Audrunas_Gruslys1", "~Michal_Valko1", "~Will_Dabney1", "~Georg_Ostrovski1", "~Eric_Moulines1", "~Remi_Munos1"]}, "abstract": {"value": "In reinforcement learning, the credit assignment problem is to distinguish luck from skill, that is, separate the inherent randomness in the environment from the controllable effects of the agent's actions. This paper proposes two novel algorithms, Quantile Credit Assignment (QCA) and Hindsight QCA (HQCA), which incorporate distributional value estimation to perform credit assignment. QCA uses a network that predicts the quantiles of the return distribution, whereas HQCA additionally incorporates information about the future. Both QCA and HQCA have the appealing interpretation of leveraging an estimate of the quantile level of the return (interpreted as the level of \"luck\") in order to derive a \"luck-dependent\" baseline for policy gradient methods. We show theoretically that this approach gives an unbiased policy gradient estimate that can yield significant variance reductions over a standard value estimate baseline. QCA and HQCA significantly outperform prior state-of-the-art methods on a range of extremely difficult credit assignment problems."}, "pdf": {"value": "/pdf/a54837439015f442ad0202847203dea4dae15047.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "mesnard|quantile_credit_assignment"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4724/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373313167, "odate": 1686841478224, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "ORyo7fxcIA", "number": 4695, "cdate": 1674746492114, "tcdate": 1674746492114, "mdate": 1686841478073, "tmdate": 1686841478073, "signatures": ["ICML.cc/2023/Conference/Submission4695/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4695/Authors"], "forum": "ORyo7fxcIA", "content": {"title": {"value": "Diffusion Models are Minimax Optimal Distribution Estimators"}, "authors": {"value": ["Kazusato Oko", "Shunta Akiyama", "Taiji Suzuki"]}, "authorids": {"value": ["~Kazusato_Oko1", "~Shunta_Akiyama1", "~Taiji_Suzuki1"]}, "abstract": {"value": "While efficient distribution learning is no doubt behind the groundbreaking success of diffusion modeling, its theoretical guarantees are quite limited. In this paper, we provide the first rigorous analysis on approximation and generalization abilities of diffusion modeling for well-known function spaces. The highlight of this paper is that when the true density function belongs to the Besov space and the empirical score matching loss is properly minimized, the generated data distribution achieves the nearly minimax optimal estimation rates in the total variation distance and in the Wasserstein distance of order one. Furthermore, we extend our theory to demonstrate how diffusion models adapt to low-dimensional data distributions. We expect these results advance theoretical understandings of diffusion modeling and its ability to generate verisimilar outputs."}, "pdf": {"value": "/pdf/bdfd16e602e0a5ebcd7f4f8f360e24274fe96bb6.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "oko|diffusion_models_are_minimax_optimal_distribution_estimators"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4695/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373312368, "odate": 1686841478061, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "AT8Iw8KOeC", "number": 4656, "cdate": 1674745840064, "tcdate": 1674745840064, "mdate": 1686841477858, "tmdate": 1686841477858, "signatures": ["ICML.cc/2023/Conference/Submission4656/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4656/Authors"], "forum": "AT8Iw8KOeC", "content": {"title": {"value": "Pretraining Language Models with Human Preferences"}, "authors": {"value": ["Tomasz Korbak", "Kejian Shi", "Angelica Chen", "Rasika Vinayak Bhalerao", "Christopher Buckley", "Jason Phang", "Samuel R. Bowman", "Ethan Perez"]}, "authorids": {"value": ["~Tomasz_Korbak1", "~Kejian_Shi2", "~Angelica_Chen1", "~Rasika_Vinayak_Bhalerao1", "~Christopher_Buckley1", "~Jason_Phang1", "~Samuel_R._Bowman1", "~Ethan_Perez1"]}, "abstract": {"value": "Language models (LMs) are pretrained to imitate text from large and diverse datasets that contain content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, among others. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training."}, "pdf": {"value": "/pdf/307d5e5a6e6bc1bce2c9ddee7f4bf65cd90b01a1.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "korbak|pretraining_language_models_with_human_preferences"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4656/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373311322, "odate": 1686841477845, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "neTWpgvVbo", "number": 4632, "cdate": 1674745345291, "tcdate": 1674745345291, "mdate": 1686841477715, "tmdate": 1686841477715, "signatures": ["ICML.cc/2023/Conference/Submission4632/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4632/Authors"], "forum": "neTWpgvVbo", "content": {"title": {"value": "RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank"}, "authors": {"value": ["Quentin Garrido", "Randall Balestriero", "Laurent Najman", "Yann LeCun"]}, "authorids": {"value": ["~Quentin_Garrido1", "~Randall_Balestriero1", "~Laurent_Najman1", "~Yann_LeCun1"]}, "abstract": {"value": "Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method ---coined RankMe--- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-parameters to tune. Through thorough empirical experiments involving hundreds of training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no reduction in final performance compared to the current selection method that involve a dataset's labels. We hope that RankMe will facilitate the deployment of JE-SSL towards domains that do not have the opportunity to rely on labels for representations' quality assessment."}, "pdf": {"value": "/pdf/fa0faebf278b140163f122eb20ceaf3bfb64220d.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "garrido|rankme_assessing_the_downstream_performance_of_pretrained_selfsupervised_representations_by_their_rank"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4632/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373310711, "odate": 1686841477703, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "5h42xM0pwn", "number": 4624, "cdate": 1674745213511, "tcdate": 1674745213511, "mdate": 1686841477673, "tmdate": 1686841477673, "signatures": ["ICML.cc/2023/Conference/Submission4624/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4624/Authors"], "forum": "5h42xM0pwn", "content": {"title": {"value": "Robust Budget Pacing with a Single Sample"}, "authors": {"value": ["Santiago R. Balseiro", "Rachitesh Kumar", "Vahab Mirrokni", "Balasubramanian Sivan", "Di Wang"]}, "authorids": {"value": ["~Santiago_R._Balseiro1", "~Rachitesh_Kumar1", "~Vahab_Mirrokni2", "~Balasubramanian_Sivan1", "~Di_Wang4"]}, "abstract": {"value": "Major Internet advertising platforms offer budget pacing tools as a standard service for advertisers to manage their ad campaigns. Given the inherent non-stationarity in an advertiser's value and also competing advertisers' values over time, a commonly used approach is to learn a target expenditure plan that specifies a target spend as a function of time, and then run a controller that tracks this plan. This raises the question: *how many historical samples are required to learn a good expenditure plan*? We study this question by considering an advertiser repeatedly participating in $T$ second-price auctions, where the tuple of her value and the highest competing bid is drawn from an unknown time-varying distribution. The advertiser seeks to maximize her total utility subject to her budget constraint. Prior work has shown the sufficiency of *$T\\log T$ samples per distribution* to achieve the optimal $O(\\sqrt{T})$-regret. We dramatically improve this state-of-the-art and show that *just one sample per distribution* is enough to achieve the near-optimal $\\tilde O(\\sqrt{T})$-regret, while still being robust to noise in the sampling distributions."}, "pdf": {"value": "/pdf/c0003867a4a666d4744f17633f7929edeefaa464.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "balseiro|robust_budget_pacing_with_a_single_sample"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4624/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373310495, "odate": 1686841477661, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "gsP05g8IeK", "number": 4622, "cdate": 1674745192229, "tcdate": 1674745192229, "mdate": 1687336687353, "tmdate": 1687336687353, "signatures": ["ICML.cc/2023/Conference/Submission4622/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4622/Authors"], "forum": "gsP05g8IeK", "content": {"title": {"value": "SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot"}, "authors": {"value": ["Elias Frantar", "Dan Alistarh"]}, "authorids": {"value": ["~Elias_Frantar1", "~Dan_Alistarh7"]}, "abstract": {"value": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in *one-shot, without any retraining*, at minimal loss of accuracy. This is achieved via a new pruning method called `SparseGPT`, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute `SparseGPT` on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. `SparseGPT` generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."}, "pdf": {"value": "/pdf/f15a946f97d40f05f1ad1932dc17d493d44bcd68.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "frantar|sparsegpt_massive_language_models_can_be_accurately_pruned_in_oneshot"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4622/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373310493, "odate": 1686841477635, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "kWS8mpioS9", "number": 4619, "cdate": 1674745138549, "tcdate": 1674745138549, "mdate": 1687336686804, "tmdate": 1687336686804, "signatures": ["ICML.cc/2023/Conference/Submission4619/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4619/Authors"], "forum": "kWS8mpioS9", "content": {"title": {"value": "Efficient RL via Disentangled Environment and Agent Representations"}, "authors": {"value": ["Kevin Gmelin", "Shikhar Bahl", "Russell Mendonca", "Deepak Pathak"]}, "authorids": {"value": ["~Kevin_Gmelin1", "~Shikhar_Bahl1", "~Russell_Mendonca1", "~Deepak_Pathak1"]}, "abstract": {"value": "Agents that are aware of the separation between the environments and themselves can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, which is often inexpensive to obtain, such as its shape or mask. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, SEAR (Structured Environment-Agent Representations), outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots."}, "pdf": {"value": "/pdf/06ab7aa959437088fc7431901b98b4d52ef41123.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "gmelin|efficient_rl_via_disentangled_environment_and_agent_representations"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4619/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373310426, "odate": 1686841477617, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "dEjB1SLDnt", "number": 4600, "cdate": 1674744821414, "tcdate": 1674744821414, "mdate": 1687336684285, "tmdate": 1687336684285, "signatures": ["ICML.cc/2023/Conference/Submission4600/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4600/Authors"], "forum": "dEjB1SLDnt", "content": {"title": {"value": "Evaluating Self-Supervised Learning via Risk Decomposition"}, "authors": {"value": ["Yann Dubois", "Tatsunori Hashimoto", "Percy Liang"]}, "authorids": {"value": ["~Yann_Dubois1", "~Tatsunori_Hashimoto1", "~Percy_Liang1"]}, "abstract": {"value": "Self-supervised learning (SSL) is typically evaluated using a single metric (linear probing on ImageNet), which neither provides insight into tradeoffs between models nor highlights how to improve them. To address this, we propose an SSL risk decomposition, which generalizes the classical approximation-estimation decomposition. Our decomposition consists of four error terms: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each term and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main source of errors and shows how to improve SSL in specific settings (full- vs few-shot) by trading off error components."}, "pdf": {"value": "/pdf/60fe2491b40299948a0a3ee3f243c229bfb3760e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "dubois|evaluating_selfsupervised_learning_via_risk_decomposition"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4600/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373309976, "odate": 1686841477457, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "RrusCGfAZ1", "number": 4585, "cdate": 1674744623628, "tcdate": 1674744623628, "mdate": 1687336683497, "tmdate": 1687336683497, "signatures": ["ICML.cc/2023/Conference/Submission4585/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4585/Authors"], "forum": "RrusCGfAZ1", "content": {"title": {"value": "Multicalibration as Boosting for Regression"}, "authors": {"value": ["Ira Globus-Harris", "Declan Harrison", "Michael Kearns", "Aaron Roth", "Jessica Sorrell"]}, "authorids": {"value": ["~Ira_Globus-Harris1", "declanharrison@gmail.com", "~Michael_Kearns2", "~Aaron_Roth1", "~Jessica_Sorrell1"]}, "abstract": {"value": "We study the connection between multicalibration and boosting for squared error regression. First we prove a useful characterization of multicalibration in terms of a ``swap regret'' like condition on squared error. Using this characterization, we give an exceedingly simple algorithm that can be analyzed both as a boosting algorithm for regression and as a multicalibration algorithm for a class $\\mathcal{H}$ that makes use only of a standard squared error regression oracle for $\\mathcal{H}$. We give a weak learning assumption on $\\mathcal{H}$ that ensures convergence to Bayes optimality without the need to make any realizability assumptions --- giving us an agnostic boosting algorithm for regression. We then show that our weak learning assumption on $\\mathcal{H}$ is both necessary and sufficient for multicalibration with respect to $\\mathcal{H}$ to imply Bayes optimality, answering an open question. We also show that if $\\mathcal{H}$ satisfies our weak learning condition relative to another class $\\mathcal{C}$ then multicalibration with respect to $\\mathcal{H}$ implies multicalibration with respect to $\\mathcal{C}$. Finally we investigate the empirical performance of our algorithm experimentally."}, "pdf": {"value": "/pdf/e7229543bcb721c3177d11514a300e63fc824601.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "globusharris|multicalibration_as_boosting_for_regression"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4585/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373309701, "odate": 1686841477412, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "9TbDVDX7de", "number": 4556, "cdate": 1674744019869, "tcdate": 1674744019869, "mdate": 1687336681013, "tmdate": 1687336681013, "signatures": ["ICML.cc/2023/Conference/Submission4556/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4556/Authors"], "forum": "9TbDVDX7de", "content": {"title": {"value": "Do Perceptually Aligned Gradients Imply Robustness?"}, "authors": {"value": ["Roy Ganz", "Bahjat Kawar", "Michael Elad"]}, "authorids": {"value": ["~Roy_Ganz1", "~Bahjat_Kawar1", "~Michael_Elad1"]}, "abstract": {"value": "Adversarially robust classifiers possess a trait that non-robust models do not - Perceptually Aligned Gradients (PAG). Their gradients with respect to the input align well with human perception. Several works have identified PAG as a byproduct of robust training, but none have considered it as a standalone phenomenon nor studied its own implications. In this work, we focus on this trait and test whether Perceptually Aligned Gradients imply Robustness. To this end, we develop a novel objective to directly promote PAG in training classifiers and examine whether models with such gradients are more robust to adversarial attacks. Extensive experiments on multiple datasets and architectures validate that models with aligned gradients exhibit significant robustness, exposing the surprising bidirectional connection between PAG and robustness. Lastly, we show that better gradient alignment leads to increased robustness and harness this observation to boost the robustness of existing adversarial training techniques."}, "pdf": {"value": "/pdf/c14f1c3ab7b428bf0c9372581e6c8ab80b81024c.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ganz|do_perceptually_aligned_gradients_imply_robustness"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4556/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373309030, "odate": 1686841477215, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "s58a6Pxw7V", "number": 4512, "cdate": 1674743183424, "tcdate": 1674743183424, "mdate": 1686841476950, "tmdate": 1686841476950, "signatures": ["ICML.cc/2023/Conference/Submission4512/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4512/Authors"], "forum": "s58a6Pxw7V", "content": {"title": {"value": "Spherical Inducing Features for Orthogonally-Decoupled Gaussian Processes"}, "authors": {"value": ["Louis C. Tiao", "Vincent Dutordoir", "Victor Picheny"]}, "authorids": {"value": ["~Louis_C._Tiao1", "~Vincent_Dutordoir1", "~Victor_Picheny2"]}, "abstract": {"value": "Despite their many desirable properties, Gaussian processes (GPs) are often compared unfavorably to deep neural networks (NNs) for lacking the ability to learn representations. Recent efforts to bridge the gap between GPs and deep NNs have yielded a new class of inter-domain variational GPs in which the inducing variables correspond to hidden units of a feedforward NN. In this work, we examine some practical issues associated with this approach and propose an extension that leverages the orthogonal decomposition of GPs to mitigate these limitations. In particular, we introduce spherical inter-domain features to construct more flexible data-dependent basis functions for both the principal and orthogonal components of the GP approximation and show that incorporating NN activation features under this framework not only alleviates these shortcomings but is more scalable than alternative strategies. Experiments on multiple benchmark datasets demonstrate the effectiveness of our approach."}, "pdf": {"value": "/pdf/fb18aa979a003629a19be7ea751a11b4a939f3f1.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "tiao|spherical_inducing_features_for_orthogonallydecoupled_gaussian_processes"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4512/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373307794, "odate": 1686841476937, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "eSpbTG0TZN", "number": 4497, "cdate": 1674742814243, "tcdate": 1674742814243, "mdate": 1687336677041, "tmdate": 1687336677041, "signatures": ["ICML.cc/2023/Conference/Submission4497/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4497/Authors"], "forum": "eSpbTG0TZN", "content": {"title": {"value": "Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels"}, "authors": {"value": ["Sai Rajeswar", "Pietro Mazzaglia", "Tim Verbelen", "Alexandre Pich\u00e9", "Bart Dhoedt", "Aaron Courville", "Alexandre Lacoste"]}, "authorids": {"value": ["~Sai_Rajeswar2", "~Pietro_Mazzaglia1", "~Tim_Verbelen1", "~Alexandre_Pich\u00e91", "~Bart_Dhoedt1", "~Aaron_Courville3", "~Alexandre_Lacoste1"]}, "abstract": {"value": "Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-scale empirical study, which we use to validate our design choices and analyze our models. We also show robust performance on the Real-Word RL benchmark, hinting at resiliency to environment perturbations during adaptation. Project website: https://masteringurlb.github.io/"}, "pdf": {"value": "/pdf/ff14c76e8ebbd5bdb8dd7d085def2b860e2a18a9.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "rajeswar|mastering_the_unsupervised_reinforcement_learning_benchmark_from_pixels"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4497/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373307393, "odate": 1686841476859, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "JSTp7NiuYi", "number": 4491, "cdate": 1674742693843, "tcdate": 1674742693843, "mdate": 1686841476803, "tmdate": 1686841476803, "signatures": ["ICML.cc/2023/Conference/Submission4491/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4491/Authors"], "forum": "JSTp7NiuYi", "content": {"title": {"value": "SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural Networks at the Edge"}, "authors": {"value": ["Mahdi Nikdan", "Tommaso Pegolotti", "Eugenia Iofinova", "Eldar Kurtic", "Dan Alistarh"]}, "authorids": {"value": ["~Mahdi_Nikdan1", "~Tommaso_Pegolotti1", "~Eugenia_Iofinova1", "~Eldar_Kurtic1", "~Dan_Alistarh7"]}, "abstract": {"value": "We provide an efficient implementation of the backpropagation algorithm, specialized to the case where the weights of the neural network being trained are _sparse_. Our algorithm is general, as it applies to arbitrary (unstructured) sparsity and common layer types (e.g., convolutional or linear). We provide a fast vectorized implementation on commodity CPUs, and show that it can yield speedups in end-to-end runtime experiments, both in transfer learning using already-sparsified networks, and in training sparse networks from scratch. Thus, our results provide the first support for sparse training on commodity hardware."}, "pdf": {"value": "/pdf/d096e1538d205745275662fb001fa0440be498fb.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "nikdan|sparseprop_efficient_sparse_backpropagation_for_faster_training_of_neural_networks_at_the_edge"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4491/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373307191, "odate": 1686841476792, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "qrH8ERUBcE", "number": 4334, "cdate": 1674739346793, "tcdate": 1674739346793, "mdate": 1686841475907, "tmdate": 1686841475907, "signatures": ["ICML.cc/2023/Conference/Submission4334/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4334/Authors"], "forum": "qrH8ERUBcE", "content": {"title": {"value": "Hierarchies of Reward Machines"}, "authors": {"value": ["Daniel Furelos-Blanco", "Mark Law", "Anders Jonsson", "Krysia Broda", "Alessandra Russo"]}, "authorids": {"value": ["~Daniel_Furelos-Blanco1", "~Mark_Law1", "~Anders_Jonsson1", "~Krysia_Broda1", "~Alessandra_Russo1"]}, "abstract": {"value": "Reward machines (RMs) are a recent formalism for representing the reward function of a reinforcement learning task through a finite-state machine whose edges encode subgoals of the task using high-level events. The structure of RMs enables the decomposition of a task into simpler and independently solvable subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a formalism for further abstracting the subtask structure by endowing an RM with the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We exploit HRMs by treating each call to an RM as an independently solvable subtask using the options framework, and describe a curriculum-based method to learn HRMs from traces observed by the agent. Our experiments reveal that exploiting a handcrafted HRM leads to faster convergence than with a flat HRM, and that learning an HRM is feasible in cases where its equivalent flat representation is not."}, "pdf": {"value": "/pdf/958a268177386f4bd53c1f26a0419c7fc45fb707.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "furelosblanco|hierarchies_of_reward_machines"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4334/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373303541, "odate": 1686841475894, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "bpRTAnJ8LW", "number": 4332, "cdate": 1674739316011, "tcdate": 1674739316011, "mdate": 1687336658806, "tmdate": 1687336658806, "signatures": ["ICML.cc/2023/Conference/Submission4332/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4332/Authors"], "forum": "bpRTAnJ8LW", "content": {"title": {"value": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, "authors": {"value": ["Stella Biderman", "Hailey Schoelkopf", "Quentin Gregory Anthony", "Herbie Bradley", "Kyle O'Brien", "Eric Hallahan", "Mohammad Aflah Khan", "Shivanshu Purohit", "USVSN Sai Prashanth", "Edward Raff", "Aviya Skowron", "Lintang Sutawika", "Oskar van der Wal"]}, "authorids": {"value": ["~Stella_Biderman1", "~Hailey_Schoelkopf1", "~Quentin_Gregory_Anthony1", "~Herbie_Bradley1", "~Kyle_O'Brien1", "~Eric_Hallahan1", "~Mohammad_Aflah_Khan1", "~Shivanshu_Purohit1", "~USVSN_Sai_Prashanth1", "~Edward_Raff1", "aviyaskowron@gmail.com", "~Lintang_Sutawika1", "~Oskar_van_der_Wal1"]}, "abstract": {"value": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce *Pythia*, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend *Pythia* to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia."}, "pdf": {"value": "/pdf/2d4dbe85261707b9652bcb7fb5cc97b6af2e59e3.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "biderman|pythia_a_suite_for_analyzing_large_language_models_across_training_and_scaling"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4332/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373303458, "odate": 1686841475853, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "CXkJh2ITml", "number": 4280, "cdate": 1674737886077, "tcdate": 1674737886077, "mdate": 1687336653184, "tmdate": 1687336653184, "signatures": ["ICML.cc/2023/Conference/Submission4280/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4280/Authors"], "forum": "CXkJh2ITml", "content": {"title": {"value": "Bayes-optimal Learning of Deep Random Networks of Extensive-width"}, "authors": {"value": ["Hugo Cui", "Florent Krzakala", "Lenka Zdeborova"]}, "authorids": {"value": ["~Hugo_Cui1", "~Florent_Krzakala1", "~Lenka_Zdeborova1"]}, "abstract": {"value": "We consider the problem of learning a target function corresponding to a deep, extensive-width, non-linear neural network with random Gaussian weights. We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large and propose a closed-form expression for the Bayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, achieve Bayes-optimal performances, while the logistic loss yields a near-optimal test error for classification. We further show numerically that when the number of samples grows faster than the dimension, ridge and kernel methods become suboptimal, while neural networks achieve test error close to zero from quadratically many samples."}, "pdf": {"value": "/pdf/18a6b02f794508a39c8bab7e44e2bff038c378d2.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "cui|bayesoptimal_learning_of_deep_random_networks_of_extensivewidth"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4280/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373302110, "odate": 1686841475492, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "1F2Opw8CGA", "number": 4219, "cdate": 1674736209295, "tcdate": 1674736209295, "mdate": 1687336649125, "tmdate": 1687336649125, "signatures": ["ICML.cc/2023/Conference/Submission4219/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4219/Authors"], "forum": "1F2Opw8CGA", "content": {"title": {"value": "Structure-informed Language Models Are Protein Designers"}, "authors": {"value": ["Zaixiang Zheng", "Yifan Deng", "Dongyu Xue", "Yi Zhou", "Fei YE", "Quanquan Gu"]}, "authorids": {"value": ["~Zaixiang_Zheng2", "~Yifan_Deng1", "~Dongyu_Xue1", "~Yi_Zhou11", "~Fei_YE4", "~Quanquan_Gu1"]}, "abstract": {"value": "This paper demonstrates that language models are strong structure-based protein designers. We present LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds. We conduct a structural surgery on pLMs, where a lightweight structural adapter is implanted into pLMs and endows it with structural awareness. During inference, iterative refinement is performed to effectively optimize the generated protein sequences. Experiments show that LM-Design improves the state-of-the-art results by a large margin, leading to 4% to 12% accuracy gains in sequence recovery (e.g., 55.65%/56.63% on CATH 4.2/4.3 single-chain benchmarks, and >60% when designing protein complexes). We provide extensive and in-depth analyses, which verify that LM-Design can (1) indeed leverage both structural and sequential knowledge to accurately handle structurally non-deterministic regions, (2) benefit from scaling data and model size, and (3) generalize to other proteins (e.g., antibodies and de novo proteins)."}, "pdf": {"value": "/pdf/379821fde2133a619e6751756327edbb482f5f08.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "zheng|structureinformed_language_models_are_protein_designers"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4219/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373300621, "odate": 1686841475213, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "4RvcXByvnR", "number": 4173, "cdate": 1674735232957, "tcdate": 1674735232957, "mdate": 1686841474827, "tmdate": 1686841474827, "signatures": ["ICML.cc/2023/Conference/Submission4173/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4173/Authors"], "forum": "4RvcXByvnR", "content": {"title": {"value": "Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning"}, "authors": {"value": ["Sam Lobel", "Akhil Bagaria", "George Konidaris"]}, "authorids": {"value": ["~Sam_Lobel1", "~Akhil_Bagaria1", "~George_Konidaris1"]}, "abstract": {"value": "We propose a new method for count-based exploration in high-dimensional state spaces. Unlike previous work which relies on density models, we show that counts can be derived by averaging samples from the Rademacher distribution (or coin flips). This insight is used to set up a simple supervised learning objective which, when optimized, yields a state's visitation count. We show that our method is significantly more effective at deducing ground-truth visitation counts than previous work; when used as an exploration bonus for a model-free reinforcement learning algorithm, it outperforms existing approaches on most of 9 challenging exploration tasks, including the Atari game Montezuma's Revenge."}, "pdf": {"value": "/pdf/3d7b5fe2f7c2df1ad6865d3758fd653b85a6e8cd.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "lobel|flipping_coins_to_estimate_pseudocounts_for_exploration_in_reinforcement_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4173/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373299336, "odate": 1686841474814, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "1sxiBaGEtg", "number": 4157, "cdate": 1674734713452, "tcdate": 1674734713452, "mdate": 1686927951153, "tmdate": 1686927951153, "signatures": ["ICML.cc/2023/Conference/Submission4157/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4157/Authors"], "forum": "1sxiBaGEtg", "content": {"title": {"value": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, "authors": {"value": ["Michael Poli", "Stefano Massaroli", "Eric Nguyen", "Daniel Y Fu", "Tri Dao", "Stephen Baccus", "Yoshua Bengio", "Stefano Ermon", "Christopher Re"]}, "authorids": {"value": ["~Michael_Poli1", "~Stefano_Massaroli1", "~Eric_Nguyen1", "~Daniel_Y_Fu1", "~Tri_Dao1", "~Stephen_Baccus2", "~Yoshua_Bengio1", "~Stefano_Ermon1", "~Christopher_Re1"]}, "abstract": {"value": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers at scale, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In challenging reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-space models, transfer functions, and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets WikiText103 and The Pile, reaching Transformer quality with a 20% reduction in training compute required at sequence length 2k. Hyena operators are 2x faster than highly optimized attention at sequence length 8k, with speedups of 100x at 64k."}, "pdf": {"value": "/pdf/2cbbcd07a0a9df3e388b001f95e5fa80295a6040.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "poli|hyena_hierarchy_towards_larger_convolutional_language_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4157/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373298926, "odate": 1686841474685, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "4weSHLFgtZ", "number": 4138, "cdate": 1674734313252, "tcdate": 1674734313252, "mdate": 1686841474504, "tmdate": 1686841474504, "signatures": ["ICML.cc/2023/Conference/Submission4138/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4138/Authors"], "forum": "4weSHLFgtZ", "content": {"title": {"value": "GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration"}, "authors": {"value": ["Naoki Murata", "Koichi Saito", "Chieh-Hsin Lai", "Yuhta Takida", "Toshimitsu Uesaka", "Yuki Mitsufuji", "Stefano Ermon"]}, "authorids": {"value": ["~Naoki_Murata1", "~Koichi_Saito1", "~Chieh-Hsin_Lai2", "~Yuhta_Takida1", "~Toshimitsu_Uesaka1", "~Yuki_Mitsufuji1", "~Stefano_Ermon1"]}, "abstract": {"value": "Pre-trained diffusion models have been successfully used as priors in a variety of linear inverse problems, where the goal is to reconstruct a signal from noisy linear measurements. However, existing approaches require knowledge of the linear operator. In this paper, we propose GibbsDDRM, an extension of Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the linear measurement operator is unknown. GibbsDDRM constructs a joint distribution of the data, measurements, and linear operator by using a pre-trained diffusion model for the data prior, and it solves the problem by posterior sampling with an efficient variant of a Gibbs sampler. The proposed method is problem-agnostic, meaning that a pre-trained diffusion model can be applied to various inverse problems without fine-tuning. In experiments, it achieved high performance on both blind image deblurring and vocal dereverberation tasks, despite the use of simple generic priors for the underlying linear operators."}, "pdf": {"value": "/pdf/c6bc1117aa7cd57c9cea3ba9dc4e07b2fe55d68a.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "murata|gibbsddrm_a_partially_collapsed_gibbs_sampler_for_solving_blind_inverse_problems_with_denoising_diffusion_restoration"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4138/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373298423, "odate": 1686841474492, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "LZvsnGH0eG", "number": 4066, "cdate": 1674732101411, "tcdate": 1674732101411, "mdate": 1687336636740, "tmdate": 1687336636740, "signatures": ["ICML.cc/2023/Conference/Submission4066/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4066/Authors"], "forum": "LZvsnGH0eG", "content": {"title": {"value": "Unifying Nesterov's Accelerated Gradient Methods for Convex and Strongly Convex Objective Functions"}, "authors": {"value": ["Jungbin Kim", "Insoon Yang"]}, "authorids": {"value": ["~Jungbin_Kim1", "~Insoon_Yang2"]}, "abstract": {"value": "Although Nesterov's accelerated gradient method (AGM) has been studied from various perspectives, it remains unclear why the most popular forms of AGMs must handle convex and strongly convex objective functions separately. To address this inconsistency, we propose a novel unified framework for Lagrangians, ordinary differential equation (ODE) models, and algorithms. As a special case, our new simple momentum algorithm, which we call the unified AGM, seamlessly bridges the gap between the two most popular forms of Nesterov's AGM and has a superior convergence guarantee compared to existing algorithms for non-strongly convex objective functions. This property is beneficial in practice when considering ill-conditioned $\\mu$-strongly convex objective functions (with small $\\mu$). Furthermore, we generalize this algorithm and the corresponding ODE model to the higher-order non-Euclidean setting. Last but not least, our unified framework is used to construct the unified AGM-G ODE, a novel ODE model for minimizing the gradient norm of strongly convex functions."}, "pdf": {"value": "/pdf/b389f775ea39902dd7639824108f9eebfa759b74.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kim|unifying_nesterovs_accelerated_gradient_methods_for_convex_and_strongly_convex_objective_functions"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4066/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373294612, "odate": 1686841474074, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "jJXuL3hQvt", "number": 4043, "cdate": 1674731396911, "tcdate": 1674731396911, "mdate": 1686841473861, "tmdate": 1686841473861, "signatures": ["ICML.cc/2023/Conference/Submission4043/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4043/Authors"], "forum": "jJXuL3hQvt", "content": {"title": {"value": "HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption"}, "authors": {"value": ["Seewoo Lee", "Garam Lee", "Jung Woo Kim", "Junbum Shin", "Mun-Kyu Lee"]}, "authorids": {"value": ["~Seewoo_Lee1", "~Garam_Lee2", "~Jung_Woo_Kim1", "~Junbum_Shin1", "~Mun-Kyu_Lee1"]}, "abstract": {"value": "Transfer learning is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and fine-tuning new classification layers to a model pre-trained on large datasets. Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in transfer learning in the machine learning as a service setting, most of them only focused on encrypted inference. In this study, we present HETAL, an efficient Homomorphic Encryption based Transfer Learning algorithm, that protects the client's privacy in training tasks by encrypting the client data using the CKKS homomorphic encryption scheme. HETAL is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training. We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster than prior methods, and a highly precise softmax approximation algorithm with increased coverage. The experimental results for five well-known benchmark datasets show total training times of 567--3442 seconds, which is less than an hour."}, "pdf": {"value": "/pdf/e3d4114ef758eb52c8ae9193997152d1e28fedc0.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "lee|hetal_efficient_privacypreserving_transfer_learning_with_homomorphic_encryption"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4043/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373294017, "odate": 1686841473849, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "1lqOZrdXeG", "number": 4018, "cdate": 1674730791345, "tcdate": 1674730791345, "mdate": 1687336631360, "tmdate": 1687336631360, "signatures": ["ICML.cc/2023/Conference/Submission4018/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4018/Authors"], "forum": "1lqOZrdXeG", "content": {"title": {"value": "Direct Parameterization of Lipschitz-Bounded Deep Networks"}, "authors": {"value": ["Ruigang Wang", "Ian Manchester"]}, "authorids": {"value": ["~Ruigang_Wang2", "~Ian_Manchester1"]}, "abstract": {"value": "This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed $\\ell^2$ Lipschitz bounds, i.e. limited sensitivity to input perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP). We provide a ``direct'' parameterization, i.e., a smooth mapping from $\\mathbb R^N$ onto the set of weights satisfying the SDP-based bound. Moreover, our parameterization is complete, i.e. a neural network satisfies the SDP bound if and only if it can be represented via our parameterization. This enables training using standard gradient methods, without any inner approximation or computationally intensive tasks (e.g. projections or barrier terms) for the SDP constraint. The new parameterization can equivalently be thought of as either a new layer type (the *sandwich layer*), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. A comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on both empirical and certified robust accuracy. Code is available at https://github.com/acfr/LBDN."}, "pdf": {"value": "/pdf/4fcf45f19491336b6516c24d105aa7c97dec7444.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wang|direct_parameterization_of_lipschitzbounded_deep_networks"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4018/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373293252, "odate": 1686841473673, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "dx5rPfq6Hr", "number": 4007, "cdate": 1674730508050, "tcdate": 1674730508050, "mdate": 1687336629239, "tmdate": 1687336629239, "signatures": ["ICML.cc/2023/Conference/Submission4007/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission4007/Authors"], "forum": "dx5rPfq6Hr", "content": {"title": {"value": "Towards Theoretical Understanding of Inverse Reinforcement Learning"}, "authors": {"value": ["Alberto Maria Metelli", "Filippo Lazzati", "Marcello Restelli"]}, "authorids": {"value": ["~Alberto_Maria_Metelli2", "~Filippo_Lazzati2", "~Marcello_Restelli1"]}, "abstract": {"value": "Inverse reinforcement learning (IRL) denotes a powerful family of algorithms for recovering a reward function justifying the behavior demonstrated by an expert agent. A well-known limitation of IRL is the ambiguity in the choice of the reward function, due to the existence of multiple rewards that explain the observed behavior. This limitation has been recently circumvented by formulating IRL as the problem of estimating the feasible reward set, i.e., the region of the rewards compatible with the expert's behavior. In this paper, we make a step towards closing the theory gap of IRL in the case of finite-horizon problems with a generative model. We start by formally introducing the problem of estimating the feasible reward set, the corresponding PAC requirement, and discussing the properties of particular classes of rewards. Then, we provide the first minimax lower bound on the sample complexity for the problem of estimating the feasible reward set of order ${\\Omega}\\left( \\frac{H^3SA}{\\epsilon^2} \\left( \\log \\left(\\frac{1}{\\delta}\\right) + S \\right)\\right)$, being $S$ and $A$ the number of states and actions respectively, $H$ the horizon, $\\epsilon$ the desired accuracy, and $\\delta$ the confidence. We analyze the sample complexity of a uniform sampling strategy (US-IRL), proving a matching upper bound up to logarithmic factors. Finally, we outline several open questions in IRL and propose future research directions."}, "pdf": {"value": "/pdf/b0c202d6375bc6563929d43e5e0270d490079b41.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "metelli|towards_theoretical_understanding_of_inverse_reinforcement_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission4007/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373292988, "odate": 1686841473574, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "22WDLG6fBO", "number": 3999, "cdate": 1674730314665, "tcdate": 1674730314665, "mdate": 1686841473549, "tmdate": 1686841473549, "signatures": ["ICML.cc/2023/Conference/Submission3999/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3999/Authors"], "forum": "22WDLG6fBO", "content": {"title": {"value": "Pre-training for Speech Translation: CTC Meets Optimal Transport"}, "authors": {"value": ["Phuong-Hang Le", "Hongyu Gong", "Changhan Wang", "Juan Pino", "Benjamin Lecouteux", "Didier Schwab"]}, "authorids": {"value": ["~Phuong-Hang_Le1", "~Hongyu_Gong1", "~Changhan_Wang1", "~Juan_Pino2", "~Benjamin_Lecouteux1", "~Didier_Schwab1"]}, "abstract": {"value": "The gap between speech and text modalities is a major challenge in speech-to-text translation (ST). Different methods have been proposed to reduce this gap, but most of them require architectural changes in ST training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the ST model. First, we show that the connectionist temporal classification (CTC) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with CTC consistently achieves better final ST accuracy. Nevertheless, CTC is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining CTC and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wasserstein space. Extensive experiments on the standard CoVoST-2 and MuST-C datasets show that our pre-training method applied to the vanilla encoder-decoder Transformer achieves state-of-the-art performance under the no-external-data setting, and performs on par with recent strong multi-task learning systems trained with external data. Finally, our method can also be applied on top of these multi-task systems, leading to further improvements for these models."}, "pdf": {"value": "/pdf/9d42954658b084558be6b5d8ca9826543fc58e08.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "le|pretraining_for_speech_translation_ctc_meets_optimal_transport"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3999/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373292754, "odate": 1686841473538, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "9UCTB84L6e", "number": 3906, "cdate": 1674727463675, "tcdate": 1674727463675, "mdate": 1687336621402, "tmdate": 1687336621402, "signatures": ["ICML.cc/2023/Conference/Submission3906/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3906/Authors"], "forum": "9UCTB84L6e", "content": {"title": {"value": "Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models"}, "authors": {"value": ["Hong Liu", "Sang Michael Xie", "Zhiyuan Li", "Tengyu Ma"]}, "authorids": {"value": ["~Hong_Liu5", "~Sang_Michael_Xie1", "~Zhiyuan_Li2", "~Tengyu_Ma1"]}, "abstract": {"value": "Language modeling on large-scale datasets improves performance of various downstream tasks. The validation pre-training loss is often used as the evaluation metric for language models since the pre-training loss tends to be well-correlated with downstream performance (which is itself hard to evaluate comprehensively). Contrary to the conventional wisdom, this paper shows that 1) pre-training loss cannot fully explain downstream performance and 2) flatness of the model is well-correlated with downstream performance where pre-training loss is not. We identify three ways to produce models with the same pre-training loss but different downstream performance: continue pre-training after convergence, increasing the model size, and changing the pre-training algorithms. These experiments demonstrate the existence of implicit bias of pre-training algorithms---among models with the same minimal pre-training loss, they implicitly prefer more transferable ones. Toward understanding this implicit bias, we prove that SGD with standard mini-batch noise implicitly prefers flatter minima of pre-training loss in language models, and empirically observe a strong correlation between flatness (measured by the trace of Hessian) and downstream performance among models with the same pre-training loss. We also prove in a synthetic language setting that among models with the minimal pre-training loss, the flattest model transfers to downstream tasks."}, "pdf": {"value": "/pdf/6d9b4d4d83f765a05ac05914f9857a4abdbd7ba5.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "liu|same_pretraining_loss_better_downstream_implicit_bias_matters_for_language_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3906/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373290543, "odate": 1686841473011, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "DH11pt7S2t", "number": 3868, "cdate": 1674726444924, "tcdate": 1674726444924, "mdate": 1686841472738, "tmdate": 1686841472738, "signatures": ["ICML.cc/2023/Conference/Submission3868/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3868/Authors"], "forum": "DH11pt7S2t", "content": {"title": {"value": "Facial Expression Recognition with Adaptive Frame Rate based on Multiple Testing Correction"}, "authors": {"value": ["Andrey Savchenko"]}, "authorids": {"value": ["~Andrey_Savchenko1"]}, "abstract": {"value": "In this paper, we consider the problem of the high computational complexity of video-based facial expression recognition. A novel sequential procedure is proposed with an adaptive frame rate selection in a short video fragment to speed up decision-making. We automatically adjust the frame rate and process fewer frames with a low frame rate for more straightforward videos and more frames for complex ones. To determine the frame rate at which an inference is sufficiently reliable, the Benjamini-Hochberg procedure from multiple comparisons theory is employed to control the false discovery rate. The main advantages of our method are an improvement of the trustworthiness of decision-making by maintaining only one hyper-parameter (false acceptance rate) and its applicability with arbitrary neural network models used as facial feature extractors without the need to re-train these models. An experimental study on datasets from ABAW and EmotiW challenges proves the superior performance (1.5-40 times faster) of the proposed approach compared to processing all frames and existing techniques with early exiting and adaptive frame selection."}, "pdf": {"value": "/pdf/a250c6805d810e4063526a6f43d58ed7be557fb5.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "savchenko|facial_expression_recognition_with_adaptive_frame_rate_based_on_multiple_testing_correction"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3868/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373289560, "odate": 1686841472725, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "lKoEeUpkVm", "number": 3867, "cdate": 1674726389713, "tcdate": 1674726389713, "mdate": 1686841472714, "tmdate": 1686841472714, "signatures": ["ICML.cc/2023/Conference/Submission3867/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3867/Authors"], "forum": "lKoEeUpkVm", "content": {"title": {"value": "Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization"}, "authors": {"value": ["Chanyeong Kim", "Jongwoong Park", "Hyunglip Bae", "Woo Chang Kim"]}, "authorids": {"value": ["~Chanyeong_Kim1", "~Jongwoong_Park1", "~Hyunglip_Bae1", "~Woo_Chang_Kim1"]}, "abstract": {"value": "Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase. Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems. Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based stagewise decomposition algorithm. This innovative approach leverages the structural advantages of the Transformer model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function. Through our numerical experiments, we affirm TranSDDP's effectiveness in addressing MSP problems. It efficiently generates a piecewise linear approximation for the value function, significantly reducing computation time while preserving solution quality, thus marking a promising progression in the treatment of large-scale multistage stochastic programming problems."}, "pdf": {"value": "/pdf/a556e787a905fbce1da3eaa6d1cd8172f6d9a36e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kim|transformerbased_stagewise_decomposition_for_largescale_multistage_stochastic_optimization"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3867/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373289522, "odate": 1686841472702, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "O1j4uFuSVW", "number": 3848, "cdate": 1674725881837, "tcdate": 1674725881837, "mdate": 1686841472567, "tmdate": 1686841472567, "signatures": ["ICML.cc/2023/Conference/Submission3848/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3848/Authors"], "forum": "O1j4uFuSVW", "content": {"title": {"value": "Adapting to game trees in zero-sum imperfect information games"}, "authors": {"value": ["C\u00f4me Fiegel", "Pierre MENARD", "Tadashi Kozuno", "Remi Munos", "Vianney Perchet", "Michal Valko"]}, "authorids": {"value": ["~C\u00f4me_Fiegel1", "~Pierre_MENARD1", "~Tadashi_Kozuno1", "~Remi_Munos1", "~Vianney_Perchet3", "~Michal_Valko1"]}, "abstract": {"value": "Imperfect information games (IIG) are games in which each player only partially observes the current game state. We study how to learn $\\epsilon$-optimal strategies in a zero-sum IIG through self-play with trajectory feedback. We give a problem-independent lower bound $\\widetilde{\\mathcal{O}}(H(A_{\\mathcal{X}}+B_{\\mathcal{Y}})/\\epsilon^2)$ on the required number of realizations to learn these strategies with high probability, where $H$ is the length of the game, $A_{\\mathcal{X}}$ and $B_{\\mathcal{Y}}$ are the total number of actions for the two players. We also propose two Follow the Regularized leader (FTRL) algorithms for this setting: Balanced FTRL which matches this lower bound, but requires the knowledge of the information set structure beforehand to define the regularization; and Adaptive FTRL which needs $\\widetilde{\\mathcal{O}}(H^2(A_{\\mathcal{X}}+B_{\\mathcal{Y}})/\\epsilon^2)$ realizations without this requirement by progressively adapting the regularization to the observations."}, "pdf": {"value": "/pdf/5eae52f717399458e1365d0f39947a20d4d64682.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "fiegel|adapting_to_game_trees_in_zerosum_imperfect_information_games"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3848/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373289100, "odate": 1686841472555, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "eIQIcUKs0T", "number": 3818, "cdate": 1674724855604, "tcdate": 1674724855604, "mdate": 1687336615289, "tmdate": 1687336615289, "signatures": ["ICML.cc/2023/Conference/Submission3818/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3818/Authors"], "forum": "eIQIcUKs0T", "content": {"title": {"value": " Mu$^2$SLAM: Multitask, Multilingual Speech and Language Models"}, "authors": {"value": ["Yong Cheng", "Yu Zhang", "Melvin Johnson", "Wolfgang Macherey", "Ankur Bapna"]}, "authorids": {"value": ["~Yong_Cheng3", "~Yu_Zhang2", "~Melvin_Johnson1", "~Wolfgang_Macherey1", "~Ankur_Bapna1"]}, "abstract": {"value": "We present Mu$^2$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^2$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling objective (MLM) on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^2$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-T decoder, despite using a relatively weaker Transformer decoder. On text understanding tasks, our model improves by more than 6% over mSLAM on XNLI, getting closer to the performance of mT5 models of comparable capacity on XNLI and TydiQA, paving the way towards a single model for all speech and text understanding tasks."}, "pdf": {"value": "/pdf/c0a3bffb47dd1c64d72b676d8edd80d05d428f6b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "cheng|mu^2slam_multitask_multilingual_speech_and_language_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3818/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373288324, "odate": 1686841472381, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Rw8OOwatgy", "number": 3801, "cdate": 1674724284360, "tcdate": 1674724284360, "mdate": 1687336614415, "tmdate": 1687336614415, "signatures": ["ICML.cc/2023/Conference/Submission3801/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3801/Authors"], "forum": "Rw8OOwatgy", "content": {"title": {"value": "Semi Bandit dynamics in Congestion Games: Convergence to Nash Equilibrium and No-Regret Guarantees."}, "authors": {"value": ["Ioannis Panageas", "Stratis Skoulakis", "Luca Viano", "Xiao Wang", "Volkan Cevher"]}, "authorids": {"value": ["~Ioannis_Panageas1", "~Stratis_Skoulakis2", "~Luca_Viano1", "~Xiao_Wang4", "~Volkan_Cevher1"]}, "abstract": {"value": "In this work, we propose introduce a variant of online stochastic gradient descent and prove it converges to Nash equilibria and simultaneously it has sublinear regret for the class of congestion games in the semi-bandit feedback setting. Our proposed method admits convergence rates depending only polynomially on the number of players and the number of facilities, but not on the size of the action set, which can be exponentially large in terms of the number of facilities. Moreover, the running time of our method has polynomial-time dependence on the implicit description of the game. Our analysis exploits techniques from convex geometry, in particular Caratheodory's theorem and recent advances in non-convex stochastic optimization. This work improves upon and answers an open question from (Cui et al 2022)."}, "pdf": {"value": "/pdf/0c2c6e1b20e134225b65e5d35bfa57984c695be3.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "panageas|semi_bandit_dynamics_in_congestion_games_convergence_to_nash_equilibrium_and_noregret_guarantees"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3801/-/Camera_Ready_Revision", "ICML.cc/2023/Conference/-/PC_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373287887, "odate": 1686841472311, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2, "hidden": true}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "ayBKRjGDEI", "number": 3790, "cdate": 1674723917678, "tcdate": 1674723917678, "mdate": 1687336613439, "tmdate": 1687336613439, "signatures": ["ICML.cc/2023/Conference/Submission3790/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3790/Authors"], "forum": "ayBKRjGDEI", "content": {"title": {"value": "Differentially Private Hierarchical Clustering with Provable Approximation Guarantees"}, "authors": {"value": ["Jacob Imola", "Alessandro Epasto", "Mohammad Mahdian", "Vincent Cohen-Addad", "Vahab Mirrokni"]}, "authorids": {"value": ["~Jacob_Imola1", "~Alessandro_Epasto3", "~Mohammad_Mahdian1", "~Vincent_Cohen-Addad1", "~Vahab_Mirrokni2"]}, "abstract": {"value": "Hierarchical Clustering is a popular unsupervised machine learning method with decades of history and numerous applications. We initiate the study of *differentially-private* approximation algorithms for hierarchical clustering under the rigorous framework introduced by Dasgupta (2016). We show strong lower bounds for the problem: that any $\\epsilon$-DP algorithm must exhibit $O(|V|^2/ \\epsilon)$-additive error for an input dataset $V$. Then, we exhibit a polynomial-time approximation algorithm with $O(|V|^{2.5}/ \\epsilon)$-additive error, and an exponential-time algorithm that meets the lower bound. To overcome the lower bound, we focus on the stochastic block model, a popular model of graphs, and, with a separation assumption on the blocks, propose a private $1+o(1)$ approximation algorithm which also recovers the blocks exactly. Finally, we perform an empirical study of our algorithms and validate their performance."}, "pdf": {"value": "/pdf/e6db69046c2fec000bfd76cc6cba386f250c9767.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "imola|differentially_private_hierarchical_clustering_with_provable_approximation_guarantees"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3790/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373287592, "odate": 1686841472286, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "oVwFwXO9Kg", "number": 3753, "cdate": 1674722737174, "tcdate": 1674722737174, "mdate": 1687336608120, "tmdate": 1687336608120, "signatures": ["ICML.cc/2023/Conference/Submission3753/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3753/Authors"], "forum": "oVwFwXO9Kg", "content": {"title": {"value": "Equivariant Polynomials for Graph Neural Networks"}, "authors": {"value": ["Omri Puny", "Derek Lim", "Bobak Kiani", "Haggai Maron", "Yaron Lipman"]}, "authorids": {"value": ["~Omri_Puny1", "~Derek_Lim1", "~Bobak_Kiani1", "~Haggai_Maron1", "~Yaron_Lipman1"]}, "abstract": {"value": "Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we propose algorithmic tools for evaluating the expressiveness of GNNs using tensor contraction sequences, and calculate the expressive power of popular GNNs. Finally, we enhance the expressivity of common GNN architectures by adding polynomial features or additional operations / aggregations inspired by our theory. These enhanced GNNs demonstrate state-of-the-art results in experiments across multiple graph learning benchmarks."}, "pdf": {"value": "/pdf/cb5597920bf6103d86a6cc3a6d82d32b1988a843.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "puny|equivariant_polynomials_for_graph_neural_networks"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3753/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373286672, "odate": 1686841472086, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "XAK3238obr", "number": 3709, "cdate": 1674721322567, "tcdate": 1674721322567, "mdate": 1686841471800, "tmdate": 1686841471800, "signatures": ["ICML.cc/2023/Conference/Submission3709/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3709/Authors"], "forum": "XAK3238obr", "content": {"title": {"value": "How Bad is Top-$K$ Recommendation under Competing Content Creators?"}, "authors": {"value": ["Fan Yao", "Chuanhao Li", "Denis Nekipelov", "Hongning Wang", "Haifeng Xu"]}, "authorids": {"value": ["~Fan_Yao2", "~Chuanhao_Li1", "~Denis_Nekipelov1", "~Hongning_Wang1", "~Haifeng_Xu1"]}, "abstract": {"value": "This study explores the impact of content creators' competition on user welfare in recommendation platforms, as well as the long-term dynamics of relevance-driven recommendations. We establish a model of creator competition, under the setting where the platform uses a top-$K$ recommendation policy, user decisions are guided by the Random Utility model, and creators, in absence of explicit utility functions, employ arbitrary no-regret learning algorithms for strategy updates. We study the user welfare guarantee through the lens of Price of Anarchy and show that the fraction of user welfare loss due to creator competition is always upper bounded by a small constant depending on $K$ and randomness in user decisions; we also prove the tightness of this bound. Our result discloses an intrinsic merit of the relevance-driven recommendation policy, as long as users' decisions involve randomness and the platform provides reasonably many alternatives to its users."}, "pdf": {"value": "/pdf/d1c19fea818e15f945c49b67bacce13c5bc7f73c.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "yao|how_bad_is_topk_recommendation_under_competing_content_creators"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3709/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373285562, "odate": 1686841471787, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "EfhmBBrXY2", "number": 3706, "cdate": 1674721282511, "tcdate": 1674721282511, "mdate": 1687336604851, "tmdate": 1687336604851, "signatures": ["ICML.cc/2023/Conference/Submission3706/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3706/Authors"], "forum": "EfhmBBrXY2", "content": {"title": {"value": "Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models"}, "authors": {"value": ["Luke Vilnis", "Yury Zemlyanskiy", "Patrick Murray", "Alexandre Tachard Passos", "Sumit Sanghai"]}, "authorids": {"value": ["~Luke_Vilnis1", "~Yury_Zemlyanskiy1", "pcmurray@google.com", "~Alexandre_Tachard_Passos1", "sumitsanghai@google.com"]}, "abstract": {"value": "Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, more than halving the standard deviation when estimating expected BLEU score reward, and closing the BLEU score gap between independent sampling and beam search by up to 63%."}, "pdf": {"value": "/pdf/db0ba24f231b5da18662fa2061a022a759b8d027.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "vilnis|arithmetic_sampling_parallel_diverse_decoding_for_large_language_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3706/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373285479, "odate": 1686841471742, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "70sWtujQAU", "number": 3692, "cdate": 1674720631079, "tcdate": 1674720631079, "mdate": 1687336603895, "tmdate": 1687336603895, "signatures": ["ICML.cc/2023/Conference/Submission3692/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3692/Authors"], "forum": "70sWtujQAU", "content": {"title": {"value": "When Personalization Harms Performance: Reconsidering the Use of Group Attributes in Prediction"}, "authors": {"value": ["Vinith Menon Suriyakumar", "Marzyeh Ghassemi", "Berk Ustun"]}, "authorids": {"value": ["~Vinith_Menon_Suriyakumar1", "~Marzyeh_Ghassemi2", "~Berk_Ustun1"]}, "abstract": {"value": "Machine learning models are often personalized with categorical attributes that define groups. In this work, we show that personalization with *group attributes* can inadvertently reduce performance at a *group level* -- i.e., groups may receive unnecessarily inaccurate predictions by sharing their personal characteristics. We present formal conditions to ensure the *fair use* of group attributes in a prediction task, and describe how they can be checked by training one additional model. We characterize how fair use conditions be violated due to standard practices in model development, and study the prevalence of fair use violations in clinical prediction tasks. Our results show that personalization often fails to produce a tailored performance gain for every group who reports personal data, and underscore the need to evaluate fair use when personalizing models with characteristics that are protected, sensitive, self-reported, or costly to acquire."}, "pdf": {"value": "/pdf/2e12fa6fbcafc5f975a7e292f729a4c7bdfc49ce.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "suriyakumar|when_personalization_harms_performance_reconsidering_the_use_of_group_attributes_in_prediction"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3692/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373285072, "odate": 1686841471592, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "8JXMDw2xGa", "number": 3617, "cdate": 1674718010594, "tcdate": 1674718010594, "mdate": 1686841471253, "tmdate": 1686841471253, "signatures": ["ICML.cc/2023/Conference/Submission3617/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3617/Authors"], "forum": "8JXMDw2xGa", "content": {"title": {"value": "Data Feedback Loops: Model-driven Amplification of Dataset Biases"}, "authors": {"value": ["Rohan Taori", "Tatsunori Hashimoto"]}, "authorids": {"value": ["~Rohan_Taori1", "~Tatsunori_Hashimoto1"]}, "abstract": {"value": "Datasets scraped from the internet have been critical to large-scale machine learning. Yet, its success puts the utility of future internet-derived datasets at potential risk, as model outputs begin to replace human annotations as a source of supervision. In this work, we formalize a system where interactions with one model are recorded as history and scraped as training data in the future. We then analyze its stability over time by tracking changes to a test-time bias statistic (e.g. gender bias of model predictions). We find that the degree of bias amplification is closely linked to whether the model's outputs behave like samples from the training distribution, a behavior which we characterize and define as uniform faithfulness. Experiments in three conditional prediction scenarios -- image classification, visual role-labeling, and language generation -- demonstrate that models that exhibit a sampling-like behavior are more faithful and thus more stable. Based on this insight, we propose an intervention to help mitigate and stabilize unstable feedback systems."}, "pdf": {"value": "/pdf/8a30f3ae654017b692625f7f30ba9b2f0b62f829.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "taori|data_feedback_loops_modeldriven_amplification_of_dataset_biases"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3617/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373283154, "odate": 1686841471241, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "HxN8K1esES", "number": 3604, "cdate": 1674717645954, "tcdate": 1674717645954, "mdate": 1686841471130, "tmdate": 1686841471130, "signatures": ["ICML.cc/2023/Conference/Submission3604/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3604/Authors"], "forum": "HxN8K1esES", "content": {"title": {"value": "Mimetic Initialization of Self-Attention Layers"}, "authors": {"value": ["Asher Trockman", "J Zico Kolter"]}, "authorids": {"value": ["~Asher_Trockman1", "~J_Zico_Kolter1"]}, "abstract": {"value": "It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they \"look\" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique \"mimetic initialization\"."}, "pdf": {"value": "/pdf/fa22726bc5e1204d7ab7684ba5fc13c85b7d0ace.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "trockman|mimetic_initialization_of_selfattention_layers"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3604/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373282794, "odate": 1686841471110, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "qlAtMW9jIh", "number": 3603, "cdate": 1674717559365, "tcdate": 1674717559365, "mdate": 1686841471088, "tmdate": 1686841471088, "signatures": ["ICML.cc/2023/Conference/Submission3603/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3603/Authors"], "forum": "qlAtMW9jIh", "content": {"title": {"value": "Uncertain Evidence in Probabilistic Models and Stochastic Simulators"}, "authors": {"value": ["Andreas Munk", "Alexander Mead", "Frank Wood"]}, "authorids": {"value": ["~Andreas_Munk1", "alexander.j.mead@googlemail.com", "~Frank_Wood2"]}, "abstract": {"value": "We consider the problem of performing Bayesian inference in probabilistic models where observations are accompanied by uncertainty, referred to as \"uncertain evidence.'' We explore how to interpret uncertain evidence, and by extension the importance of proper interpretation as it pertains to inference about latent variables. We consider a recently-proposed method \"distributional evidence'' as well as revisit two older methods: Jeffrey's rule and virtual evidence. We devise guidelines on how to account for uncertain evidence and we provide new insights, particularly regarding consistency. To showcase the impact of different interpretations of the same uncertain evidence, we carry out experiments in which one interpretation is defined as \"correct.'' We then compare inference results from each different interpretation illustrating the importance of careful consideration of uncertain evidence."}, "pdf": {"value": "/pdf/3edec376a6709deb3d5c7d479d2d1e95450498cb.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "munk|uncertain_evidence_in_probabilistic_models_and_stochastic_simulators"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3603/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373282765, "odate": 1686841471076, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "7IRybndMLU", "number": 3567, "cdate": 1674716466177, "tcdate": 1674716466177, "mdate": 1686841470858, "tmdate": 1686841470858, "signatures": ["ICML.cc/2023/Conference/Submission3567/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3567/Authors"], "forum": "7IRybndMLU", "content": {"title": {"value": "Whose Opinions Do Language Models Reflect?"}, "authors": {"value": ["Shibani Santurkar", "Esin Durmus", "Faisal Ladhak", "Cinoo Lee", "Percy Liang", "Tatsunori Hashimoto"]}, "authorids": {"value": ["~Shibani_Santurkar1", "~Esin_Durmus1", "~Faisal_Ladhak2", "~Cinoo_Lee1", "~Percy_Liang1", "~Tatsunori_Hashimoto1"]}, "abstract": {"value": "Language models (LMs) are increasingly being used in open-ended contexts, where the opinions they reflect in response to subjective queries can have a profound impact, both on user satisfaction, and shaping the views of society at large. We put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls. Using this framework, we create OpinionQA, a dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals)."}, "pdf": {"value": "/pdf/e826f6e5666a6ce2aef38bdc972201b440afaf33.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "santurkar|whose_opinions_do_language_models_reflect"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3567/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373281747, "odate": 1686841470846, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "gKxXNAVZeF", "number": 3480, "cdate": 1674713532103, "tcdate": 1674713532103, "mdate": 1686841470273, "tmdate": 1686841470273, "signatures": ["ICML.cc/2023/Conference/Submission3480/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3480/Authors"], "forum": "gKxXNAVZeF", "content": {"title": {"value": "Nonparametric Extensions of Randomized Response for Private Confidence Sets"}, "authors": {"value": ["Ian Waudby-Smith", "Steven Wu", "Aaditya Ramdas"]}, "authorids": {"value": ["~Ian_Waudby-Smith1", "~Steven_Wu1", "~Aaditya_Ramdas2"]}, "abstract": {"value": "This work derives methods for performing nonparametric, nonasymptotic statistical inference for population means under the constraint of local differential privacy (LDP). Given bounded observations $(X_1, \\dots, X_n)$ with mean $\\mu^\\star$ that are privatized into $(Z_1, \\dots, Z_n)$, we present confidence intervals (CI) and time-uniform confidence sequences (CS) for $\\mu^\\star$ when only given access to the privatized data. To achieve this, we introduce a nonparametric and sequentially interactive generalization of Warner's famous ``randomized response'' mechanism, satisfying LDP for arbitrary bounded random variables, and then provide CIs and CSs for their means given access to the resulting privatized observations. For example, our results yield private analogues of Hoeffding's inequality in both fixed-time and time-uniform regimes. We extend these Hoeffding-type CSs to capture time-varying (non-stationary) means, and conclude by illustrating how these methods can be used to conduct private online A/B tests."}, "pdf": {"value": "/pdf/7f7e31c4c19cdeb07e86ef1890ad4294f30a5328.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "waudbysmith|nonparametric_extensions_of_randomized_response_for_private_confidence_sets"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3480/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373279758, "odate": 1686841470261, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "2C8Y6iao2I", "number": 3479, "cdate": 1674713487538, "tcdate": 1674713487538, "mdate": 1687336575499, "tmdate": 1687336575499, "signatures": ["ICML.cc/2023/Conference/Submission3479/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3479/Authors"], "forum": "2C8Y6iao2I", "content": {"title": {"value": "Cross-Modal Fine-Tuning: Align then Refine"}, "authors": {"value": ["Junhong Shen", "Liam Li", "Lucio M. Dery", "Corey Staten", "Mikhail Khodak", "Graham Neubig", "Ameet Talwalkar"]}, "authorids": {"value": ["~Junhong_Shen1", "~Liam_Li1", "~Lucio_M._Dery1", "~Corey_Staten1", "~Mikhail_Khodak1", "~Graham_Neubig1", "~Ameet_Talwalkar1"]}, "abstract": {"value": "Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific cross-modal methods. We highlight the importance of data alignment via a series of ablation studies and exemplify ORCA's utility in data-limited regimes."}, "pdf": {"value": "/pdf/9f14f671055b98c96173a534dc624153ac030794.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "shen|crossmodal_finetuning_align_then_refine"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3479/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373279722, "odate": 1686841470242, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "wZdykpFd6U", "number": 3363, "cdate": 1674709687708, "tcdate": 1674709687708, "mdate": 1687336562839, "tmdate": 1687336562839, "signatures": ["ICML.cc/2023/Conference/Submission3363/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3363/Authors"], "forum": "wZdykpFd6U", "content": {"title": {"value": "Learning Mixtures of Markov Chains and MDPs"}, "authors": {"value": ["Chinmaya Kausik", "Kevin Tan", "Ambuj Tewari"]}, "authorids": {"value": ["~Chinmaya_Kausik1", "kevtan@umich.edu", "~Ambuj_Tewari1"]}, "abstract": {"value": "We present an algorithm for learning mixtures of Markov chains and Markov decision processes (MDPs) from short unlabeled trajectories. Specifically, our method handles mixtures of Markov chains with optional control input by going through a multi-step process, involving (1) a subspace estimation step, (2) spectral clustering of trajectories using \"pairwise distance estimators,\" along with refinement using the EM algorithm, (3) a model estimation step, and (4) a classification step for predicting labels of new trajectories. We provide end-to-end performance guarantees, where we only explicitly require the length of trajectories to be linear in the number of states and the number of trajectories to be linear in a mixing time parameter. Experimental results support these guarantees, where we attain 96.6% average accuracy on a mixture of two MDPs in gridworld, outperforming the EM algorithm with random initialization (73.2% average accuracy). We also significantly outperform the EM algorithm on real data from the LastFM song dataset."}, "pdf": {"value": "/pdf/2b8a35762aae52e93cf6c99fc91f4eaae6b36244.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kausik|learning_mixtures_of_markov_chains_and_mdps"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3363/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373276637, "odate": 1686841469056, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "vZh3aw4TaF", "number": 3276, "cdate": 1674706436118, "tcdate": 1674706436118, "mdate": 1687336555381, "tmdate": 1687336555381, "signatures": ["ICML.cc/2023/Conference/Submission3276/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3276/Authors"], "forum": "vZh3aw4TaF", "content": {"title": {"value": "Tight Data Access Bounds for Private Top-$k$ Selection"}, "authors": {"value": ["Hao WU", "Olga Ohrimenko", "Anthony Wirth"]}, "authorids": {"value": ["~Hao_WU21", "~Olga_Ohrimenko1", "~Anthony_Wirth1"]}, "abstract": {"value": "We study the top-$k$ selection problem under the differential privacy model: $m$ items are rated according to votes of a set of clients. We consider a setting in which algorithms can retrieve data via a sequence of accesses, each either a random access or a sorted access; the goal is to minimize the total number of data accesses. Our algorithm requires only $O(\\sqrt{mk})$ expected accesses: to our knowledge, this is the first sublinear data-access upper bound for this problem. Our analysis also shows that the well-known exponential mechanism requires only $O(\\sqrt{m})$ expected accesses. Accompanying this, we develop the first lower bounds for the problem, in three settings: only random accesses; only sorted accesses; a sequence of accesses of either kind. We show that, to avoid $\\Omega(m)$ access cost, supporting *both* kinds of access is necessary, and that in this case our algorithm's access cost is optimal."}, "pdf": {"value": "/pdf/b4005b34558ddae3c5a2a86e23f6929b7ed75691.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wu|tight_data_access_bounds_for_private_topk_selection"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3276/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373274449, "odate": 1686841468518, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "odqQB2OXsG", "number": 3264, "cdate": 1674705972794, "tcdate": 1674705972794, "mdate": 1686841468516, "tmdate": 1686841468516, "signatures": ["ICML.cc/2023/Conference/Submission3264/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3264/Authors"], "forum": "odqQB2OXsG", "content": {"title": {"value": "Understanding Plasticity in Neural Networks"}, "authors": {"value": ["Clare Lyle", "Zeyu Zheng", "Evgenii Nikishin", "Bernardo Avila Pires", "Razvan Pascanu", "Will Dabney"]}, "authorids": {"value": ["~Clare_Lyle1", "~Zeyu_Zheng1", "~Evgenii_Nikishin1", "~Bernardo_Avila_Pires1", "~Razvan_Pascanu1", "~Will_Dabney1"]}, "abstract": {"value": "Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it often occurs in the absence of saturated units. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these findings on larger-scale RL benchmarks in the Arcade Learning Environment."}, "pdf": {"value": "/pdf/078d3e46408e48e8b54344309c432351c1f36440.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "lyle|understanding_plasticity_in_neural_networks"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3264/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373274161, "odate": 1686841468501, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "s7me1XxUqd", "number": 3136, "cdate": 1674700772332, "tcdate": 1674700772332, "mdate": 1686841467313, "tmdate": 1686841467313, "signatures": ["ICML.cc/2023/Conference/Submission3136/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3136/Authors"], "forum": "s7me1XxUqd", "content": {"title": {"value": "Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation"}, "authors": {"value": ["Jin-Hong Du", "Pratik Patil", "Arun K. Kuchibhotla"]}, "authorids": {"value": ["~Jin-Hong_Du1", "~Pratik_Patil1", "~Arun_K._Kuchibhotla1"]}, "abstract": {"value": "We study subsampling-based ridge ensembles in the proportional asymptotics regime, where the feature size grows proportionally with the sample size such that their ratio converges to a constant. By analyzing the squared prediction risk of ridge ensembles as a function of the explicit penalty $\\lambda$ and the limiting subsample aspect ratio $\\phi_s$ (the ratio of the feature size to the subsample size), we characterize contours in the $(\\lambda, \\phi_s)$-plane at any achievable risk. As a consequence, we prove that the risk of the optimal full ridgeless ensemble (fitted on all possible subsamples) matches that of the optimal ridge predictor. In addition, we prove strong uniform consistency of generalized cross-validation (GCV) over the subsample sizes for estimating the prediction risk of ridge ensembles. This allows for GCV-based tuning of full ridgeless ensembles without sample splitting and yields a predictor whose risk matches optimal ridge risk."}, "pdf": {"value": "/pdf/c629755e08bcb50e45ecb46ea510ca8eef41a3e5.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "du|subsample_ridge_ensembles_equivalences_and_generalized_crossvalidation"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3136/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373270923, "odate": 1686841467301, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "qAW0AD6qYA", "number": 3008, "cdate": 1674695951869, "tcdate": 1674695951869, "mdate": 1687323269728, "tmdate": 1687323269728, "signatures": ["ICML.cc/2023/Conference/Submission3008/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission3008/Authors"], "forum": "qAW0AD6qYA", "content": {"title": {"value": "Delving into Noisy Label Detection with Clean Data"}, "authors": {"value": ["Chenglin Yu", "Xinsong Ma", "Weiwei Liu"]}, "authorids": {"value": ["~Chenglin_Yu1", "~Xinsong_Ma1", "~Weiwei_Liu1"]}, "abstract": {"value": "A critical element of learning with noisy labels is noisy label detection. Notably, numerous previous works assume that no source of labels can be clean in a noisy label detection context. In this work, we relax this assumption and assume that a small subset of the training data is clean, which enables substantial noisy label detection performance gains. Specifically, we propose a novel framework that leverages clean data by framing the problem of noisy label detection with clean data as a multiple hypothesis testing problem. Moreover, we propose BHN, a simple yet effective approach for noisy label detection that integrates the Benjamini-Hochberg (BH) procedure into deep neural networks. BHN achieves $\\textit{state-of-the-art}$ performance and outperforms baselines by $\\textbf{28.48}$% in terms of false discovery rate (FDR) and by $\\textbf{18.99}$% in terms of F1 on CIFAR-10. Extensive ablation studies further demonstrate the superiority of BHN. Our code is available at https://github.com/ChenglinYu/BHN."}, "pdf": {"value": "/pdf/bf5950c3b4bb9f24f0bd39f4a40e26c0a53bacce.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "yu|delving_into_noisy_label_detection_with_clean_data"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission3008/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373267752, "odate": 1686841466481, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "3dqwXb1te4", "number": 2902, "cdate": 1674690562667, "tcdate": 1674690562667, "mdate": 1686841465542, "tmdate": 1686841465542, "signatures": ["ICML.cc/2023/Conference/Submission2902/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2902/Authors"], "forum": "3dqwXb1te4", "content": {"title": {"value": "Generalization on the Unseen, Logic Reasoning and Degree Curriculum"}, "authors": {"value": ["Emmanuel Abbe", "Samy Bengio", "Aryo Lotfi", "Kevin Rizk"]}, "authorids": {"value": ["~Emmanuel_Abbe1", "~Samy_Bengio1", "~Aryo_Lotfi1", "~Kevin_Rizk1"]}, "abstract": {"value": "This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length generalization problem (e.g., Anil et al. 2022); (2) we introduce a curriculum learning algorithm called Degree-Curriculum that learns monomials more efficiently by incrementing supports."}, "pdf": {"value": "/pdf/3a89448666467b1b2ee9a344b07987b8c8846cd5.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "abbe|generalization_on_the_unseen_logic_reasoning_and_degree_curriculum"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2902/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373265103, "odate": 1686841465530, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "LX3VAhXNTw", "number": 2900, "cdate": 1674690458291, "tcdate": 1674690458291, "mdate": 1686841465500, "tmdate": 1686841465500, "signatures": ["ICML.cc/2023/Conference/Submission2900/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2900/Authors"], "forum": "LX3VAhXNTw", "content": {"title": {"value": "Adversarial Policies Beat Superhuman Go AIs"}, "authors": {"value": ["Tony Tong Wang", "Adam Gleave", "Tom Tseng", "Kellin Pelrine", "Nora Belrose", "Joseph Miller", "Michael D Dennis", "Yawen Duan", "Viktor Pogrebniak", "Sergey Levine", "Stuart Russell"]}, "authorids": {"value": ["~Tony_Tong_Wang1", "~Adam_Gleave1", "~Tom_Tseng1", "~Kellin_Pelrine1", "~Nora_Belrose1", "~Joseph_Miller3", "~Michael_D_Dennis1", "~Yawen_Duan1", "~Viktor_Pogrebniak1", "~Sergey_Levine1", "~Stuart_Russell1"]}, "abstract": {"value": "We attack the state-of-the-art Go-playing AI system KataGo by training adversarial policies against it, achieving a >97% win rate against KataGo running at superhuman settings. Our adversaries do not win by playing Go well. Instead, they trick KataGo into making serious blunders. Our attack transfers zero-shot to other superhuman Go-playing AIs, and is comprehensible to the extent that human experts can implement it without algorithmic assistance to consistently beat superhuman AIs. The core vulnerability uncovered by our attack persists even in KataGo agents adversarially trained to defend against our attack. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes. Example games are available https://goattack.far.ai/."}, "pdf": {"value": "/pdf/4f68d3c0d0d2db82c35a401f5bdfd364c9c33aff.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wang|adversarial_policies_beat_superhuman_go_ais"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2900/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373265043, "odate": 1686841465483, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "b2GYLlhH4a", "number": 2897, "cdate": 1674690270445, "tcdate": 1674690270445, "mdate": 1687336516854, "tmdate": 1687336516854, "signatures": ["ICML.cc/2023/Conference/Submission2897/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2897/Authors"], "forum": "b2GYLlhH4a", "content": {"title": {"value": "Why does Throwing Away Data Improve Worst-Group Error?"}, "authors": {"value": ["Kamalika Chaudhuri", "Kartik Ahuja", "Martin Arjovsky", "David Lopez-Paz"]}, "authorids": {"value": ["~Kamalika_Chaudhuri1", "~Kartik_Ahuja1", "~Martin_Arjovsky1", "~David_Lopez-Paz2"]}, "abstract": {"value": "When facing data with imbalanced classes or groups, practitioners follow an intriguing strategy to achieve best results. They throw away examples until the classes or groups are balanced in size, and then perform empirical risk minimization on the reduced training set. This opposes common wisdom in learning theory, where the expected error is supposed to decrease as the dataset grows in size. In this work, we leverage extreme value theory to address this apparent contradiction. Our results show that the tails of the data distribution play an important role in determining the worst-group-accuracy of linear classifiers. When learning on data with heavy tails, throwing away data restores the geometric symmetry of the resulting classifier, and therefore improves its worst-group generalization."}, "pdf": {"value": "/pdf/8008bc80ebf7495ebbda283d83fb98e70da75279.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "chaudhuri|why_does_throwing_away_data_improve_worstgroup_error"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2897/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373264956, "odate": 1686841465428, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "YiWzhu9pl6", "number": 2882, "cdate": 1674689565669, "tcdate": 1674689565669, "mdate": 1687336515349, "tmdate": 1687336515349, "signatures": ["ICML.cc/2023/Conference/Submission2882/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2882/Authors"], "forum": "YiWzhu9pl6", "content": {"title": {"value": "Interventional Causal Representation Learning"}, "authors": {"value": ["Kartik Ahuja", "Divyat Mahajan", "Yixin Wang", "Yoshua Bengio"]}, "authorids": {"value": ["~Kartik_Ahuja1", "~Divyat_Mahajan1", "~Yixin_Wang1", "~Yoshua_Bengio1"]}, "abstract": {"value": "Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors' support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents' support and their ancestors'. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect do interventions. Moreover, we can achieve block affine identification, namely the estimated latent factors are only entangled with a few other latents if we have access to data from imperfect interventions. These results highlight the unique power of interventional data in causal representation learning; they can enable provable identification of latent factors without any assumptions about their distributions or dependency structure."}, "pdf": {"value": "/pdf/5dc9558df15e65ad0ce058f9f1c33eaa765600d6.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ahuja|interventional_causal_representation_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2882/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373264571, "odate": 1686841465337, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "aX8ig9X2a7", "number": 2841, "cdate": 1674687668317, "tcdate": 1674687668317, "mdate": 1686841465057, "tmdate": 1686841465057, "signatures": ["ICML.cc/2023/Conference/Submission2841/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2841/Authors"], "forum": "aX8ig9X2a7", "content": {"title": {"value": "A Watermark for Large Language Models"}, "authors": {"value": ["John Kirchenbauer", "Jonas Geiping", "Yuxin Wen", "Jonathan Katz", "Ian Miers", "Tom Goldstein"]}, "authorids": {"value": ["~John_Kirchenbauer1", "~Jonas_Geiping1", "~Yuxin_Wen2", "~Jonathan_Katz1", "~Ian_Miers1", "~Tom_Goldstein1"]}, "abstract": {"value": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."}, "pdf": {"value": "/pdf/3e5ba9e242875bfa037ed84c67433a7831ff4476.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kirchenbauer|a_watermark_for_large_language_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2841/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373263687, "odate": 1686841465046, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "UiAyIILXRd", "number": 2819, "cdate": 1674686493643, "tcdate": 1674686493643, "mdate": 1686841464848, "tmdate": 1686841464848, "signatures": ["ICML.cc/2023/Conference/Submission2819/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2819/Authors"], "forum": "UiAyIILXRd", "content": {"title": {"value": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature"}, "authors": {"value": ["Eric Mitchell", "Yoonho Lee", "Alexander Khazatsky", "Christopher D Manning", "Chelsea Finn"]}, "authorids": {"value": ["~Eric_Mitchell1", "~Yoonho_Lee1", "~Alexander_Khazatsky1", "~Christopher_D_Manning1", "~Chelsea_Finn1"]}, "abstract": {"value": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT."}, "pdf": {"value": "/pdf/647b6df6699abb86e5161978e500d06e1f249df8.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "mitchell|detectgpt_zeroshot_machinegenerated_text_detection_using_probability_curvature"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2819/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373263210, "odate": 1686841464828, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "PBRArApxMh", "number": 2808, "cdate": 1674686009327, "tcdate": 1674686009327, "mdate": 1687336508465, "tmdate": 1687336508465, "signatures": ["ICML.cc/2023/Conference/Submission2808/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2808/Authors"], "forum": "PBRArApxMh", "content": {"title": {"value": "TRAK: Attributing Model Behavior at Scale"}, "authors": {"value": ["Sung Min Park", "Kristian Georgiev", "Andrew Ilyas", "Guillaume Leclerc", "Aleksander Madry"]}, "authorids": {"value": ["~Sung_Min_Park2", "~Kristian_Georgiev1", "~Andrew_Ilyas1", "~Guillaume_Leclerc1", "~Aleksander_Madry1"]}, "abstract": {"value": "The goal of *data attribution* is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets. In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective *and* computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scales: image classifiers trained on ImageNet, vision-language models (CLIP), and language models (BERT and mT5). We provide code for using TRAK (and reproducing our work) at https://github.com/MadryLab/trak ."}, "pdf": {"value": "/pdf/54e7f4a5ff8447434c14ae64f924c28e7e71a321.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "park|trak_attributing_model_behavior_at_scale"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2808/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373262896, "odate": 1686841464724, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "mjYZd6SgZS", "number": 2792, "cdate": 1674684856855, "tcdate": 1674684856855, "mdate": 1687336505985, "tmdate": 1687336505985, "signatures": ["ICML.cc/2023/Conference/Submission2792/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2792/Authors"], "forum": "mjYZd6SgZS", "content": {"title": {"value": "On the Statistical Benefits of Temporal Difference Learning"}, "authors": {"value": ["David Cheikhi", "Daniel Russo"]}, "authorids": {"value": ["~David_Cheikhi1", "~Daniel_Russo1"]}, "abstract": {"value": "Given a dataset on actions and resulting long-term rewards, a direct estimation approach fits value functions that minimize prediction error on the training data. Temporal difference learning (TD) methods instead fit value functions by minimizing the degree of temporal inconsistency between estimates made at successive time-steps. Focusing on finite state Markov chains, we provide a crisp asymptotic theory of the statistical advantages of this approach. First, we show that an intuitive inverse trajectory pooling coefficient completely characterizes the percent reduction in mean-squared error of value estimates. Depending on problem structure, the reduction could be enormous or nonexistent. Next, we prove that there can be dramatic improvements in estimates of the difference in value-to-go for two states: TD's errors are bounded in terms of a novel measure -- the problem's trajectory crossing time -- which can be much smaller than the problem's time horizon."}, "pdf": {"value": "/pdf/a9bc8e97a916eb93f7a2be1ef9ff504844cf7d50.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "cheikhi|on_the_statistical_benefits_of_temporal_difference_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2792/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373262538, "odate": 1686841464606, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "SCU1xlr9Y4", "number": 2744, "cdate": 1674682028832, "tcdate": 1674682028832, "mdate": 1687336501204, "tmdate": 1687336501204, "signatures": ["ICML.cc/2023/Conference/Submission2744/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2744/Authors"], "forum": "SCU1xlr9Y4", "content": {"title": {"value": "Equivariant Architectures for Learning in Deep Weight Spaces"}, "authors": {"value": ["Aviv Navon", "Aviv Shamsian", "Idan Achituve", "Ethan Fetaya", "Gal Chechik", "Haggai Maron"]}, "authorids": {"value": ["~Aviv_Navon1", "~Aviv_Shamsian1", "~Idan_Achituve1", "~Ethan_Fetaya1", "~Gal_Chechik1", "~Haggai_Maron1"]}, "abstract": {"value": "Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show how these layers can be implemented using three basic operations: pooling, broadcasting, and fully connected layers applied to the input in an appropriate manner. We demonstrate the effectiveness of our architecture and its advantages over natural baselines in a variety of learning tasks."}, "pdf": {"value": "/pdf/a405e3754a1632e635afa57f1874e9ca97dd4008.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "navon|equivariant_architectures_for_learning_in_deep_weight_spaces"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2744/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373261084, "odate": 1686841464181, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "eyTrDtchE1", "number": 2640, "cdate": 1674676079179, "tcdate": 1674676079179, "mdate": 1686841463544, "tmdate": 1686841463544, "signatures": ["ICML.cc/2023/Conference/Submission2640/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2640/Authors"], "forum": "eyTrDtchE1", "content": {"title": {"value": "Generalized Teacher Forcing for Learning Chaotic Dynamics"}, "authors": {"value": ["Florian Hess", "Zahra Monfared", "Manuel Brenner", "Daniel Durstewitz"]}, "authorids": {"value": ["~Florian_Hess1", "~Zahra_Monfared1", "~Manuel_Brenner1", "~Daniel_Durstewitz1"]}, "abstract": {"value": "Chaotic dynamical systems (DS) are ubiquitous in nature and society. Often we are interested in reconstructing such systems from observed time series for prediction or mechanistic insight, where by reconstruction we mean learning geometrical and invariant temporal properties of the system in question (like attractors). However, training reconstruction algorithms like recurrent neural networks (RNNs) on such systems by gradient-descent based techniques faces severe challenges. This is mainly due to exploding gradients caused by the exponential divergence of trajectories in chaotic systems. Moreover, for (scientific) interpretability we wish to have as low dimensional reconstructions as possible, preferably in a model which is mathematically tractable. Here we report that a surprisingly simple modification of teacher forcing leads to provably strictly all-time bounded gradients in training on chaotic systems, and, when paired with a simple architectural rearrangement of a tractable RNN design, piecewise-linear RNNs (PLRNNs), allows for faithful reconstruction in spaces of at most the dimensionality of the observed system. We show on several DS that with these amendments we can reconstruct DS better than current SOTA algorithms, in much lower dimensions. Performance differences were particularly compelling on real world data with which most other methods severely struggled. This work thus led to a simple yet powerful DS reconstruction algorithm which is highly interpretable at the same time."}, "pdf": {"value": "/pdf/955eee9bb4f3f9f090ebb4c38b177ba428b6683e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "hess|generalized_teacher_forcing_for_learning_chaotic_dynamics"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2640/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373258451, "odate": 1686841463532, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "thUjOwfzzv", "number": 2569, "cdate": 1674670945364, "tcdate": 1674670945364, "mdate": 1687336489261, "tmdate": 1687336489261, "signatures": ["ICML.cc/2023/Conference/Submission2569/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2569/Authors"], "forum": "thUjOwfzzv", "content": {"title": {"value": "Human-Timescale Adaptation in an Open-Ended Task Space"}, "authors": {"value": ["Jakob Bauer", "Kate Baumli", "Feryal Behbahani", "Avishkar Bhoopchand", "Nathalie Bradley-Schmieg", "Michael Chang", "Natalie Clay", "Adrian Collister", "Vibhavari Dasagi", "Lucy Gonzalez", "Karol Gregor", "Edward Hughes", "Sheleem Kashem", "Maria Loks-Thompson", "Hannah Openshaw", "Jack Parker-Holder", "Shreya Pathak", "Nicolas Perez-Nieves", "Nemanja Rakicevic", "Tim Rockt\u00e4schel", "Yannick Schroecker", "Satinder Singh", "Jakub Sygnowski", "Karl Tuyls", "Sarah York", "Alexander Zacherl", "Lei M Zhang"]}, "authorids": {"value": ["~Jakob_Bauer1", "~Kate_Baumli1", "~Feryal_Behbahani1", "~Avishkar_Bhoopchand1", "nschmieg@google.com", "michaelchangc@deepmind.com", "~Natalie_Clay3", "acollister@deepmind.com", "~Vibhavari_Dasagi1", "lucygps@deepmind.com", "~Karol_Gregor1", "~Edward_Hughes1", "sheleem@deepmind.com", "mzlt@deepmind.com", "hopenshaw@deepmind.com", "~Jack_Parker-Holder1", "shreyapa@deepmind.com", "~Nicolas_Perez-Nieves1", "~Nemanja_Rakicevic1", "~Tim_Rockt\u00e4schel1", "~Yannick_Schroecker1", "~Satinder_Singh2", "~Jakub_Sygnowski1", "~Karl_Tuyls1", "syork@deepmind.com", "azacherl@deepmind.com", "~Lei_M_Zhang1"]}, "abstract": {"value": "Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains."}, "pdf": {"value": "/pdf/a33ca20365624ef4006337dc632103b20addad4e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "bauer|humantimescale_adaptation_in_an_openended_task_space"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2569/-/Camera_Ready_Revision", "ICML.cc/2023/Conference/-/PC_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373256664, "odate": 1686841463183, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2, "hidden": true}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "BgRiBwPKHC", "number": 2539, "cdate": 1674669137230, "tcdate": 1674669137230, "mdate": 1686841462962, "tmdate": 1686841462962, "signatures": ["ICML.cc/2023/Conference/Submission2539/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2539/Authors"], "forum": "BgRiBwPKHC", "content": {"title": {"value": "OCD: Learning to Overfit with Conditional Diffusion Models"}, "authors": {"value": ["Shahar Lutati", "Lior Wolf"]}, "authorids": {"value": ["~Shahar_Lutati1", "~Lior_Wolf1"]}, "abstract": {"value": "We present a dynamic model in which the weights are conditioned on an input sample x and are learned to match those that would be obtained by finetuning a base model on x and its label y. This mapping between an input sample and network weights is approximated by a denoising diffusion model. The diffusion model we employ focuses on modifying a single layer of the base model and is conditioned on the input, activations, and output of this layer. Since the diffusion model is stochastic in nature, multiple initializations generate different networks, forming an ensemble, which leads to further improvements. Our experiments demonstrate the wide applicability of the method for image classification, 3D reconstruction, tabular data, speech separation, and natural language processing."}, "pdf": {"value": "/pdf/0965c4827d5fd723fa5a97a2f8c8017df7f3a9fe.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "lutati|ocd_learning_to_overfit_with_conditional_diffusion_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2539/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373255894, "odate": 1686841462950, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "vD1R00hROK", "number": 2496, "cdate": 1674666860147, "tcdate": 1674666860147, "mdate": 1687336485193, "tmdate": 1687336485193, "signatures": ["ICML.cc/2023/Conference/Submission2496/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2496/Authors"], "forum": "vD1R00hROK", "content": {"title": {"value": "Dynamic Regularized Sharpness Aware Minimization in Federated Learning:  Approaching Global Consistency and Smooth Landscape"}, "authors": {"value": ["Yan Sun", "Li Shen", "Shixiang Chen", "Liang Ding", "Dacheng Tao"]}, "authorids": {"value": ["~Yan_Sun3", "~Li_Shen1", "~Shixiang_Chen1", "~Liang_Ding3", "~Dacheng_Tao1"]}, "abstract": {"value": "In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance. Most previous works only focus on enhancing the consistency between the local and global objectives to alleviate this prejudicial client drifts from the perspective of the optimization view, whose performance would be prominently deteriorated on the high heterogeneity. In this work, we propose a novel and general algorithm FedSMOO by jointly considering the optimization and generalization targets to efficiently improve the performance in FL. Concretely, FedSMOO adopts a dynamic regularizer to guarantee the local optima towards the global objective, which is meanwhile revised by the global Sharpness Aware Minimization (SAM) optimizer to search for the consistent flat minima. Our theoretical analysis indicates that FedSMOO achieves fast $\\mathcal{O}(1/T)$ convergence rate with low generalization bound. Extensive numerical studies are conducted on the real-world dataset to verify its peerless efficiency and excellent generality."}, "pdf": {"value": "/pdf/af34605ba483ea4b747e84f974ce5138b14a2553.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "sun|dynamic_regularized_sharpness_aware_minimization_in_federated_learning_approaching_global_consistency_and_smooth_landscape"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2496/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373254906, "odate": 1686841462643, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "dZA7WtCULT", "number": 2383, "cdate": 1674661411466, "tcdate": 1674661411466, "mdate": 1686841462068, "tmdate": 1686841462068, "signatures": ["ICML.cc/2023/Conference/Submission2383/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2383/Authors"], "forum": "dZA7WtCULT", "content": {"title": {"value": "Bidirectional Adaptation for Robust Semi-Supervised Learning with Inconsistent Data Distributions"}, "authors": {"value": ["Lin-Han Jia", "Lan-Zhe Guo", "Zhi Zhou", "Jie-Jing Shao", "Yuke Xiang", "Yu-Feng Li"]}, "authorids": {"value": ["~Lin-Han_Jia1", "~Lan-Zhe_Guo2", "~Zhi_Zhou2", "~Jie-Jing_Shao1", "~Yuke_Xiang1", "~Yu-Feng_Li1"]}, "abstract": {"value": "Semi-supervised learning (SSL) suffers from severe performance degradation when labeled and unlabeled data come from inconsistent data distributions. However, there is still a lack of sufficient theoretical guidance on how to alleviate this problem. In this paper, we propose a general theoretical framework that demonstrates how distribution discrepancies caused by pseudo-label predictions and target predictions can lead to severe generalization errors. Through theoretical analysis, we identify three main reasons why previous SSL algorithms cannot perform well with inconsistent distributions: coupling between the pseudo-label predictor and the target predictor, biased pseudo labels, and restricted sample weights. To address these challenges, we introduce a practical framework called Bidirectional Adaptation that can adapt to the distribution of unlabeled data for debiased pseudo-label prediction and to the target distribution for debiased target prediction, thereby mitigating these shortcomings. Extensive experimental results demonstrate the effectiveness of our proposed framework."}, "pdf": {"value": "/pdf/942825bd2c9f0e0d0acc7454f0d19a136c5379c0.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "jia|bidirectional_adaptation_for_robust_semisupervised_learning_with_inconsistent_data_distributions"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2383/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373252073, "odate": 1686841462057, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "GtoeseQjtY", "number": 2261, "cdate": 1674656016298, "tcdate": 1674656016298, "mdate": 1686927940543, "tmdate": 1686927940543, "signatures": ["ICML.cc/2023/Conference/Submission2261/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2261/Authors"], "forum": "GtoeseQjtY", "content": {"title": {"value": "Settling the Reward Hypothesis"}, "authors": {"value": ["Michael Bowling", "John D Martin", "David Abel", "Will Dabney"]}, "authorids": {"value": ["~Michael_Bowling1", "~John_D_Martin1", "~David_Abel1", "~Will_Dabney1"]}, "abstract": {"value": "The *reward hypothesis* posits that, \"all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\" We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds."}, "pdf": {"value": "/pdf/033ce35a70ded67b6d98a34284c33a1ec2c16770.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "bowling|settling_the_reward_hypothesis"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2261/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373249014, "odate": 1686841461275, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "ZvKWki48yP", "number": 2243, "cdate": 1674654870275, "tcdate": 1674654870275, "mdate": 1686841461176, "tmdate": 1686841461176, "signatures": ["ICML.cc/2023/Conference/Submission2243/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2243/Authors"], "forum": "ZvKWki48yP", "content": {"title": {"value": "On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness"}, "authors": {"value": ["Haotian Ye", "Xiaoyu Chen", "Liwei Wang", "Simon Shaolei Du"]}, "authorids": {"value": ["~Haotian_Ye1", "~Xiaoyu_Chen2", "~Liwei_Wang1", "~Simon_Shaolei_Du1"]}, "abstract": {"value": "Generalization in Reinforcement Learning (RL) aims to train an agent during training that generalizes to the target environment. In this work, we first point out that RL generalization is fundamentally different from the generalization in supervised learning, and fine-tuning on the target environment is necessary for good test performance. Therefore, we seek to answer the following question: how much can we expect pre-training over training environments to be helpful for efficient and effective fine-tuning? On one hand, we give a surprising result showing that asymptotically, the improvement from pre-training is at most a constant factor. On the other hand, we show that pre-training can be indeed helpful in the non-asymptotic regime by designing a policy collection-elimination (PCE) algorithm and proving a distribution-dependent regret bound that is independent of the state-action space. We hope our theoretical results can provide insight towards understanding pre-training and generalization in RL."}, "pdf": {"value": "/pdf/b51294340555dde61f63b75d2c013c61f9354077.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ye|on_the_power_of_pretraining_for_generalization_in_rl_provable_benefits_and_hardness"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2243/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373248632, "odate": 1686841461164, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "jwy77xkyPt", "number": 2200, "cdate": 1674651737029, "tcdate": 1674651737029, "mdate": 1686841460729, "tmdate": 1686841460729, "signatures": ["ICML.cc/2023/Conference/Submission2200/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2200/Authors"], "forum": "jwy77xkyPt", "content": {"title": {"value": "Information-Theoretic State Space Model for Multi-View Reinforcement Learning"}, "authors": {"value": ["HyeongJoo Hwang", "Seokin Seo", "Youngsoo Jang", "Sungyoon Kim", "Geon-Hyeong Kim", "Seunghoon Hong", "Kee-Eung Kim"]}, "authorids": {"value": ["~HyeongJoo_Hwang1", "~Seokin_Seo1", "~Youngsoo_Jang2", "~Sungyoon_Kim1", "~Geon-Hyeong_Kim2", "~Seunghoon_Hong2", "~Kee-Eung_Kim2"]}, "abstract": {"value": "Multi-View Reinforcement Learning (MVRL) seeks to find an optimal control for an agent given multi-view observations from various sources. Despite recent advances in multi-view learning that aim to extract the latent representation from multi-view data, it is not straightforward to apply them to control tasks, especially when the observations are temporally dependent on one another. The problem can be even more challenging if the observations are intermittently missing for a subset of views. In this paper, we introduce Fuse2Control (F2C), an information-theoretic approach to capturing the underlying state space model from the sequences of multi-view observations. We conduct an extensive set of experiments in various control tasks showing that our method is highly effective in aggregating task-relevant information across many views, that scales linearly with the number of views while retaining robustness to arbitrary missing view scenarios."}, "pdf": {"value": "/pdf/a52c3a560a890938fca31018bdf152a6f844805a.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "hwang|informationtheoretic_state_space_model_for_multiview_reinforcement_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2200/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373247568, "odate": 1686841460718, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "BTGTEEkPsV", "number": 2194, "cdate": 1674651388638, "tcdate": 1674651388638, "mdate": 1686841460706, "tmdate": 1686841460706, "signatures": ["ICML.cc/2023/Conference/Submission2194/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2194/Authors"], "forum": "BTGTEEkPsV", "content": {"title": {"value": "Continuation Path Learning for Homotopy Optimization"}, "authors": {"value": ["Xi Lin", "Zhiyuan Yang", "Xiaoyuan Zhang", "Qingfu Zhang"]}, "authorids": {"value": ["~Xi_Lin2", "~Zhiyuan_Yang2", "~Xiaoyuan_Zhang2", "~Qingfu_Zhang1"]}, "abstract": {"value": "Homotopy optimization is a traditional method to deal with a complicated optimization problem by solving a sequence of easy-to-hard surrogate subproblems. However, this method can be very sensitive to the continuation schedule design and might lead to a suboptimal solution to the original problem. In addition, the intermediate solutions, often ignored by classic homotopy optimization, could be useful for many real-world applications. In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which contains infinite intermediate solutions for any surrogate subproblems. Rather than the classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports the real-time generation of any intermediate solution, which could be desirable for many applications. Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making."}, "pdf": {"value": "/pdf/f69dd259989c88f77571003b3f68382052a2ef5b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "lin|continuation_path_learning_for_homotopy_optimization"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2194/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373247447, "odate": 1686841460694, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "XMer44w2u9", "number": 2190, "cdate": 1674651051416, "tcdate": 1674651051416, "mdate": 1687336469317, "tmdate": 1687336469317, "signatures": ["ICML.cc/2023/Conference/Submission2190/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2190/Authors"], "forum": "XMer44w2u9", "content": {"title": {"value": "Fourmer: An Efficient Global Modeling Paradigm for Image Restoration"}, "authors": {"value": ["man zhou", "Jie Huang", "Chun-Le Guo", "Chongyi Li"]}, "authorids": {"value": ["~man_zhou1", "~Jie_Huang4", "~Chun-Le_Guo1", "~Chongyi_Li1"]}, "abstract": {"value": "Global modeling-based image restoration frameworks have become popular. However, they often require a high memory footprint and do not consider task-specific degradation. Our work presents an alternative approach to global modeling that is more efficient for image restoration. The key insights which motivate our study are two-fold: 1) Fourier transform is capable of disentangling image degradation and content component to a certain extent, serving as the image degradation prior, and 2) Fourier domain innately embraces global properties, where each pixel in the Fourier space is involved with all spatial pixels. While adhering to the ``spatial interaction + channel evolution'' rule of previous studies, we customize the core designs with Fourier spatial interaction modeling and Fourier channel evolution. Our paradigm, Fourmer, achieves competitive performance on common image restoration tasks such as image de-raining, image enhancement, image dehazing, and guided image super-resolution, while requiring fewer computational resources. The code for Fourmer will be made publicly available."}, "pdf": {"value": "/pdf/ea096602db7f9507fc944e8cafdc9f4e9e46713b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "zhou|fourmer_an_efficient_global_modeling_paradigm_for_image_restoration"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2190/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373247316, "odate": 1686841460634, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Aev7tepsqx", "number": 2177, "cdate": 1674649416832, "tcdate": 1674649416832, "mdate": 1686841460559, "tmdate": 1686841460559, "signatures": ["ICML.cc/2023/Conference/Submission2177/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2177/Authors"], "forum": "Aev7tepsqx", "content": {"title": {"value": "Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?"}, "authors": {"value": ["Victor Boutin", "Thomas FEL", "Lakshya Singhal", "Rishav Mukherji", "Akash Nagaraj", "Julien Colin", "Thomas Serre"]}, "authorids": {"value": ["~Victor_Boutin2", "~Thomas_FEL1", "~Lakshya_Singhal1", "~Rishav_Mukherji1", "~Akash_Nagaraj2", "~Julien_Colin2", "~Thomas_Serre1"]}, "abstract": {"value": "An important milestone for AI is the development of algorithms that can produce drawings that are indistinguishable from those of humans. Here, we adapt the ''diversity vs. recognizability'' scoring framework from Boutin et al (2022) and find that one-shot diffusion models have indeed started to close the gap between humans and machines. However, using a finer-grained measure of the originality of individual samples, we show that strengthening the guidance of diffusion models helps improve the humanness of their drawings, but they still fall short of approximating the originality and recognizability of human drawings. Comparing human category diagnostic features, collected through an online psychophysics experiment, against those derived from diffusion models reveals that humans rely on fewer and more localized features. Overall, our study suggests that diffusion models have significantly helped improve the quality of machine-generated drawings; however, a gap between humans and machines remains -- in part explainable by discrepancies in visual strategies."}, "pdf": {"value": "/pdf/5f2ae77e2fead335cb6205186b9e8f206d223ccd.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "boutin|diffusion_models_as_artists_are_we_closing_the_gap_between_humans_and_machines"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2177/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373246988, "odate": 1686841460548, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "GOousx8DUL", "number": 2137, "cdate": 1674643473158, "tcdate": 1674643473158, "mdate": 1687336468220, "tmdate": 1687336468220, "signatures": ["ICML.cc/2023/Conference/Submission2137/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2137/Authors"], "forum": "GOousx8DUL", "content": {"title": {"value": "Denoising MCMC for Accelerating Diffusion-Based Generative Models"}, "authors": {"value": ["Beomsu Kim", "Jong Chul Ye"]}, "authorids": {"value": ["~Beomsu_Kim1", "~Jong_Chul_Ye1"]}, "abstract": {"value": "The sampling process of diffusion models can be interpreted as solving the reverse stochastic differential equation (SDE) or the ordinary differential equation (ODE) of the diffusion process, which often requires up to thousands of discretization steps to generate a single image. This has sparked a great interest in developing efficient integration techniques for reverse-S/ODEs. Here, we propose an orthogonal approach to accelerating score-based sampling: Denoising MCMC (DMCMC). DMCMC first uses MCMC to produce initialization points for reverse-S/ODE in the product space of data and diffusion time. Then, a reverse-S/ODE integrator is used to denoise the initialization points. Since MCMC traverses close to the data manifold, the cost of producing a clean sample for DMCMC is much less than that of producing a clean sample from noise. Denoising Langevin Gibbs, an instance of DMCMC, successfully accelerates all six reverse-S/ODE integrators considered in this work, and achieves state-of-the-art results: in the limited number of score function evaluation (NFE) setting on CIFAR10, we have $3.25$ FID with $\\approx 10$ NFE and $2.49$ FID with $\\approx 16$ NFE. On CelebA-HQ-256, we have $6.99$ FID with $\\approx 160$ NFE, which beats the current best record of Kim et al. (2022) among score-based models, $7.16$ FID with $4000$ NFE. Code: https://github.com/1202kbs/DMCMC"}, "pdf": {"value": "/pdf/d0aa126202ef3d1c0360100945e6e9013fccf915.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kim|denoising_mcmc_for_accelerating_diffusionbased_generative_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2137/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373246065, "odate": 1686841460431, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "4PgzyLz6hi", "number": 2121, "cdate": 1674641563280, "tcdate": 1674641563280, "mdate": 1686841460358, "tmdate": 1686841460358, "signatures": ["ICML.cc/2023/Conference/Submission2121/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission2121/Authors"], "forum": "4PgzyLz6hi", "content": {"title": {"value": "Calibrating Multimodal Learning"}, "authors": {"value": ["Huan Ma", "Qingyang Zhang", "Changqing Zhang", "Bingzhe Wu", "Huazhu Fu", "Joey Tianyi Zhou", "Qinghua Hu"]}, "authorids": {"value": ["~Huan_Ma1", "~Qingyang_Zhang1", "~Changqing_Zhang1", "~Bingzhe_Wu1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1", "~Qinghua_Hu1"]}, "abstract": {"value": "Multimodal machine learning has achieved remarkable progress in a wide range of scenarios. However, the reliability of multimodal learning remains largely unexplored. In this paper, through extensive empirical studies, we identify current multimodal classification methods suffer from unreliable predictive confidence that tend to rely on partial modalities when estimating confidence. Specifically, we find that the confidence estimated by current models could even increase when some modalities are corrupted. To address the issue, we introduce an intuitive principle for multimodal learning, i.e., the confidence should not increase when one modality is removed. Accordingly, we propose a novel regularization technique, i.e., Calibrating Multimodal Learning (CML) regularization, to calibrate the predictive confidence of previous methods. This technique could be flexibly equipped by existing models and improve the performance in terms of confidence calibration, classification accuracy, and model robustness."}, "pdf": {"value": "/pdf/cb059215ea99bbd581a702923c433c0bfe4fe618.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ma|calibrating_multimodal_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission2121/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373245596, "odate": 1686841460347, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "2qflscc6A8", "number": 1992, "cdate": 1674624484855, "tcdate": 1674624484855, "mdate": 1686841459708, "tmdate": 1686841459708, "signatures": ["ICML.cc/2023/Conference/Submission1992/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1992/Authors"], "forum": "2qflscc6A8", "content": {"title": {"value": "Learning Signed Distance Functions from Noisy 3D Point Clouds via Noise to Noise Mapping"}, "authors": {"value": ["Baorui Ma", "Yu-Shen Liu", "Zhizhong Han"]}, "authorids": {"value": ["~Baorui_Ma1", "~Yu-Shen_Liu1", "~Zhizhong_Han2"]}, "abstract": {"value": "Learning signed distance functions (SDFs) from 3D point clouds is an important task in 3D computer vision. However, without ground truth signed distances, point normals or clean point clouds, current methods still struggle from learning SDFs from noisy point clouds. To overcome this challenge, we propose to learn SDFs via a noise to noise mapping, which does not require any clean point cloud or ground truth supervision for training. Our novelty lies in the noise to noise mapping which can infer a highly accurate SDF of a single object or scene from its multiple or even single noisy point cloud observations. Our novel learning manner is supported by modern Lidar systems which capture multiple noisy observations per second. We achieve this by a novel loss which enables statistical reasoning on point clouds and maintains geometric consistency although point clouds are irregular, unordered and have no point correspondence among noisy observations. Our evaluation under the widely used benchmarks demonstrates our superiority over the state-of-the-art methods in surface reconstruction, point cloud denoising and upsampling. Our code, data, and pre-trained models are available at https://github.com/mabaorui/Noise2NoiseMapping/ ."}, "pdf": {"value": "/pdf/ade711dbbc98f2d8762c4f8512e4f33ac5acd4fd.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ma|learning_signed_distance_functions_from_noisy_3d_point_clouds_via_noise_to_noise_mapping"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1992/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373241889, "odate": 1686841459695, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "X2JJxvcAfT", "number": 1965, "cdate": 1674620950921, "tcdate": 1674620950921, "mdate": 1686841459574, "tmdate": 1686841459574, "signatures": ["ICML.cc/2023/Conference/Submission1965/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1965/Authors"], "forum": "X2JJxvcAfT", "content": {"title": {"value": "Over-parametrization via Lifting for Low-rank Matrix Sensing: Conversion of Spurious Solutions to Strict Saddle Points"}, "authors": {"value": ["Ziye Ma", "Igor Molybog", "Javad Lavaei", "Somayeh Sojoudi"]}, "authorids": {"value": ["~Ziye_Ma1", "igormolybog@gmail.com", "~Javad_Lavaei1", "~Somayeh_Sojoudi1"]}, "abstract": {"value": "This paper studies the role of over-parametrization in solving non-convex optimization problems. The focus is on the important class of low-rank matrix sensing, where we propose an infinite hierarchy of non-convex problems via the lifting technique and the Burer-Monteiro factorization. This contrasts with the existing over-parametrization technique where the search rank is limited by the dimension of the matrix and it does not allow a rich over-parametrization of an arbitrary degree. We show that although the spurious solutions of the problem remain stationary points through the hierarchy, they will be transformed into strict saddle points (under some technical conditions) and can be escaped via local search methods. This is the first result in the literature showing that over-parametrization creates a negative curvature for escaping spurious solutions. We also derive a bound on how much over-parametrization is requited to enable the elimination of spurious solutions."}, "pdf": {"value": "/pdf/a76fd2a30a5ca0d1599887ac5421627a7b79e0b4.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ma|overparametrization_via_lifting_for_lowrank_matrix_sensing_conversion_of_spurious_solutions_to_strict_saddle_points"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1965/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373241295, "odate": 1686841459562, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "CuWORvLAnp", "number": 1892, "cdate": 1674611152578, "tcdate": 1674611152578, "mdate": 1687336445177, "tmdate": 1687336445177, "signatures": ["ICML.cc/2023/Conference/Submission1892/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1892/Authors"], "forum": "CuWORvLAnp", "content": {"title": {"value": "Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets from Large Models"}, "authors": {"value": ["AJAY KUMAR JAISWAL", "Shiwei Liu", "Tianlong Chen", "Ying Ding", "Zhangyang Wang"]}, "authorids": {"value": ["~AJAY_KUMAR_JAISWAL1", "~Shiwei_Liu2", "~Tianlong_Chen1", "~Ying_Ding4", "~Zhangyang_Wang1"]}, "abstract": {"value": "Large pre-trained transformers have been receiving explosive attention in the past few years, due to their acculturation for numerous downstream applications via fine-tuning, but their exponentially increasing parameter counts are becoming a primary hurdle to even just fine-tune them without industry-standard hardware. Recently, Lottery Ticket Hypothesis (LTH) and its variants, have been exploited to prune these large pre-trained models generating subnetworks which can achieve similar performance as their dense counterparts, but LTH pragmatism is enormously inhibited by repetitive full training and pruning routine of iterative magnitude pruning (IMP) which worsens with increasing model size. Motivated by the recent observations of model soups, which suggest that fine-tuned weights of multiple models can be merged to a better minima, we propose **Instant Soup Pruning (ISP)** to generate lottery ticket quality subnetworks, using a fraction of the original IMP cost by replacing the expensive intermediate pruning stages of IMP with computationally efficient weak mask generation and aggregation routine. More specifically, during the mask generation stage, ISP takes a small handful of iterations using varying training protocols and data subsets to generate many weak and noisy subnetworks, and superpose them to average out the noise creating a high-quality denoised subnetwork. Our extensive experiments and ablation on two popular large-scale pre-trained models: $\\texttt{CLIP} (unexplored in pruning till date)$ and $\\texttt{BERT}$ across multiple benchmark vision $\\texttt{\\{MNIST, SVHN, Cars, GTSRB, CIFAR-10, CIFAR-100\\}}$ and language datasets $\\texttt{\\{MNLI, QNLI, QQP, SST, ...\\}}$ validate the effectiveness of ISP compared to several state-of-the-art pruning methods. Additionally, we show that ISP can be easily modified with minimal overhead to produce benefits comparable to model soups, without the prerequisite to generate multiple candidates fine-tuned models. Codes are available at: https://github.com/VITA-Group/instant_soup."}, "pdf": {"value": "/pdf/f762d018661acc30d9d83c54a7f6c2ebaf5c5936.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "jaiswal|instant_soup_cheap_pruning_ensembles_in_a_single_pass_can_draw_lottery_tickets_from_large_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1892/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373239608, "odate": 1686841459143, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "DyCRGRlmNc", "number": 1863, "cdate": 1674603809912, "tcdate": 1674603809912, "mdate": 1687336443016, "tmdate": 1687336443016, "signatures": ["ICML.cc/2023/Conference/Submission1863/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1863/Authors"], "forum": "DyCRGRlmNc", "content": {"title": {"value": "Buying Information for Stochastic Optimization"}, "authors": {"value": ["Mingchen Ma", "Christos Tzamos"]}, "authorids": {"value": ["~Mingchen_Ma1", "~Christos_Tzamos1"]}, "abstract": {"value": "Stochastic optimization is one of the central problems in Machine Learning and Theoretical Computer Science. In the standard model, the algorithm is given a fixed distribution known in advance. In practice though, one may acquire at a cost extra information to make better decisions. In this paper, we study how to buy information for stochastic optimization and formulate this question as an online learning problem. Assuming the learner has an oracle for the original optimization problem, we design a $2$-competitive deterministic algorithm and a $e/(e-1)$-competitive randomized algorithm for buying information. We show that this ratio is tight as the problem is equivalent to a robust generalization of the ski-rental problem, which we call super-martingale stopping. We also consider an adaptive setting where the learner can choose to buy information after taking some actions for the underlying optimization problem. We focus on the classic optimization problem, Min-Sum Set Cover, where the goal is to quickly find an action that covers a given request drawn from a known distribution. We provide an $8$-competitive algorithm running in polynomial time that chooses actions and decides when to buy information about the underlying request."}, "pdf": {"value": "/pdf/6366c0252340b649cd5c11cd08565bd9b1dcb3a7.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ma|buying_information_for_stochastic_optimization"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1863/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373238956, "odate": 1686841459034, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "C9NEblP8vS", "number": 1841, "cdate": 1674600218006, "tcdate": 1674600218006, "mdate": 1686841458806, "tmdate": 1686841458806, "signatures": ["ICML.cc/2023/Conference/Submission1841/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1841/Authors"], "forum": "C9NEblP8vS", "content": {"title": {"value": "Fast Inference from Transformers via Speculative Decoding"}, "authors": {"value": ["Yaniv Leviathan", "Matan Kalman", "Yossi Matias"]}, "authorids": {"value": ["~Yaniv_Leviathan1", "~Matan_Kalman1", "~Yossi_Matias2"]}, "abstract": {"value": "Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs."}, "pdf": {"value": "/pdf/8316558dacb37b485267c6e530a4817924a0dc8f.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "leviathan|fast_inference_from_transformers_via_speculative_decoding"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1841/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373238479, "odate": 1686841458756, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "3onrj9ua4l", "number": 1820, "cdate": 1674596828653, "tcdate": 1674596828653, "mdate": 1687336437985, "tmdate": 1687336437985, "signatures": ["ICML.cc/2023/Conference/Submission1820/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1820/Authors"], "forum": "3onrj9ua4l", "content": {"title": {"value": "Sharper Bounds for $\\ell_p$ Sensitivity Sampling"}, "authors": {"value": ["David Woodruff", "Taisuke Yasuda"]}, "authorids": {"value": ["~David_Woodruff1", "~Taisuke_Yasuda1"]}, "abstract": {"value": "In large scale machine learning, *random sampling* is a popular way to approximate datasets by a small representative subset of examples. In particular, *sensitivity sampling* is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the *VC dimension* $d$ and the *total sensitivity* $\\mathfrak{S}$ in remarkably general settings. However, guarantees going beyond this general bound of $\\mathfrak{S} d$ are known in perhaps only one setting, for *$\\ell_2$ subspace embeddings*, despite intense study of sensitivity sampling in prior work. In this work, we show the first bounds for sensitivity sampling for $\\ell_p$ subspace embeddings for $p\\neq 2$ that improve over the general $\\mathfrak{S} d$ bound, achieving a bound of roughly $\\mathfrak{S}^{2/p}$ for $1\\leq p<2$ and $\\mathfrak{S}^{2-2/p}$ for $2<p<\\infty$. For $1\\leq p<2$, we show that this bound is tight, in the sense that there exist matrices for which $\\mathfrak{S}^{2/p}$ samples is necessary. Furthermore, our techniques yield further new results in the study of sampling algorithms, showing that the *root leverage score sampling* algorithm achieves a bound of roughly $d$ for $1\\leq p<2$, and that a combination of leverage score and sensitivity sampling achieves an improved bound of roughly $d^{2/p}\\mathfrak{S}^{2-4/p}$ for $2<p<\\infty$. Our sensitivity sampling results yield the best known sample complexity for a wide class of structured matrices that have small $\\ell_p$ sensitivity."}, "pdf": {"value": "/pdf/74cb3eeb57b7947ba5ea41b3a3321b2c2dcfc023.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "woodruff|sharper_bounds_for_\\ell_p_sensitivity_sampling"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1820/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373237986, "odate": 1686841458579, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "FZ1C5BXrfz", "number": 1784, "cdate": 1674588647234, "tcdate": 1674588647234, "mdate": 1687336434479, "tmdate": 1687336434479, "signatures": ["ICML.cc/2023/Conference/Submission1784/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1784/Authors"], "forum": "FZ1C5BXrfz", "content": {"title": {"value": "A Fully First-Order Method for Stochastic Bilevel Optimization"}, "authors": {"value": ["Jeongyeol Kwon", "Dohyun Kwon", "Stephen Wright", "Robert D Nowak"]}, "authorids": {"value": ["~Jeongyeol_Kwon1", "~Dohyun_Kwon1", "~Stephen_Wright1", "~Robert_D_Nowak1"]}, "abstract": {"value": "We consider stochastic unconstrained bilevel optimization problems when only the first-order gradient oracles are available. While numerous optimization methods have been proposed for tackling bilevel problems, existing methods either tend to require possibly expensive calculations regarding Hessians of lower-level objectives, or lack rigorous finite-time performance guarantees. In this work, we propose a Fully First-order Stochastic Approximation (F2SA) method, and study its non-asymptotic convergence properties. Specifically, we show that F2SA converges to an $\\epsilon$-stationary solution of the bilevel problem after $\\epsilon^{-7/2}, \\epsilon^{-5/2}$, and $\\epsilon^{-3/2}$ iterations (each iteration using $O(1)$ samples) when stochastic noises are in both level objectives, only in the upper-level objective, and not present (deterministic settings), respectively. We further show that if we employ momentum-assisted gradient estimators, the iteration complexities can be improved to $\\epsilon^{-5/2}, \\epsilon^{-4/2}$, and $\\epsilon^{-3/2}$, respectively. We demonstrate even superior practical performance of the proposed method over existing second-order based approaches on MNIST data-hypercleaning experiments."}, "pdf": {"value": "/pdf/9b20fce63a25efa6a4e37a3a6d2c9d7d0ea5a77e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kwon|a_fully_firstorder_method_for_stochastic_bilevel_optimization"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1784/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373237144, "odate": 1686841458337, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Phjti0QbkZ", "number": 1758, "cdate": 1674582318963, "tcdate": 1674582318963, "mdate": 1687336431616, "tmdate": 1687336431616, "signatures": ["ICML.cc/2023/Conference/Submission1758/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1758/Authors"], "forum": "Phjti0QbkZ", "content": {"title": {"value": "ODS: Test-Time Adaptation in the Presence of Open-World Data Shift"}, "authors": {"value": ["Zhi Zhou", "Lan-Zhe Guo", "Lin-Han Jia", "Dingchu Zhang", "Yu-Feng Li"]}, "authorids": {"value": ["~Zhi_Zhou2", "~Lan-Zhe_Guo2", "~Lin-Han_Jia1", "~Dingchu_Zhang1", "~Yu-Feng_Li1"]}, "abstract": {"value": "Test-time adaptation (TTA) adapts a source model to the distribution shift in testing data without using any source data. There have been plenty of algorithms concentrated on covariate shift in the last decade, i.e., $\\mathcal{D}_t(X)$, the distribution of the test data is different from the source data. Nonetheless, in real application scenarios, it is necessary to consider the influence of label distribution shift, i.e., both $\\mathcal{D}_t(X)$ and $\\mathcal{D}_t(Y)$ are shifted, which has not been sufficiently explored yet. To remedy this, we study a new problem setup, namely, TTA with Open-world Data Shift (AODS). The goal of AODS is simultaneously adapting a model to covariate and label distribution shifts in the test phase. In this paper, we first analyze the relationship between classification error and distribution shifts. Motivated by this, we hence propose a new framework, namely ODS, which decouples the mixed distribution shift and then addresses covariate and label distribution shifts accordingly. We conduct experiments on multiple benchmarks with different types of shifts, and the results demonstrate the superior performance of our method against the state of the arts. Moreover, ODS is suitable for many TTA algorithms."}, "pdf": {"value": "/pdf/5e6e180508e35874ed935122b50790f25375bda5.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "zhou|ods_testtime_adaptation_in_the_presence_of_openworld_data_shift"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1758/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373236553, "odate": 1686841458167, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "qw8zAw6mzJ", "number": 1749, "cdate": 1674580353511, "tcdate": 1674580353511, "mdate": 1686841458037, "tmdate": 1686841458037, "signatures": ["ICML.cc/2023/Conference/Submission1749/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1749/Authors"], "forum": "qw8zAw6mzJ", "content": {"title": {"value": "Simplex Random Features"}, "authors": {"value": ["Isaac Reid", "Krzysztof Marcin Choromanski", "Valerii Likhosherstov", "Adrian Weller"]}, "authorids": {"value": ["~Isaac_Reid3", "~Krzysztof_Marcin_Choromanski1", "~Valerii_Likhosherstov2", "~Adrian_Weller1"]}, "abstract": {"value": "We present Simplex Random Features (SimRFs), a new random feature (RF) mechanism for unbiased approximation of the softmax and Gaussian kernels by geometrical correlation of random projection vectors. We prove that SimRFs provide the smallest possible mean square error (MSE) on unbiased estimates of these kernels among the class of weight-independent geometrically-coupled positive random feature (PRF) mechanisms, substantially outperforming the previously most accurate Orthogonal Random Features (ORFs) at no observable extra cost. We present a more computationally expensive SimRFs+ variant, which we prove is asymptotically optimal in the broader family of weight-dependent geometrical coupling schemes (which permit correlations between random vector directions and norms). In extensive empirical studies, we show consistent gains provided by SimRFs in settings including pointwise kernel estimation, nonparametric classification and scalable Transformers."}, "pdf": {"value": "/pdf/d699eeb174a684fef4648c41cfce431a2a90d192.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "reid|simplex_random_features"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1749/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373236357, "odate": 1686841458026, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "kNzaZ0jbIg", "number": 1740, "cdate": 1674579479451, "tcdate": 1674579479451, "mdate": 1687336430003, "tmdate": 1687336430003, "signatures": ["ICML.cc/2023/Conference/Submission1740/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1740/Authors"], "forum": "kNzaZ0jbIg", "content": {"title": {"value": "Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks"}, "authors": {"value": ["Mohammed Nowaz Rabbani Chowdhury", "Shuai Zhang", "Meng Wang", "Sijia Liu", "Pin-Yu Chen"]}, "authorids": {"value": ["~Mohammed_Nowaz_Rabbani_Chowdhury1", "~Shuai_Zhang6", "~Meng_Wang4", "~Sijia_Liu1", "~Pin-Yu_Chen1"]}, "abstract": {"value": "In deep learning, mixture-of-experts (MoE) activates one or few experts (sub-networks) on a per-sample or per-token basis, resulting in significant computation reduction. The recently proposed patch-level routing in MoE (pMoE) divides each input into $n$ patches (or tokens) and sends $l$ patches ($l\\ll n$) to each expert through prioritized routing. pMoE has demonstrated great empirical success in reducing training and inference costs while maintaining test accuracy. However, the theoretical explanation of pMoE and the general MoE remains elusive. Focusing on a supervised classification task using a mixture of two-layer convolutional neural networks (CNNs), we show for the first time that pMoE provably reduces the required number of training samples to achieve desirable generalization (referred to as the sample complexity) by a factor in the polynomial order of $n/l$, and outperforms its single-expert counterpart of the same or even larger capacity. The advantage results from the discriminative routing property, which is justified in both theory and practice that pMoE routers can filter label-irrelevant patches and route similar class-discriminative patches to the same expert. Our experimental results on MNIST, CIFAR-10, and CelebA support our theoretical findings on pMoE's generalization and show that pMoE can avoid learning spurious correlations."}, "pdf": {"value": "/pdf/a652ff9ae5bc41f84a6185580eddd15a859811d2.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "chowdhury|patchlevel_routing_in_mixtureofexperts_is_provably_sampleefficient_for_convolutional_neural_networks"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1740/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373236203, "odate": 1686841457983, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "e1lKKjkNMj", "number": 1675, "cdate": 1674570261341, "tcdate": 1674570261341, "mdate": 1687336422624, "tmdate": 1687336422624, "signatures": ["ICML.cc/2023/Conference/Submission1675/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1675/Authors"], "forum": "e1lKKjkNMj", "content": {"title": {"value": "Difference of submodular minimization via DC programming"}, "authors": {"value": ["Marwa El Halabi", "George Orfanides", "Tim Hoheisel"]}, "authorids": {"value": ["~Marwa_El_Halabi2", "~George_Orfanides1", "tim.hoheisel@mcgill.ca"]}, "abstract": {"value": "Minimizing the difference of two submodular (DS) functions is a problem that naturally occurs in various machine learning problems. Although it is well known that a DS problem can be equivalently formulated as the minimization of the difference of two convex (DC) functions, existing algorithms do not fully exploit this connection. A classical algorithm for DC problems is called the DC algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that we apply to the DC program corresponding to DS minimization. We extend existing convergence properties of DCA, and connect them to convergence properties on the DS problem. Our results on DCA match the theoretical guarantees satisfied by existing DS algorithms, while providing a more complete characterization of convergence properties. In the case of CDCA, we obtain a stronger local minimality guarantee. Our numerical results show that our proposed algorithms outperform existing baselines on two applications: speech corpus selection and feature selection."}, "pdf": {"value": "/pdf/130ae482f7584417af4a8934f5ec8f57188e4616.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "halabi|difference_of_submodular_minimization_via_dc_programming"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1675/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373234517, "odate": 1686841457525, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Q3Rmfuj4vf", "number": 1631, "cdate": 1674558546534, "tcdate": 1674558546534, "mdate": 1686841457218, "tmdate": 1686841457218, "signatures": ["ICML.cc/2023/Conference/Submission1631/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1631/Authors"], "forum": "Q3Rmfuj4vf", "content": {"title": {"value": "Returning The Favour: When Regression Benefits From Probabilistic Causal Knowledge"}, "authors": {"value": ["Shahine Bouabid", "Jake Fawkes", "Dino Sejdinovic"]}, "authorids": {"value": ["~Shahine_Bouabid1", "~Jake_Fawkes1", "~Dino_Sejdinovic1"]}, "abstract": {"value": "A directed acyclic graph (DAG) provides valuable prior knowledge that is often discarded in regression tasks in machine learning. We show that the independences arising from the presence of collider structures in DAGs provide meaningful inductive biases, which constrain the regression hypothesis space and improve predictive performance. We introduce collider regression, a framework to incorporate probabilistic causal knowledge from a collider in a regression problem. When the hypothesis space is a reproducing kernel Hilbert space, we prove a strictly positive generalisation benefit under mild assumptions and provide closed-form estimators of the empirical risk minimiser. Experiments on synthetic and climate model data demonstrate performance gains of the proposed methodology."}, "pdf": {"value": "/pdf/67d0f5b54bca4929ed83f7a7a7f626f1c888cad3.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "bouabid|returning_the_favour_when_regression_benefits_from_probabilistic_causal_knowledge"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1631/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373233410, "odate": 1686841457206, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "GTos8jbYUa", "number": 1509, "cdate": 1674511599180, "tcdate": 1674511599180, "mdate": 1686841456366, "tmdate": 1686841456366, "signatures": ["ICML.cc/2023/Conference/Submission1509/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1509/Authors"], "forum": "GTos8jbYUa", "content": {"title": {"value": "Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series"}, "authors": {"value": ["Abdul Fatir Ansari", "Alvin Heng", "Andre Lim", "Harold Soh"]}, "authorids": {"value": ["~Abdul_Fatir_Ansari2", "~Alvin_Heng1", "~Andre_Lim1", "~Harold_Soh1"]}, "abstract": {"value": "Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved imputation and forecasting performance of NCDSSM over existing models."}, "pdf": {"value": "/pdf/144a3c35da280f7c1bd91a816550d077d56f2efa.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ansari|neural_continuousdiscrete_state_space_models_for_irregularlysampled_time_series"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1509/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373230235, "odate": 1686841456354, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Fj0PRtd4e6", "number": 1458, "cdate": 1674495763076, "tcdate": 1674495763076, "mdate": 1686841455932, "tmdate": 1686841455932, "signatures": ["ICML.cc/2023/Conference/Submission1458/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1458/Authors"], "forum": "Fj0PRtd4e6", "content": {"title": {"value": "BEATs: Audio Pre-Training with Acoustic Tokenizers"}, "authors": {"value": ["Sanyuan Chen", "Yu Wu", "Chengyi Wang", "Shujie LIU", "Daniel Tompkins", "Zhuo Chen", "Wanxiang Che", "Xiangzhan Yu", "Furu Wei"]}, "authorids": {"value": ["~Sanyuan_Chen1", "~Yu_Wu1", "~Chengyi_Wang1", "~Shujie_LIU1", "datompki@microsoft.com", "~Zhuo_Chen14", "~Wanxiang_Che1", "yxz@hit.edu.cn", "~Furu_Wei1"]}, "abstract": {"value": "We introduce a self-supervised learning (SSL) framework BEATs for general audio representation pre-training, where we optimize an acoustic tokenizer and an audio SSL model by iterations. Unlike the previous audio SSL models that employ reconstruction loss for pre-training, our audio SSL model is trained with the discrete label prediction task, where the labels are generated by a semantic-rich acoustic tokenizer. We propose an iterative pipeline to jointly optimize the tokenizer and the pre-trained model, aiming to abstract high-level semantics and discard the redundant details for audio. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art (SOTA) results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new SOTA mAP 50.6% on AudioSet-2M without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats."}, "pdf": {"value": "/pdf/5730ae36509094417a38469432936a284ca2e948.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "chen|beats_audio_pretraining_with_acoustic_tokenizers"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1458/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373228914, "odate": 1686841455919, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "PZahJfBVNB", "number": 1439, "cdate": 1674490294458, "tcdate": 1674490294458, "mdate": 1686841455840, "tmdate": 1686841455840, "signatures": ["ICML.cc/2023/Conference/Submission1439/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1439/Authors"], "forum": "PZahJfBVNB", "content": {"title": {"value": "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"}, "authors": {"value": ["Axel Sauer", "Tero Karras", "Samuli Laine", "Andreas Geiger", "Timo Aila"]}, "authorids": {"value": ["~Axel_Sauer1", "~Tero_Karras1", "~Samuli_Laine1", "~Andreas_Geiger3", "~Timo_Aila1"]}, "abstract": {"value": "Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed."}, "pdf": {"value": "/pdf/8271796d0deeb23b7ab06bbb9f5429c24ea5d89e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "sauer|stylegant_unlocking_the_power_of_gans_for_fast_largescale_texttoimage_synthesis"}}, "pdate": 1682373228406, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1439/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "odate": 1686841455828, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "gLH40bhHpm", "number": 1301, "cdate": 1674444656557, "tcdate": 1674444656557, "mdate": 1687336390729, "tmdate": 1687336390729, "signatures": ["ICML.cc/2023/Conference/Submission1301/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1301/Authors"], "forum": "gLH40bhHpm", "content": {"title": {"value": "Fast Private Kernel Density Estimation via Locality Sensitive Quantization"}, "authors": {"value": ["Tal Wagner", "Yonatan Naamad", "Nina Mishra"]}, "authorids": {"value": ["~Tal_Wagner1", "~Yonatan_Naamad1", "~Nina_Mishra2"]}, "abstract": {"value": "We study efficient mechanisms for differentially private kernel density estimation (DP-KDE). Prior work for the Gaussian kernel described algorithms that run in time exponential in the number of dimensions $d$. This paper breaks the exponential barrier, and shows how the KDE can privately be approximated in time linear in $d$, making it feasible for high-dimensional data. We also present improved bounds for low-dimensional data. Our results are obtained through a general framework, which we term Locality Sensitive Quantization (LSQ), for constructing private KDE mechanisms where existing KDE approximation techniques can be applied. It lets us leverage several efficient non-private KDE methods---like Random Fourier Features, the Fast Gauss Transform, and Locality Sensitive Hashing---and ``privatize'' them in a black-box manner. Our experiments demonstrate that our resulting DP-KDE mechanisms are fast and accurate on large datasets in both high and low dimensions."}, "pdf": {"value": "/pdf/464a98dfac02ffa5f4f4b7d7ea0ec35594b51b01.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wagner|fast_private_kernel_density_estimation_via_locality_sensitive_quantization"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1301/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373224987, "odate": 1686841455127, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "dT7uMuZJjf", "number": 1219, "cdate": 1674387994280, "tcdate": 1674387994280, "mdate": 1686841454620, "tmdate": 1686841454620, "signatures": ["ICML.cc/2023/Conference/Submission1219/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1219/Authors"], "forum": "dT7uMuZJjf", "content": {"title": {"value": "Sequential Underspecified Instrument Selection for Cause-Effect Estimation"}, "authors": {"value": ["Elisabeth Ailer", "Jason Hartford", "Niki Kilbertus"]}, "authorids": {"value": ["~Elisabeth_Ailer1", "~Jason_Hartford1", "~Niki_Kilbertus1"]}, "abstract": {"value": "Instrumental variable (IV) methods are used to estimate causal effects in settings with unobserved confounding, where we cannot directly experiment on the treatment variable. Instruments are variables which only affect the outcome indirectly via the treatment variable(s). Most IV applications focus on low-dimensional treatments and crucially require at least as many instruments as treatments. This assumption is restrictive: in the natural sciences we often seek to infer causal effects of high-dimensional treatments (e.g., the effect of gene expressions or microbiota on health and disease), but can only run few experiments with a limited number of instruments (e.g., drugs or antibiotics). In such under-specified problems, the full treatment effect is not identifiable in a single experiment even in the linear case. We show that one can still reliably recover the projection of the treatment effect onto the instrumented subspace and develop techniques to consistently combine such partial estimates from different sets of instruments. We then leverage our combined estimators in an algorithm that iteratively proposes the most informative instruments at each round of experimentation to maximize the overall information about the full causal effect."}, "pdf": {"value": "/pdf/7948717f41d0246ee05b9fa5281f4c643472a200.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ailer|sequential_underspecified_instrument_selection_for_causeeffect_estimation"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1219/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373222906, "odate": 1686841454608, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "4YYgtY1APK", "number": 1211, "cdate": 1674379225771, "tcdate": 1674379225771, "mdate": 1686841454551, "tmdate": 1686841454551, "signatures": ["ICML.cc/2023/Conference/Submission1211/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1211/Authors"], "forum": "4YYgtY1APK", "content": {"title": {"value": "Subequivariant Graph Reinforcement Learning in 3D Environments"}, "authors": {"value": ["Runfa Chen", "Jiaqi Han", "Fuchun Sun", "Wenbing Huang"]}, "authorids": {"value": ["~Runfa_Chen1", "~Jiaqi_Han2", "~Fuchun_Sun1", "~Wenbing_Huang1"]}, "abstract": {"value": "Learning a shared policy that guides the locomotion of different agents is of core interest in Reinforcement Learning (RL), which leads to the study of morphology-agnostic RL. However, existing benchmarks are highly restrictive in the choice of starting point and target point, constraining the movement of the agents within 2D space. In this work, we propose a novel setup for morphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments (3D-SGRL). Specifically, we first introduce a new set of more practical yet challenging benchmarks in 3D space that allows the agent to have full Degree-of-Freedoms to explore in arbitrary directions starting from arbitrary configurations. Moreover, to optimize the policy over the enlarged state-action space, we propose to inject geometric symmetry, i.e., subequivariance, into the modeling of the policy and Q-function such that the policy can generalize to all directions, improving exploration efficiency. This goal is achieved by a novel SubEquivariant Transformer (SET) that permits expressive message exchange. Finally, we evaluate the proposed method on the proposed benchmarks, where our method consistently and significantly outperforms existing approaches on single-task, multi-task, and zero-shot generalization scenarios. Extensive ablations are also conducted to verify our design."}, "pdf": {"value": "/pdf/82396655b96debffce32f3aafdd0394c470acb22.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "chen|subequivariant_graph_reinforcement_learning_in_3d_environments"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1211/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373222648, "odate": 1686841454538, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "YxpkGn5Oly", "number": 1177, "cdate": 1674353780556, "tcdate": 1674353780556, "mdate": 1686841454430, "tmdate": 1686841454430, "signatures": ["ICML.cc/2023/Conference/Submission1177/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1177/Authors"], "forum": "YxpkGn5Oly", "content": {"title": {"value": "Tilted Sparse Additive Models"}, "authors": {"value": ["Yingjie Wang", "Hong Chen", "Weifeng Liu", "Fengxiang He", "Tieliang Gong", "Youcheng Fu", "Dacheng Tao"]}, "authorids": {"value": ["~Yingjie_Wang1", "~Hong_Chen1", "~Weifeng_Liu1", "~Fengxiang_He1", "~Tieliang_Gong2", "~Youcheng_Fu2", "~Dacheng_Tao1"]}, "abstract": {"value": "Additive models have been burgeoning in data analysis due to their flexible representation and desirable interpretability. However, most existing approaches are constructed under empirical risk minimization (ERM), and thus perform poorly in situations where average performance is not a suitable criterion for the problems of interest, e.g., data with complex non-Gaussian noise, imbalanced labels or both of them. In this paper, a novel class of sparse additive models is proposed under tilted empirical risk minimization (TERM), which addresses the deficiencies in ERM by imposing tilted impact on individual losses, and is flexibly capable of achieving a variety of learning objectives, e.g., variable selection, robust estimation, imbalanced classification and multiobjective learning. On the theoretical side, a learning theory analysis which is centered around the generalization bound and function approximation error bound (under some specific data distributions) is conducted rigorously. On the practical side, an accelerated optimization algorithm is designed by integrating Prox-SVRG and random Fourier acceleration technique. The empirical assessments verify the competitive performance of our approach on both synthetic and real data."}, "pdf": {"value": "/pdf/fbeb18ae7795aefc5ba2d0d5d572f197831ff25b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wang|tilted_sparse_additive_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1177/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373221921, "odate": 1686841454416, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "ZVxT2ToHR5", "number": 1163, "cdate": 1674344620395, "tcdate": 1674344620395, "mdate": 1687336385268, "tmdate": 1687336385268, "signatures": ["ICML.cc/2023/Conference/Submission1163/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1163/Authors"], "forum": "ZVxT2ToHR5", "content": {"title": {"value": "Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning"}, "authors": {"value": ["Christopher A. Choquette-Choo", "Hugh Brendan McMahan", "J Keith Rush", "Abhradeep Guha Thakurta"]}, "authorids": {"value": ["~Christopher_A._Choquette-Choo1", "~Hugh_Brendan_McMahan1", "~J_Keith_Rush1", "~Abhradeep_Guha_Thakurta1"]}, "abstract": {"value": "We introduce new differentially private (DP) mechanisms for gradient-based machine learning (ML) with multiple passes (epochs) over a dataset, substantially improving the achievable privacy-utility-computation tradeoffs. We formalize the problem of DP mechanisms for adaptive streams with multiple participations and introduce a non-trivial extension of online matrix factorization DP mechanisms to our setting. This includes establishing the necessary theory for sensitivity calculations and efficient computation of optimal matrices. For some applications like $>\\!\\! 10,000$ SGD steps, applying these optimal techniques becomes computationally expensive. We thus design an efficient Fourier-transform-based mechanism with only a minor utility loss. Extensive empirical evaluation on both example-level DP for image classification and user-level DP for language modeling demonstrate substantial improvements over all previous methods, including the widely-used DP-SGD. Though our primary application is to ML, our main DP results are applicable to arbitrary linear queries and hence may have much broader applicability."}, "pdf": {"value": "/pdf/124164e0f1e140ecf90c596e64714c8a725cb1d0.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "choquettechoo|multiepoch_matrix_factorization_mechanisms_for_private_machine_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1163/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373221559, "odate": 1686841454394, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "pRQOVucM8e", "number": 1117, "cdate": 1674295582012, "tcdate": 1674295582012, "mdate": 1686841454089, "tmdate": 1686841454089, "signatures": ["ICML.cc/2023/Conference/Submission1117/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1117/Authors"], "forum": "pRQOVucM8e", "content": {"title": {"value": "Dynamics-inspired Neuromorphic Visual Representation Learning"}, "authors": {"value": ["Zhengqi Pei", "Shuhui Wang"]}, "authorids": {"value": ["~Zhengqi_Pei1", "~Shuhui_Wang1"]}, "abstract": {"value": "This paper investigates the dynamics-inspired neuromorphic architecture for visual representation learning following Hamilton's principle. Our method converts weight-based neural structure to its dynamics-based form that consists of finite sub-models, whose mutual relations measured by computing path integrals amongst their dynamical states are equivalent to the typical neural weights. Based on the entropy reduction process derived from the Euler-Lagrange equations, the feedback signals interpreted as stress forces amongst sub-models push them to move. We first train a dynamics-based neural model from scratch and observe that this model outperforms traditional neural models on MNIST. We then convert several pre-trained neural structures into dynamics-based forms, followed by fine-tuning via entropy reduction to obtain the stabilized dynamical states. We observe consistent improvements in these transformed models over their weight-based counterparts on ImageNet and WebVision in terms of computational complexity, parameter size, testing accuracy, and robustness. Besides, we show the correlation between model performance and structural entropy, providing deeper insight into weight-free neuromorphic learning."}, "pdf": {"value": "/pdf/fa5507d6907c06d905b3dc971db9df6e225a9b1a.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "pei|dynamicsinspired_neuromorphic_visual_representation_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1117/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373220430, "odate": 1686841454078, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "XQuXfVR5Wx", "number": 1046, "cdate": 1674257618395, "tcdate": 1674257618395, "mdate": 1686841453701, "tmdate": 1686841453701, "signatures": ["ICML.cc/2023/Conference/Submission1046/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1046/Authors"], "forum": "XQuXfVR5Wx", "content": {"title": {"value": "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles"}, "authors": {"value": ["Chaitanya Ryali", "Yuan-Ting Hu", "Daniel Bolya", "Chen Wei", "Haoqi Fan", "Po-Yao Huang", "Vaibhav Aggarwal", "Arkabandhu Chowdhury", "Omid Poursaeed", "Judy Hoffman", "Jitendra Malik", "Yanghao Li", "Christoph Feichtenhofer"]}, "authorids": {"value": ["~Chaitanya_Ryali1", "~Yuan-Ting_Hu1", "~Daniel_Bolya1", "~Chen_Wei2", "~Haoqi_Fan2", "~Po-Yao_Huang2", "~Vaibhav_Aggarwal1", "~Arkabandhu_Chowdhury1", "~Omid_Poursaeed2", "~Judy_Hoffman1", "~Jitendra_Malik2", "~Yanghao_Li1", "~Christoph_Feichtenhofer4"]}, "abstract": {"value": "Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera."}, "pdf": {"value": "/pdf/5ce7798463c794be6b22d707ceb6ddb58a264d3f.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "ryali|hiera_a_hierarchical_vision_transformer_without_the_bellsandwhistles"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1046/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373218872, "odate": 1686841453686, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "skb34O7hFp", "number": 1026, "cdate": 1674249014528, "tcdate": 1674249014528, "mdate": 1686841453541, "tmdate": 1686841453541, "signatures": ["ICML.cc/2023/Conference/Submission1026/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1026/Authors"], "forum": "skb34O7hFp", "content": {"title": {"value": "The Dormant Neuron Phenomenon in Deep Reinforcement Learning"}, "authors": {"value": ["Ghada Sokar", "Rishabh Agarwal", "Pablo Samuel Castro", "Utku Evci"]}, "authorids": {"value": ["~Ghada_Sokar1", "~Rishabh_Agarwal2", "~Pablo_Samuel_Castro1", "~Utku_Evci1"]}, "abstract": {"value": "In this work we identify the dormant neuron phenomenon in deep reinforcement learning, where an agent's network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training. Our experiments demonstrate that ReDo maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance."}, "pdf": {"value": "/pdf/f9422610749769827875dd1291e330a23a2bed34.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "sokar|the_dormant_neuron_phenomenon_in_deep_reinforcement_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1026/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373218331, "odate": 1686841453527, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "RRntzKrBTp", "number": 1019, "cdate": 1674245829570, "tcdate": 1674245829570, "mdate": 1686841453493, "tmdate": 1686841453493, "signatures": ["ICML.cc/2023/Conference/Submission1019/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission1019/Authors"], "forum": "RRntzKrBTp", "content": {"title": {"value": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU"}, "authors": {"value": ["Ying Sheng", "Lianmin Zheng", "Binhang Yuan", "Zhuohan Li", "Max Ryabinin", "Beidi Chen", "Percy Liang", "Christopher Re", "Ion Stoica", "Ce Zhang"]}, "authorids": {"value": ["~Ying_Sheng1", "~Lianmin_Zheng2", "~Binhang_Yuan1", "~Zhuohan_Li1", "~Max_Ryabinin1", "~Beidi_Chen1", "~Percy_Liang1", "~Christopher_Re1", "~Ion_Stoica2", "~Ce_Zhang1"]}, "abstract": {"value": "The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen."}, "pdf": {"value": "/pdf/636b0f6623f434f5ac61f4a2e261b0c10ceb5af6.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "sheng|flexgen_highthroughput_generative_inference_of_large_language_models_with_a_single_gpu"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission1019/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373218115, "odate": 1686841453480, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "UxQsrlM6mY", "number": 982, "cdate": 1674227524217, "tcdate": 1674227524217, "mdate": 1686841453228, "tmdate": 1686841453228, "signatures": ["ICML.cc/2023/Conference/Submission982/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission982/Authors"], "forum": "UxQsrlM6mY", "content": {"title": {"value": "Memory-Based Dual Gaussian Processes for Sequential Learning"}, "authors": {"value": ["Paul Edmund Chang", "Prakhar Verma", "S. T. John", "Arno Solin", "Mohammad Emtiyaz Khan"]}, "authorids": {"value": ["~Paul_Edmund_Chang1", "~Prakhar_Verma1", "~S._T._John1", "~Arno_Solin1", "~Mohammad_Emtiyaz_Khan1"]}, "abstract": {"value": "Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning."}, "pdf": {"value": "/pdf/531ff80b548a71f8b59bfe0bce601ec21dc68855.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "chang|memorybased_dual_gaussian_processes_for_sequential_learning"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission982/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373217131, "odate": 1686841453211, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "xH0zbCNR5E", "number": 973, "cdate": 1674224355309, "tcdate": 1674224355309, "mdate": 1687336372237, "tmdate": 1687336372237, "signatures": ["ICML.cc/2023/Conference/Submission973/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission973/Authors"], "forum": "xH0zbCNR5E", "content": {"title": {"value": "AdaBoost is not an Optimal Weak to Strong Learner"}, "authors": {"value": ["Mikael M\u00f8ller H\u00f8gsgaard", "Kasper Green Larsen", "Martin Ritzert"]}, "authorids": {"value": ["~Mikael_M\u00f8ller_H\u00f8gsgaard1", "~Kasper_Green_Larsen1", "~Martin_Ritzert1"]}, "abstract": {"value": "AdaBoost is a classic boosting algorithm for combining multiple inaccurate classifiers produced by a weak learner, to produce a strong learner with arbitrarily high accuracy when given enough training data. Determining the optimal number of samples necessary to obtain a given accuracy of the strong learner, is a basic learning theoretic question. Larsen and Ritzert (NeurIPS'22) recently presented the first provably optimal weak-to-strong learner. However, their algorithm is somewhat complicated and it remains an intriguing question whether the prototypical boosting algorithm AdaBoost also makes optimal use of training samples. In this work, we answer this question in the negative. Concretely, we show that the sample complexity of AdaBoost, and other classic variations thereof, are sub-optimal by at least one logarithmic factor in the desired accuracy of the strong learner."}, "pdf": {"value": "/pdf/370aa7f074fbd9010ee99077f25a84fbcc167d6e.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "h\u00f8gsgaard|adaboost_is_not_an_optimal_weak_to_strong_learner"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission973/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373216884, "odate": 1686841453137, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "HVKmLi1iR4", "number": 900, "cdate": 1674183071716, "tcdate": 1674183071716, "mdate": 1686841452869, "tmdate": 1686841452869, "signatures": ["ICML.cc/2023/Conference/Submission900/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission900/Authors"], "forum": "HVKmLi1iR4", "content": {"title": {"value": "BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models"}, "authors": {"value": ["Taebum Kim", "Hyoungjoo Kim", "Gyeong-In Yu", "Byung-Gon Chun"]}, "authorids": {"value": ["~Taebum_Kim1", "~Hyoungjoo_Kim1", "~Gyeong-In_Yu1", "~Byung-Gon_Chun1"]}, "abstract": {"value": "Pipeline parallelism is a key technique for training large language models within GPU clusters. However, it often leads to a memory imbalance problem, where certain GPUs face high memory pressure while others underutilize their capacity. This imbalance results in suboptimal training performance, even when the overall GPU memory capacity is sufficient for more efficient setups. To address this inefficiency, we propose BPipe, a novel approach for achieving memory balance in pipeline parallelism. BPipe employs an activation balancing method to transfer intermediate activations between GPUs during training, enabling all GPUs to utilize comparable amounts of memory. With balanced memory utilization, BPipe enhances the training efficiency of large language models like GPT-3 by eliminating redundant recomputations or increasing the micro-batch size. Our evaluation conducted on 48 A100 GPUs across six nodes interconnected with HDR InfiniBand shows that BPipe accelerates the training of GPT-3 96B and GPT-3 134B models by 1.25x-2.17x compared to Megatron-LM, a state-of-the-art framework for training large language models."}, "pdf": {"value": "/pdf/d75effc213f423e79a754031ce434e287b71b8f8.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kim|bpipe_memorybalanced_pipeline_parallelism_for_training_large_language_models"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission900/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373215290, "odate": 1686841452856, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "5Akrk9Ln6N", "number": 877, "cdate": 1674169152550, "tcdate": 1674169152550, "mdate": 1686841452682, "tmdate": 1686841452682, "signatures": ["ICML.cc/2023/Conference/Submission877/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission877/Authors"], "forum": "5Akrk9Ln6N", "content": {"title": {"value": "Reparameterized Policy Learning for Multimodal Trajectory Optimization"}, "authors": {"value": ["Zhiao Huang", "Litian Liang", "Zhan Ling", "Xuanlin Li", "Chuang Gan", "Hao Su"]}, "authorids": {"value": ["~Zhiao_Huang1", "~Litian_Liang1", "~Zhan_Ling2", "~Xuanlin_Li1", "~Chuang_Gan1", "~Hao_Su1"]}, "abstract": {"value": "We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward. Our method consistently outperforms previous approaches across a range of tasks. Code and supplementary materials are available on the project page https://haosulab.github.io/RPG/"}, "pdf": {"value": "/pdf/01ca6c794fb3c60cec707bca29c019b57bfaadde.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "huang|reparameterized_policy_learning_for_multimodal_trajectory_optimization"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission877/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373214741, "odate": 1686841452668, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "NUtErghzv4", "number": 740, "cdate": 1674064018560, "tcdate": 1674064018560, "mdate": 1687336363979, "tmdate": 1687336363979, "signatures": ["ICML.cc/2023/Conference/Submission740/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission740/Authors"], "forum": "NUtErghzv4", "content": {"title": {"value": "Marginalization is not Marginal: No Bad VAE Local Minima when Learning Optimal Sparse Representations"}, "authors": {"value": ["David Wipf"]}, "authorids": {"value": ["~David_Wipf1"]}, "abstract": {"value": "Although the variational autoencoder (VAE) represents a widely-used deep generative model, the underlying energy function when applied to continuous data remains poorly understood. In fact, most prior theoretical analysis has assumed a simplified affine decoder such that the model collapses to probabilistic PCA, a restricted regime whereby existing classical algorithms can also be trivially applied to guarantee globally optimal solutions. To push our understanding into more complex, practically-relevant settings, this paper instead adopts a deceptively sophisticated single-layer decoder that nonetheless allows the VAE to address the fundamental challenge of learning optimally sparse representations of continuous data originating from popular multiple-response regression models. In doing so, we can then examine VAE properties within the non-trivial context of solving difficult, NP-hard inverse problems. More specifically, we prove rigorous conditions which guarantee that any minimum of the VAE energy (local or global) will produce the optimally sparse latent representation, meaning zero reconstruction error using a minimal number of active latent dimensions. This is ultimately possible because VAE marginalization over the latent posterior selectively smooths away bad local minima as has been conjectured but not actually proven in prior work. We then discuss how equivalent-capacity deterministic autoencoders, even with appropriate sparsity-promoting regularization of the latent space, maintain bad local minima that do not correspond with such parsimonious representations. Overall, these results serve to elucidate key properties of the VAE loss surface relative to finding low-dimensional structure in data."}, "pdf": {"value": "/pdf/bdf6a88ad39f61b2500931f410daec023efa5ba2.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "wipf|marginalization_is_not_marginal_no_bad_vae_local_minima_when_learning_optimal_sparse_representations"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission740/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373211751, "odate": 1686841452043, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "5ivhVPY8RC", "number": 730, "cdate": 1674053620577, "tcdate": 1674053620577, "mdate": 1686841452013, "tmdate": 1686841452013, "signatures": ["ICML.cc/2023/Conference/Submission730/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission730/Authors"], "forum": "5ivhVPY8RC", "content": {"title": {"value": "Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients"}, "authors": {"value": ["Marc Harkonen", "Markus Lange-Hegermann", "Bogdan Raita"]}, "authorids": {"value": ["~Marc_Harkonen1", "~Markus_Lange-Hegermann1", "bogdanraita@gmail.com"]}, "abstract": {"value": "Partial differential equations (PDEs) are important tools to model physical systems and including them into machine learning models is an important way of incorporating physical knowledge. Given any system of linear PDEs with constant coefficients, we propose a family of Gaussian process (GP) priors, which we call EPGP, such that all realizations are exact solutions of this system. We apply the Ehrenpreis-Palamodov fundamental principle, which works as a non-linear Fourier transform, to construct GP kernels mirroring standard spectral methods for GPs. Our approach can infer probable solutions of linear PDE systems from any data such as noisy measurements, or pointwise defined initial and boundary conditions. Constructing EPGP-priors is algorithmic, generally applicable, and comes with a sparse version (S-EPGP) that learns the relevant spectral frequencies and works better for big data sets. We demonstrate our approach on three families of systems of PDEs, the heat equation, wave equation, and Maxwell's equations, where we improve upon the state of the art in computation time and precision, in some experiments by several orders of magnitude."}, "pdf": {"value": "/pdf/6142eece8a5343ddba26020c5686dbd41a43955b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "harkonen|gaussian_process_priors_for_systems_of_linear_partial_differential_equations_with_constant_coefficients"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission730/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373211564, "odate": 1686841452000, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "1UaGAhLAsL", "number": 725, "cdate": 1674050429099, "tcdate": 1674050429099, "mdate": 1686841451965, "tmdate": 1686841451965, "signatures": ["ICML.cc/2023/Conference/Submission725/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission725/Authors"], "forum": "1UaGAhLAsL", "content": {"title": {"value": "Random Classification Noise does not defeat All Convex Potential Boosters Irrespective of Model Choice"}, "authors": {"value": ["Yishay Mansour", "Richard Nock", "Robert Williamson"]}, "authorids": {"value": ["~Yishay_Mansour2", "~Richard_Nock1", "~Robert_Williamson1"]}, "abstract": {"value": "A landmark negative result of Long and Servedio has had a considerable impact on research and development in boosting algorithms, around the now famous tagline that \"noise defeats all convex boosters\". In this paper, we appeal to the half-century+ founding theory of losses for class probability estimation, an extension of Long and Servedio's results and a new general convex booster to demonstrate that the source of their negative result is in fact the *model class*, linear separators. Losses or algorithms are neither to blame. This leads us to a discussion on an otherwise praised aspect of ML, *parameterisation*."}, "pdf": {"value": "/pdf/55dc7daba6247b4345f428f1fa9ff6be59f1115c.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "mansour|random_classification_noise_does_not_defeat_all_convex_potential_boosters_irrespective_of_model_choice"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission725/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373211443, "odate": 1686841451954, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "RNRbovY8zV", "number": 636, "cdate": 1673973198016, "tcdate": 1673973198016, "mdate": 1687336358047, "tmdate": 1687336358047, "signatures": ["ICML.cc/2023/Conference/Submission636/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission636/Authors"], "forum": "RNRbovY8zV", "content": {"title": {"value": "Delayed Feedback in Kernel Bandits"}, "authors": {"value": ["Sattar Vakili", "Danyal Ahmed", "Alberto Bernacchia", "Ciara Pike-Burke"]}, "authorids": {"value": ["~Sattar_Vakili1", "~Danyal_Ahmed1", "~Alberto_Bernacchia1", "~Ciara_Pike-Burke2"]}, "abstract": {"value": "Black box optimisation of an unknown function from expensive and noisy evaluations is a ubiquitous problem in machine learning, academic research and industrial production. An abstraction of the problem can be formulated as a kernel based bandit problem (also known as Bayesian optimisation), where a learner aims at optimising a kernelized function through sequential noisy observations. The existing work predominantly assumes feedback is immediately available; an assumption which fails in many real world situations, including recommendation systems, clinical trials and hyperparameter tuning. We consider a kernel bandit problem under stochastically delayed feedback, and propose an algorithm with $\\tilde{\\mathcal{O}}\\left(\\sqrt{\\Gamma_k(T) T}+\\mathbb{E}[\\tau]\\right)$ regret, where $T$ is the number of time steps, $\\Gamma_k(T)$ is the maximum information gain of the kernel with $T$ observations, and $\\tau$ is the delay random variable. This represents a significant improvement over the state of the art regret bound of $\\tilde{\\mathcal{O}}\\left(\\Gamma_k(T)\\sqrt{ T}+\\mathbb{E}[\\tau]\\Gamma_k(T)\\right)$ reported in (Verma et al., 2022). In particular, for very non-smooth kernels, the information gain grows almost linearly in time, trivializing the existing results. We also validate our theoretical results with simulations."}, "pdf": {"value": "/pdf/068480588ebbcf995abfc4a667071c2cd65a649a.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "vakili|delayed_feedback_in_kernel_bandits"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission636/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373209154, "odate": 1686841451412, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "3ETNXs54HB", "number": 575, "cdate": 1673936662731, "tcdate": 1673936662731, "mdate": 1686841451022, "tmdate": 1686841451022, "signatures": ["ICML.cc/2023/Conference/Submission575/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission575/Authors"], "forum": "3ETNXs54HB", "content": {"title": {"value": "AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners"}, "authors": {"value": ["Zhixuan Liang", "Yao Mu", "Mingyu Ding", "Fei Ni", "Masayoshi Tomizuka", "Ping Luo"]}, "authorids": {"value": ["~Zhixuan_Liang2", "~Yao_Mu1", "~Mingyu_Ding1", "~Fei_Ni1", "~Masayoshi_Tomizuka1", "~Ping_Luo2"]}, "abstract": {"value": "Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For example, AdaptDiffuser not only outperforms the previous art Diffuser by 20.8% on Maze2D and 7.5% on MuJoCo locomotion, but also adapts better to new tasks, e.g., KUKA pick-and-place, by 27.9% without requiring additional expert data. More visualization results and demo videos could be found on our project page."}, "pdf": {"value": "/pdf/18713cbce5faf7d8bdeaa82c584d78d6126325e2.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "liang|adaptdiffuser_diffusion_models_as_adaptive_selfevolving_planners"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission575/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373207569, "odate": 1686841451010, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "uY7F5bouCN", "number": 308, "cdate": 1673543212146, "tcdate": 1673543212146, "mdate": 1686841449423, "tmdate": 1686841449423, "signatures": ["ICML.cc/2023/Conference/Submission308/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission308/Authors"], "forum": "uY7F5bouCN", "content": {"title": {"value": "Brauer's Group Equivariant Neural Networks"}, "authors": {"value": ["Edward Pearce-Crump"]}, "authorids": {"value": ["~Edward_Pearce-Crump1"]}, "abstract": {"value": "We provide a full characterisation of all of the possible group equivariant neural networks whose layers are some tensor power of $\\mathbb{R}^{n}$ for three symmetry groups that are missing from the machine learning literature: $O(n)$, the orthogonal group; $SO(n)$, the special orthogonal group; and $Sp(n)$, the symplectic group. In particular, we find a spanning set of matrices for the learnable, linear, equivariant layer functions between such tensor power spaces in the standard basis of $\\mathbb{R}^{n}$ when the group is $O(n)$ or $SO(n)$, and in the symplectic basis of $\\mathbb{R}^{n}$ when the group is $Sp(n)$."}, "pdf": {"value": "/pdf/df7a110a745a9645434e7f8758a09c83b490f15b.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "pearcecrump|brauers_group_equivariant_neural_networks"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission308/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373201519, "odate": 1686841449405, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "VXIRjBCV4Z", "number": 249, "cdate": 1673468450119, "tcdate": 1673468450119, "mdate": 1686841449125, "tmdate": 1686841449125, "signatures": ["ICML.cc/2023/Conference/Submission249/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission249/Authors"], "forum": "VXIRjBCV4Z", "content": {"title": {"value": "Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference"}, "authors": {"value": ["Kyurae Kim", "Kaiwen Wu", "Jisu Oh", "Jacob R. Gardner"]}, "authorids": {"value": ["~Kyurae_Kim1", "~Kaiwen_Wu2", "~Jisu_Oh1", "~Jacob_R._Gardner1"]}, "abstract": {"value": "Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the ABC condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence."}, "pdf": {"value": "/pdf/16056d3f0072b01f2f66aa40b79692ff270194de.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "kim|practical_and_matching_gradient_variance_bounds_for_blackbox_variational_bayesian_inference"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission249/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373200380, "odate": 1686841449021, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "eStrtvtXiN", "number": 237, "cdate": 1673455809940, "tcdate": 1673455809940, "mdate": 1687336330920, "tmdate": 1687336330920, "signatures": ["ICML.cc/2023/Conference/Submission237/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission237/Authors"], "forum": "eStrtvtXiN", "content": {"title": {"value": "Fundamental Limits of Two-layer Autoencoders, and Achieving Them with Gradient Methods"}, "authors": {"value": ["Aleksandr Shevchenko", "Kevin K\u00f6gler", "Hamed Hassani", "Marco Mondelli"]}, "authorids": {"value": ["~Aleksandr_Shevchenko1", "~Kevin_K\u00f6gler1", "~Hamed_Hassani2", "~Marco_Mondelli1"]}, "abstract": {"value": "Autoencoders are a popular model in many branches of machine learning and lossy data compression. However, their fundamental limits, the performance of gradient methods and the features learnt during optimization remain poorly understood, even in the two-layer setting. In fact, earlier work has considered either linear autoencoders or specific training regimes (leading to vanishing or diverging compression rates). Our paper addresses this gap by focusing on non-linear two-layer autoencoders trained in the challenging proportional regime in which the input dimension scales linearly with the size of the representation. Our results characterize the minimizers of the population risk, and show that such minimizers are achieved by gradient methods; their structure is also unveiled, thus leading to a concise description of the features obtained via training. For the special case of a sign activation function, our analysis establishes the fundamental limits for the lossy compression of Gaussian sources via (shallow) autoencoders. Finally, while the results are proved for Gaussian data, numerical simulations on standard datasets display the universality of the theoretical predictions."}, "pdf": {"value": "/pdf/f592ce178e43c06e0ac76eba538e120aee6a2080.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "shevchenko|fundamental_limits_of_twolayer_autoencoders_and_achieving_them_with_gradient_methods"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission237/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373200234, "odate": 1686841448920, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "ZOOwHgxfR4", "number": 108, "cdate": 1673321379846, "tcdate": 1673321379846, "mdate": 1687336322944, "tmdate": 1687336322944, "signatures": ["ICML.cc/2023/Conference/Submission108/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission108/Authors"], "forum": "ZOOwHgxfR4", "content": {"title": {"value": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts"}, "authors": {"value": ["Minghao Xu", "Xinyu Yuan", "Santiago Miret", "Jian Tang"]}, "authorids": {"value": ["~Minghao_Xu1", "~Xinyu_Yuan2", "~Santiago_Miret1", "~Jian_Tang1"]}, "abstract": {"value": "Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM's original representation power. On downstream tasks, ProtST enables both supervised learning and zero-shot prediction. We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks. Under the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein classification, and ProtST also enables functional protein retrieval from a large-scale database without any function annotation."}, "pdf": {"value": "/pdf/9080450d5404d52ca740f60fa2f19c4b08deea98.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "xu|protst_multimodality_learning_of_protein_sequences_and_biomedical_texts"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission108/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373197713, "odate": 1686841448187, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "Jc5QwxfyyQ", "number": 59, "cdate": 1673289406301, "tcdate": 1673289406301, "mdate": 1686841447971, "tmdate": 1686841447971, "signatures": ["ICML.cc/2023/Conference/Submission59/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission59/Authors"], "forum": "Jc5QwxfyyQ", "content": {"title": {"value": "Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language"}, "authors": {"value": ["Alexei Baevski", "Arun Babu", "Wei-Ning Hsu", "Michael Auli"]}, "authorids": {"value": ["~Alexei_Baevski1", "~Arun_Babu1", "~Wei-Ning_Hsu2", "~Michael_Auli1"]}, "abstract": {"value": "Current self-supervised learning algorithms are often modality-specific and require large amounts of computational resources. To address these issues, we increase the training efficiency of data2vec, a learning objective that generalizes across several modalities. We do not encode masked tokens, use a fast convolutional decoder and amortize the effort to build teacher representations. data2vec 2.0 benefits from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner. Experiments on ImageNet-1K image classification show that data2vec 2.0 matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time, on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x less time, and on GLUE natural language understanding it matches a retrained RoBERTa model in half the time. Trading some speed for accuracy results in ImageNet-1K top-1 accuracy of 86.8% with a ViT-L model trained for 150 epochs."}, "pdf": {"value": "/pdf/82664148104b799414370efbd450b4100f364cba.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "baevski|efficient_selfsupervised_learning_with_contextualized_target_representations_for_vision_speech_and_language"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission59/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373196828, "odate": 1686841447958, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "SdOn9JSyTx", "number": 42, "cdate": 1673282502608, "tcdate": 1673282502608, "mdate": 1687336318103, "tmdate": 1687336318103, "signatures": ["ICML.cc/2023/Conference/Submission42/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission42/Authors"], "forum": "SdOn9JSyTx", "content": {"title": {"value": "Learning GFlowNets From Partial Episodes For Improved Convergence And Stability"}, "authors": {"value": ["Kanika Madan", "Jarrid Rector-Brooks", "Maksym Korablyov", "Emmanuel Bengio", "Moksh Jain", "Andrei Cristian Nica", "Tom Bosc", "Yoshua Bengio", "Nikolay Malkin"]}, "authorids": {"value": ["~Kanika_Madan3", "~Jarrid_Rector-Brooks2", "~Maksym_Korablyov1", "~Emmanuel_Bengio1", "~Moksh_Jain1", "~Andrei_Cristian_Nica1", "~Tom_Bosc1", "~Yoshua_Bengio1", "~Nikolay_Malkin1"]}, "abstract": {"value": "Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\\lambda$) algorithm in reinforcement learning, we introduce *subtrajectory balance* or SubTB($\\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance."}, "pdf": {"value": "/pdf/b385be827f4f645e57b1423196bd3c4ec7997ccc.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "madan|learning_gflownets_from_partial_episodes_for_improved_convergence_and_stability"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission42/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373196501, "odate": 1686841447831, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "GXZ6cT5cvY", "number": 32, "cdate": 1673276948860, "tcdate": 1673276948860, "mdate": 1687336316590, "tmdate": 1687336316590, "signatures": ["ICML.cc/2023/Conference/Submission32/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission32/Authors"], "forum": "GXZ6cT5cvY", "content": {"title": {"value": "Learning-Rate-Free Learning by D-Adaptation"}, "authors": {"value": ["Aaron Defazio", "Konstantin Mishchenko"]}, "authorids": {"value": ["~Aaron_Defazio1", "~Konstantin_Mishchenko1"]}, "abstract": {"value": "The speed of gradient descent for convex Lipschitz functions is highly dependent on the choice of learning rate. Setting the learning rate to achieve the optimal convergence rate requires knowing the distance D from the initial point to the solution set. In this work, we describe a single-loop method, with no back-tracking or line searches, which does not require knowledge of D yet asymptotically achieves the optimal rate of convergence for the complexity class of convex Lipschitz functions. Our approach is the first parameter-free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems. Our method is practical, efficient and requires no additional function value or gradient evaluations each step. An implementation is provided in the supplementary material."}, "pdf": {"value": "/pdf/7b2fcf3a3fe28e2f75d8e81ca8e2675e142a4a8d.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "defazio|learningratefree_learning_by_dadaptation"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission32/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373196282, "odate": 1686841447802, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}, {"id": "31H0gfU8Ar", "number": 20, "cdate": 1673274114976, "tcdate": 1673274114976, "mdate": 1687336314437, "tmdate": 1687336314437, "signatures": ["ICML.cc/2023/Conference/Submission20/Authors"], "readers": ["everyone"], "writers": ["ICML.cc/2023/Conference", "ICML.cc/2023/Conference/Submission20/Authors"], "forum": "31H0gfU8Ar", "content": {"title": {"value": "Cones: Concept Neurons in Diffusion Models for Customized Generation"}, "authors": {"value": ["Zhiheng Liu", "Ruili Feng", "Kai Zhu", "Yifei Zhang", "Kecheng Zheng", "Yu Liu", "Deli Zhao", "Jingren Zhou", "Yang Cao"]}, "authorids": {"value": ["~Zhiheng_Liu1", "~Ruili_Feng1", "~Kai_Zhu4", "~Yifei_Zhang4", "~Kecheng_Zheng2", "~Yu_Liu23", "~Deli_Zhao1", "~Jingren_Zhou1", "~Yang_Cao5"]}, "abstract": {"value": "Human brains respond to semantic features of presented stimuli with different neurons. This raises the question of whether deep neural networks admit a similar behavior pattern. To investigate this phenomenon, this paper identifies a small cluster of neurons associated with a specific subject in a diffusion model. We call those neurons the concept neurons. They can be identified by statistics of network gradients to a stimulation connected with the given subject. The concept neurons demonstrate magnetic properties in interpreting and manipulating generation results. Shutting them can directly yield the related subject contextualized in different scenes. Concatenating multiple clusters of concept neurons can vividly generate all related concepts in a single image. Our method attains impressive performance for multi-subject customization, even four or more subjects. For large-scale applications, the concept neurons are environmentally friendly as we only need to store a sparse cluster of int index instead of dense float32 parameter values, reducing storage consumption by 90% compared with previous customized generation methods. Extensive qualitative and quantitative studies on diverse scenarios show the superiority of our method in interpreting and manipulating diffusion models."}, "pdf": {"value": "/pdf/8765e5d3b8ea50fa8c5dd1b78a32d041c36fcbc4.pdf"}, "venue": {"value": "ICML 2023 OralPoster"}, "venueid": {"value": "ICML.cc/2023/Conference"}, "paperhash": {"value": "liu|cones_concept_neurons_in_diffusion_models_for_customized_generation"}}, "invitations": ["ICML.cc/2023/Conference/-/Submission", "ICML.cc/2023/Conference/-/Edit", "ICML.cc/2023/Conference/Submission20/-/Camera_Ready_Revision"], "domain": "ICML.cc/2023/Conference", "pdate": 1682373195668, "odate": 1686841447706, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "authors", "order": 3}, {"name": "authorids", "order": 4}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "title", "order": 10}, {"name": "supplementary_material", "order": 10}, {"name": "abstract", "order": 11, "input": "textarea", "markdown": true}, {"name": "financial_aid", "order": 11}, {"name": "paper_checklist_guidelines", "order": 12, "input": "checkbox"}, {"name": "verify_author_names", "order": 12, "input": "checkbox"}, {"name": "no_additional_revisions", "order": 13, "input": "checkbox"}, {"name": "pdf_appendices", "order": 14, "input": "checkbox"}, {"name": "latest_style_file", "order": 15, "input": "checkbox"}, {"name": "pdf", "order": 16}, {"name": "paper_verification_code", "order": 17}, {"name": "permissions_form", "order": 18}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}]}}], "count": 155}