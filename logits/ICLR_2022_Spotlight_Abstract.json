{"notes": [{"id": "g1SzIRLQXMM", "original": "-iEEvvwzZ1k", "number": 4724, "cdate": 1632965333637, "mdate": null, "ddate": null, "tcdate": 1633409729580, "tmdate": 1676330441883, "tddate": null, "forum": "g1SzIRLQXMM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream", "authorids": ["~Franziska_Geiger1", "~Martin_Schrimpf1", "~Tiago_Marques2", "~James_J._DiCarlo1"], "authors": ["Franziska Geiger", "Martin Schrimpf", "Tiago Marques", "James J. DiCarlo"], "keywords": ["computational neuroscience", "primate visual ventral stream", "convolutional neural networks", "biologically plausible learning"], "abstract": "After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are considered poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image.  While ongoing research is pursuing the use of unsupervised proxies for labels, we here explore a complementary strategy of reducing the required number of supervised synaptic updates to produce an adult-like ventral visual stream (as judged by the match to V1, V2, V4, IT, and behavior). Such models might require less precise machinery and energy expenditure to coordinate these updates and would thus move us closer to viable neuroscientific hypotheses about how the visual system wires itself up. Relative to standard model training on labeled images in ImageNet, we here demonstrate that the total number of supervised weight updates can be substantially reduced using three complementary strategies: First, we find that only 2% of supervised updates (epochs and images) are needed to achieve 80% of the match to adult ventral stream. Specifically, training benefits predictions of higher visual cortex the most whereas early visual cortex predictions only improve marginally over the course of training. Second, by improving the random distribution of synaptic connectivity, we find that 54% of the brain match can already be achieved \u201cat birth\" (i.e. no training at all). Third, we find that, by training only 5% of model synapses, we can still achieve nearly 80% of the match to the ventral stream. This approach further improves on ImageNet performance over previous attempts in computer vision of minimizing trained components without substantially increasing the relative number of trained parameters. These results reflect first steps in modeling not just primate adult visual processing during inference, but also how the ventral visual stream might be \"wired up\" by evolution (a model's \"birth\" state) and by developmental learning (a model's updates based on visual experience).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "geiger|wiring_up_vision_minimizing_supervised_synaptic_updates_needed_to_produce_a_primate_ventral_stream", "pdf": "/pdf/52d6dfe1e693892f0382d80002c972022ced3a49.pdf", "one-sentence_summary": "We develop biologically-motivated initialization and training procedures to train models with 200x fewer synaptic updates (epochs x labeled images x weights) while maintaining 80% of brain predictivity on a set of neural and behavioral benchmarks.", "_bibtex": "@inproceedings{\ngeiger2022wiring,\ntitle={Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream},\nauthor={Franziska Geiger and Martin Schrimpf and Tiago Marques and James J. DiCarlo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=g1SzIRLQXMM}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "CALFyKVs87", "original": "mMayc6ur-u4W", "number": 4616, "cdate": 1632875764623, "mdate": null, "ddate": null, "tcdate": 1632875764623, "tmdate": 1676330444589, "tddate": null, "forum": "CALFyKVs87", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Dynamics-Aware Comparison of Learned Reward Functions", "authorids": ["~Blake_Wulfe1", "logan.ellis@tri.global", "jean.mercat@tri.global", "~Rowan_Thomas_McAllister1", "~Adrien_Gaidon1"], "authors": ["Blake Wulfe", "Logan Michael Ellis", "Jean Mercat", "Rowan Thomas McAllister", "Adrien Gaidon"], "keywords": ["Reward Learning", "Inverse Reinforcement Learning", "Reinforcement Learning", "Comparing Reward Functions"], "abstract": "The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, $\\textit{comparing}$ reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach conflates deficiencies in the reward function with those of the policy search algorithm used to optimize it. To address this challenge, Gleave et al. (2020) propose the Equivalent-Policy Invariant Comparison (EPIC) distance. EPIC avoids policy optimization, but in doing so requires computing reward values at transitions that may be impossible under the system dynamics. This is problematic for learned reward functions because it entails evaluating them outside of their training distribution, resulting in inaccurate reward values that we show can render EPIC ineffective at comparing rewards. To address this problem, we propose the Dynamics-Aware Reward Distance (DARD), a new reward pseudometric. DARD uses an approximate transition model of the environment to transform reward functions into a form that allows for comparisons that are invariant to reward shaping while only evaluating reward functions on transitions close to their training distribution. Experiments in simulated physical domains demonstrate that DARD enables reliable reward comparisons without policy optimization and is significantly more predictive than baseline methods of downstream policy performance when dealing with learned reward functions.", "one-sentence_summary": "We propose a method for quantifying the similarity of learned reward functions without performing policy learning and evaluation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wulfe|dynamicsaware_comparison_of_learned_reward_functions", "pdf": "/pdf/14a7ecb3498b71a8fba347a8d3438e054084f561.pdf", "supplementary_material": "/attachment/66f4d83cb036753660d3553547f09c84531de4b2.zip", "_bibtex": "@inproceedings{\nwulfe2022dynamicsaware,\ntitle={Dynamics-Aware Comparison of Learned Reward Functions},\nauthor={Blake Wulfe and Logan Michael Ellis and Jean Mercat and Rowan Thomas McAllister and Adrien Gaidon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CALFyKVs87}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "5LXw_QplBiF", "original": "YZ6mnkhtLFf", "number": 4554, "cdate": 1632875760596, "mdate": null, "ddate": null, "tcdate": 1632875760596, "tmdate": 1676330446556, "tddate": null, "forum": "5LXw_QplBiF", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Hierarchical Structures with Differentiable Nondeterministic Stacks", "authorids": ["~Brian_DuSell1", "~David_Chiang1"], "authors": ["Brian DuSell", "David Chiang"], "keywords": ["RNN", "pushdown automata", "nondeterminism", "formal languages", "language modeling"], "abstract": "Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank.", "one-sentence_summary": "We present a new stack-augmented RNN with strong results on CFL language modeling tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dusell|learning_hierarchical_structures_with_differentiable_nondeterministic_stacks", "pdf": "/pdf/bfc2ff0a81fd70d01a09a0cb018dddf36e401060.pdf", "supplementary_material": "/attachment/fbc0f04369e9911a9ace7abc5a8f3be541ece0f8.zip", "_bibtex": "@inproceedings{\ndusell2022learning,\ntitle={Learning Hierarchical Structures with Differentiable Nondeterministic Stacks},\nauthor={Brian DuSell and David Chiang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5LXw_QplBiF}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "eMudnJsb1T5", "original": "9lpaM4qcJyez", "number": 4500, "cdate": 1632875756938, "mdate": null, "ddate": null, "tcdate": 1632875756938, "tmdate": 1697934514652, "tddate": null, "forum": "eMudnJsb1T5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sampling with Mirrored Stein Operators", "authorids": ["~Jiaxin_Shi1", "~Chang_Liu10", "~Lester_Mackey1"], "authors": ["Jiaxin Shi", "Chang Liu", "Lester Mackey"], "keywords": ["Stein's method", "Sampling", "Mirror descent", "Natural gradient descent", "Probabilistic inference", "Bayesian inference", "Post-selection inference", "Stein operators"], "abstract": "We introduce a new family of particle evolution samplers suitable for constrained domains and non-Euclidean geometries. Stein Variational Mirror Descent and Mirrored Stein Variational Gradient Descent minimize the Kullback-Leibler (KL) divergence to constrained target distributions by evolving particles in a dual space defined by a mirror map. Stein Variational Natural Gradient exploits non-Euclidean geometry to more efficiently minimize the KL divergence to unconstrained targets. We derive these samplers from a new class of mirrored Stein operators and adaptive kernels developed in this work. We demonstrate that these new samplers yield accurate approximations to distributions on the simplex, deliver valid confidence intervals in post-selection inference, and converge more rapidly than prior methods in large-scale unconstrained posterior inference. Finally, we establish the convergence of our new procedures under verifiable conditions on the target distribution.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shi|sampling_with_mirrored_stein_operators", "pdf": "/pdf/5c867e64ede248dde6dfa23bae9c1f0365022fdb.pdf", "one-sentence_summary": "We introduce multi-particle generalization of mirror descent for sampling in constrained domains and non-Euclidean geometries.", "supplementary_material": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.12506/code)", "_bibtex": "@inproceedings{\nshi2022sampling,\ntitle={Sampling with Mirrored Stein Operators},\nauthor={Jiaxin Shi and Chang Liu and Lester Mackey},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=eMudnJsb1T5}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "X6D9bAHhBQ1", "original": "-XbEHFpj4WLt", "number": 4498, "cdate": 1632875756804, "mdate": null, "ddate": null, "tcdate": 1632875756804, "tmdate": 1676330448426, "tddate": null, "forum": "X6D9bAHhBQ1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Planning in Stochastic Environments with a Learned Model", "authorids": ["~Ioannis_Antonoglou1", "~Julian_Schrittwieser1", "~Sherjil_Ozair1", "~Thomas_K_Hubert1", "~David_Silver1"], "authors": ["Ioannis Antonoglou", "Julian Schrittwieser", "Sherjil Ozair", "Thomas K Hubert", "David Silver"], "keywords": ["model-based reinforcement learning", "deep reinforcement learning", "tree based search", "MCTS"], "abstract": "Model-based reinforcement learning has proven highly successful. However, learning a model in isolation from its use during planning is problematic in complex environments. To date, the most effective techniques have instead combined value-equivalent model learning with powerful tree-search methods. This approach is exemplified by MuZero, which has achieved state-of-the-art performance in a wide range of domains, from board games to visually rich environments, with discrete and continuous action spaces, in online and offline settings. However, previous instantiations of this approach were limited to the use of deterministic models. This limits their performance in environments that are inherently stochastic, partially observed, or so large and complex that they appear stochastic to a finite agent. In this paper we extend this approach to learn and plan with stochastic models. Specifically, we introduce a new algorithm, Stochastic MuZero, that learns a stochastic model incorporating afterstates, and uses this model to perform a stochastic tree search. Stochastic MuZero matched or exceeded the state of the art in a set of canonical single and multi-agent environments, including 2048 and backgammon, while maintaining the same performance as standard MuZero in the game of Go.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "antonoglou|planning_in_stochastic_environments_with_a_learned_model", "pdf": "/pdf/f49fc80947707469997960f573102cea38cafb0f.pdf", "_bibtex": "@inproceedings{\nantonoglou2022planning,\ntitle={Planning in Stochastic Environments with a Learned Model},\nauthor={Ioannis Antonoglou and Julian Schrittwieser and Sherjil Ozair and Thomas K Hubert and David Silver},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=X6D9bAHhBQ1}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "T8wHz4rnuGL", "original": "A-M1FLwsdI_", "number": 4402, "cdate": 1632875750536, "mdate": null, "ddate": null, "tcdate": 1632875750536, "tmdate": 1676330452809, "tddate": null, "forum": "T8wHz4rnuGL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "RotoGrad: Gradient Homogenization in Multitask Learning", "authorids": ["~Adri\u00e1n_Javaloy1", "~Isabel_Valera1"], "authors": ["Adri\u00e1n Javaloy", "Isabel Valera"], "keywords": ["multitask learning", "conflicting gradients", "negative transfer"], "abstract": "Multitask learning is being increasingly adopted in applications domains like computer vision and reinforcement learning. However, optimally exploiting its advantages remains a major challenge due to the effect of negative transfer. Previous works have tracked down this issue to the disparities in gradient magnitudes and directions across tasks, when optimizing the shared network parameters. While recent work has acknowledged that negative transfer is a two-fold problem, existing approaches fall short as they only focus on either homogenizing the gradient magnitude across tasks; or greedily change the gradient directions, overlooking future conflicts. In this work, we introduce RotoGrad, an algorithm that tackles negative transfer as a whole: it jointly homogenizes gradient magnitudes and directions, while ensuring training convergence. We show that RotoGrad outperforms competing methods in complex problems, including multi-label classification in CelebA and computer vision tasks in the NYUv2 dataset. A Pytorch implementation can be found in https://github.com/adrianjav/rotograd.", "one-sentence_summary": "We propose an algorithm to simultaneously homogenize gradient magnitudes and directions across tasks in MTL.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "javaloy|rotograd_gradient_homogenization_in_multitask_learning", "pdf": "/pdf/288f1ffa5c44be1b70664610932a5019dd24b6a1.pdf", "supplementary_material": "/attachment/c7dadc4d23625f83e84924c0cb66a1ebb3e7760c.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\njavaloy2022rotograd,\ntitle={RotoGrad: Gradient Homogenization in Multitask Learning},\nauthor={Adri{\\'a}n Javaloy and Isabel Valera},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=T8wHz4rnuGL}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "D6nH3719vZy", "original": "r6vHA_ZEZD_T", "number": 4395, "cdate": 1632875750141, "mdate": null, "ddate": null, "tcdate": 1632875750141, "tmdate": 1697934523225, "tddate": null, "forum": "D6nH3719vZy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Improving Adversarial Transferability of Vision Transformers ", "authorids": ["~Muzammal_Naseer1", "~Kanchana_Ranasinghe1", "~Salman_Khan4", "~Fahad_Khan1", "~Fatih_Porikli2"], "authors": ["Muzammal Naseer", "Kanchana Ranasinghe", "Salman Khan", "Fahad Khan", "Fatih Porikli"], "keywords": ["Vision Transformers", "Adversarial Perturbations"], "abstract": "Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs).  This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very \\emph{low} black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs.Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies specific to the architecture of ViT models.  \\emph{(i) Self-Ensemble:} We propose a method to find multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-specific information at each ViT block. \\emph{(ii) Token Refinement:} We then propose to refine the tokens to further enhance the discriminative capacity at each block of ViT.Our token refinement systematically combines the class tokens with structural information preserved within the patch tokens. An adversarial attack when applied to such refined tokens within the ensemble of classifiers found in a single vision transformer has significantly higher transferability and thereby brings out the true generalization potential of the ViT's adversarial space. Code: https://t.ly/hBbW.", "one-sentence_summary": "Novel approach to improve transferability of adversarial perturbations found in vision transformers via self-ensemble and token refinement.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "naseer|on_improving_adversarial_transferability_of_vision_transformers", "pdf": "/pdf/b40e4df19f3e58593a885adc8809af1ba9864da8.pdf", "supplementary_material": "", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2106.04169/code)", "_bibtex": "@inproceedings{\nnaseer2022on,\ntitle={On Improving Adversarial Transferability of Vision Transformers },\nauthor={Muzammal Naseer and Kanchana Ranasinghe and Salman Khan and Fahad Khan and Fatih Porikli},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=D6nH3719vZy}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "eW5R4Cek6y6", "original": "sg5EJ7MjdQWY", "number": 4393, "cdate": 1632875750009, "mdate": null, "ddate": null, "tcdate": 1632875750009, "tmdate": 1697934523229, "tddate": null, "forum": "eW5R4Cek6y6", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Predicting Generalization using GANs", "authorids": ["~Yi_Zhang1", "~Arushi_Gupta1", "~Nikunj_Saunshi1", "~Sanjeev_Arora1"], "authors": ["Yi Zhang", "Arushi Gupta", "Nikunj Saunshi", "Sanjeev Arora"], "keywords": ["generalization", "generative adversarial network"], "abstract": "Research on generalization bounds for deep networks seeks to give ways to predict test error using just the training dataset and the network parameters. While generalization bounds can give many insights about architecture design, training algorithms etc., what they do not currently do is yield good predictions for actual test error. A recently introduced Predicting Generalization in Deep Learning competition aims to encourage discovery of methods to better predict test error. The current paper investigates a simple idea: can test error be predicted using {\\em synthetic data,} produced using a Generative Adversarial Network (GAN) that was trained on the same training dataset? Upon investigating several GAN models and architectures, we find that this turns out to be the case. \n\nIn fact, using GANs pre-trained on standard datasets, the test error can be predicted without requiring any additional hyper-parameter tuning. This result is surprising because GANs have well-known limitations (e.g. mode collapse) and are known to not learn the data distribution accurately. Yet the generated samples are good enough to substitute for test data. Several additional experiments are presented to explore reasons why GANs do well at this task. In addition to a new approach for predicting generalization, the counter-intuitive phenomena presented in our work may also call for a better understanding of GANs' strengths and limitations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|on_predicting_generalization_using_gans", "pdf": "/pdf/c02dd2fa195251e1b7cc85379208fea1bc5f6a53.pdf", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2111.14212/code)", "_bibtex": "@inproceedings{\nzhang2022on,\ntitle={On Predicting Generalization using {GAN}s},\nauthor={Yi Zhang and Arushi Gupta and Nikunj Saunshi and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=eW5R4Cek6y6}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "L3_SsSNMmy", "original": "v7O7jW_EvV2N", "number": 4356, "cdate": 1632875747517, "mdate": null, "ddate": null, "tcdate": 1632875747517, "tmdate": 1676330455305, "tddate": null, "forum": "L3_SsSNMmy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Connection between Local Attention and Dynamic Depth-wise Convolution", "authorids": ["~Qi_Han3", "~Zejia_Fan1", "~Qi_Dai4", "~Lei_Sun4", "~Ming-Ming_Cheng3", "~Jiaying_Liu1", "~Jingdong_Wang1"], "authors": ["Qi Han", "Zejia Fan", "Qi Dai", "Lei Sun", "Ming-Ming Cheng", "Jiaying Liu", "Jingdong Wang"], "keywords": ["local attention", "depth-wise convolution", "dynamic depth-wise convolution", "weight sharing", "dynamic weight"], "abstract": "Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as dynamic weight computation. We point out that local attention resembles depth-wise convolution and its dynamic variants in sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. The main differences lie in (i) weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners - local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation. The connection between local attention and dynamic depth-wise convolution is empirically verified by the ablation study about weight sharing and dynamic weight computation in Local Vision Transformer and (dynamic) depth-wise convolution. We empirically observe that the models based on depth-wise convolution and the dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "han|on_the_connection_between_local_attention_and_dynamic_depthwise_convolution", "pdf": "/pdf/b5b230d05deb5ca8dcfd87f952bca5621cf5cced.pdf", "one-sentence_summary": "We study the connection between local attention and dynamic depth-wise convolution in terms of sparse connectivity, weight sharing, and dynamic weight", "data": "", "_bibtex": "@inproceedings{\nhan2022on,\ntitle={On the Connection between Local Attention and Dynamic Depth-wise Convolution},\nauthor={Qi Han and Zejia Fan and Qi Dai and Lei Sun and Ming-Ming Cheng and Jiaying Liu and Jingdong Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=L3_SsSNMmy}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "uorVGbWV5sw", "original": "Cy0Z8Lb2foKt", "number": 4355, "cdate": 1632875747449, "mdate": null, "ddate": null, "tcdate": 1632875747449, "tmdate": 1676330455372, "tddate": null, "forum": "uorVGbWV5sw", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Strength of Minibatch Noise in SGD", "authorids": ["~Liu_Ziyin1", "~Kangqiao_Liu1", "~Takashi_Mori1", "~Masahito_Ueda1"], "authors": ["Liu Ziyin", "Kangqiao Liu", "Takashi Mori", "Masahito Ueda"], "keywords": ["stochastic gradient descent", "minibatch noise", "discrete-time SGD", "noise and fluctuation", "exact solvable models"], "abstract": "The noise in stochastic gradient descent (SGD), caused by minibatch sampling, is poorly understood despite its practical importance in deep learning. This work presents the first systematic study of the SGD noise and fluctuations close to a local minimum. We first analyze the SGD noise in linear regression in detail and then derive a general formula for approximating SGD noise in different types of minima. For application, our results (1) provide insight into the stability of training a neural network, (2) suggest that a large learning rate can help generalization by introducing an implicit regularization, (3) explain why the linear learning rate-batchsize scaling law fails at a large learning rate or at a small batchsize and (4) can provide an understanding of how discrete-time nature of SGD affects the recently discovered power-law phenomenon of SGD.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ziyin|strength_of_minibatch_noise_in_sgd", "pdf": "/pdf/d9f2b7e88fec2d057ac6f54cbbd1ecdff0f06afb.pdf", "one-sentence_summary": "We solve the strength and shape of the minibatch noise in SGD exactly. ", "_bibtex": "@inproceedings{\nziyin2022strength,\ntitle={Strength of Minibatch Noise in {SGD}},\nauthor={Liu Ziyin and Kangqiao Liu and Takashi Mori and Masahito Ueda},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=uorVGbWV5sw}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "cU8rknuhxc", "original": "1zW7iYM_WkI", "number": 4352, "cdate": 1632875747246, "mdate": null, "ddate": null, "tcdate": 1632875747246, "tmdate": 1697934526493, "tddate": null, "forum": "cU8rknuhxc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning more skills through optimistic exploration", "authorids": ["~DJ_Strouse1", "~Kate_Baumli1", "~David_Warde-Farley1", "~Volodymyr_Mnih1", "~Steven_Stenberg_Hansen1"], "authors": ["DJ Strouse", "Kate Baumli", "David Warde-Farley", "Volodymyr Mnih", "Steven Stenberg Hansen"], "keywords": ["intrinsic control", "skill discovery", "unsupervised skill learning", "uncertainty estimation", "optimistic exploration", "variational information maximization"], "abstract": "Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from trajectories. The hope is for the agent to explore and master the environment by encouraging each skill (latent) to reliably reach different states. However, an inherent exploration problem lingers: when a novel state is actually encountered, the discriminator will necessarily not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al., 2019). We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage researchers to treat pessimism with DISDAIN.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "strouse|learning_more_skills_through_optimistic_exploration", "pdf": "/pdf/fd77361933d33f5982e69d08631cf6222a3c48ce.pdf", "one-sentence_summary": "Learn more skills by adding an information gain exploration bonus based on discriminator ensemble disagreement.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.14226/code)", "_bibtex": "@inproceedings{\nstrouse2022learning,\ntitle={Learning more skills through optimistic exploration},\nauthor={DJ Strouse and Kate Baumli and David Warde-Farley and Volodymyr Mnih and Steven Stenberg Hansen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cU8rknuhxc}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "PLDOnFoVm4", "original": "gS3gYOGz7hd", "number": 4323, "cdate": 1632875745359, "mdate": null, "ddate": null, "tcdate": 1632875745359, "tmdate": 1676330456791, "tddate": null, "forum": "PLDOnFoVm4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory", "authorids": ["~Zhi_Zhang1", "~Zhuoran_Yang1", "~Han_Liu4", "~Pratap_Tokekar1", "~Furong_Huang1"], "authors": ["Zhi Zhang", "Zhuoran Yang", "Han Liu", "Pratap Tokekar", "Furong Huang"], "keywords": ["Multi-agent Reinforcement Learning", "Predictive State Representation", "Dynamic Interaction Graph"], "abstract": "We study reinforcement learning for partially observable multi-agent systems where each agent only has access to its own observation and reward and aims to maximize its cumulative rewards. To handle partial observations, we propose graph-assisted predictive state representations (GAPSR), a scalable multi-agent representation learning framework that leverages the agent connectivity graphs to aggregate local representations computed by each agent. In addition, our representations are readily able to incorporate dynamic interaction graphs and kernel space embeddings of the predictive states, and thus have strong flexibility and representation power. \nBased on GAPSR, we propose an end-to-end  MARL algorithm that simultaneously infers the predictive representations and uses the representations as the input of a policy optimization algorithm. Empirically, we demonstrate the efficacy of the proposed algorithm provided on both a MAMuJoCo robotic learning experiment and a multi-agent particle learning environment.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|reinforcement_learning_under_a_multiagent_predictive_state_representation_model_method_and_theory", "pdf": "/pdf/abd7a0683441b4eb75fb4381e8ac583f2bff2b90.pdf", "one-sentence_summary": "We propose a new algorithm for MARL under a multi-agent predictive state representation model, where we incorporate a dynamic interaction graph; we provide the theoretical guarantees of our model and run various experiments to support our algorithm.", "supplementary_material": "/attachment/92296e44ac86e038a7149ca9414df5ef0270ef18.zip", "_bibtex": "@inproceedings{\nzhang2022reinforcement,\ntitle={Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory},\nauthor={Zhi Zhang and Zhuoran Yang and Han Liu and Pratap Tokekar and Furong Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PLDOnFoVm4}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "26gKg6x-ie", "original": "A0RTkrTZtv8", "number": 4292, "cdate": 1632875743343, "mdate": null, "ddate": null, "tcdate": 1632875743343, "tmdate": 1697934531727, "tddate": null, "forum": "26gKg6x-ie", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Adversarial Support Alignment", "authorids": ["~Shangyuan_Tong1", "~Timur_Garipov1", "~Yang_Zhang3", "~Shiyu_Chang2", "~Tommi_S._Jaakkola1"], "authors": ["Shangyuan Tong", "Timur Garipov", "Yang Zhang", "Shiyu Chang", "Tommi S. Jaakkola"], "keywords": ["support alignment", "distribution alignment", "optimal transport", "domain adaptation"], "abstract": "We study the problem of aligning the supports of distributions. Compared to the existing work on distribution alignment, support alignment does not require the densities to be matched. We propose symmetric support difference as a divergence measure to quantify the mismatch between supports. We show that select discriminators (e.g. discriminator trained for Jensen-Shannon divergence) are able to map support differences as support differences in their one-dimensional output space. Following this result, our method aligns supports by minimizing a symmetrized relaxed optimal transport cost in the discriminator 1D space via an adversarial process. Furthermore, we show that our approach can be viewed as a limit of existing notions of alignment by increasing transportation assignment tolerance. We quantitatively evaluate the method across domain adaptation tasks with shifts in label distributions. Our experiments show that the proposed method is more robust against these shifts than other alignment-based baselines.", "one-sentence_summary": "We study the problem of aligning the supports of distributions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tong|adversarial_support_alignment", "pdf": "/pdf/baf81164820438550e81b120efdf1f7f96cd349d.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.08908/code)", "_bibtex": "@inproceedings{\ntong2022adversarial,\ntitle={Adversarial Support Alignment},\nauthor={Shangyuan Tong and Timur Garipov and Yang Zhang and Shiyu Chang and Tommi S. Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=26gKg6x-ie}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "41e9o6cQPj", "original": "DLpK6mnFPpm_", "number": 4280, "cdate": 1632875742537, "mdate": null, "ddate": null, "tcdate": 1632875742537, "tmdate": 1676330459435, "tddate": null, "forum": "41e9o6cQPj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GreaseLM: Graph REASoning Enhanced Language Models", "authorids": ["~Xikun_Zhang1", "~Antoine_Bosselut1", "~Michihiro_Yasunaga1", "~Hongyu_Ren1", "~Percy_Liang1", "~Christopher_D_Manning1", "~Jure_Leskovec1"], "authors": ["Xikun Zhang", "Antoine Bosselut", "Michihiro Yasunaga", "Hongyu Ren", "Percy Liang", "Christopher D Manning", "Jure Leskovec"], "keywords": ["language models", "commonsense", "question answering", "knowledge graphs", "KG augmentation"], "abstract": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|greaselm_graph_reasoning_enhanced_language_models", "pdf": "/pdf/1a023786aa33b14412cd0596ee9247b562f4f4fe.pdf", "one-sentence_summary": "We propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and GNNs over multiple layers of modality interaction operations, allowing both modalities to bidirectionally inform the representation of the other.", "data": "", "_bibtex": "@inproceedings{\nzhang2022greaselm,\ntitle={Grease{LM}: Graph {REAS}oning Enhanced Language Models},\nauthor={Xikun Zhang and Antoine Bosselut and Michihiro Yasunaga and Hongyu Ren and Percy Liang and Christopher D Manning and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=41e9o6cQPj}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "DTkEfj0Ygb8", "original": "XOwtIdojlvj", "number": 4263, "cdate": 1632875741531, "mdate": null, "ddate": null, "tcdate": 1632875741531, "tmdate": 1676330459899, "tddate": null, "forum": "DTkEfj0Ygb8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning meta-features for AutoML", "authorids": ["~Herilalaina_Rakotoarison1", "~Louisot_Milijaona1", "~Andry_RASOANAIVO1", "~Michele_Sebag1", "~Marc_Schoenauer1"], "authors": ["Herilalaina Rakotoarison", "Louisot Milijaona", "Andry RASOANAIVO", "Michele Sebag", "Marc Schoenauer"], "keywords": ["AutoML", "Meta-features", "Hyper-parameter Optimization", "Optimal Transport"], "abstract": "This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed \\mf s with the space of distributions on the hyper-parameter configurations. MetaBu meta-features, learned once and for all, induce a topology on the set of datasets that is exploited to define a distribution of promising hyper-parameter configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta-features boosts the performance of state of the art AutoML systems, AutoSklearn (Feurer et al. 2015) and Probabilistic Matrix Factorization (Fusi et al. 2018). Furthermore, the inspection of MetaBu meta-features gives some hints into when an ML algorithm does well. Finally, the topology based on MetaBu meta-features enables to estimate the intrinsic dimensionality of the OpenML benchmark w.r.t. a given ML algorithm or pipeline. The source code is available at https://github.com/luxusg1/metabu.", "one-sentence_summary": "We propose a novel approach to learn dataset meta-features for AutoML.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rakotoarison|learning_metafeatures_for_automl", "pdf": "/pdf/2c85e44817e8f27a5434ef26f8089b6dbaec6dab.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nrakotoarison2022learning,\ntitle={Learning meta-features for Auto{ML}},\nauthor={Herilalaina Rakotoarison and Louisot Milijaona and Andry RASOANAIVO and Michele Sebag and Marc Schoenauer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DTkEfj0Ygb8}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "Dup_dDqkZC5", "original": "luDE5MZUqJ", "number": 4221, "cdate": 1632875738693, "mdate": null, "ddate": null, "tcdate": 1632875738693, "tmdate": 1676330462574, "tddate": null, "forum": "Dup_dDqkZC5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction", "authorids": ["~Roger_Girgis1", "~Florian_Golemo1", "~Felipe_Codevilla1", "~Martin_Weiss4", "~Jim_Aldon_D'Souza1", "~Samira_Ebrahimi_Kahou1", "~Felix_Heide2", "~Christopher_Pal1"], "authors": ["Roger Girgis", "Florian Golemo", "Felipe Codevilla", "Martin Weiss", "Jim Aldon D'Souza", "Samira Ebrahimi Kahou", "Felix Heide", "Christopher Pal"], "keywords": ["trajectory prediction", "motion forecasting", "transformers", "latent variable models"], "abstract": "Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent trajectories. We refer to these architectures as \u201cAutoBots\u201d. The encoder is a stack of interleaved temporal and social multi-head self-attention (MHSA) modules which alternately perform equivariant processing across the temporal and social dimensions. The decoder employs learnable seed parameters in combination with temporal and social MHSA modules allowing it to perform inference over the\nentire future scene in a single forward pass efficiently. AutoBots can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. For the single-agent prediction case, our model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on the Argoverse vehicle prediction challenge. In the multi-agent setting, we evaluate on the synthetic partition of TrajNet++ dataset to showcase the model\u2019s socially-consistent predictions. We also demonstrate our model on general sequences of sets and provide illustrative experiments modelling the sequential structure of the multiple strokes that make up symbols in the Omniglot data. A distinguishing feature of AutoBots is that all models are trainable on a\nsingle desktop GPU (1080 Ti) in under 48h.", "one-sentence_summary": "New Transformer-based architecture for socially consistent motion forecasting. Achieves SotA performance on NuScenes at a fraction of the compute of competing methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "girgis|latent_variable_sequential_set_transformers_for_joint_multiagent_motion_prediction", "pdf": "/pdf/1ab1260f39e79ac98b52759c8221374f595af7aa.pdf", "supplementary_material": "/attachment/fa09bc152c94a14f94ed217063872f9875575325.zip", "data": "", "_bibtex": "@inproceedings{\ngirgis2022latent,\ntitle={Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction},\nauthor={Roger Girgis and Florian Golemo and Felipe Codevilla and Martin Weiss and Jim Aldon D'Souza and Samira Ebrahimi Kahou and Felix Heide and Christopher Pal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Dup_dDqkZC5}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "5FUq05QRc5b", "original": "F3LweAqW6yn", "number": 4176, "cdate": 1632875735672, "mdate": null, "ddate": null, "tcdate": 1632875735672, "tmdate": 1676330464285, "tddate": null, "forum": "5FUq05QRc5b", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective", "authorids": ["~Qi_Lyu2", "~Xiao_Fu1", "~Weiran_Wang1", "~Songtao_Lu1"], "authors": ["Qi Lyu", "Xiao Fu", "Weiran Wang", "Songtao Lu"], "keywords": [], "abstract": "Multiple views of data, both naturally acquired (e.g., image and audio) and artificially produced (e.g., via adding different noise to data samples), have proven useful in enhancing representation learning. Natural views are often handled by multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA], while the artificial ones are frequently used in self-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins. Both types of approaches often involve learning neural feature extractors such that the embeddings of data exhibit high cross-view correlations. Although intuitive, the effectiveness of correlation-based neural embedding is mostly empirically validated. \nThis work aims to understand latent correlation maximization-based deep multiview learning from a latent component identification viewpoint. An intuitive generative model of multiview data is adopted, where the views are different nonlinear mixtures of shared and private components. Since the shared components are view/distortion-invariant, representing the data using such components is believed to reveal the identity of the samples effectively and robustly. Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across views (up to certain ambiguities). In addition, it is further shown that the private information in each view can be provably disentangled from the shared using proper regularization design. A finite sample analysis, which has been rare in nonlinear mixture identifiability study, is also presented. The theoretical results and newly designed regularization are tested on a series of tasks. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lyu|understanding_latent_correlationbased_multiview_learning_and_selfsupervision_an_identifiability_perspective", "pdf": "/pdf/a4489765925696bd54b9091ab12640cebd4b76ce.pdf", "supplementary_material": "/attachment/2dcad8168bd71de571a245b23b3e5d651bcffb8b.zip", "data": "", "_bibtex": "@inproceedings{\nlyu2022understanding,\ntitle={Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective},\nauthor={Qi Lyu and Xiao Fu and Weiran Wang and Songtao Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5FUq05QRc5b}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "EDeVYpT42oS", "original": "KPdhMifDD8E", "number": 4137, "cdate": 1632875733041, "mdate": null, "ddate": null, "tcdate": 1632875733041, "tmdate": 1697934547372, "tddate": null, "forum": "EDeVYpT42oS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Deconstructing the Inductive Biases of Hamiltonian Neural Networks", "authorids": ["~Nate_Gruver1", "~Marc_Anton_Finzi1", "~Samuel_Don_Stanton1", "~Andrew_Gordon_Wilson1"], "authors": ["Nate Gruver", "Marc Anton Finzi", "Samuel Don Stanton", "Andrew Gordon Wilson"], "keywords": [], "abstract": "Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that don\u2019t conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In this paper, we examine the inductive biases that make physics-inspired models successful in practice. We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation. We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems. We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gruver|deconstructing_the_inductive_biases_of_hamiltonian_neural_networks", "pdf": "/pdf/0c5f008200cc427b5b6c416a2ec60e6adf6df996.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2202.04836/code)", "_bibtex": "@inproceedings{\ngruver2022deconstructing,\ntitle={Deconstructing the Inductive Biases of Hamiltonian Neural Networks},\nauthor={Nate Gruver and Marc Anton Finzi and Samuel Don Stanton and Andrew Gordon Wilson},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EDeVYpT42oS}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "TrjbxzRcnf-", "original": "vpFwqzOBbQz", "number": 4131, "cdate": 1632875732637, "mdate": null, "ddate": null, "tcdate": 1632875732637, "tmdate": 1697934548069, "tddate": null, "forum": "TrjbxzRcnf-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Memorizing Transformers", "authorids": ["~Yuhuai_Wu1", "~Markus_Norman_Rabe1", "~DeLesley_Hutchins1", "~Christian_Szegedy1"], "authors": ["Yuhuai Wu", "Markus Norman Rabe", "DeLesley Hutchins", "Christian Szegedy"], "keywords": ["Transformer", "architecture", "memorization."], "abstract": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights.  \nWe instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate $k$NN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. \nOn benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wu|memorizing_transformers", "pdf": "/pdf/33d84d1024126d6a7d4098f2f3beffdbe7057caa.pdf", "one-sentence_summary": "We propose to use an external memory module to allow instant utilization of newly acquired knowledge.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.08913/code)", "_bibtex": "@inproceedings{\nwu2022memorizing,\ntitle={Memorizing Transformers},\nauthor={Yuhuai Wu and Markus Norman Rabe and DeLesley Hutchins and Christian Szegedy},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TrjbxzRcnf-}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "X8cLTHexYyY", "original": "0ATQN7v7SaZx", "number": 4103, "cdate": 1632875730802, "mdate": null, "ddate": null, "tcdate": 1632875730802, "tmdate": 1697934550476, "tddate": null, "forum": "X8cLTHexYyY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning-Augmented $k$-means Clustering", "authorids": ["~Jon_Ergun1", "~Zhili_Feng1", "~Sandeep_Silwal1", "~David_Woodruff1", "~Samson_Zhou1"], "authors": ["Jon C. Ergun", "Zhili Feng", "Sandeep Silwal", "David Woodruff", "Samson Zhou"], "keywords": ["clustering", "learning-augmented algorithms"], "abstract": "$k$-means clustering is a well-studied problem due to its wide applicability. Unfortunately, there exist strong theoretical limits on the performance of any algorithm for the $k$-means problem on worst-case inputs. To overcome this barrier, we consider a scenario where ``advice'' is provided to help perform clustering. Specifically, we consider the $k$-means problem augmented with a predictor that, given any point, returns its cluster label in an approximately optimal clustering up to some, possibly adversarial, error. We present an algorithm whose performance improves along with the accuracy of the predictor, even though na\\\"{i}vely following the accurate predictor can still lead to a high clustering cost. Thus if the predictor is sufficiently accurate, we can retrieve a close to optimal clustering with nearly optimal runtime, breaking known computational barriers for algorithms that do not have access to such advice. We evaluate our algorithms on real datasets and show significant improvements in the quality of clustering.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ergun|learningaugmented_kmeans_clustering", "pdf": "/pdf/aec5563bc92ef6f5f5b441eec312315315b468c9.pdf", "one-sentence_summary": "We study the $k$-means problem augmented with a learning-based predictor that gives noisy information about true labels.", "supplementary_material": "/attachment/5909fc23c1364bc92b1b7f261715bb13083672b5.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.14094/code)", "_bibtex": "@inproceedings{\nergun2022learningaugmented,\ntitle={Learning-Augmented \\$k\\$-means Clustering},\nauthor={Jon C. Ergun and Zhili Feng and Sandeep Silwal and David Woodruff and Samson Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=X8cLTHexYyY}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "SsPCtEY6yCl", "original": "_68WGwMMYN", "number": 4060, "cdate": 1632875728039, "mdate": null, "ddate": null, "tcdate": 1632875728039, "tmdate": 1676330473288, "tddate": null, "forum": "SsPCtEY6yCl", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Uncomputability of Partition Functions in Energy-Based Sequence Models", "authorids": ["~Chu-Cheng_Lin1", "~Arya_D._McCarthy1"], "authors": ["Chu-Cheng Lin", "Arya D. McCarthy"], "keywords": ["energy-based models", "turing completeness", "model capacity", "sequence models", "autoregressive models", "partition function", "parameter estimation", "model selection"], "abstract": "In this paper, we argue that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions. Among other things, this makes model selection--and therefore learning model parameters--not only difficult, but generally _undecidable_. The reason is that there are no good deterministic or randomized estimates of partition functions. Specifically, we exhibit a pathological example where under common assumptions, _no_ useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, we consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness. Our theoretical results suggest that statistical procedures with asymptotic guarantees and sheer (but finite) amounts of compute are not the only things that make sequence modeling work; computability concerns must not be neglected as we consider more expressive model parametrizations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lin|on_the_uncomputability_of_partition_functions_in_energybased_sequence_models", "pdf": "/pdf/886d78756e355f194640b5e7bf0bfbd8482e5623.pdf", "one-sentence_summary": "EBMs over sequences have several theoretical limitations as learnable probabilistic sequence models.", "_bibtex": "@inproceedings{\nlin2022on,\ntitle={On the Uncomputability of Partition Functions in Energy-Based Sequence Models},\nauthor={Chu-Cheng Lin and Arya D. McCarthy},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SsPCtEY6yCl}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "fILj7WpI-g", "original": "9A1Lpv9FOzP", "number": 4039, "cdate": 1632875726631, "mdate": null, "ddate": null, "tcdate": 1632875726631, "tmdate": 1697934557186, "tddate": null, "forum": "fILj7WpI-g", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs", "authorids": ["~Andrew_Jaegle2", "~Sebastian_Borgeaud1", "~Jean-Baptiste_Alayrac2", "~Carl_Doersch1", "~Catalin_Ionescu1", "~David_Ding2", "~Skanda_Koppula1", "~Daniel_Zoran1", "~Andrew_Brock1", "~Evan_Shelhamer2", "~Olivier_J_Henaff1", "~Matthew_Botvinick1", "~Andrew_Zisserman1", "~Oriol_Vinyals1", "~Joao_Carreira1"], "authors": ["Andrew Jaegle", "Sebastian Borgeaud", "Jean-Baptiste Alayrac", "Carl Doersch", "Catalin Ionescu", "David Ding", "Skanda Koppula", "Daniel Zoran", "Andrew Brock", "Evan Shelhamer", "Olivier J Henaff", "Matthew Botvinick", "Andrew Zisserman", "Oriol Vinyals", "Joao Carreira"], "keywords": ["Perceiver", "BERT", "natural language processing", "optical flow", "computer vision", "multimodal", "GLUE", "ImageNet", "StarCraft"], "abstract": "A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jaegle|perceiver_io_a_general_architecture_for_structured_inputs_outputs", "pdf": "/pdf/be7bf6b12e6abb37fb7853467cc6ef71ea5a1659.pdf", "one-sentence_summary": "We propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs.", "supplementary_material": "/attachment/3fc874e2f3eaa32a9fb8a6eecc715613204436b1.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.14795/code)", "_bibtex": "@inproceedings{\njaegle2022perceiver,\ntitle={Perceiver {IO}: A General Architecture for Structured Inputs \\& Outputs},\nauthor={Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Daniel Zoran and Andrew Brock and Evan Shelhamer and Olivier J Henaff and Matthew Botvinick and Andrew Zisserman and Oriol Vinyals and Joao Carreira},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=fILj7WpI-g}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "POvMvLi91f", "original": "IXNKrPoNJRnt", "number": 4011, "cdate": 1632875724660, "mdate": null, "ddate": null, "tcdate": 1632875724660, "tmdate": 1676330476000, "tddate": null, "forum": "POvMvLi91f", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization", "authorids": ["~Aviral_Kumar2", "~Rishabh_Agarwal2", "~Tengyu_Ma1", "~Aaron_Courville3", "~George_Tucker1", "~Sergey_Levine1"], "authors": ["Aviral Kumar", "Rishabh Agarwal", "Tengyu Ma", "Aaron Courville", "George Tucker", "Sergey Levine"], "keywords": ["Q-learning", "offline RL", "regularization"], "abstract": "Despite overparameterization, deep networks trained via supervised learning are surprisingly easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive aliasing, in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images.", "one-sentence_summary": "We show that implicit regularization effects can lead to poor performance in value-based offline RL and propose an explicit regularizer to mitigate these effects.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kumar|dr3_valuebased_deep_reinforcement_learning_requires_explicit_regularization", "pdf": "/pdf/4682f104a198c9218cf0cdcdbdea5d55d4cf56d8.pdf", "_bibtex": "@inproceedings{\nkumar2022dr,\ntitle={{DR}3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization},\nauthor={Aviral Kumar and Rishabh Agarwal and Tengyu Ma and Aaron Courville and George Tucker and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=POvMvLi91f}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "iMSjopcOn0p", "original": "bObqWk5EuW4", "number": 3989, "cdate": 1632875723259, "mdate": null, "ddate": null, "tcdate": 1632875723259, "tmdate": 1697934562669, "tddate": null, "forum": "iMSjopcOn0p", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "MT3: Multi-Task Multitrack Music Transcription", "authorids": ["~Joshua_P_Gardner1", "~Ian_Simon1", "~Ethan_Manilow1", "~Curtis_Hawthorne1", "~Jesse_Engel1"], "authors": ["Joshua P Gardner", "Ian Simon", "Ethan Manilow", "Curtis Hawthorne", "Jesse Engel"], "keywords": ["music transcription", "transformer", "multi-task learning", "low resource learning", "music understanding", "music information retrieval"], "abstract": "Automatic Music Transcription (AMT), inferring musical notes from raw audio, is a challenging task at the core of music understanding. Unlike Automatic Speech Recognition (ASR), which typically focuses on the words of a single speaker, AMT often requires transcribing multiple instruments simultaneously, all while preserving fine-scale pitch and timing information. Further, many AMT datasets are ``low-resource'', as even expert musicians find music transcription difficult and time-consuming. Thus, prior work has focused on task-specific architectures, tailored to the individual instruments of each task. In this work, motivated by the promising results of sequence-to-sequence transfer learning for low-resource Natural Language Processing (NLP), we demonstrate that a general-purpose Transformer model can perform multi-task AMT, jointly transcribing arbitrary combinations of musical instruments across several transcription datasets. We show this unified training framework achieves high-quality transcription results across a range of datasets, dramatically improving performance for low-resource instruments (such as guitar), while preserving strong performance for abundant instruments (such as piano). Finally, by expanding the scope of AMT, we expose the need for more consistent evaluation metrics and better dataset alignment, and provide a strong baseline for this new direction of multi-task AMT.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gardner|mt3_multitask_multitrack_music_transcription", "pdf": "/pdf/d2fbcd8e79c33510066015e1639aa7edbd4a0dac.pdf", "one-sentence_summary": "Unified framework for music transcription, jointly training a single model on six multi-instrument datasets and establishing a new SOTA for low-resource music transcription.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2111.03017/code)", "_bibtex": "@inproceedings{\ngardner2022mt,\ntitle={{MT}3: Multi-Task Multitrack Music Transcription},\nauthor={Joshua P Gardner and Ian Simon and Ethan Manilow and Curtis Hawthorne and Jesse Engel},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iMSjopcOn0p}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "nHpzE7DqAnG", "original": "qUJMWpPSUs3T", "number": 3935, "cdate": 1632875719617, "mdate": null, "ddate": null, "tcdate": 1632875719617, "tmdate": 1676330480162, "tddate": null, "forum": "nHpzE7DqAnG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Does your graph need a confidence boost?  Convergent boosted smoothing on graphs with tabular node features", "authorids": ["~Jiuhai_Chen1", "~Jonas_Mueller1", "~Vassilis_N._Ioannidis1", "~Soji_Adeshina1", "~Yangkun_Wang1", "~Tom_Goldstein1", "~David_Wipf1"], "authors": ["Jiuhai Chen", "Jonas Mueller", "Vassilis N. Ioannidis", "Soji Adeshina", "Yangkun Wang", "Tom Goldstein", "David Wipf"], "keywords": ["Graph Neural Network", "Boosting", "Node classification", "Tabular Data"], "abstract": "Many practical modeling tasks require making predictions using tabular data composed of heterogeneous feature types (e.g., text-based, categorical, continuous, etc.).  In this setting boosted decision trees and related ensembling techniques generally dominate real-world applications involving iid training/test sets.  However, when there are relations between samples and the iid assumption is no longer reasonable, it remains unclear how to incorporate these dependencies within existing boosting pipelines.  To this end, we propose a generalized framework for combining boosted trees and more general model ensembling techniques, with graph propagation layers that share  node/sample information across edges connecting related samples.  And unlike previous efforts to integrate graph-based models with boosting, our approach is anchored to a principled meta loss function such that provable convergence can be guaranteed under relatively mild assumptions. Across a variety of benchmarks involving non-iid graph data with tabular node features, our framework achieves comparable or superior performance.", "one-sentence_summary": "We develop a convergent method for combining boosting and graph propagation layers. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|does_your_graph_need_a_confidence_boost_convergent_boosted_smoothing_on_graphs_with_tabular_node_features", "pdf": "/pdf/1c7555405291429ac38f31042735a6536dd9bfb5.pdf", "_bibtex": "@inproceedings{\nchen2022does,\ntitle={Does your graph need a confidence boost?  Convergent boosted smoothing on graphs with tabular node features},\nauthor={Jiuhai Chen and Jonas Mueller and Vassilis N. Ioannidis and Soji Adeshina and Yangkun Wang and Tom Goldstein and David Wipf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nHpzE7DqAnG}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "_xwr8gOBeV1", "original": "W23o2ZOD_cx", "number": 3929, "cdate": 1632875719209, "mdate": null, "ddate": null, "tcdate": 1632875719209, "tmdate": 1676330480996, "tddate": null, "forum": "_xwr8gOBeV1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Geometric and Physical Quantities improve E(3) Equivariant Message Passing", "authorids": ["~Johannes_Brandstetter1", "~Rob_Hesselink1", "~Elise_van_der_Pol1", "~Erik_J_Bekkers1", "~Max_Welling1"], "authors": ["Johannes Brandstetter", "Rob Hesselink", "Elise van der Pol", "Erik J Bekkers", "Max Welling"], "keywords": ["equivariant graph neural networks", "steerable message passing", "non-linear convolutions", "molecular modeling", "covariant information"], "abstract": "Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E($3$) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. Our model, composed of steerable MLPs, is able to incorporate geometric and physical information in both the message and update functions.\nThrough the definition of steerable node attributes, the MLPs provide a new class of activation functions for general use with steerable feature fields. We discuss ours and related work through the lens of equivariant non-linear convolutions, which further allows us to pin-point the successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable) point convolutions; steerable messages improve upon recent equivariant graph networks that send invariant messages. We demonstrate the effectiveness of our method on several tasks in computational physics and chemistry and provide extensive ablation studies.", "one-sentence_summary": "We generalise equivariant graph networks such that node and edge updates are able to leverage covariant information.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "brandstetter|geometric_and_physical_quantities_improve_e3_equivariant_message_passing", "pdf": "/pdf/65770d511d6363ed18177d5f6ad2c21985a7884b.pdf", "supplementary_material": "/attachment/6d3c1183d7b9d9a521fab1347ddcf929b55234db.zip", "code": "", "_bibtex": "@inproceedings{\nbrandstetter2022geometric,\ntitle={Geometric and Physical Quantities improve E(3) Equivariant Message Passing},\nauthor={Johannes Brandstetter and Rob Hesselink and Elise van der Pol and Erik J Bekkers and Max Welling},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_xwr8gOBeV1}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "l3SDgUh7qZO", "original": "XZc7SbpQTRO", "number": 3925, "cdate": 1632875718926, "mdate": null, "ddate": null, "tcdate": 1632875718926, "tmdate": 1676330481303, "tddate": null, "forum": "l3SDgUh7qZO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SphereFace2: Binary Classification is All You Need for Deep Face Recognition", "authorids": ["~Yandong_Wen1", "~Weiyang_Liu1", "~Adrian_Weller1", "~Bhiksha_Raj1", "~Rita_Singh1"], "authors": ["Yandong Wen", "Weiyang Liu", "Adrian Weller", "Bhiksha Raj", "Rita Singh"], "keywords": [], "abstract": "State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we start by identifying the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the \"competitive\" nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this \"one-vs-all\" binary classification framework so that it can outperform current competitive methods. Our experiments on popular benchmarks demonstrate that SphereFace2 can consistently outperform state-of-the-art deep face recognition methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wen|sphereface2_binary_classification_is_all_you_need_for_deep_face_recognition", "pdf": "/pdf/32df823cca80df310924963131f7d3535aa80a57.pdf", "one-sentence_summary": "A novel deep face recognition framework", "data": "", "_bibtex": "@inproceedings{\nwen2022sphereface,\ntitle={SphereFace2: Binary Classification is All You Need for Deep Face Recognition},\nauthor={Yandong Wen and Weiyang Liu and Adrian Weller and Bhiksha Raj and Rita Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=l3SDgUh7qZO}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "mHu2vIds_-b", "original": "sNtxV7cSnL", "number": 3885, "cdate": 1632875716254, "mdate": null, "ddate": null, "tcdate": 1632875716254, "tmdate": 1697934572029, "tddate": null, "forum": "mHu2vIds_-b", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Boosting Randomized Smoothing with Variance Reduced Classifiers", "authorids": ["~Mikl\u00f3s_Z._Horv\u00e1th1", "~Mark_Niklas_Mueller2", "~Marc_Fischer1", "~Martin_Vechev1"], "authors": ["Mikl\u00f3s Z. Horv\u00e1th", "Mark Niklas Mueller", "Marc Fischer", "Martin Vechev"], "keywords": ["adversarial robustness", "certified robustness", "randomized smoothing"], "abstract": "Randomized Smoothing (RS) is a promising method for obtaining robustness certi\ufb01cates by evaluating a base model under noise. In this work, we: (i) theoretically motivate why ensembles are a particularly suitable choice as base models for RS, and (ii) empirically con\ufb01rm this choice, obtaining state-of-the-art results in multiple settings. The key insight of our work is that the reduced variance of ensembles over the perturbations introduced in RS leads to signi\ufb01cantly more consistent classi\ufb01cations for a given input. This, in turn, leads to substantially increased certi\ufb01able radii for samples close to the decision boundary. Additionally, we introduce key optimizations which enable an up to 55-fold decrease in sample complexity of RS for predetermined radii, thus drastically reducing its computational overhead. Experimentally, we show that ensembles of only 3 to 10 classi\ufb01ers consistently improve on their strongest constituting model with respect to their average certi\ufb01ed radius (ACR) by 5% to 21% on both CIFAR10 and ImageNet, achieving a new state-of-the-art ACR of 0.86 and 1.11, respectively. We release all code and models required to reproduce our results at https://github.com/eth-sri/smoothing-ensembles.", "one-sentence_summary": "We show -- theoretically and empirically -- that ensembles reduce variance under randomized smoothing, yielding higher certified accuracy, leading to a new state-of-the-art on CIFAR-10 and ImageNet.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "horv\u00e1th|boosting_randomized_smoothing_with_variance_reduced_classifiers", "pdf": "/pdf/23e2b6f2b0f6bf1e6d39492a2557b0b0357d6fdf.pdf", "supplementary_material": "/attachment/d413f9e7ffc5bdf88371fbcd10f6367c7be00f90.zip", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.06946/code)", "_bibtex": "@inproceedings{\nhorv{\\'a}th2022boosting,\ntitle={Boosting Randomized Smoothing with Variance Reduced Classifiers},\nauthor={Mikl{\\'o}s Z. Horv{\\'a}th and Mark Niklas Mueller and Marc Fischer and Martin Vechev},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mHu2vIds_-b}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "t5EmXZ3ZLR", "original": "t-bqE7WeBdFA", "number": 3848, "cdate": 1632875713724, "mdate": null, "ddate": null, "tcdate": 1632875713724, "tmdate": 1676330486043, "tddate": null, "forum": "t5EmXZ3ZLR", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning", "authorids": ["~Manuel_Nonnenmacher1", "~Thomas_Pfeil1", "~Ingo_Steinwart1", "~David_Reeb2"], "authors": ["Manuel Nonnenmacher", "Thomas Pfeil", "Ingo Steinwart", "David Reeb"], "keywords": ["Structured Pruning", "Saliency-based Pruning", "Network Compression", "Hessian Approximation", "Neural Architecture Search", "Deep Learning", "Computer Vision"], "abstract": "Pruning neural networks reduces inference time and memory costs. On standard hardware, these benefits will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise two novel saliency-based methods for second-order structured pruning (SOSP) which include correlations among all structures and layers. Our main method SOSP-H employs an innovative second-order approximation, which enables saliency evaluations by fast Hessian-vector products. SOSP-H thereby scales like a first-order method despite taking into account the full Hessian. We validate SOSP-H by comparing it to our second method SOSP-I that uses a well-established Hessian approximation, and to numerous state-of-the-art methods. While SOSP-H performs on par or better in terms of accuracy, it has clear advantages in terms of scalability and efficiency. This allowed us to scale SOSP-H to large-scale vision tasks, even though it captures correlations across all layers of the network. To underscore the global nature of our pruning methods, we evaluate their performance not only by removing structures from a pretrained network, but also by detecting architectural bottlenecks. We show that our algorithms allow to systematically reveal architectural bottlenecks, which we then remove to further increase the accuracy of the networks.", "one-sentence_summary": "We introduce a second-order structured pruning method which efficiently captures global correlations among structures of deep neural networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "nonnenmacher|sosp_efficiently_capturing_global_correlations_by_secondorder_structured_pruning", "pdf": "/pdf/27b7b15fa7028267ed543816aadafc963d85b09a.pdf", "_bibtex": "@inproceedings{\nnonnenmacher2022sosp,\ntitle={{SOSP}: Efficiently Capturing Global Correlations by Second-Order Structured Pruning},\nauthor={Manuel Nonnenmacher and Thomas Pfeil and Ingo Steinwart and David Reeb},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=t5EmXZ3ZLR}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "8Py-W8lSUgy", "original": "DoUz-WWD6-h", "number": 3837, "cdate": 1632875712961, "mdate": null, "ddate": null, "tcdate": 1632875712961, "tmdate": 1676330486351, "tddate": null, "forum": "8Py-W8lSUgy", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Relational Multi-Task Learning: Modeling Relations between Data and Tasks", "authorids": ["~Kaidi_Cao1", "~Jiaxuan_You2", "~Jure_Leskovec1"], "authors": ["Kaidi Cao", "Jiaxuan You", "Jure Leskovec"], "keywords": ["Graph Neural Networks", "Relational Representation Learning", "Multi-task Learning", "Meta Learning"], "abstract": "A key assumption in multi-task learning is that at the inference time the multi-task model only has access to a given data point but not to the data point\u2019s labels from other tasks. This presents an opportunity to extend multi-task learning to utilize data point\u2019s labels from other auxiliary tasks, and this way improves performance on the new task. Here we introduce a novel relational multi-task learning setting where we leverage data point labels from auxiliary tasks to make more accurate predictions on the new task. We develop MetaLink, where our key innovation is to build a knowledge graph that connects data points and tasks and thus allows us to leverage labels from auxiliary tasks. The knowledge graph consists of two types of nodes: (1) data nodes, where node features are data embeddings computed by the neural network, and (2) task nodes, with the last layer\u2019s weights for each task as node features. The edges in this knowledge graph capture data-task relationships, and the edge label captures the label of a data point on a particular task. Under MetaLink, we reformulate the new task as a link label prediction problem between a data node and a task node. The MetaLink framework provides flexibility to model knowledge transfer from auxiliary task labels to the task of interest. We evaluate MetaLink on 6 benchmark datasets in both biochemical and vision domains. Experiments demonstrate that MetaLink can successfully utilize the relations among different tasks, outperforming the state-of-the-art methods under the proposed relational multi-task learning setting, with up to 27% improvement in ROC AUC.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "cao|relational_multitask_learning_modeling_relations_between_data_and_tasks", "pdf": "/pdf/2fc25baa1e108a6b1e91d4388bfae9d417400e3f.pdf", "one-sentence_summary": "We propose MetaLink to solve a variety of multi-task learning settings, by constructing a knowledge graph over data points and tasks.", "data": "", "_bibtex": "@inproceedings{\ncao2022relational,\ntitle={Relational Multi-Task Learning: Modeling Relations between Data and Tasks},\nauthor={Kaidi Cao and Jiaxuan You and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8Py-W8lSUgy}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "sRZ3GhmegS", "original": "drhtijytS2H", "number": 3836, "cdate": 1632875712894, "mdate": null, "ddate": null, "tcdate": 1632875712894, "tmdate": 1697934577522, "tddate": null, "forum": "sRZ3GhmegS", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "CoBERL: Contrastive BERT for Reinforcement Learning", "authorids": ["~Andrea_Banino1", "~Adria_Puigdomenech_Badia2", "~Jacob_C_Walker1", "~Tim_Scholtes1", "~Jovana_Mitrovic1", "~Charles_Blundell1"], "authors": ["Andrea Banino", "Adria Puigdomenech Badia", "Jacob C Walker", "Tim Scholtes", "Jovana Mitrovic", "Charles Blundell"], "keywords": ["Reinforcement Learning", "Contrastive Learning", "Representation Learning", "Transformer", "Deep Reinforcement Learning"], "abstract": "Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. COBERL enables efficient and robust learning from pixels across a wide variety of domains. We use bidirectional masked prediction in combination with a generalization of a recent contrastive method to learn better representations for RL, without the need of hand engineered data augmentations. We find that COBERL consistently improves data efficiency across the full Atari suite, a set of control tasks and a challenging 3D environment, and often it also increases final score performance.", "one-sentence_summary": "A new loss and an improved architecture to efficiently train attentional models in reinforcement learning. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "banino|coberl_contrastive_bert_for_reinforcement_learning", "pdf": "/pdf/c833364a7435330b3ee8e71a2020d1172e9d3380.pdf", "supplementary_material": "/attachment/215de5ab9545cd4cd0521c12e9343b915630f677.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2107.05431/code)", "_bibtex": "@inproceedings{\nbanino2022coberl,\ntitle={Co{BERL}: Contrastive {BERT} for Reinforcement Learning},\nauthor={Andrea Banino and Adria Puigdomenech Badia and Jacob C Walker and Tim Scholtes and Jovana Mitrovic and Charles Blundell},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=sRZ3GhmegS}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "qwBK94cP1y", "original": "shj1n3wSaBg", "number": 3826, "cdate": 1632875712220, "mdate": null, "ddate": null, "tcdate": 1632875712220, "tmdate": 1697934579306, "tddate": null, "forum": "qwBK94cP1y", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Optimal Transport for Causal Discovery", "authorids": ["~Ruibo_Tu1", "~Kun_Zhang1", "~Hedvig_Kjellstrom1", "~Cheng_Zhang1"], "authors": ["Ruibo Tu", "Kun Zhang", "Hedvig Kjellstrom", "Cheng Zhang"], "keywords": ["causal discovery", "optimal transport", "functional causal model"], "abstract": "To determine causal relationships between two variables, approaches based on Functional Causal Models (FCMs) have been proposed by properly restricting model classes; however, the performance is sensitive to the model assumptions, which makes it difficult to use. In this paper, we provide a novel dynamical-system view of FCMs and propose a new framework for identifying causal direction in the bivariate case. We first show the connection between FCMs and optimal transport, and then study optimal transport under the constraints of FCMs. Furthermore, by exploiting the dynamical interpretation of optimal transport under the FCM constraints, we determine the corresponding underlying dynamical process of the static cause-effect pair data. It provides a new dimension for describing static causal discovery tasks while enjoying more freedom for modeling the quantitative causal influences. In particular, we show that Additive Noise Models (ANMs) correspond to volume-preserving pressureless flows. Consequently, based on their velocity field divergence, we introduce a criterion for determining causal direction. With this criterion, we propose a novel optimal transport-based algorithm for ANMs which is robust to the choice of models and extend it to post-nonlinear models. Our method demonstrated state-of-the-art results on both synthetic and causal discovery benchmark datasets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tu|optimal_transport_for_causal_discovery", "pdf": "/pdf/beb079a1e442ec0e13e5da89161a1f30ba96279f.pdf", "supplementary_material": "/attachment/e894ce6b442b67ed4947ed85203f417026f8ba5d.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.09366/code)", "_bibtex": "@inproceedings{\ntu2022optimal,\ntitle={Optimal Transport for Causal Discovery},\nauthor={Ruibo Tu and Kun Zhang and Hedvig Kjellstrom and Cheng Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qwBK94cP1y}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "I1hQbx10Kxn", "original": "eHekShQICrbc", "number": 3797, "cdate": 1632875710259, "mdate": null, "ddate": null, "tcdate": 1632875710259, "tmdate": 1676330489190, "tddate": null, "forum": "I1hQbx10Kxn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Bridging Generic and Personalized Federated Learning for Image Classification", "authorids": ["~Hong-You_Chen1", "~Wei-Lun_Chao1"], "authors": ["Hong-You Chen", "Wei-Lun Chao"], "keywords": ["federated learning", "personalization", "image classification"], "abstract": "Federated learning is promising for its capability to collaboratively train models with multiple clients without accessing their data, but vulnerable when clients' data distributions diverge from each other. This divergence further leads to a dilemma: \"Should we prioritize the learned model's generic performance (for future use at the server) or its personalized performance (for each client)?\" These two, seemingly competing goals have divided the community to focus on one or the other, yet in this paper we show that it is possible to approach both at the same time. Concretely, we propose a novel federated learning framework that explicitly decouples a model's dual duties with two prediction tasks. On the one hand, we introduce a family of losses that are robust to non-identical class distributions, enabling clients to train a generic predictor with a consistent objective across them. On the other hand, we formulate the personalized predictor as a lightweight adaptive module that is learned to minimize each client's empirical risk on top of the generic predictor. With this two-loss, two-predictor framework which we name Federated Robust Decoupling (Fed-RoD), the learned model can simultaneously achieve state-of-the-art generic and personalized performance, essentially bridging the two tasks. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|on_bridging_generic_and_personalized_federated_learning_for_image_classification", "pdf": "/pdf/770f89d1e8fedb42b47cd1ac9c8df3df92f3a178.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nchen2022on,\ntitle={On Bridging Generic and Personalized Federated Learning for Image Classification},\nauthor={Hong-You Chen and Wei-Lun Chao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=I1hQbx10Kxn}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "4-D6CZkRXxI", "original": "Bz4Cr4d42V6", "number": 3741, "cdate": 1632875706628, "mdate": null, "ddate": null, "tcdate": 1632875706628, "tmdate": 1697934588052, "tddate": null, "forum": "4-D6CZkRXxI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Value Gradient weighted Model-Based Reinforcement Learning", "authorids": ["~Claas_A_Voelcker1", "~Victor_Liao1", "~Animesh_Garg1", "~Amir-massoud_Farahmand1"], "authors": ["Claas A Voelcker", "Victor Liao", "Animesh Garg", "Amir-massoud Farahmand"], "keywords": ["model-based reinforcement learning", "reinforcment learning", "objective mismatch", "value function", "sensitivity"], "abstract": "Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches.\n", "one-sentence_summary": "We propose the Value-gradient weighted Model loss, a method for value-aware model learning in challenging settings, such as small model capacity and the presence of distracting state dimensions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "voelcker|value_gradient_weighted_modelbased_reinforcement_learning", "pdf": "/pdf/d924f4fd00b558974bf7f10d5b94c179c583225b.pdf", "supplementary_material": "/attachment/1a681542947089632a1944c54ae8b9062ca8e0cd.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.01464/code)", "_bibtex": "@inproceedings{\nvoelcker2022value,\ntitle={Value Gradient weighted Model-Based Reinforcement Learning},\nauthor={Claas A Voelcker and Victor Liao and Animesh Garg and Amir-massoud Farahmand},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=4-D6CZkRXxI}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "-llS6TiOew", "original": "P7a_mwAmCyt", "number": 3684, "cdate": 1632875703020, "mdate": null, "ddate": null, "tcdate": 1632875703020, "tmdate": 1676330494196, "tddate": null, "forum": "-llS6TiOew", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling", "authorids": ["~Ada_Wan1"], "authors": ["Ada Wan"], "keywords": ["fairness", "evaluation", "multilingual NLP / multilinguality", "representation learning for language data", "statistical comparisons", "Double Descent", "conditional language modeling", "data-centric approach", "diversity in AI", "morphology", "Transformer", "meta evaluation", "visualization or interpretation of learned representations", "character encoding", "internationalization and localization", "robustness", "statistical science for NLP", "science in the era of AI/DL (AIxScience)", "transdisciplinarity"], "abstract": "We perform systematically and fairly controlled experiments with the 6-layer Transformer to investigate  the hardness in conditional-language-modeling languages which have been traditionally considered morphologically rich (AR and RU) and poor (ZH). We evaluate through statistical comparisons across 30 possible language directions from the 6 languages of the United Nations Parallel Corpus across 5 data sizes on 3 representation levels --- character, byte, and word. Results show that performance is relative to the representation granularity of each of the languages, not to the language as a whole. On the character and byte levels, we are able to eliminate statistically significant performance disparity, hence demonstrating that a language cannot be intrinsically hard. The disparity that mirrors the morphological complexity hierarchy is shown to be a byproduct of word segmentation. Evidence from data statistics, along with the fact that word segmentation is qualitatively indeterminate, renders a decades-long debate on morphological complexity (unless it is being intentionally modeled in a word-based, meaning-driven context) irrelevant in the context of computing. The intent of our work is to help effect more objectivity and adequacy in evaluation as well as fairness and inclusivity in experimental setup in the area of language and computing so to uphold diversity in Machine Learning and Artificial Intelligence research. Multilinguality is real and relevant in computing not due to canonical, structural linguistic concepts such as morphology or \"words\" in our minds, but rather standards related to internationalization and localization, such as character encoding --- something which has thus far been sorely overlooked in our discourse and curricula. ", "one-sentence_summary": "We investigate performance disparity in multilingual NLP with Transformer conditional LMs, and find, in the context of computing, morphological complexity to be a byproduct of word segmentation and disparity arising therefrom unwarranted. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wan|fairness_in_representation_for_multilingual_nlp_insights_from_controlled_experiments_on_conditional_language_modeling", "pdf": "/pdf/1bc81aec7b25823dcaab95c24c1c5c0779bd3c7c.pdf", "data": "", "_bibtex": "@inproceedings{\nwan2022fairness,\ntitle={Fairness in Representation for Multilingual {NLP}: Insights from Controlled Experiments on Conditional Language Modeling},\nauthor={Ada Wan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-llS6TiOew}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 58}}, {"id": "YJ1WzgMVsMt", "original": "77PiKHFCoVR", "number": 3625, "cdate": 1632875699447, "mdate": null, "ddate": null, "tcdate": 1632875699447, "tmdate": 1676330496615, "tddate": null, "forum": "YJ1WzgMVsMt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration", "authorids": ["~Desik_Rengarajan1", "gargivaidya@tamu.edu", "~Akshay_Sarvesh1", "~Dileep_Kalathil1", "~Srinivas_Shakkottai1"], "authors": ["Desik Rengarajan", "Gargi Vaidya", "Akshay Sarvesh", "Dileep Kalathil", "Srinivas Shakkottai"], "keywords": ["Reinforcement Learning", "Sparse Rewards", "Learning from Demonstrations"], "abstract": "A major challenge in real-world reinforcement learning (RL) is the sparsity of reward feedback.  Often, what is available is an intuitive but sparse reward function that only indicates whether the task is completed partially or fully.  However, the lack of carefully designed, fine grain feedback implies that most existing RL algorithms fail to learn an acceptable policy in a reasonable time frame.  This is because of the large number of exploration actions that the policy has to perform before it gets any useful feedback that it can learn from.  In this work, we address this challenging problem by developing an algorithm that exploits the offline demonstration data generated by {a sub-optimal behavior policy} for faster and efficient online RL in such sparse reward settings.  The proposed algorithm, which we call the Learning Online with Guidance Offline (LOGO) algorithm, merges a policy improvement step with an additional policy guidance step by using the offline demonstration data.  The key idea is that by obtaining guidance from - not imitating - the offline {data}, LOGO orients its policy in the manner of the sub-optimal {policy}, while yet being able to learn beyond and approach optimality.  We provide a theoretical analysis of our algorithm, and provide a lower bound on the performance improvement in each learning episode.  We also extend our algorithm to the even more challenging incomplete observation setting, where the demonstration data contains only a censored version of the true state observation.  We demonstrate the superior performance of our algorithm over state-of-the-art approaches on a number of  benchmark environments with sparse rewards {and censored state}.  Further, we demonstrate the value of our approach via implementing LOGO on a mobile robot for trajectory tracking and obstacle avoidance, where it shows excellent performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rengarajan|reinforcement_learning_with_sparse_rewards_using_guidance_from_offline_demonstration", "pdf": "/pdf/3b163b1af845a3cb05504bfe3d2f3a4a205fe856.pdf", "one-sentence_summary": "Reinforcement learning in sparse reward environments  using offline guidance. ", "supplementary_material": "/attachment/e9a8a1a43323e97b23825086eb46414675c9d0b4.zip", "_bibtex": "@inproceedings{\nrengarajan2022reinforcement,\ntitle={Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration},\nauthor={Desik Rengarajan and Gargi Vaidya and Akshay Sarvesh and Dileep Kalathil and Srinivas Shakkottai},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YJ1WzgMVsMt}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "49A1Y6tRhaq", "original": "__y41L8wt9c", "number": 3613, "cdate": 1632875698775, "mdate": null, "ddate": null, "tcdate": 1632875698775, "tmdate": 1697934598252, "tddate": null, "forum": "49A1Y6tRhaq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Linking Emergent and Natural Languages via Corpus Transfer", "authorids": ["~Shunyu_Yao1", "~Mo_Yu1", "~Yang_Zhang3", "~Karthik_R_Narasimhan1", "~Joshua_B._Tenenbaum1", "~Chuang_Gan1"], "authors": ["Shunyu Yao", "Mo Yu", "Yang Zhang", "Karthik R Narasimhan", "Joshua B. Tenenbaum", "Chuang Gan"], "keywords": ["Emergent Language", "Emergent Communication", "Transfer Learning"], "abstract": "The study of language emergence aims to understand how human languages are shaped by perceptual grounding and communicative intent. Computational approaches to emergent communication (EC) predominantly consider referential games in limited domains and analyze the learned protocol within the game framework. As a result, it remains unclear how the emergent languages from these settings connect to natural languages or provide benefits in real-world language processing tasks, where statistical models trained on large text corpora dominate. In this work, we propose a novel way to establish such a link by corpus transfer, i.e. pretraining on a corpus of emergent language for downstream natural language tasks, which is in contrast to prior work that directly transfers speaker and listener parameters. Our approach showcases non-trivial transfer benefits for two different tasks \u2013 language modeling and image captioning. For example, in a low-resource setup (modeling 2 million natural language tokens), pre-training on an emergent language corpus with just 2 million tokens reduces model perplexity by 24.6% on average across ten natural languages. We also introduce a novel metric to predict the transferability of an emergent language by translating emergent messages to natural language captions grounded on the same images. We find that our translation-based metric highly correlates with the downstream performance on modeling natural languages (for instance $\\rho = 0.83$ on Hebrew), while topographic similarity, a popular metric in previous works, shows surprisingly low correlation ($\\rho = 0.003$), hinting that simple properties like attribute disentanglement from synthetic domains might not capture the full complexities of natural language. Our findings also indicate potential benefits of moving language emergence forward with natural language resources and models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yao|linking_emergent_and_natural_languages_via_corpus_transfer", "pdf": "/pdf/1a6c8cd5798d4d7939b303c278024594ccda6968.pdf", "one-sentence_summary": "We find that pre-training on an emergent language corpus improves natural language tasks in a low resource setup, and propose a metric to predict such a transferability.", "supplementary_material": "/attachment/d12c42aee122b091709b735cf313ee180c667fcf.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2203.13344/code)", "_bibtex": "@inproceedings{\nyao2022linking,\ntitle={Linking Emergent and Natural Languages via Corpus Transfer},\nauthor={Shunyu Yao and Mo Yu and Yang Zhang and Karthik R Narasimhan and Joshua B. Tenenbaum and Chuang Gan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=49A1Y6tRhaq}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "wv6g8fWLX2q", "original": "UoY2aiQ6WkR", "number": 3597, "cdate": 1632875697766, "mdate": null, "ddate": null, "tcdate": 1632875697766, "tmdate": 1676330498361, "tddate": null, "forum": "wv6g8fWLX2q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting", "authorids": ["~Yuzhou_Chen1", "~Ignacio_Segovia-Dominguez1", "~Baris_Coskunuzer1", "~Yulia_Gel1"], "authors": ["Yuzhou Chen", "Ignacio Segovia-Dominguez", "Baris Coskunuzer", "Yulia Gel"], "keywords": ["topological data analysis", "multipersistence", "graph convolutional networks", "supragraph diffusion", "multivariate time series forecasting"], "abstract": "Graph Neural Networks (GNNs) are proven to be a powerful machinery for learning complex dependencies in multivariate spatio-temporal processes. However, most existing GNNs have inherently static architectures, and as a result, do not explicitly account for time dependencies of the encoded knowledge and are limited in their ability to simultaneously infer latent time-conditioned relations among entities. We postulate that such hidden time-conditioned properties may be captured by the tools of multipersistence, i.e, a emerging machinery in topological data analysis which allows us to quantify dynamics of the data shape along multiple geometric dimensions. \n We make the first step toward integrating the two rising research directions, that is, time-aware deep learning and multipersistence, and propose a new model, Time-Aware Multipersistence Spatio-Supra Graph Convolutional Network (TAMP-S2GCNets). We summarize inherent time-conditioned topological properties of the data as time-aware multipersistence Euler-Poincar\\'e surface and prove its stability. We then construct a supragraph convolution module which simultaneously accounts for the extracted intra- and inter- spatio-temporal dependencies in the data. Our extensive experiments on highway traffic flow, Ethereum token prices, and COVID-19 hospitalizations demonstrate that TAMP-S2GCNets outperforms the state-of-the-art tools in multivariate time series forecasting tasks.", "one-sentence_summary": "We make the first step toward integrating two emerging directions, time-aware deep learning and multi-parameter persistence, allowing us to infer latent time-conditioned relations among entities in multivariate time series forecasting tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|tamps2gcnets_coupling_timeaware_multipersistence_knowledge_representation_with_spatiosupra_graph_convolutional_networks_for_timeseries_forecasting", "pdf": "/pdf/7e6227c594fe1f760bcc5ecab60120d7439ba995.pdf", "supplementary_material": "/attachment/d9d5823ad1434b6aef134edf896bb18ab77aead9.zip", "_bibtex": "@inproceedings{\nchen2022tampsgcnets,\ntitle={{TAMP}-S2{GCN}ets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting},\nauthor={Yuzhou Chen and Ignacio Segovia-Dominguez and Baris Coskunuzer and Yulia Gel},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wv6g8fWLX2q}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 4}}, {"id": "K0E_F0gFDgA", "original": "Y13XMWnxyODh", "number": 3572, "cdate": 1632875696198, "mdate": null, "ddate": null, "tcdate": 1632875696198, "tmdate": 1697934602567, "tddate": null, "forum": "K0E_F0gFDgA", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The MultiBERTs: BERT Reproductions for Robustness Analysis", "authorids": ["~Thibault_Sellam2", "~Steve_Yadlowsky1", "~Ian_Tenney1", "~Jason_Wei1", "~Naomi_Saphra1", "~Alexander_D'Amour1", "~Tal_Linzen1", "~Jasmijn_Bastings1", "~Iulia_Raluca_Turc1", "~Jacob_Eisenstein1", "~Dipanjan_Das1", "~Ellie_Pavlick1"], "authors": ["Thibault Sellam", "Steve Yadlowsky", "Ian Tenney", "Jason Wei", "Naomi Saphra", "Alexander D'Amour", "Tal Linzen", "Jasmijn Bastings", "Iulia Raluca Turc", "Jacob Eisenstein", "Dipanjan Das", "Ellie Pavlick"], "keywords": ["Pre-trained models", "BERT", "bootstrapping", "hypothesis testing", "robustness"], "abstract": "Experiments with pre-trained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact tested in the experiment (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure which includes the architecture, training data, initialization scheme, and loss function. Recent work has shown that repeating the pre-training process can lead to substantially different performance, suggesting that an alternative strategy is needed to make principled statements about procedures. To enable researchers to draw more robust conclusions, we introduce MultiBERTs, a set of 25 BERT-Base checkpoints, trained with similar hyper-parameters as the original BERT model but differing in random weight initialization and shuffling of training data. We also define the Multi-Bootstrap, a non-parametric bootstrap method for statistical inference designed for settings where there are multiple pre-trained models and limited test data. To illustrate our approach, we present a case study of gender bias in coreference resolution, in which the Multi-Bootstrap lets us measure effects that may not be detected with a single checkpoint. The models and statistical library are available online, along with an additional set of 140 intermediate checkpoints captured during pre-training to facilitate research on learning dynamics.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sellam|the_multiberts_bert_reproductions_for_robustness_analysis", "pdf": "/pdf/2e66adcaa5191446f5006eb5db26da387f29ec18.pdf", "one-sentence_summary": "We introduce MultiBERTs, 25 BERT checkpoints trained with similar hyper-parameters but different random seeds, and the Multi-Bootstrap, a bootstrapping method for experimental settings that involve multiple models and limited test data.", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2106.16163/code)", "_bibtex": "@inproceedings{\nsellam2022the,\ntitle={The Multi{BERT}s: {BERT} Reproductions for Robustness Analysis},\nauthor={Thibault Sellam and Steve Yadlowsky and Ian Tenney and Jason Wei and Naomi Saphra and Alexander D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Raluca Turc and Jacob Eisenstein and Dipanjan Das and Ellie Pavlick},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=K0E_F0gFDgA}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "vSix3HPYKSU", "original": "vybOCFQ4omVl", "number": 3519, "cdate": 1632875692664, "mdate": null, "ddate": null, "tcdate": 1632875692664, "tmdate": 1697934607325, "tddate": null, "forum": "vSix3HPYKSU", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Message Passing Neural PDE Solvers", "authorids": ["~Johannes_Brandstetter1", "~Daniel_E._Worrall1", "~Max_Welling1"], "authors": ["Johannes Brandstetter", "Daniel E. Worrall", "Max Welling"], "keywords": ["neural PDE solvers", "message passing", "autoregressive models", "zero-stability"], "abstract": "The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in the low resolution regime in terms of speed, and accuracy.", "one-sentence_summary": "This paper introduces a message passing neural PDE solver that replaces all heuristically designed components in numerical PDE solvers with backprop-optimized neural function approximators. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "brandstetter|message_passing_neural_pde_solvers", "pdf": "/pdf/dfb3e9b359e53414eb2852a5ed8ed48038f889c0.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.03376/code)", "_bibtex": "@inproceedings{\nbrandstetter2022message,\ntitle={Message Passing Neural {PDE} Solvers},\nauthor={Johannes Brandstetter and Daniel E. Worrall and Max Welling},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vSix3HPYKSU}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "Ek7PSN7Y77z", "original": "08kNIdMpjkth", "number": 3426, "cdate": 1632875686634, "mdate": null, "ddate": null, "tcdate": 1632875686634, "tmdate": 1697934613195, "tddate": null, "forum": "Ek7PSN7Y77z", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games", "authorids": ["~Jens_Tuyls1", "~Shunyu_Yao1", "~Sham_M._Kakade1", "~Karthik_R_Narasimhan1"], "authors": ["Jens Tuyls", "Shunyu Yao", "Sham M. Kakade", "Karthik R Narasimhan"], "keywords": ["reinforcement learning", "language understanding", "text-based games"], "abstract": "Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by 27% and 11% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "tuyls|multistage_episodic_control_for_strategic_exploration_in_text_games", "pdf": "/pdf/b67d131e9b9afd599358ce78865538bd83521d24.pdf", "one-sentence_summary": "We propose a multi-stage approach to playing text games that improves the score on Zork1 from around 40 to 103. ", "supplementary_material": "/attachment/bf8204996411ab421764000863c72d3222ad3f14.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.01251/code)", "_bibtex": "@inproceedings{\ntuyls2022multistage,\ntitle={Multi-Stage Episodic Control for Strategic Exploration in Text Games},\nauthor={Jens Tuyls and Shunyu Yao and Sham M. Kakade and Karthik R Narasimhan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Ek7PSN7Y77z}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "V3C8p78sDa", "original": "b_x0VZOaP4H_", "number": 3397, "cdate": 1632875684736, "mdate": null, "ddate": null, "tcdate": 1632875684736, "tmdate": 1697934616451, "tddate": null, "forum": "V3C8p78sDa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Exploring the Limits of Large Scale Pre-training", "authorids": ["~Samira_Abnar1", "~Mostafa_Dehghani1", "~Behnam_Neyshabur1", "~Hanie_Sedghi1"], "authors": ["Samira Abnar", "Mostafa Dehghani", "Behnam Neyshabur", "Hanie Sedghi"], "keywords": ["Scaling law", "Pre-training", "Transfer learning", "Large Scale", "Vision Transformer", "Few Shot", "Empirical Investigation"], "abstract": "Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might  observe that improvements in pre-training would transfer favorably to  most downstream tasks. In this work we systematically study this phenomena and establish that, as we increase the upstream accuracy, performance of downstream tasks \\emph{saturates}. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance  that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the observed saturation behavior is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, in order to have a better downstream performance, we need to hurt upstream accuracy.", "one-sentence_summary": "We perform a systematic investigation of limits of  large scale pre-training for few-shot and transfer learning in image recognition with a wide range of downstream tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "abnar|exploring_the_limits_of_large_scale_pretraining", "pdf": "/pdf/a96ab51b85d9ac8937fe9688a023e72c05b91822.pdf", "supplementary_material": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02095/code)", "_bibtex": "@inproceedings{\nabnar2022exploring,\ntitle={Exploring the Limits of Large Scale Pre-training},\nauthor={Samira Abnar and Mostafa Dehghani and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=V3C8p78sDa}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "JGO8CvG5S9", "original": "H3BG3tuKopii", "number": 3390, "cdate": 1632875684339, "mdate": null, "ddate": null, "tcdate": 1632875684339, "tmdate": 1676330507115, "tddate": null, "forum": "JGO8CvG5S9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Universal Approximation Under Constraints is Possible with Transformers", "authorids": ["~Anastasis_Kratsios1", "~Behnoosh_Zamanlooy2", "~Tianlin_Liu2", "~Ivan_Dokmani\u01071"], "authors": ["Anastasis Kratsios", "Behnoosh Zamanlooy", "Tianlin Liu", "Ivan Dokmani\u0107"], "keywords": ["Constrained Universal Approximation", "Probabilistic Attention", "Transformer Networks", "Geometric Deep Learning", "Measurable Maximum Theorem", "Non-Affine Random Projections", "Optimal Transport."], "abstract": "Many practical problems need the output of a machine learning model to satisfy a set of constraints, $K$.  Nevertheless, there is no known guarantee that classical neural network architectures can exactly encode constraints while simultaneously achieving universality.  We provide a quantitative constrained universal approximation theorem which guarantees that for any non-convex compact set $K$ and any continuous function $f:\\mathbb{R}^n\\rightarrow K$, there is a probabilistic transformer $\\hat{F}$ whose randomized outputs all lie in $K$ and whose expected output uniformly approximates $f$.  Our second main result is a ``deep neural version'' of Berge's Maximum Theorem (1963).  The result guarantees that given an objective function $L$, a constraint set $K$, and a family of soft constraint sets, there is a probabilistic transformer $\\hat{F}$ that approximately minimizes $L$ and whose outputs belong to $K$; moreover, $\\hat{F}$ approximately satisfies the soft constraints.  Our results imply the first universal approximation theorem for classical transformers with exact convex constraint satisfaction.  They also yield that a chart-free universal approximation theorem for Riemannian manifold-valued functions subject to suitable geodesically convex constraints.", "one-sentence_summary": "We provide the first universal approximation theorem with exact non-convex constraint satisfaction, and we introduce probabilistic transformer networks to do so.  ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kratsios|universal_approximation_under_constraints_is_possible_with_transformers", "pdf": "/pdf/5b02f8b0bb76868ca513d915646aba6e37d7727e.pdf", "supplementary_material": "/attachment/e5c9d9dd1be5aa9fa528dee3a89e63f79de1c645.zip", "_bibtex": "@inproceedings{\nkratsios2022universal,\ntitle={Universal Approximation Under Constraints is Possible with Transformers},\nauthor={Anastasis Kratsios and Behnoosh Zamanlooy and Tianlin Liu and Ivan Dokmani{\\'c}},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JGO8CvG5S9}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "hR_SMu8cxCV", "original": "ejEGBdaDrWx", "number": 3358, "cdate": 1632875682241, "mdate": null, "ddate": null, "tcdate": 1632875682241, "tmdate": 1676330508949, "tddate": null, "forum": "hR_SMu8cxCV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Scaling Laws for Neural Machine Translation", "authorids": ["~Behrooz_Ghorbani1", "~Orhan_Firat1", "~Markus_Freitag2", "~Ankur_Bapna1", "~Maxim_Krikun1", "~Xavier_Garcia1", "~Ciprian_Chelba2", "~Colin_Cherry1"], "authors": ["Behrooz Ghorbani", "Orhan Firat", "Markus Freitag", "Ankur Bapna", "Maxim Krikun", "Xavier Garcia", "Ciprian Chelba", "Colin Cherry"], "keywords": ["Scaling Laws", "Neural Machine Translation", "NMT", "Model Scaling"], "abstract": "We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study.", "one-sentence_summary": "We provide (model) scaling laws for neural machine translation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ghorbani|scaling_laws_for_neural_machine_translation", "pdf": "/pdf/dec3d7582a0893c49661157564fdbe66ccc0036f.pdf", "_bibtex": "@inproceedings{\nghorbani2022scaling,\ntitle={Scaling Laws for Neural Machine Translation},\nauthor={Behrooz Ghorbani and Orhan Firat and Markus Freitag and Ankur Bapna and Maxim Krikun and Xavier Garcia and Ciprian Chelba and Colin Cherry},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hR_SMu8cxCV}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "8H5bpVwvt5", "original": "wZEW6zNML6g9", "number": 3341, "cdate": 1632875681062, "mdate": null, "ddate": null, "tcdate": 1632875681062, "tmdate": 1697934623052, "tddate": null, "forum": "8H5bpVwvt5", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning", "authorids": ["~Biwei_Huang1", "~Fan_Feng2", "~Chaochao_Lu1", "~Sara_Magliacane1", "~Kun_Zhang1"], "authors": ["Biwei Huang", "Fan Feng", "Chaochao Lu", "Sara Magliacane", "Kun Zhang"], "keywords": ["Transfer RL", "Graphical models", "Efficient adaptation"], "abstract": "One practical challenge in reinforcement learning (RL) is how to make quick adaptations when faced with new environments. In this paper, we propose a principled framework for adaptive RL, called AdaRL, that adapts reliably and efficiently to changes across domains with a few samples from the target domain, even in partially observable environments. Specifically, we leverage a parsimonious graphical representation that characterizes structural relationships over variables in the RL system. Such graphical representations provide a compact way to encode what and where the changes across domains are, and furthermore inform us with a minimal set of changes that one has to consider for the purpose of policy adaptation. We show that by explicitly leveraging this compact representation to encode changes, we can efficiently adapt the policy to the target domain, in which only a few samples are needed and further policy optimization is avoided. We illustrate the efficacy of AdaRL through a series of experiments that vary factors in the observation, transition and reward functions for Cartpole and Atari games.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|adarl_what_where_and_how_to_adapt_in_transfer_reinforcement_learning", "pdf": "/pdf/d3061c36db2595696e3c5444edf46fe3a2f665e9.pdf", "one-sentence_summary": "Efficient policy adaptation across domains by learning a parsimonious graphical representation that encodes changes in a compact way.", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.02729/code)", "_bibtex": "@inproceedings{\nhuang2022adarl,\ntitle={Ada{RL}: What, Where, and How to Adapt in Transfer Reinforcement Learning},\nauthor={Biwei Huang and Fan Feng and Chaochao Lu and Sara Magliacane and Kun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8H5bpVwvt5}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "GQjaI9mLet", "original": "9E6MeHAEHFi", "number": 3282, "cdate": 1632875677205, "mdate": null, "ddate": null, "tcdate": 1632875677205, "tmdate": 1676330513388, "tddate": null, "forum": "GQjaI9mLet", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking", "authorids": ["~Octavian-Eugen_Ganea1", "~Xinyuan_Huang1", "~Charlotte_Bunne1", "~Yatao_Bian1", "~Regina_Barzilay1", "~Tommi_S._Jaakkola1", "~Andreas_Krause1"], "authors": ["Octavian-Eugen Ganea", "Xinyuan Huang", "Charlotte Bunne", "Yatao Bian", "Regina Barzilay", "Tommi S. Jaakkola", "Andreas Krause"], "keywords": ["protein complexes", "protein structure", "rigid body docking", "SE(3) equivariance", "graph neural networks"], "abstract": "Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EquiDock, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing  docking software despite not relying on heavy candidate sampling, structure refinement, or templates.", "one-sentence_summary": "We perform rigid protein docking using a novel independent SE(3)-equivariant message passing mechanism that guarantees the same resulting protein complex independent of the initial placement of the two 3D structures.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ganea|independent_se3equivariant_models_for_endtoend_rigid_protein_docking", "pdf": "/pdf/4b1b65283103b98ee8f765eb00dca5e4786fcf19.pdf", "_bibtex": "@inproceedings{\nganea2022independent,\ntitle={Independent {SE}(3)-Equivariant Models for End-to-End Rigid Protein Docking},\nauthor={Octavian-Eugen Ganea and Xinyuan Huang and Charlotte Bunne and Yatao Bian and Regina Barzilay and Tommi S. Jaakkola and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=GQjaI9mLet}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "0RDcd5Axok", "original": "zNGPsnw9yae9", "number": 3260, "cdate": 1632875675853, "mdate": null, "ddate": null, "tcdate": 1632875675853, "tmdate": 1697934631444, "tddate": null, "forum": "0RDcd5Axok", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards a Unified View of Parameter-Efficient Transfer Learning", "authorids": ["~Junxian_He1", "~Chunting_Zhou1", "~Xuezhe_Ma1", "~Taylor_Berg-Kirkpatrick1", "~Graham_Neubig1"], "authors": ["Junxian He", "Chunting Zhou", "Xuezhe Ma", "Taylor Berg-Kirkpatrick", "Graham Neubig"], "keywords": ["parameter-efficient transfer learning", "unified view", "natural language processing"], "abstract": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "he|towards_a_unified_view_of_parameterefficient_transfer_learning", "pdf": "/pdf/859577d9cab3bf7833fe6fd6ea3a66c3b424c6bb.pdf", "one-sentence_summary": "We propose a unified framework for several state-of-the-art parameter-efficient tuning methods, ", "supplementary_material": "/attachment/3c1a22f013c8c48fb14354550635473a2a78399f.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.04366/code)", "_bibtex": "@inproceedings{\nhe2022towards,\ntitle={Towards a Unified View of Parameter-Efficient Transfer Learning},\nauthor={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0RDcd5Axok}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "BS49l-B5Bql", "original": "JwVY0tO8wOG", "number": 3173, "cdate": 1632875670239, "mdate": null, "ddate": null, "tcdate": 1632875670239, "tmdate": 1697934639774, "tddate": null, "forum": "BS49l-B5Bql", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "GNN-LM: Language Modeling based on Global Contexts via GNN", "authorids": ["~Yuxian_Meng1", "~Shi_Zong1", "~Xiaoya_Li2", "~Xiaofei_Sun1", "~Tianwei_Zhang1", "~Fei_Wu1", "~Jiwei_Li1"], "authors": ["Yuxian Meng", "Shi Zong", "Xiaoya Li", "Xiaofei Sun", "Tianwei Zhang", "Fei Wu", "Jiwei Li"], "keywords": [], "abstract": "Inspired by the notion that \"it to copy is easier than to memorize\", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla  LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "meng|gnnlm_language_modeling_based_on_global_contexts_via_gnn", "pdf": "/pdf/5a848353a24b880cebcd40d2e65796e794c0ff1d.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.08743/code)", "_bibtex": "@inproceedings{\nmeng2022gnnlm,\ntitle={{GNN}-{LM}: Language Modeling based on Global Contexts via {GNN}},\nauthor={Yuxian Meng and Shi Zong and Xiaoya Li and Xiaofei Sun and Tianwei Zhang and Fei Wu and Jiwei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BS49l-B5Bql}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "metRpM4Zrcb", "original": "sYu12GfQJ4b", "number": 3077, "cdate": 1632875664154, "mdate": null, "ddate": null, "tcdate": 1632875664154, "tmdate": 1676330523767, "tddate": null, "forum": "metRpM4Zrcb", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Continual Learning with Filter Atom Swapping", "authorids": ["~Zichen_Miao1", "~Ze_Wang3", "~Wei_Chen26", "~Qiang_Qiu1"], "authors": ["Zichen Miao", "Ze Wang", "Wei Chen", "Qiang Qiu"], "keywords": ["continual learning"], "abstract": "Continual learning has been widely studied in recent years to resolve the catastrophic forgetting of deep neural networks. In this paper, we first enforce a low-rank filter subspace by decomposing convolutional filters within each network layer over a small set of filter atoms. Then, we perform continual learning with filter atom swapping. In other words, we learn for each task a new filter subspace for each convolutional layer, i.e., hundreds of parameters as filter atoms, but keep subspace coefficients shared across tasks. By maintaining a small footprint memory of filter atoms, we can easily archive models for past tasks to avoid forgetting. The effectiveness of this simple scheme for continual learning is illustrated both empirically and theoretically. The proposed atom swapping framework further enables flexible and efficient model ensemble with members selected within a task or across tasks to improve the performance in different continual learning settings. Being validated on multiple benchmark datasets with different convolutional network structures, the proposed method outperforms the state-of-the-art methods in both accuracy and scalability.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "miao|continual_learning_with_filter_atom_swapping", "pdf": "/pdf/f453901d83c3373b05abe089d047948fa6e10b9b.pdf", "data": "", "_bibtex": "@inproceedings{\nmiao2022continual,\ntitle={Continual Learning with Filter Atom Swapping},\nauthor={Zichen Miao and Ze Wang and Wei Chen and Qiang Qiu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=metRpM4Zrcb}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "7YDLgf9_zgm", "original": "WzSV_QMGEGe", "number": 3027, "cdate": 1632875660952, "mdate": null, "ddate": null, "tcdate": 1632875660952, "tmdate": 1676330526698, "tddate": null, "forum": "7YDLgf9_zgm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Continual Learning with Recursive Gradient Optimization", "authorids": ["~Hao_Liu18", "~Huaping_Liu1"], "authors": ["Hao Liu", "Huaping Liu"], "keywords": ["continual learning", "lifelong learning"], "abstract": "Learning multiple tasks sequentially without forgetting previous knowledge, called Continual Learning(CL), remains a long-standing challenge for neural networks. Most existing methods rely on additional network capacity or data replay. In contrast, we introduce a novel approach which we refer to as Recursive Gradient Optimization(RGO). RGO is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer(FEL) that represents different long-term structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines and achieves new state-of-the-art performance on 20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher average accuracy than Single-Task Learning(STL), this method is flexible and reliable to provide continual learning capabilities for learning models that rely on gradient descent.", "pdf": "/pdf/19ab5aa0cb46a13fe9b97dab46913da24e89d2fb.pdf", "one-sentence_summary": "This paper proposes a novel method for continual learning in a fixed capacity network in the non-replay regime, which minimizes the loss on the current task while also minimizing an upper bound of loss increment on previous tasks.", "supplementary_material": "/attachment/904ae61e1113f88a24f7ca54704a06406dfe99de.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|continual_learning_with_recursive_gradient_optimization", "_bibtex": "@inproceedings{\nliu2022continual,\ntitle={Continual Learning with Recursive Gradient Optimization},\nauthor={Hao Liu and Huaping Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7YDLgf9_zgm}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "g8NJR6fCCl8", "original": "dq0U729SNqB", "number": 3020, "cdate": 1632875660429, "mdate": null, "ddate": null, "tcdate": 1632875660429, "tmdate": 1676330527403, "tddate": null, "forum": "g8NJR6fCCl8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning", "authorids": ["~Chun-Hao_Chang1", "~Rich_Caruana1", "~Anna_Goldenberg1"], "authors": ["Chun-Hao Chang", "Rich Caruana", "Anna Goldenberg"], "keywords": ["Generalized Additive Model", "Deep Learning Architecture", "Interpretability"], "abstract": "Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability. Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable compared to other ensemble and deep learning models. We demonstrate that our models find interesting patterns in the data. Lastly, we show that we are able to improve model accuracy via self-supervised pre-training, an improvement that is not possible for non-differentiable GAMs.", "one-sentence_summary": "We develop a deep-learning version of Generalized Additive Model (GAM) and GA2M that is accurate, scalable and interpretable.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chang|nodegam_neural_generalized_additive_model_for_interpretable_deep_learning", "pdf": "/pdf/dca44cbebb8f51bf19f423539a640118c674db6f.pdf", "supplementary_material": "/attachment/7cbc9f15fabf2a7e90216f2567043979fc8e3994.zip", "code": "", "_bibtex": "@inproceedings{\nchang2022nodegam,\ntitle={{NODE}-{GAM}: Neural Generalized Additive Model for Interpretable Deep Learning},\nauthor={Chun-Hao Chang and Rich Caruana and Anna Goldenberg},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=g8NJR6fCCl8}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "dgxFTxuJ50e", "original": "p6gF89D2cW", "number": 2968, "cdate": 1632875656958, "mdate": null, "ddate": null, "tcdate": 1632875656958, "tmdate": 1676330530305, "tddate": null, "forum": "dgxFTxuJ50e", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness", "authorids": ["lebesgue0118@gmail.com", "~Taiji_Suzuki1"], "authors": ["Sho Okumoto", "Taiji Suzuki"], "keywords": [], "abstract": "Among a wide range of success of deep learning, convolutional neural networks have been extensively utilized in several tasks such as speech recognition, image processing, and natural language processing, which require inputs with large dimensions.\nSeveral studies have investigated function estimation capability of deep learning, but most of them have assumed that the dimensionality of the input is much smaller than the sample size. \nHowever, for typical data in applications such as those handled by the convolutional neural networks described above, \nthe dimensionality of inputs is relatively high or even infinite. \nIn this paper, we investigate the approximation and estimation errors of the (dilated) convolutional neural networks when the input is infinite dimensional. \nAlthough the approximation and estimation errors of neural networks are affected by the curse of dimensionality in the existing analyses for typical function spaces such as the \\Holder and Besov spaces, we show that, by considering anisotropic smoothness, they can alleviate exponential dependency on the dimensionality but they only depend on the smoothness of the target functions. \nOur theoretical analysis supports the great practical success of convolutional networks.  \nFurthermore, we show that the dilated convolution is advantageous when the smoothness of the target function has a sparse structure.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "okumoto|learnability_of_convolutional_neural_networks_for_infinite_dimensional_input_via_mixed_and_anisotropic_smoothness", "pdf": "/pdf/634d72e29913c5641e4d1a3370a43676421f5b75.pdf", "_bibtex": "@inproceedings{\nokumoto2022learnability,\ntitle={Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness},\nauthor={Sho Okumoto and Taiji Suzuki},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dgxFTxuJ50e}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "tD7eCtaSkR", "original": "zB5v4BVQPFE", "number": 2925, "cdate": 1632875654098, "mdate": null, "ddate": null, "tcdate": 1632875654098, "tmdate": 1676330532685, "tddate": null, "forum": "tD7eCtaSkR", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100", "authorids": ["~Sahil_Singla1", "~Surbhi_Singla1", "~Soheil_Feizi2"], "authors": ["Sahil Singla", "Surbhi Singla", "Soheil Feizi"], "keywords": ["provable robustness", "adversarial examples"], "abstract": "Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the $l_{2}$ norm is useful for provable adversarial robustness, interpretable gradients and stable training. While $1$-Lipschitz CNNs can be designed by enforcing a $1$-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to certify the robustness of $1$-Lipschitz CNNs by relaxing the orthogonalization of the last linear layer of the network that significantly advances the state of the art for both standard and provable robust accuracies on CIFAR-100 (gains of $4.80\\%$ and $4.71\\%$, respectively). We further boost their robustness by introducing (i) a novel Gradient Norm preserving activation function called the Householder activation function (that includes every $\\mathrm{GroupSort}$ activation) and (ii) a certificate regularization. On CIFAR-10, we achieve significant improvements over prior works in provable robust accuracy ($5.81\\%$) with only a minor drop in standard accuracy ($-0.29\\%$). Code for reproducing all experiments in the paper is available at \\url{https://github.com/singlasahil14/SOC}. ", "one-sentence_summary": "Improving provable robustness of 1 Lipschitz CNNs by relaxing orthogonalization of last layer, certificate regularization and a novel activation function.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "singla|improved_deterministic_l2_robustness_on_cifar10_and_cifar100", "pdf": "/pdf/666d6306f372e8bad4486c511aded6081ad8c921.pdf", "supplementary_material": "/attachment/efab4bcac7d8bba1bcf55e5b8e68313452e72a77.zip", "_bibtex": "@inproceedings{\nsingla2022improved,\ntitle={Improved deterministic l2 robustness on {CIFAR}-10 and {CIFAR}-100},\nauthor={Sahil Singla and Surbhi Singla and Soheil Feizi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tD7eCtaSkR}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "CyKHoKyvgnp", "original": "4GQJE0yctkd", "number": 2918, "cdate": 1632875653632, "mdate": null, "ddate": null, "tcdate": 1632875653632, "tmdate": 1676330532959, "tddate": null, "forum": "CyKHoKyvgnp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models", "authorids": ["~Chaoyue_Liu2", "~Libin_Zhu1", "~Misha_Belkin1"], "authors": ["Chaoyue Liu", "Libin Zhu", "Misha Belkin"], "keywords": ["Assembling", "linearity", "Transition to linearity", "wide neural networks"], "abstract": "Wide neural networks with linear output layer have been shown to be near-linear, and to have near-constant neural tangent kernel (NTK), in a region containing the optimization path of gradient descent. These findings seem counter-intuitive since in general neural networks are highly complex models. Why does a linear structure emerge when the neural networks become wide? \nIn this work, we provide a new perspective on this \"transition to linearity\" by considering a neural network as an assembly model recursively built from a set of sub-models corresponding to individual neurons. In this view, we show that the linearity of wide neural networks is, in fact, an emerging property of assembling a large number of diverse ``weak'' sub-models, none of which dominate the assembly. ", "one-sentence_summary": "Transition to linearity of wide neural networks is an emerging property of assembling weak models corresponding to individual neurons", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|transition_to_linearity_of_wide_neural_networks_is_an_emerging_property_of_assembling_weak_models", "pdf": "/pdf/d4e2a924c1b35ff107a22198b053db66c9f1ae3d.pdf", "_bibtex": "@inproceedings{\nliu2022transition,\ntitle={Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models},\nauthor={Chaoyue Liu and Libin Zhu and Misha Belkin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CyKHoKyvgnp}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "RxplU3vmBx", "original": "tTYv4aiewr", "number": 2875, "cdate": 1632875650626, "mdate": null, "ddate": null, "tcdate": 1632875650626, "tmdate": 1676330535088, "tddate": null, "forum": "RxplU3vmBx", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Looking Back on Learned Experiences  For Class/task Incremental Learning", "authorids": ["m.pourkeshavarz@gmail.com", "~Guoying_Zhao3", "~Mohammad_Sabokrou1"], "authors": ["Mozhgan PourKeshavarzi", "Guoying Zhao", "Mohammad Sabokrou"], "keywords": ["Deepl Learning", "Class Incremental learning", "Continual learning", "Experiences"], "abstract": "Classical deep neural networks are limited in their ability to learn from emerging streams of training data. When trained sequentially on new or evolving tasks, their performance degrades sharply, making them inappropriate in real-world use cases. Existing methods tackle it by either storing old data samples or only updating a parameter set of deep neural networks, which, however, demands a large memory budget or spoils the flexibility of models to learn the incremented task distribution. In this paper, we shed light on an on-call transfer set to provide past experiences whenever a new task arises in the data stream. In particular, we propose a Cost-Free Incremental Learning (CF-IL) not only to replay past experiences the model has learned but also to perform this in a cost free manner. Towards this end, we introduced a memory recovery paradigm in which we query the network to synthesize past exemplars whenever a new task emerges. Thus, our method needs no extra memory for data buffering or network growing, besides calls the proposed memory recovery paradigm to provide past exemplars, named a transfer set in order to mitigate catastrophically forgetting the former tasks in the Incremental Learning (IL) setup. Moreover, in contrast with recently proposed methods, the suggested paradigm does not desire a parallel architecture since it only relies on the learner network. Compared to the state-of-the-art data techniques without buffering past data samples, CF-IL demonstrates significantly better performance on the well-known datasets whether a task oracle is available in test time (Task-IL) or not (Class-IL).", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pourkeshavarzi|looking_back_on_learned_experiences_for_classtask_incremental_learning", "pdf": "/pdf/6a7c60c7a81fef15a85782af93ea2e117db6ac6b.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\npourkeshavarzi2022looking,\ntitle={Looking Back on Learned Experiences  For Class/task Incremental Learning},\nauthor={Mozhgan PourKeshavarzi and Guoying Zhao and Mohammad Sabokrou},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RxplU3vmBx}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "US2rTP5nm_", "original": "tAHAT4oxFj", "number": 2853, "cdate": 1632875649104, "mdate": null, "ddate": null, "tcdate": 1632875649104, "tmdate": 1697934674912, "tddate": null, "forum": "US2rTP5nm_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "EntQA: Entity Linking as Question Answering", "authorids": ["wenzheng.zhang@rutgers.edu", "wenyue.hua@rutgers.edu", "~Karl_Stratos2"], "authors": ["Wenzheng Zhang", "Wenyue Hua", "Karl Stratos"], "keywords": ["Entity linking", "open-domain question answering", "dense retrieval", "reading comprehension", "information extraction", "natural language processing"], "abstract": "A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called $\\textbf{EntQA}$, which stands for $\\mbox{\\textbf{Ent}ity}$ linking as $\\mbox{\\textbf{Q}uestion}$ $\\mbox{\\textbf{A}nswering}$. EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module. Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on pretrained models for dense entity retrieval and reading comprehension. Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision. EntQA achieves strong results on the GERBIL benchmarking platform.\n", "one-sentence_summary": "We frame entity linking as inverse open-domain question answering and solve the dilemma of having to predict mentions before entities.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|entqa_entity_linking_as_question_answering", "pdf": "/pdf/644a6b82d054eb9827a94841c9a51dc4ba75e7b4.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02369/code)", "_bibtex": "@inproceedings{\nzhang2022entqa,\ntitle={Ent{QA}: Entity Linking as Question Answering},\nauthor={Wenzheng Zhang and Wenyue Hua and Karl Stratos},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=US2rTP5nm_}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "2_vhkAMARk", "original": "aL3im2piKtN", "number": 2776, "cdate": 1632875644188, "mdate": null, "ddate": null, "tcdate": 1632875644188, "tmdate": 1697934682960, "tddate": null, "forum": "2_vhkAMARk", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems", "authorids": ["~Thomas_Pethick1", "~Puya_Latafat1", "panos.patrinos@kuleuven.be", "~Olivier_Fercoq1", "~Volkan_Cevher1"], "authors": ["Thomas Pethick", "Puya Latafat", "Panos Patrinos", "Olivier Fercoq", "Volkan Cevher"], "keywords": ["Minimax", "Nonconvex-Nonconcave", "Variational inequilities", "Saddle point problem", "First-order methods", "Limit cycles"], "abstract": "This paper introduces a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. It is well-known that finding a local solution for general minimax problems is computationally intractable. This observation has recently motivated the study of structures sufficient for convergence of first order methods in the more general setting of variational inequalities when the so-called weak Minty variational inequality (MVI) holds. This problem class captures non-trivial structures as we demonstrate with examples, for which a large family of existing algorithms provably converge to limit cycles. Our results require a less restrictive parameter range in the weak MVI compared to what is previously known, thus extending the applicability of our scheme. The proposed algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. Our scheme also converges globally even in settings where the underlying operator exhibits limit cycles.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pethick|escaping_limit_cycles_global_convergence_for_constrained_nonconvexnonconcave_minimax_problems", "pdf": "/pdf/2b1d67d765bf24cb72760db6f0bcaff4cefc8032.pdf", "one-sentence_summary": "Under weak MVI we introduce a new extragradient-type algorithm that avoids limit cycles", "supplementary_material": "/attachment/9565873500530ddbed5595f34938b390ceec5c9d.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2302.09831/code)", "_bibtex": "@inproceedings{\npethick2022escaping,\ntitle={Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems},\nauthor={Thomas Pethick and Puya Latafat and Panos Patrinos and Olivier Fercoq and Volkan Cevher},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=2_vhkAMARk}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "IwJPj2MBcIa", "original": "B9DIph4sRG", "number": 2702, "cdate": 1632875639327, "mdate": null, "ddate": null, "tcdate": 1632875639327, "tmdate": 1697934690656, "tddate": null, "forum": "IwJPj2MBcIa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Compositional Attention: Disentangling Search and Retrieval", "authorids": ["~Sarthak_Mittal1", "~Sharath_Chandra_Raparthy1", "~Irina_Rish1", "~Yoshua_Bengio1", "~Guillaume_Lajoie1"], "authors": ["Sarthak Mittal", "Sharath Chandra Raparthy", "Irina Rish", "Yoshua Bengio", "Guillaume Lajoie"], "keywords": ["compositional attention", "flexible search and retrieval", "better generalization"], "abstract": "Multi-head, key-value attention is the backbone of transformer-like model architectures which have proven to be widely successful in recent years. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interaction, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism,  called Compositional Attention, that replaces the standard head structure. The proposed mechanism  disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval and is easy to implement in a variety of established network architectures.", "one-sentence_summary": "Recombining search and retrieval mechanisms of multi-head attention in a disentangled and flexible manner for better representational capacity and generalization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mittal|compositional_attention_disentangling_search_and_retrieval", "pdf": "/pdf/0ac34f592d10eb8cc8a3486322f340c5e0a456ba.pdf", "supplementary_material": "/attachment/75e1b9d639848540262157374fb6695aaecc0d57.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/arxiv:2110.09419/code)", "_bibtex": "@inproceedings{\nmittal2022compositional,\ntitle={Compositional Attention: Disentangling Search and Retrieval},\nauthor={Sarthak Mittal and Sharath Chandra Raparthy and Irina Rish and Yoshua Bengio and Guillaume Lajoie},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=IwJPj2MBcIa}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "XWODe7ZLn8f", "original": "84ko6vOEUZU", "number": 2694, "cdate": 1632875638764, "mdate": null, "ddate": null, "tcdate": 1632875638764, "tmdate": 1676330544881, "tddate": null, "forum": "XWODe7ZLn8f", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Contrastive Fine-grained Class Clustering via Generative Adversarial Networks", "authorids": ["~Yunji_Kim1", "~Jung-Woo_Ha1"], "authors": ["Yunji Kim", "Jung-Woo Ha"], "keywords": ["Unsupervised Fine-grained Class Clustering", "Disentangled Representation Learning", "Generative Adversarial Networks"], "abstract": "Unsupervised fine-grained class clustering is a practical yet challenging task due to the difficulty of feature representations learning of subtle object details. We introduce C3-GAN, a method that leverages the categorical inference power of InfoGAN with contrastive learning. We aim to learn feature representations that encourage a dataset to form distinct cluster boundaries in the embedding space, while also maximizing the mutual information between the latent code and its image observation. Our approach is to train a discriminator, which is also used for inferring clusters, to optimize the contrastive loss, where image-latent pairs that maximize the mutual information are considered as positive pairs and the rest as negative pairs. Specifically, we map the input of a generator, which was sampled from the categorical distribution, to the embedding space of the discriminator and let them act as a cluster centroid. In this way, C3-GAN succeeded in learning a clustering-friendly embedding space where each cluster is distinctively separable. Experimental results show that C3-GAN achieved the state-of-the-art clustering performance on four fine-grained image datasets, while also alleviating the mode collapse phenomenon. Code is available at https://github.com/naver-ai/c3-gan.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kim|contrastive_finegrained_class_clustering_via_generative_adversarial_networks", "pdf": "/pdf/e0f34f45a561d5470192a2a430af4c5ad2ade5d9.pdf", "one-sentence_summary": "We proposed a method for unsupervised fine-grained class clustering that leverages the information-theoretic regularization term based on contrastive loss. ", "_bibtex": "@inproceedings{\nkim2022contrastive,\ntitle={Contrastive Fine-grained Class Clustering via Generative Adversarial Networks},\nauthor={Yunji Kim and Jung-Woo Ha},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XWODe7ZLn8f}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "1xXvPrAshao", "original": "IKOKolC_TVH", "number": 2584, "cdate": 1632875631362, "mdate": null, "ddate": null, "tcdate": 1632875631362, "tmdate": 1697934701578, "tddate": null, "forum": "1xXvPrAshao", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Multimodal VAEs through Mutual Supervision", "authorids": ["~Tom_Joy1", "~Yuge_Shi1", "~Philip_Torr1", "~Tom_Rainforth1", "~Sebastian_M_Schmon1", "~Siddharth_N1"], "authors": ["Tom Joy", "Yuge Shi", "Philip Torr", "Tom Rainforth", "Sebastian M Schmon", "Siddharth N"], "keywords": ["Multimodal Variational Autoencoder", "Variational Autoencoder"], "abstract": "Multimodal VAEs seek to model the joint distribution over heterogeneous data (e.g.\\ vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the MEME, that avoids such explicit combinations by repurposing semi-supervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing---something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image--image) and CUB (image--text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data.", "one-sentence_summary": "Here we re-purpose semi-supervised VAEs to leverage mutual supervision between encoding distributions, allowing us to learn multi-modal VAEs with partially obsereved data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "joy|learning_multimodal_vaes_through_mutual_supervision", "pdf": "/pdf/6e3f8005627ed71e89af82b1e6d063771b707c3e.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.12570/code)", "_bibtex": "@inproceedings{\njoy2022learning,\ntitle={Learning Multimodal {VAE}s through Mutual Supervision},\nauthor={Tom Joy and Yuge Shi and Philip Torr and Tom Rainforth and Sebastian M Schmon and Siddharth N},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1xXvPrAshao}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "dEwfxt14bca", "original": "94dFWynRwFl", "number": 2580, "cdate": 1632875631084, "mdate": null, "ddate": null, "tcdate": 1632875631084, "tmdate": 1697934701870, "tddate": null, "forum": "dEwfxt14bca", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "When should agents explore?", "authorids": ["~Miruna_Pislar1", "~David_Szepesvari1", "~Georg_Ostrovski1", "~Diana_L_Borsa1", "~Tom_Schaul2"], "authors": ["Miruna Pislar", "David Szepesvari", "Georg Ostrovski", "Diana L Borsa", "Tom Schaul"], "keywords": ["exploration", "mode-switching", "reinforcement learning", "Atari"], "abstract": "Exploration remains a central challenge for reinforcement learning (RL). Virtually all existing methods share the feature of a *monolithic* behaviour policy that changes only gradually (at best). In contrast, the exploratory behaviours of animals and humans exhibit a rich diversity, namely including forms of *switching* between modes. This paper presents an initial study of mode-switching, non-monolithic exploration for RL. We investigate different modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. We also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, we report a promising initial study on Atari, using two-mode exploration and switching at sub-episodic time-scales.", "pdf": "/pdf/5628a68890630dcf1b41b8c21140287bb6d2e9eb.pdf", "one-sentence_summary": "A fresh look at the question of *when* to switch into exploration mode, and for how long.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pislar|when_should_agents_explore", "supplementary_material": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2108.11811/code)", "_bibtex": "@inproceedings{\npislar2022when,\ntitle={When should agents explore?},\nauthor={Miruna Pislar and David Szepesvari and Georg Ostrovski and Diana L Borsa and Tom Schaul},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dEwfxt14bca}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "zz9hXVhf40", "original": "3ZRa3rMYNpD", "number": 2558, "cdate": 1632875629651, "mdate": null, "ddate": null, "tcdate": 1632875629651, "tmdate": 1676330551666, "tddate": null, "forum": "zz9hXVhf40", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Revisiting Design Choices in Offline Model Based Reinforcement Learning", "authorids": ["~Cong_Lu1", "~Philip_Ball1", "~Jack_Parker-Holder1", "~Michael_Osborne1", "~Stephen_J._Roberts1"], "authors": ["Cong Lu", "Philip Ball", "Jack Parker-Holder", "Michael Osborne", "Stephen J. Roberts"], "keywords": ["Model-Based Reinforcement Learning", "Offline Reinforcement Learning", "Uncertainty Quantification"], "abstract": "Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. This has spawned a variety of uncertainty heuristics, with little to no comparison between differing approaches. In this paper, we compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. Using these insights, we show that selecting these key hyperparameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lu|revisiting_design_choices_in_offline_model_based_reinforcement_learning", "pdf": "/pdf/1fd710c82e5202735a840dcabdf897afa2030b34.pdf", "supplementary_material": "/attachment/22c20ba5d91e95042595781655a6800dd25b1632.zip", "_bibtex": "@inproceedings{\nlu2022revisiting,\ntitle={Revisiting Design Choices in Offline Model Based Reinforcement Learning},\nauthor={Cong Lu and Philip Ball and Jack Parker-Holder and Michael Osborne and Stephen J. Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zz9hXVhf40}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 28}}, {"id": "KhLK0sHMgXK", "original": "tvj5MR-04pT", "number": 2555, "cdate": 1632875629450, "mdate": null, "ddate": null, "tcdate": 1632875629450, "tmdate": 1676330551762, "tddate": null, "forum": "KhLK0sHMgXK", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "NASPY: Automated Extraction of Automated Machine Learning Models", "authorids": ["~Xiaoxuan_Lou1", "~Shangwei_Guo1", "~Jiwei_Li1", "~Yaoxin_Wu2", "~Tianwei_Zhang1"], "authors": ["Xiaoxuan Lou", "Shangwei Guo", "Jiwei Li", "Yaoxin Wu", "Tianwei Zhang"], "keywords": [], "abstract": "We present NASPY, an end-to-end adversarial framework to extract the networkarchitecture of deep learning models from Neural Architecture Search (NAS). Existing works about model extraction attacks mainly focus on conventional DNN models with very simple operations, or require heavy manual analysis with lots of domain knowledge.  In contrast, NASPY introduces seq2seq models to automatically identify novel and complicated operations (e.g., separable convolution,dilated convolution) from hardware side-channel sequences. We design two models (RNN-CTC and transformer), which can achieve only 3.2% and 11.3% error rates for operation prediction.  We further present methods to recover the model hyper-parameters and topology from the operation sequence .  With these techniques, NASPY is able to extract the complete NAS model architecture with high fidelity and automation, which are rarely analyzed before.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lou|naspy_automated_extraction_of_automated_machine_learning_models", "pdf": "/pdf/86d83b714af5dd7905a8be1ae5d3722feeb303ab.pdf", "one-sentence_summary": "We present NASPY, an end-to-end adversarial framework to extract the networkarchitecture of deep learning models from Neural Architecture Search (NAS).", "_bibtex": "@inproceedings{\nlou2022naspy,\ntitle={{NASPY}: Automated Extraction of Automated Machine Learning Models},\nauthor={Xiaoxuan Lou and Shangwei Guo and Jiwei Li and Yaoxin Wu and Tianwei Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KhLK0sHMgXK}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "FLA55mBee6Q", "original": "S3rZridG-8E", "number": 2550, "cdate": 1632875629165, "mdate": null, "ddate": null, "tcdate": 1632875629165, "tmdate": 1676330552255, "tddate": null, "forum": "FLA55mBee6Q", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation", "authorids": ["~Jongmin_Lee1", "~Cosmin_Paduraru1", "~Daniel_J_Mankowitz2", "~Nicolas_Heess1", "~Doina_Precup1", "~Kee-Eung_Kim2", "~Arthur_Guez1"], "authors": ["Jongmin Lee", "Cosmin Paduraru", "Daniel J Mankowitz", "Nicolas Heess", "Doina Precup", "Kee-Eung Kim", "Arthur Guez"], "keywords": ["Offline Reinforcement Learning", "Offline Constrained Reinforcement Learning", "Stationary Distribution Correction Estimation"], "abstract": "We consider the offline constrained reinforcement learning (RL) problem, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute a policy that guarantees satisfying the cost constraints in the offline RL setting, since the off-policy evaluation inherently has an estimation error. In this paper, we present an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. Our algorithm, COptiDICE, directly estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experimental results show that COptiDICE attains better policies in terms of constraint satisfaction and return-maximization, outperforming baseline algorithms.", "one-sentence_summary": "We present an offline constrained RL algorithm, which estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|coptidice_offline_constrained_reinforcement_learning_via_stationary_distribution_correction_estimation", "pdf": "/pdf/072227698fafd08a9854a8816e3cc5d5a8eb5754.pdf", "_bibtex": "@inproceedings{\nlee2022coptidice,\ntitle={{CO}pti{DICE}: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation},\nauthor={Jongmin Lee and Cosmin Paduraru and Daniel J Mankowitz and Nicolas Heess and Doina Precup and Kee-Eung Kim and Arthur Guez},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FLA55mBee6Q}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "nhnJ3oo6AB", "original": "DMCNzblufpY", "number": 2485, "cdate": 1632875625056, "mdate": null, "ddate": null, "tcdate": 1632875625056, "tmdate": 1697934710808, "tddate": null, "forum": "nhnJ3oo6AB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers", "authorids": ["~Ruihan_Yang2", "~Minghao_Zhang1", "~Nicklas_Hansen1", "~Huazhe_Xu1", "~Xiaolong_Wang3"], "authors": ["Ruihan Yang", "Minghao Zhang", "Nicklas Hansen", "Huazhe Xu", "Xiaolong Wang"], "keywords": ["Reinforcement Learning", "Robotics", "Locomotion Control", "Multi-Modal Transformer"], "abstract": "We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoor and in-the-wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yang|learning_visionguided_quadrupedal_locomotion_endtoend_with_crossmodal_transformers", "pdf": "/pdf/44a74037c4089730c34de558a6d679925f866a56.pdf", "one-sentence_summary": "We introduce a novel end-to-end Reinforcement Learning approach called LocoTransformer, leveraging both visual inputs and proprioceptive states, for locomotion control in both simulation and with real robots.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2107.03996/code)", "_bibtex": "@inproceedings{\nyang2022learning,\ntitle={Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers},\nauthor={Ruihan Yang and Minghao Zhang and Nicklas Hansen and Huazhe Xu and Xiaolong Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=nhnJ3oo6AB}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "dwg5rXg1WS_", "original": "dxyGBFFezh5", "number": 2472, "cdate": 1632875624125, "mdate": null, "ddate": null, "tcdate": 1632875624125, "tmdate": 1676330555992, "tddate": null, "forum": "dwg5rXg1WS_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "ViTGAN: Training GANs with Vision Transformers", "authorids": ["~Kwonjoon_Lee1", "~Huiwen_Chang2", "~Lu_Jiang1", "~Han_Zhang1", "~Zhuowen_Tu1", "~Ce_Liu1"], "authors": ["Kwonjoon Lee", "Huiwen Chang", "Lu Jiang", "Han Zhang", "Zhuowen Tu", "Ce Liu"], "keywords": [], "abstract": "Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to faciliate convergence. Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNN- based GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|vitgan_training_gans_with_vision_transformers", "pdf": "/pdf/12b5f82c142cc00c08e950c5a49db1948f14aa54.pdf", "data": "", "_bibtex": "@inproceedings{\nlee2022vitgan,\ntitle={Vi{TGAN}: Training {GAN}s with Vision Transformers},\nauthor={Kwonjoon Lee and Huiwen Chang and Lu Jiang and Han Zhang and Zhuowen Tu and Ce Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dwg5rXg1WS_}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "AJsI-ymaKn_", "original": "ssaGijO_9gW", "number": 2413, "cdate": 1632875620145, "mdate": null, "ddate": null, "tcdate": 1632875620145, "tmdate": 1697934717562, "tddate": null, "forum": "AJsI-ymaKn_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "POETREE: Interpretable Policy Learning with Adaptive Decision Trees", "authorids": ["~Aliz\u00e9e_Pace1", "~Alex_Chan2", "~Mihaela_van_der_Schaar2"], "authors": ["Aliz\u00e9e Pace", "Alex Chan", "Mihaela van der Schaar"], "keywords": ["Imitation Learning", "Interpretable ML", "Clinical Decision Support", "Sequential Decision-Making"], "abstract": "Building models of human decision-making from observed behaviour is critical to better understand, diagnose and support real-world policies such as clinical care. As established policy learning approaches remain focused on imitation performance, they fall short of explaining the demonstrated decision-making process. Policy Extraction through decision Trees (POETREE) is a novel framework for interpretable policy learning, compatible with fully-offline and partially-observable clinical decision environments -- and builds probabilistic tree policies determining physician actions based on patients' observations and medical history. Fully-differentiable tree architectures are grown incrementally during optimization to adapt their complexity to the modelling task, and learn a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. This policy learning method outperforms the state-of-the-art on real and synthetic medical datasets, both in terms of understanding, quantifying and evaluating observed behaviour as well as in accurately replicating it -- with potential to improve future decision support systems.", "pdf": "/pdf/18b6779789f76ccc8b7ff5e0a77ec0fa2a5d4057.pdf", "one-sentence_summary": "Policy Extraction through decision Trees (POETREE) is a novel framework for interpretable policy learning, compatible with fully-offline and partially-observable clinical decision environments.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "pace|poetree_interpretable_policy_learning_with_adaptive_decision_trees", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2203.08057/code)", "_bibtex": "@inproceedings{\npace2022poetree,\ntitle={{POETREE}: Interpretable Policy Learning with Adaptive Decision Trees},\nauthor={Aliz{\\'e}e Pace and Alex Chan and Mihaela van der Schaar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AJsI-ymaKn_}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 29}}, {"id": "iEvAf8i6JjO", "original": "B7IzwxKtUJH", "number": 2365, "cdate": 1632875617126, "mdate": null, "ddate": null, "tcdate": 1632875617126, "tmdate": 1676330560957, "tddate": null, "forum": "iEvAf8i6JjO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "TRGP: Trust Region Gradient Projection for Continual Learning", "authorids": ["~Sen_Lin1", "~Li_Yang6", "~Deliang_Fan1", "~Junshan_Zhang1"], "authors": ["Sen Lin", "Li Yang", "Deliang Fan", "Junshan Zhang"], "keywords": ["trust region", "gradient projection", "scaled weight projection", "continual learning", "forward knowledge transfer", "task correlation"], "abstract": "Catastrophic forgetting is one of the major challenges in continual learning. To address this issue, some existing methods put restrictive constraints on the optimization space of the new task for minimizing the interference to old tasks. However, this may lead to unsatisfactory performance for the new task, especially when the new task is strongly correlated with old tasks. To tackle this challenge, we propose Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. Particularly, we introduce a notion of 'trust region' to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layer-wise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks,  TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that our approach achieves significant improvement over related state-of-the-art methods.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lin|trgp_trust_region_gradient_projection_for_continual_learning", "pdf": "/pdf/7291715ef56df6f6a92491a5977eb95a2db1ca86.pdf", "one-sentence_summary": "We propose a novel continual learning approach to facilitate the forward knowledge transfer, based on an efficient characterization of task correlation using a novel notion of 'trust region'.", "supplementary_material": "/attachment/5df19db8c9f9004aef610580629148c20f55fe0d.zip", "_bibtex": "@inproceedings{\nlin2022trgp,\ntitle={{TRGP}: Trust Region Gradient Projection for Continual Learning},\nauthor={Sen Lin and Li Yang and Deliang Fan and Junshan Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iEvAf8i6JjO}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "g5ynW-jMq4M", "original": "qe4AjISt8mw", "number": 2324, "cdate": 1632875614572, "mdate": null, "ddate": null, "tcdate": 1632875614572, "tmdate": 1676330563214, "tddate": null, "forum": "g5ynW-jMq4M", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Properties from mechanisms: an equivariance perspective on identifiable representation learning", "authorids": ["~Kartik_Ahuja1", "~Jason_Hartford1", "~Yoshua_Bengio1"], "authors": ["Kartik Ahuja", "Jason Hartford", "Yoshua Bengio"], "keywords": ["representation learning", "equivariance", "independent component analysis", "ICA", "autoencoders"], "abstract": "A key goal of unsupervised representation learning is ``inverting'' a data generating process to recover its latent properties.  Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the problem and ask,  ``Can we instead identify latent properties by leveraging knowledge of the mechanisms that govern their evolution?'' We provide a complete characterization of the sources of non-identifiability as we vary knowledge about a set of possible mechanisms. In particular, we prove that if we know the exact mechanisms under which the latent properties evolve, then identification can be achieved up to any equivariances that are shared by the underlying mechanisms. We generalize this characterization to settings where we only know some hypothesis class over possible mechanisms, as well as settings where the mechanisms are stochastic. We demonstrate the power of this mechanism-based perspective by showing that we can leverage our results to generalize existing identifiable representation learning results. These results suggest that by exploiting inductive biases on mechanisms, it is possible to design a range of new identifiable representation learning approaches.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ahuja|properties_from_mechanisms_an_equivariance_perspective_on_identifiable_representation_learning", "pdf": "/pdf/2b2b18f71be3fd973de88f72c37e764d69dde18a.pdf", "one-sentence_summary": "Representation learning is identifiable up to any equivariances of the (known) mechanisms that govern an environment's evolution.", "_bibtex": "@inproceedings{\nahuja2022properties,\ntitle={Properties from mechanisms: an equivariance perspective on identifiable representation learning},\nauthor={Kartik Ahuja and Jason Hartford and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=g5ynW-jMq4M}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 4}}, {"id": "dUV91uaXm3", "original": "GaXCyTHmIX-", "number": 2321, "cdate": 1632875614370, "mdate": null, "ddate": null, "tcdate": 1632875614370, "tmdate": 1676330563363, "tddate": null, "forum": "dUV91uaXm3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Revisiting Over-smoothing in BERT from the Perspective of Graph", "authorids": ["~Han_Shi1", "~JIAHUI_GAO1", "~Hang_Xu1", "~Xiaodan_Liang2", "~Zhenguo_Li1", "~Lingpeng_Kong1", "~Stephen_M._S._Lee1", "~James_Kwok1"], "authors": ["Han Shi", "JIAHUI GAO", "Hang Xu", "Xiaodan Liang", "Zhenguo Li", "Lingpeng Kong", "Stephen M. S. Lee", "James Kwok"], "keywords": ["BERT", "Over-smoothing", "Transformer"], "abstract": "Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shi|revisiting_oversmoothing_in_bert_from_the_perspective_of_graph", "pdf": "/pdf/c6d11173508af7016a361ef28688912c6b0a80a8.pdf", "one-sentence_summary": "We theoretically analyze the over-smoothing phenomenon of transformer-based models (e.g., BERT) and propose a novel hierarchical fusion strategy to alleviate it.", "data": "", "_bibtex": "@inproceedings{\nshi2022revisiting,\ntitle={Revisiting Over-smoothing in {BERT} from the Perspective of Graph},\nauthor={Han Shi and JIAHUI GAO and Hang Xu and Xiaodan Liang and Zhenguo Li and Lingpeng Kong and Stephen M. S. Lee and James Kwok},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dUV91uaXm3}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "XEW8CQgArno", "original": "M41c21j98CR", "number": 2264, "cdate": 1632875610573, "mdate": null, "ddate": null, "tcdate": 1632875610573, "tmdate": 1676330566218, "tddate": null, "forum": "XEW8CQgArno", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Training invariances and the low-rank phenomenon: beyond linear networks", "authorids": ["~Thien_Le1", "~Stefanie_Jegelka3"], "authors": ["Thien Le", "Stefanie Jegelka"], "keywords": ["deep learning", "nonsmooth analysis", "Clarke subdifferential", "implicit regularization", "low rank bias", "alignment", "training invariance"], "abstract": "The implicit bias induced by the training of neural networks has become a topic of rigorous study. In the limit of gradient flow and gradient descent with appropriate step size, it has been shown that when one trains a deep linear network with logistic or exponential loss on linearly separable data, the weights converge to rank-$1$ matrices. In this paper, we extend this theoretical result to the last few linear layers of the much wider class of nonlinear ReLU-activated feedforward networks containing fully-connected layers and skip connections.  Similar to the linear case, the proof relies on specific local training invariances, sometimes referred to as alignment, which we show to hold for submatrices where neurons are stably-activated in all training examples, and it reflects empirical results in the literature. We also show this is not true in general for the full matrix of ReLU fully-connected layers. Our proof relies on a specific decomposition of the network into a multilinear function and another ReLU network whose weights are constant under a certain parameter directional convergence.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "le|training_invariances_and_the_lowrank_phenomenon_beyond_linear_networks", "pdf": "/pdf/09cd317c13e66c6829a08a5b598324211771fd4b.pdf", "one-sentence_summary": "We extend theoretical results regarding the low-rank bias of deep linear neural networks trained with gradient-based algorithm to non-linear architectures, reflecting empirical results in the literature.", "_bibtex": "@inproceedings{\nle2022training,\ntitle={Training invariances and the low-rank phenomenon: beyond linear networks},\nauthor={Thien Le and Stefanie Jegelka},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XEW8CQgArno}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "lpkGn3k2YdD", "original": "-eLob5P14Es", "number": 2261, "cdate": 1632875610360, "mdate": null, "ddate": null, "tcdate": 1632875610360, "tmdate": 1676330566373, "tddate": null, "forum": "lpkGn3k2YdD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Long-Term Reward Redistribution via Randomized Return Decomposition", "authorids": ["~Zhizhou_Ren1", "~Ruihan_Guo1", "~Yuan_Zhou1", "~Jian_Peng1"], "authors": ["Zhizhou Ren", "Ruihan Guo", "Yuan Zhou", "Jian Peng"], "keywords": ["Reinforcement Learning", "Long-Term Credit Assignment", "Reward Redistribution", "Return Decomposition"], "abstract": "Many practical applications of reinforcement learning require agents to learn from sparse and delayed rewards. It challenges the ability of agents to attribute their actions to future outcomes. In this paper, we consider the problem formulation of episodic reinforcement learning with trajectory feedback. It refers to an extreme delay of reward signals, in which the agent can only obtain one reward signal at the end of each trajectory. A popular paradigm for this problem setting is learning with a designed auxiliary dense reward function, namely proxy reward, instead of sparse environmental signals. Based on this framework, this paper proposes a novel reward redistribution algorithm, randomized return decomposition (RRD), to learn a proxy reward function for episodic reinforcement learning. We establish a surrogate problem by Monte-Carlo sampling that scales up least-squares-based reward redistribution to long-horizon problems. We analyze our surrogate loss function by connection with existing methods in the literature, which illustrates the algorithmic properties of our approach. In experiments, we extensively evaluate our proposed method on a variety of benchmark tasks with episodic rewards and demonstrate substantial improvement over baseline algorithms.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ren|learning_longterm_reward_redistribution_via_randomized_return_decomposition", "pdf": "/pdf/9d37ef647d9bba0c4b6fe1976563d07da74d311e.pdf", "one-sentence_summary": "We propose randomized return decomposition, a novel reward redistribution algorithm, which establishes a surrogate optimization problem to scale up learning in long-horizon tasks.", "supplementary_material": "", "_bibtex": "@inproceedings{\nren2022learning,\ntitle={Learning Long-Term Reward Redistribution via Randomized Return Decomposition},\nauthor={Zhizhou Ren and Ruihan Guo and Yuan Zhou and Jian Peng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lpkGn3k2YdD}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "siCt4xZn5Ve", "original": "oECjPKkvFAL", "number": 2250, "cdate": 1632875609599, "mdate": null, "ddate": null, "tcdate": 1632875609599, "tmdate": 1676330567166, "tddate": null, "forum": "siCt4xZn5Ve", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "What Happens after SGD Reaches Zero Loss? --A Mathematical Framework", "authorids": ["~Zhiyuan_Li2", "~Tianhao_Wang1", "~Sanjeev_Arora1"], "authors": ["Zhiyuan Li", "Tianhao Wang", "Sanjeev Arora"], "keywords": ["SGD", "implicit bias", "generalization", "deep learning", "implicit regularization", "manifold"], "abstract": "Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function $L$ can form a manifold. Intuitively, with a sufficiently small learning rate $\\eta$, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, $\\text{tr}[\\nabla^2 L]$. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold---i.e., the \"implicit bias\"---using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a *global* analysis of the implicit bias valid for $\\eta^{-2}$ steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for $\\eta^{-1.6}$ steps and (2) allowing *arbitrary* noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires $O(\\kappa\\ln d)$ samples for learning an $\\kappa$-sparse overparametrized linear model in $\\mathbb{R}^d$ (Woodworth et al., 2020), while GD initialized in the kernel regime requires $\\Omega(d)$ samples. This upper bound is minimax optimal and improves the previous $\\widetilde{O}(\\kappa^2)$ upper bound (HaoChen et al., 2020).", "one-sentence_summary": "We propose a mathematical framework to study the implicit bias of SGD after reaching zero loss, based on which we prove label noise can help SGD escape the kernel regime and achieve optimal sample complexity for overparametrized linear model.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|what_happens_after_sgd_reaches_zero_loss_a_mathematical_framework", "pdf": "/pdf/053e833d0de895f0839c04aabd053b4d678ffb02.pdf", "supplementary_material": "/attachment/7bbee31b806c9688b15216dca1fa441cb9f56fcf.zip", "_bibtex": "@inproceedings{\nli2022what,\ntitle={What Happens after {SGD} Reaches Zero Loss? --A Mathematical Framework},\nauthor={Zhiyuan Li and Tianhao Wang and Sanjeev Arora},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=siCt4xZn5Ve}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "45L_dgP48Vd", "original": "eRRbto5n1ly", "number": 2219, "cdate": 1632875607498, "mdate": null, "ddate": null, "tcdate": 1632875607498, "tmdate": 1697934736813, "tddate": null, "forum": "45L_dgP48Vd", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series", "authorids": ["~Enyan_Dai1", "~Jie_Chen1"], "authors": ["Enyan Dai", "Jie Chen"], "keywords": ["Anomaly Detection", "Normalizing Flow", "DAG", "Multiple Time Series"], "abstract": "Anomaly detection is a widely studied task for a broad variety of data types; among them, multiple time series appear frequently in applications, including for example, power grids and traffic networks. Detecting anomalies for multiple time series, however, is a challenging subject, owing to the intricate interdependencies among the constituent series. We hypothesize that anomalies occur in low density regions of a distribution and explore the use of normalizing flows for unsupervised anomaly detection, because of their superior quality in density estimation. Moreover, we propose a novel flow model by imposing a Bayesian network among constituent series. A Bayesian network is a directed acyclic graph (DAG) that models causal relationships; it factorizes the joint probability of the series into the product of easy-to-evaluate conditional probabilities. We call such a graph-augmented normalizing flow approach GANF and propose joint estimation of the DAG with flow parameters. We conduct extensive experiments on real-world datasets and demonstrate the effectiveness of GANF for density estimation, anomaly detection, and identification of time series distribution drift.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dai|graphaugmented_normalizing_flows_for_anomaly_detection_of_multiple_time_series", "pdf": "/pdf/b66a4e577ef3e2caf4cb8c17ee02ca6b15eaca26.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.07857/code)", "_bibtex": "@inproceedings{\ndai2022graphaugmented,\ntitle={Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series},\nauthor={Enyan Dai and Jie Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=45L_dgP48Vd}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "z1-I6rOKv1S", "original": "3oNcOGXUc9z", "number": 2201, "cdate": 1632875606140, "mdate": null, "ddate": null, "tcdate": 1632875606140, "tmdate": 1676330569632, "tddate": null, "forum": "z1-I6rOKv1S", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Autoregressive Quantile Flows for Predictive Uncertainty Estimation", "authorids": ["~Phillip_Si1", "~Allan_Bishop1", "~Volodymyr_Kuleshov1"], "authors": ["Phillip Si", "Allan Bishop", "Volodymyr Kuleshov"], "keywords": [], "abstract": "Numerous applications of machine learning involve representing probability distributions over high-dimensional data. We propose autoregressive quantile flows, a flexible class of normalizing flow models trained using a novel objective based on proper scoring rules. Our objective does not require calculating computationally expensive determinants of Jacobians during training and supports new types of neural architectures, such as neural autoregressive flows from which it is easy to sample. \n    We leverage these models in quantile flow regression, an approach that parameterizes predictive conditional distributions with flows, resulting in improved probabilistic predictions on tasks such as time series forecasting and object detection.\n    Our novel objective functions and neural flow parameterizations also yield improvements on popular generation and density estimation tasks, and represent a step beyond maximum likelihood learning of flows.", "pdf": "/pdf/564438790b7385df4f30b72aafb410555a14f948.pdf", "one-sentence_summary": "Using Quantile Flows for Predictive and Generative Data Modeling and Generation", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "si|autoregressive_quantile_flows_for_predictive_uncertainty_estimation", "_bibtex": "@inproceedings{\nsi2022autoregressive,\ntitle={Autoregressive Quantile Flows for Predictive Uncertainty Estimation},\nauthor={Phillip Si and Volodymyr Kuleshov and Allan Bishop},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=z1-I6rOKv1S}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "DNRADop4ksB", "original": "J9f5GTv_hEV", "number": 2170, "cdate": 1632875603356, "mdate": null, "ddate": null, "tcdate": 1632875603356, "tmdate": 1697934743028, "tddate": null, "forum": "DNRADop4ksB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Importance of Firth Bias Reduction in Few-Shot Classification", "authorids": ["~Saba_Ghaffari1", "~Ehsan_Saleh1", "~David_Forsyth1", "~Yu-Xiong_Wang1"], "authors": ["Saba Ghaffari", "Ehsan Saleh", "David Forsyth", "Yu-Xiong Wang"], "keywords": ["Few-shot Classification", "Firth Regularization", "MLE Bias"], "abstract": "Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classification suffers from the bias in the estimation of classifier parameters; however, an effective underlying bias reduction technique that could alleviate this issue in training few-shot classifiers has been overlooked. In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classification. Theoretically, Firth bias reduction removes the $O(N^{-1})$ first order term from the small-sample bias of the Maximum Likelihood Estimator. Here we show that the general Firth bias reduction technique simplifies to encouraging uniform class assignment probabilities for multinomial logistic classification, and almost has the same effect in cosine classifiers. We derive an easy-to-implement optimization objective for Firth penalized multinomial logistic and cosine classifiers, which is equivalent to penalizing the cross-entropy loss with a KL-divergence between the predictions and the uniform label distribution. Then, we empirically evaluate that it is consistently effective across the board for few-shot image classification, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes. Furthermore, we demonstrate the effectiveness of Firth bias reduction on cross-domain and imbalanced data settings. Our implementation is available at https://github.com/ehsansaleh/firth_bias_reduction.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ghaffari|on_the_importance_of_firth_bias_reduction_in_fewshot_classification", "pdf": "/pdf/fafc00da00fec3e0c8db049a4e2e14a588fa4aa7.pdf", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.02529/code)", "_bibtex": "@inproceedings{\nghaffari2022on,\ntitle={On the Importance of Firth Bias Reduction in Few-Shot Classification},\nauthor={Saba Ghaffari and Ehsan Saleh and David Forsyth and Yu-Xiong Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DNRADop4ksB}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "04pGUg0-pdZ", "original": "u9KnXTVGLfR", "number": 2163, "cdate": 1632875602593, "mdate": null, "ddate": null, "tcdate": 1632875602593, "tmdate": 1676330571660, "tddate": null, "forum": "04pGUg0-pdZ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Finite-Time Convergence and Sample Complexity of Multi-Agent Actor-Critic Reinforcement Learning with Average Reward", "authorids": ["~FNU_Hairi1", "~Jia_Liu1", "~Songtao_Lu1"], "authors": ["FNU Hairi", "Jia Liu", "Songtao Lu"], "keywords": [], "abstract": "In this paper, we establish the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. \nIn this problem, a set of $N$ agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network.\nWe consider a practical MARL setting, where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. \nToward this end, we propose a mini-batch Markovian sampled fully decentralized actor-critic algorithm and analyze its finite-time convergence and sample complexity.\nWe show that the sample complexity of this algorithm is $\\mathcal{O}(N^{2}/\\epsilon^{2}\\log(N/\\epsilon))$.\nInterestingly, this sample complexity bound matches that of the state-of-the-art single-agent actor-critic algorithms for reinforcement learning. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hairi|finitetime_convergence_and_sample_complexity_of_multiagent_actorcritic_reinforcement_learning_with_average_reward", "pdf": "/pdf/88771b44b2b7e3d534226c24b2a2e7d9739fc960.pdf", "_bibtex": "@inproceedings{\nhairi2022finitetime,\ntitle={Finite-Time Convergence and Sample Complexity of Multi-Agent Actor-Critic Reinforcement Learning with Average Reward},\nauthor={FNU Hairi and Jia Liu and Songtao Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=04pGUg0-pdZ}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "ieNJYujcGDO", "original": "KkuaUjZWzec", "number": 2144, "cdate": 1632875600580, "mdate": null, "ddate": null, "tcdate": 1632875600580, "tmdate": 1697934744746, "tddate": null, "forum": "ieNJYujcGDO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Understanding the Data Dependency of Mixup-style Training", "authorids": ["~Muthu_Chidambaram1", "~Xiang_Wang1", "~Yuzheng_Hu1", "~Chenwei_Wu1", "~Rong_Ge1"], "authors": ["Muthu Chidambaram", "Xiang Wang", "Yuzheng Hu", "Chenwei Wu", "Rong Ge"], "keywords": ["mixup", "deep learning", "semi-supervised learning", "empirical risk minimization", "generalization", "margin", "counterexample"], "abstract": "In the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained using Mixup seem to still minimize the original empirical risk and exhibit better generalization and robustness on various tasks when compared to standard training. In this paper, we investigate how these benefits of Mixup training rely on properties of the data in the context of classification. For minimizing the original empirical risk, we compute a closed form for the Mixup-optimal classification, which allows us to construct a simple dataset on which minimizing the Mixup loss leads to learning a classifier that does not minimize the empirical loss on the data. On the other hand, we also give sufficient conditions for Mixup training to also minimize the original empirical risk. For generalization, we characterize the margin of a Mixup classifier, and use this to understand why the decision boundary of a Mixup classifier can adapt better to the full structure of the training data when compared to standard training. In contrast, we also show that, for a large class of linear models and linearly separable datasets, Mixup training leads to learning the same classifier as standard training.", "one-sentence_summary": "A theoretical analysis of data conditions under which mixup can perform worse, better, and identically when compared to empirical risk minimization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chidambaram|towards_understanding_the_data_dependency_of_mixupstyle_training", "pdf": "/pdf/da2d0598034ce31ec61b7322e921e2900518e9f3.pdf", "supplementary_material": "/attachment/0b39a7f8d327f5ede92ee48b1eb15e2616e48ce2.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.07647/code)", "_bibtex": "@inproceedings{\nchidambaram2022towards,\ntitle={Towards Understanding the Data Dependency of Mixup-style Training},\nauthor={Muthu Chidambaram and Xiang Wang and Yuzheng Hu and Chenwei Wu and Rong Ge},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ieNJYujcGDO}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "oDFvtxzPOx", "original": "pGHc6MhLD2o", "number": 2114, "cdate": 1632875596701, "mdate": null, "ddate": null, "tcdate": 1632875596701, "tmdate": 1676330574253, "tddate": null, "forum": "oDFvtxzPOx", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Self-Supervision Enhanced Feature Selection with Correlated Gates", "authorids": ["~Changhee_Lee1", "~Fergus_Imrie1", "~Mihaela_van_der_Schaar2"], "authors": ["Changhee Lee", "Fergus Imrie", "Mihaela van der Schaar"], "keywords": ["Feature Selection", "Feature Importance", "Self-Supervised Learning"], "abstract": "Discovering relevant input features for predicting a target variable is a key scientific question. However, in many domains, such as medicine and biology, feature selection is confounded by a scarcity of labeled samples coupled with significant correlations among features. In this paper, we propose a novel deep learning approach to feature selection that addresses both challenges simultaneously. First, we pre-train the network using unlabeled samples within a self-supervised learning framework by solving pretext tasks that require the network to learn informative representations from partial feature sets. Then, we fine-tune the pre-trained network to discover relevant features using labeled samples. During both training phases, we explicitly account for the correlation structure of the input features by generating correlated gate vectors from a multivariate Bernoulli distribution. Experiments on multiple real-world datasets including clinical and omics demonstrate that our model discovers relevant features that provide superior prediction performance compared to the state-of-the-art benchmarks in practical scenarios where there is often limited labeled data and high correlations among features.", "one-sentence_summary": "We propose a novel DL-based feature selection method using self-supervised learning and multivariate Bernoulli distribution to address common challenges in feature selection: a scarcity of labeled samples and significant correlations among features.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|selfsupervision_enhanced_feature_selection_with_correlated_gates", "pdf": "/pdf/eeb0bd632ffa4f23e0076ad9369b3943bbf31efc.pdf", "supplementary_material": "/attachment/f8284301f79e18548a4d2c1d486f6825ffd71039.zip", "_bibtex": "@inproceedings{\nlee2022selfsupervision,\ntitle={Self-Supervision Enhanced Feature Selection with Correlated Gates},\nauthor={Changhee Lee and Fergus Imrie and Mihaela van der Schaar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oDFvtxzPOx}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "CzceR82CYc", "original": "TeitYpm9h6Q", "number": 2112, "cdate": 1632875596454, "mdate": null, "ddate": null, "tcdate": 1632875596454, "tmdate": 1697934748182, "tddate": null, "forum": "CzceR82CYc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion", "authorids": ["~Tim_Dockhorn1", "~Arash_Vahdat3", "~Karsten_Kreis1"], "authors": ["Tim Dockhorn", "Arash Vahdat", "Karsten Kreis"], "keywords": ["Score-based generative modeling", "denoising diffusion models", "image synthesis"], "abstract": "Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered \"velocities\" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler\u2013Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dockhorn|scorebased_generative_modeling_with_criticallydamped_langevin_diffusion", "pdf": "/pdf/b0abf219a709e60170ee3d79e0068b682e8696dd.pdf", "one-sentence_summary": "In this work, we propose a novel diffusion process ideally suited for score-based generative models and provide new insights into score-based denoising diffusion models.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.07068/code)", "_bibtex": "@inproceedings{\ndockhorn2022scorebased,\ntitle={Score-Based Generative Modeling with Critically-Damped Langevin Diffusion},\nauthor={Tim Dockhorn and Arash Vahdat and Karsten Kreis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CzceR82CYc}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "DIjCrlsu6Z", "original": "OrPoNxodvt9", "number": 2104, "cdate": 1632875595569, "mdate": null, "ddate": null, "tcdate": 1632875595569, "tmdate": 1697934748979, "tddate": null, "forum": "DIjCrlsu6Z", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Controlling Directions Orthogonal to a Classifier", "authorids": ["~Yilun_Xu1", "~Hao_He1", "~Tianxiao_Shen1", "~Tommi_S._Jaakkola1"], "authors": ["Yilun Xu", "Hao He", "Tianxiao Shen", "Tommi S. Jaakkola"], "keywords": ["orthogonal classifier", "invariance"], "abstract": "We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than those of the given classifier). Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness. The orthogonal classifier enables desired style transfer when domains vary in multiple aspects, improves domain adaptation with label shifts and mitigates the unfairness as a predictor. The code is available at https://github.com/Newbeeer/orthogonal_classifier", "one-sentence_summary": "We develop a notion of orthogonality in classifier, and the corresponding construction and utility.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|controlling_directions_orthogonal_to_a_classifier", "pdf": "/pdf/259173e97dc2f0da3f9b4879faaede603f13d98a.pdf", "supplementary_material": "/attachment/9dd0aa8269f9ea535a0d6590ab6567747052c9c6.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2201.11259/code)", "_bibtex": "@inproceedings{\nxu2022controlling,\ntitle={Controlling Directions Orthogonal to a Classifier},\nauthor={Yilun Xu and Hao He and Tianxiao Shen and Tommi S. Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DIjCrlsu6Z}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "2eXhNpHeW6E", "original": "QtmIakz2AbP", "number": 2083, "cdate": 1632875582941, "mdate": null, "ddate": null, "tcdate": 1632875582941, "tmdate": 1697934751479, "tddate": null, "forum": "2eXhNpHeW6E", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "R5: Rule Discovery with Reinforced and Recurrent Relational Reasoning", "authorids": ["~Shengyao_Lu1", "~Bang_Liu1", "~Keith_G_Mills1", "~SHANGLING_JUI1", "~Di_Niu1"], "authors": ["Shengyao Lu", "Bang Liu", "Keith G Mills", "SHANGLING JUI", "Di Niu"], "keywords": ["systematicity", "graph reasoning"], "abstract": "Systematicity, i.e., the ability to recombine known parts and rules to form new sequences while reasoning over relational data, is critical to machine intelligence. A model with strong systematicity is able to train on small-scale tasks and generalize to large-scale tasks. In this paper, we propose R5, a relational reasoning framework based on reinforcement learning that reasons over relational graph data and explicitly mines underlying compositional logical rules from observations. R5 has strong systematicity and being robust to noisy data. It consists of a policy value network equipped with Monte Carlo Tree Search to perform recurrent relational prediction and a backtrack rewriting mechanism for rule mining. By alternately applying the two components, R5 progressively learns a set of explicit rules from data and performs explainable and generalizable relation prediction. We conduct extensive evaluations on multiple datasets. Experimental results show that R5 outperforms various embedding-based and rule induction baselines on relation prediction tasks while achieving a high recall rate in discovering ground truth rules. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lu|r5_rule_discovery_with_reinforced_and_recurrent_relational_reasoning", "pdf": "/pdf/08116e08b73b5c728213b5d350ddbbcf4154bb9f.pdf", "supplementary_material": "/attachment/c6b012eaca2260ea3246a34633c60cb211b98152.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2205.06454/code)", "_bibtex": "@inproceedings{\nlu2022r,\ntitle={R5: Rule Discovery with Reinforced and Recurrent Relational Reasoning},\nauthor={Shengyao Lu and Bang Liu and Keith G Mills and SHANGLING JUI and Di Niu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=2eXhNpHeW6E}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "J4iSIR9fhY0", "original": "9bgHe-Q8U_9", "number": 2069, "cdate": 1632875571445, "mdate": null, "ddate": null, "tcdate": 1632875571445, "tmdate": 1676330576693, "tddate": null, "forum": "J4iSIR9fhY0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Representation Learning for Online and Offline RL in Low-rank MDPs", "authorids": ["~Masatoshi_Uehara1", "~Xuezhou_Zhang2", "~Wen_Sun1"], "authors": ["Masatoshi Uehara", "Xuezhou Zhang", "Wen Sun"], "keywords": ["Provably sample efficient Reinforcement Learning", "PAC bounds", "Representation learning", "Low-rank MDP"], "abstract": "This work studies the question of Representation Learning in RL: how can we learn a compact low-dimensional representation such that on top of the representation we can perform RL procedures such as exploration and exploitation, in a sample efficient manner. We focus on the low-rank Markov Decision Processes (MDPs) where the transition dynamics correspond to a low-rank transition matrix. Unlike prior works that assume the representation is known (e.g., linear MDPs), here we need to learn the representation for the low-rank MDP. We study both the online RL and offline RL settings. For the online setting, operating with the same computational oracles used in FLAMBE (Agarwal et.al), the state-of-art algorithm for learning representations in low-rank MDPs, we propose an algorithm REP-UCB Upper Confidence Bound driven Representation learning for RL), which significantly improves the sample complexity from $\\widetilde{O}( A^9 d^7 / (\\epsilon^{10} (1-\\gamma)^{22}))$ for FLAMBE to $\\widetilde{O}( A^4 d^4 / (\\epsilon^2 (1-\\gamma)^{2})  )$ with $d$ being the rank of the transition matrix (or dimension of the ground truth representation), $A$ being the number of actions, and $\\gamma$ being the discounted factor. Notably, REP-UCB is simpler than FLAMBE, as it directly balances the interplay between representation learning, exploration, and exploitation, while FLAMBE is an explore-then-commit style approach and has to perform reward-free exploration step-by-step forward in time. For the offline RL setting, we develop an algorithm that leverages pessimism to learn under a partial coverage condition: our algorithm is able to compete against any policy as long as it is covered by the offline distribution.", "one-sentence_summary": "We study representation learning in low-rank MDP in both online setting and offline setting, and propose statistically and computationally efficient algorithms.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "uehara|representation_learning_for_online_and_offline_rl_in_lowrank_mdps", "pdf": "/pdf/5efca5979678883d593d4e2a7fd7c48f0add1015.pdf", "supplementary_material": "/attachment/973cc093bf6b3fb1c46d3433387ee95d4d7f2561.zip", "_bibtex": "@inproceedings{\nuehara2022representation,\ntitle={Representation Learning for Online and Offline {RL} in Low-rank {MDP}s},\nauthor={Masatoshi Uehara and Xuezhou Zhang and Wen Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=J4iSIR9fhY0}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "X_hByk2-5je", "original": "zzuBom9m8az", "number": 2053, "cdate": 1632875566296, "mdate": null, "ddate": null, "tcdate": 1632875566296, "tmdate": 1697934754068, "tddate": null, "forum": "X_hByk2-5je", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Lossless Compression with Probabilistic Circuits", "authorids": ["~Anji_Liu1", "~Stephan_Mandt1", "~Guy_Van_den_Broeck1"], "authors": ["Anji Liu", "Stephan Mandt", "Guy Van den Broeck"], "keywords": [], "abstract": "Despite extensive progress on image generation, common deep generative model architectures are not easily applied to lossless compression. For example, VAEs suffer from a compression cost overhead due to their latent variables. This overhead can only be partially eliminated with elaborate schemes such as bits-back coding, often resulting in poor single-sample compression rates. To overcome such problems, we establish a new class of tractable lossless compression models that permit efficient encoding and decoding: Probabilistic Circuits (PCs). These are a class of neural networks involving $|p|$ computational units that support efficient marginalization over arbitrary subsets of the $D$ feature dimensions, enabling efficient arithmetic coding. We derive efficient encoding and decoding schemes that both have time complexity $\\mathcal{O} (\\log(D) \\cdot |p|)$, where a naive scheme would have linear costs in $D$ and $|p|$, making the approach highly scalable. Empirically, our PC-based (de)compression algorithm runs 5-40 times faster than neural compression algorithms that achieve similar bitrates. By scaling up the traditional PC structure learning pipeline, we achieve state-of-the-art results on image datasets such as MNIST. Furthermore, PCs can be naturally integrated with existing neural compression algorithms to improve the performance of these base models on natural image datasets. Our results highlight the potential impact that non-standard learning architectures may have on neural data compression.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|lossless_compression_with_probabilistic_circuits", "pdf": "/pdf/7cccb2cf8c807b3d5eeee9e05f70c8b5ea9ab246.pdf", "supplementary_material": "/attachment/47fa820f74007d38214ffa0555e1cfbe2ca8b6a3.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2111.11632/code)", "_bibtex": "@inproceedings{\nliu2022lossless,\ntitle={Lossless Compression with Probabilistic Circuits},\nauthor={Anji Liu and Stephan Mandt and Guy Van den Broeck},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=X_hByk2-5je}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "T8vZHIRTrY", "original": "fu5LqC8GIs", "number": 2012, "cdate": 1632875557363, "mdate": null, "ddate": null, "tcdate": 1632875557363, "tmdate": 1676330579850, "tddate": null, "forum": "T8vZHIRTrY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding Domain Randomization for Sim-to-real Transfer", "authorids": ["~Xiaoyu_Chen2", "~Jiachen_Hu1", "~Chi_Jin1", "~Lihong_Li1", "~Liwei_Wang1"], "authors": ["Xiaoyu Chen", "Jiachen Hu", "Chi Jin", "Lihong Li", "Liwei Wang"], "keywords": ["domain randomization", "sim-to-real transfer", "learning theory"], "abstract": "Reinforcement learning encounters many challenges when applied directly in the real world. Sim-to-real transfer is widely used to transfer the knowledge learned from simulation to the real world. Domain randomization---one of the most popular algorithms for sim-to-real transfer---has been demonstrated to be effective in various tasks in robotics and  autonomous driving. Despite its empirical successes, theoretical understanding on why this simple algorithm works is largely missing. In this paper, we propose a  theoretical framework for sim-to-real transfers, in which the simulator is modeled as a set of MDPs with tunable parameters (corresponding to unknown physical parameters such as friction).  We provide sharp bounds on the sim-to-real gap---the difference between the value of policy returned by domain randomization and the value of an optimal policy for the real world. We prove that sim-to-real transfer can succeed under mild conditions without any real-world training samples. Our theory also highlights the importance of using memory (i.e., history-dependent policies) in domain randomization. Our proof is based on novel techniques that reduce the problem of bounding the sim-to-real gap to the problem of designing efficient learning algorithms for infinite-horizon MDPs, which we believe are of independent interest.", "one-sentence_summary": "We propose theoretical frameworks for sim-to-real transfer and domain randomization, and provide bounds on the sub-optimality gap of the policy returned by domain randomization.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|understanding_domain_randomization_for_simtoreal_transfer", "pdf": "/pdf/4e09871da1715277fa6f29c516b944b9e97b0c16.pdf", "_bibtex": "@inproceedings{\nchen2022understanding,\ntitle={Understanding Domain Randomization for Sim-to-real Transfer},\nauthor={Xiaoyu Chen and Jiachen Hu and Chi Jin and Lihong Li and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=T8vZHIRTrY}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "7F9cOhdvfk_", "original": "3LuQjcIPRck", "number": 1994, "cdate": 1632875554760, "mdate": null, "ddate": null, "tcdate": 1632875554760, "tmdate": 1697934759903, "tddate": null, "forum": "7F9cOhdvfk_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "$\\mathrm{SO}(2)$-Equivariant Reinforcement Learning", "authorids": ["~Dian_Wang1", "~Robin_Walters1", "~Robert_Platt1"], "authors": ["Dian Wang", "Robin Walters", "Robert Platt"], "keywords": ["Reinforcement Learning", "Equivariance", "Robotic Manipulation"], "abstract": "Equivariant neural networks enforce symmetry within the structure of their convolutional layers, resulting in a substantial improvement in sample efficiency when learning an equivariant or invariant function. Such models are applicable to robotic manipulation learning which can often be formulated as a rotationally symmetric problem. This paper studies equivariant model architectures in the context of $Q$-learning and actor-critic reinforcement learning. We identify equivariant and invariant characteristics of the optimal $Q$-function and the optimal policy and propose equivariant DQN and SAC algorithms that leverage this structure. We present experiments that demonstrate that our equivariant versions of DQN and SAC can be significantly more sample efficient than competing algorithms on an important class of robotic manipulation problems.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|\\mathrmso2equivariant_reinforcement_learning", "pdf": "/pdf/9f58959cef1dc2c685298e532713a5104f2df44b.pdf", "one-sentence_summary": "This paper proposes equivariant DQN and equivariant SAC that significantly improve the sample efficiency of RL in robotic manipulation.", "supplementary_material": "/attachment/3a0036825585896b12b650624b8c513f3c7d408e.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.04439/code)", "_bibtex": "@inproceedings{\nwang2022mathrmsoequivariant,\ntitle={\\${\\textbackslash}mathrm\\{{SO}\\}(2)\\$-Equivariant Reinforcement Learning},\nauthor={Dian Wang and Robin Walters and Robert Platt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7F9cOhdvfk_}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "CuV_qYkmKb3", "original": "6-XMi5i-KAO", "number": 1990, "cdate": 1632875554476, "mdate": null, "ddate": null, "tcdate": 1632875554476, "tmdate": 1676330581152, "tddate": null, "forum": "CuV_qYkmKb3", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption", "authorids": ["~Dara_Bahri1", "~Heinrich_Jiang1", "~Yi_Tay1", "~Donald_Metzler1"], "authors": ["Dara Bahri", "Heinrich Jiang", "Yi Tay", "Donald Metzler"], "keywords": ["self-supervised learning", "tabular data", "pre-training", "contrastive learning", "openML"], "abstract": "Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world \\emph{tabular} datasets. We propose \\textsc{Scarf}, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, \\textsc{Scarf} not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that \\textsc{Scarf} complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.", "pdf": "/pdf/b63986fb73f0b81b950ec3c4c84a0977ad6bdee1.pdf", "one-sentence_summary": "Scarf is a self-supervised, contrastive pre-training method for neural networks applied to tabular classification tasks that boosts performance, even when labeled data is limited or noisy.", "supplementary_material": "", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bahri|scarf_selfsupervised_contrastive_learning_using_random_feature_corruption", "data": "", "_bibtex": "@inproceedings{\nbahri2022scarf,\ntitle={Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption},\nauthor={Dara Bahri and Heinrich Jiang and Yi Tay and Donald Metzler},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CuV_qYkmKb3}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "sOK-zS6WHB", "original": "m2-sbblp23", "number": 1981, "cdate": 1632875554004, "mdate": null, "ddate": null, "tcdate": 1632875554004, "tmdate": 1676330581453, "tddate": null, "forum": "sOK-zS6WHB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Responsible Disclosure of Generative Models Using Scalable Fingerprinting", "authorids": ["~Ning_Yu2", "~Vladislav_Skripniuk1", "~Dingfan_Chen1", "~Larry_S._Davis1", "~Mario_Fritz1"], "authors": ["Ning Yu", "Vladislav Skripniuk", "Dingfan Chen", "Larry S. Davis", "Mario Fritz"], "keywords": ["Generative models", "fingerprinting", "responsible disclosure", "deep fake detection and attribution"], "abstract": "Over the past years, deep generative models have achieved a new level of performance. Generated data has become difficult, if not impossible, to be distinguished from real data. While there are plenty of use cases that benefit from this technology, there are also strong concerns on how this new technology can be misused to generate deep fakes and enable misinformation at scale. Unfortunately, current deep fake detection methods are not sustainable, as the gap between real and fake continues to close. In contrast, our work enables a responsible disclosure of such state-of-the-art generative models, that allows model inventors to fingerprint their models, so that the generated samples containing a fingerprint can be accurately detected and attributed to a source. Our technique achieves this by an efficient and scalable ad-hoc generation of a large population of models with distinct fingerprints. Our recommended operation point uses a 128-bit fingerprint which in principle results in more than 10^{38} identifiable models. Experiments show that our method fulfills key properties of a fingerprinting mechanism and achieves effectiveness in deep fake detection and attribution. Code and models are available at https://github.com/ningyu1991/ScalableGANFingerprints.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yu|responsible_disclosure_of_generative_models_using_scalable_fingerprinting", "pdf": "/pdf/e17ae78a19a967b27b17c7545fd71dfff5109784.pdf", "one-sentence_summary": "Our work enables a responsible disclosure of generative models, that allows model inventors to fingerprint their models, so that the generated samples containing a fingerprint can be accurately detected and attributed to a source.", "data": "", "code": "", "_bibtex": "@inproceedings{\nyu2022responsible,\ntitle={Responsible Disclosure of Generative Models Using Scalable Fingerprinting},\nauthor={Ning Yu and Vladislav Skripniuk and Dingfan Chen and Larry S. Davis and Mario Fritz},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=sOK-zS6WHB}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "JSR-YDImK95", "original": "Gara6LzOml", "number": 1945, "cdate": 1632875551862, "mdate": null, "ddate": null, "tcdate": 1632875551862, "tmdate": 1676330583855, "tddate": null, "forum": "JSR-YDImK95", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Path Auxiliary Proposal for MCMC in Discrete Space", "authorids": ["~Haoran_Sun2", "~Hanjun_Dai1", "~Wei_Xia8", "~Arun_Ramamurthy1"], "authors": ["Haoran Sun", "Hanjun Dai", "Wei Xia", "Arun Ramamurthy"], "keywords": [], "abstract": "Energy-based Model (EBM) offers a powerful approach for modeling discrete structure, but both inference and learning of EBM are hard as it involves sampling from discrete distributions. Recent work shows Markov Chain Monte Carlo (MCMC) with the informed proposal is a powerful tool for such sampling. However, an informed proposal only allows local updates as it requires evaluating all energy changes in the neighborhood.\nIn this work, we present a path auxiliary algorithm that uses a composition of local moves to efficiently explore large neighborhoods. We also give a fast version of our algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function. Empirically, we show that our path auxiliary algorithms considerably outperform other generic samplers on various discrete models for sampling, inference, and learning. Our method can also be used to train deep EBMs for high-dimensional discrete data.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sun|path_auxiliary_proposal_for_mcmc_in_discrete_space", "pdf": "/pdf/03e5897fe57ea115623975dd15807e2738f12501.pdf", "supplementary_material": "/attachment/a826a656d5887cf7c3db599b1c2f2b6192443522.zip", "_bibtex": "@inproceedings{\nsun2022path,\ntitle={Path Auxiliary Proposal for {MCMC} in Discrete Space},\nauthor={Haoran Sun and Hanjun Dai and Wei Xia and Arun Ramamurthy},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JSR-YDImK95}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "7b4zxUnrO2N", "original": "ZoKnhB0DEM", "number": 1927, "cdate": 1632875550616, "mdate": null, "ddate": null, "tcdate": 1632875550616, "tmdate": 1697934768421, "tddate": null, "forum": "7b4zxUnrO2N", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Possibility Before Utility: Learning And Using Hierarchical Affordances", "authorids": ["~Robby_Costales1", "~Shariq_Iqbal1", "~Fei_Sha3"], "authors": ["Robby Costales", "Shariq Iqbal", "Fei Sha"], "keywords": ["RL", "HRL", "reinforcement learning", "hierarchical reinforcement learning", "affordances", "hierarchical affordances"], "abstract": "Reinforcement learning algorithms struggle on tasks with complex hierarchical dependency structures. Humans and other intelligent agents do not waste time assessing the utility of every high-level action in existence, but instead only consider ones they deem possible in the first place. By focusing only on what is feasible, or \"afforded'', at the present moment, an agent can spend more time both evaluating the utility of and acting on what matters. To this end, we present Hierarchical Affordance Learning (HAL), a method that learns a model of hierarchical affordances in order to prune impossible subtasks for more effective learning. Existing works in hierarchical reinforcement learning provide agents with structural representations of subtasks but are not affordance-aware, and by grounding our definition of hierarchical affordances in the present state, our approach is more flexible than the multitude of approaches that ground their subtask dependencies in a symbolic history. While these logic-based methods often require complete knowledge of the subtask hierarchy, our approach is able to utilize incomplete and varying symbolic specifications. Furthermore, we demonstrate that relative to non-affordance-aware methods, HAL agents are better able to efficiently learn complex tasks, navigate environment stochasticity, and acquire diverse skills in the absence of extrinsic supervision---all of which are hallmarks of human learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "costales|possibility_before_utility_learning_and_using_hierarchical_affordances", "pdf": "/pdf/f4b5c96c2948ff7fcea521e9713644691c27bab2.pdf", "one-sentence_summary": "We introduce a method that achieves superior performance in complex hierarchical tasks by utilizing a notion of subtask dependency grounded in the present state.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2203.12686/code)", "_bibtex": "@inproceedings{\ncostales2022possibility,\ntitle={Possibility Before Utility: Learning And Using Hierarchical Affordances},\nauthor={Robby Costales and Shariq Iqbal and Fei Sha},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7b4zxUnrO2N}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "DfMqlB0PXjM", "original": "D6zdfSC3r7F", "number": 1913, "cdate": 1632875549637, "mdate": null, "ddate": null, "tcdate": 1632875549637, "tmdate": 1676330586158, "tddate": null, "forum": "DfMqlB0PXjM", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Interpretable Unsupervised Diversity Denoising and Artefact Removal", "authorids": ["~Mangal_Prakash1", "~Mauricio_Delbracio1", "~Peyman_Milanfar1", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Mauricio Delbracio", "Peyman Milanfar", "Florian Jug"], "keywords": ["Interpretable Unsupervised Image Restoration", "Diversity Image Restoration", "Unsupervised Image Denoising", "Unsupervised Artefact Removal"], "abstract": "Image denoising and artefact removal are complex inverse problems admitting multiple valid solutions. Unsupervised diversity restoration, that is, obtaining a diverse set of possible restorations given a corrupted image, is important for ambiguity removal in many applications such as microscopy where paired data for supervised training are often unobtainable. In real world applications, imaging noise and artefacts are typically hard to model, leading to unsatisfactory performance of existing unsupervised approaches. This work presents an interpretable approach for unsupervised and diverse image restoration. To this end, we introduce a capable architecture called Hierarchical DivNoising (HDN) based on hierarchical Variational Autoencoder. We show that HDN learns an interpretable multi-scale representation of artefacts  and we leverage this interpretability to remove imaging artefacts commonly occurring in microscopy data. Our method achieves state-of-the-art results on twelve benchmark image denoising datasets while providing access to a whole distribution of sensibly restored solutions.\nAdditionally, we demonstrate on three real microscopy datasets that HDN removes artefacts without supervision, being the first method capable of doing so while generating multiple plausible restorations all consistent with the given corrupted image.", "one-sentence_summary": "This work proposes a new architecture for unsupervised, interpretable and diverse image restoration while achieving state-of-the-art results on numerous commonly used benchmarks across multiple image domains.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "prakash|interpretable_unsupervised_diversity_denoising_and_artefact_removal", "pdf": "/pdf/3fb037cb33af2bbe50cc241272e4f9313aaf3552.pdf", "data": "", "_bibtex": "@inproceedings{\nprakash2022interpretable,\ntitle={Interpretable Unsupervised Diversity Denoising and Artefact Removal},\nauthor={Mangal Prakash and Mauricio Delbracio and Peyman Milanfar and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DfMqlB0PXjM}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "HTx7vrlLBEj", "original": "W20Tnsg0BoU", "number": 1857, "cdate": 1632875545982, "mdate": null, "ddate": null, "tcdate": 1632875545982, "tmdate": 1697934777456, "tddate": null, "forum": "HTx7vrlLBEj", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Half-Inverse Gradients for Physical Deep Learning", "authorids": ["~Patrick_Schnell1", "~Philipp_Holl1", "~Nils_Thuerey1"], "authors": ["Patrick Schnell", "Philipp Holl", "Nils Thuerey"], "keywords": ["physical simulation", "partial differential equations", "physical loss functions", "optimization"], "abstract": "Recent works in deep learning have shown that integrating differentiable physics simulators into the training process can greatly improve the quality of results. Although this combination represents a more complex optimization task than usual neural network training, the same gradient-based optimizers are used to minimize the loss function. However, the integrated physics solvers have a profound effect on the gradient flow as manipulating scales in magnitude and direction is an inherent property of many physical processes. Consequently, the gradient flow is often highly unbalanced and creates an environment in which existing gradient-based optimizers perform poorly. In this work, we analyze the characteristics of both physical and neural network optimizations separately to derive a new method based on a half-inversion of the Jacobian. Our approach combines principles of both classical network and physics optimizers to solve the combined optimization task. Compared to state-of-the-art neural network optimizers, our method converges more quickly and to better solutions, which we demonstrate on three complex learning problems involving nonlinear oscillators, the Schroedinger equation and the Poisson problem.", "one-sentence_summary": "By proposing a novel Jacobian-based optimizer, we question the current practice of using the state-of-the-art gradient-based methods for the optimization of neural networks with physics objectives.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "schnell|halfinverse_gradients_for_physical_deep_learning", "pdf": "/pdf/30d8e38a4f776ba1a0480b58ae7a48fc34a42760.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.10131/code)", "_bibtex": "@inproceedings{\nschnell2022halfinverse,\ntitle={Half-Inverse Gradients for Physical Deep Learning},\nauthor={Patrick Schnell and Philipp Holl and Nils Thuerey},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HTx7vrlLBEj}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "X_ch3VrNSRg", "original": "-hPWIJpgRpm", "number": 1849, "cdate": 1632875545434, "mdate": null, "ddate": null, "tcdate": 1632875545434, "tmdate": 1697934778243, "tddate": null, "forum": "X_ch3VrNSRg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits", "authorids": ["~Yikun_Ban1", "~Yuchen_Yan1", "~Arindam_Banerjee4", "~Jingrui_He1"], "authors": ["Yikun Ban", "Yuchen Yan", "Arindam Banerjee", "Jingrui He"], "keywords": ["Contextual Bandits", "Exploration Strategy", "Neural Networks"], "abstract": "In this paper, we propose a novel neural exploration strategy in contextual bandits, EE-Net, distinct from the standard UCB-based and TS-based approaches. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration tradeoff in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound (UCB). In recent literature, linear contextual bandits have adopted ridge regression to estimate the reward function and combine it with TS or UCB strategies for exploration. However, this line of works explicitly assumes the reward is based on a linear function of arm vectors, which may not be true in real-world datasets. To overcome this challenge, a series of neural bandit algorithms have been proposed, where a neural network is used to learn the underlying reward function and TS or UCB are adapted for exploration. Instead of calculating a large-deviation based statistical bound for exploration like previous methods,  we propose \"EE-Net\", a novel neural-based exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn potential gains compared to the currently estimated reward for exploration. Then, a decision-maker is constructed to combine the outputs from the Exploitation and Exploration networks. We prove that EE-Net can achieve $\\mathcal{O}(\\sqrt{T\\log T})$ regret and show that EE-Net outperforms existing linear and neural contextual bandit baselines on real-world datasets. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ban|eenet_exploitationexploration_neural_networks_in_contextual_bandits", "pdf": "/pdf/c6228ff8fe747650e5b549f73f34d2306402b787.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.03177/code)", "_bibtex": "@inproceedings{\nban2022eenet,\ntitle={{EE}-Net: Exploitation-Exploration Neural Networks in Contextual Bandits},\nauthor={Yikun Ban and Yuchen Yan and Arindam Banerjee and Jingrui He},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=X_ch3VrNSRg}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "iMH1e5k7n3L", "original": "FN9Rzxv45YQ", "number": 1831, "cdate": 1632875544185, "mdate": null, "ddate": null, "tcdate": 1632875544185, "tmdate": 1676330591290, "tddate": null, "forum": "iMH1e5k7n3L", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Spike-inspired rank coding for fast and accurate recurrent neural networks", "authorids": ["~Alan_Jeffares1", "~Qinghai_Guo1", "~Pontus_Stenetorp1", "~Timoleon_Moraitis1"], "authors": ["Alan Jeffares", "Qinghai Guo", "Pontus Stenetorp", "Timoleon Moraitis"], "keywords": [], "abstract": "Biological spiking neural networks (SNNs) can temporally encode information in their outputs, e.g. in the rank order in which neurons fire, whereas artificial neural networks (ANNs) conventionally do not. As a result, models of SNNs for neuromorphic computing are regarded as potentially more rapid and efficient than ANNs when dealing with temporal input. On the other hand, ANNs are simpler to train, and usually achieve superior performance. Here we show that temporal coding such as rank coding (RC) inspired by SNNs can also be applied to conventional ANNs such as LSTMs, and leads to computational savings and speedups.\nIn our RC for ANNs, we apply backpropagation through time using the standard real-valued activations, but only from a strategically early time step of each sequential input example, decided by a threshold-crossing event. Learning then incorporates naturally also when to produce an output, without other changes to the model or the algorithm. Both the forward and the backward training pass can be significantly shortened by skipping the remaining input sequence after that first event. RC-training also significantly reduces time-to-insight during inference, with a minimal decrease in accuracy. The desired speed-accuracy trade-off is tunable by varying the threshold or a regularization parameter that rewards output entropy. We demonstrate these in two toy problems of sequence classification, and in a temporally-encoded MNIST dataset where our RC model achieves 99.19% accuracy after the first input time-step, outperforming the state of the art in temporal coding with SNNs, as well as in spoken-word classification of Google Speech Commands, outperforming non-RC-trained early inference with LSTMs.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jeffares|spikeinspired_rank_coding_for_fast_and_accurate_recurrent_neural_networks", "pdf": "/pdf/c1c6fc25f1bbf5574f5a49723ca38dafe70660ca.pdf", "one-sentence_summary": "Learning to infer fast in LSTMs inspired by SNNs, and applied in speech recognition", "supplementary_material": "/attachment/c03cca126ba37b9d00278e5e9c2cc8f0d1497e1d.zip", "data": "", "_bibtex": "@inproceedings{\njeffares2022spikeinspired,\ntitle={Spike-inspired rank coding for fast and accurate recurrent neural networks},\nauthor={Alan Jeffares and Qinghai Guo and Pontus Stenetorp and Timoleon Moraitis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iMH1e5k7n3L}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "W9G_ImpHlQd", "original": "C0Bm9-5YoJ_", "number": 1824, "cdate": 1632875543835, "mdate": null, "ddate": null, "tcdate": 1632875543835, "tmdate": 1697934780895, "tddate": null, "forum": "W9G_ImpHlQd", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective", "authorids": ["~Yimeng_Zhang2", "~Yuguang_Yao1", "~Jinghan_Jia1", "~Jinfeng_Yi1", "~Mingyi_Hong1", "~Shiyu_Chang2", "~Sijia_Liu1"], "authors": ["Yimeng Zhang", "Yuguang Yao", "Jinghan Jia", "Jinfeng Yi", "Mingyi Hong", "Shiyu Chang", "Sijia Liu"], "keywords": ["Zeroth-Order Optimization", "Black-Box Defense", "Gradient-Free", "Adversarial Robustness", "Certified Defense"], "abstract": "The lack of adversarial robustness has been recognized as an important issue for state-of-the-art machine learning (ML) models, e.g., deep neural networks (DNNs). Thereby, robustifying ML models against adversarial attacks is now a major focus of research. However, nearly all existing defense methods, particularly for robust training, made the white-box assumption that the defender has the access to the details of an ML model (or its surrogate alternatives if available), e.g., its architectures and parameters. Beyond existing works, in this paper we aim to address the problem of black-box defense: How to robustify a black-box model using just input queries and output feedback? Such a problem arises in practical scenarios, where the owner of the predictive model is reluctant to share model information in order to preserve privacy. To this end, we propose a general notion of defensive operation that can be applied to black-box models, and design it through the lens of denoised smoothing (DS), a \ufb01rst-order (FO) certi\ufb01ed defense technique. To allow the design of merely using model queries, we further integrate DS with the zeroth-order (gradient-free) optimization. However, a direct implementation of zeroth-order (ZO) optimization suffers a high variance of gradient estimates, and thus leads to ineffective defense. To tackle this problem, we next propose to prepend an autoencoder (AE) to a given (black-box) model so that DS can be trained using variance-reduced ZO optimization. We term the eventual defense as ZO-AE-DS. In practice, we empirically show that ZO-AE-DS can achieve improved accuracy, certi\ufb01ed robustness, and query complexity over existing baselines. And the effectiveness of our approach is justi\ufb01ed under both image classi\ufb01cation and image reconstruction tasks.", "one-sentence_summary": "We propose a general notion of defensive operation that can be applied to black-box models, and design it through the lens of denoised smoothing (DS), a first-order (FO) certified defense technique.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|how_to_robustify_blackbox_ml_models_a_zerothorder_optimization_perspective", "pdf": "/pdf/892f86851a58432caba514751380a19f53458d67.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.14195/code)", "_bibtex": "@inproceedings{\nzhang2022how,\ntitle={How to Robustify Black-Box {ML} Models? A Zeroth-Order Optimization Perspective},\nauthor={Yimeng Zhang and Yuguang Yao and Jinghan Jia and Jinfeng Yi and Mingyi Hong and Shiyu Chang and Sijia Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=W9G_ImpHlQd}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "FEDfGWVZYIn", "original": "yCQ-wXeps6D", "number": 1815, "cdate": 1632875543204, "mdate": null, "ddate": null, "tcdate": 1632875543204, "tmdate": 1697934782044, "tddate": null, "forum": "FEDfGWVZYIn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "RelaxLoss: Defending Membership Inference Attacks without Losing Utility", "authorids": ["~Dingfan_Chen1", "~Ning_Yu2", "~Mario_Fritz1"], "authors": ["Dingfan Chen", "Ning Yu", "Mario Fritz"], "keywords": ["membership inference attack", "defense"], "abstract": "As a long-term threat to the privacy of training data, membership inference attacks (MIAs) emerge ubiquitously in machine learning models.\nExisting works evidence strong connection between the distinguishability of the training and testing loss distributions and the model's vulnerability to MIAs. Motivated by existing results, we propose a novel training framework based on a relaxed loss ($\\textbf{RelaxLoss}$) with a more achievable learning target, which leads to narrowed generalization gap and reduced privacy leakage. RelaxLoss is applicable to any classification model with added benefits of easy implementation and negligible overhead. Through extensive evaluations on five datasets with diverse modalities (images, medical data, transaction records), our approach consistently outperforms state-of-the-art defense mechanisms in terms of resilience against MIAs as well as model utility. Our defense is the first that can withstand a wide range of attacks while preserving (or even improving) the target model's utility.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|relaxloss_defending_membership_inference_attacks_without_losing_utility", "pdf": "/pdf/e3ac303ac886fc33aba9568f6cb7a74e2c021f00.pdf", "one-sentence_summary": "We propose a novel training scheme that is highly effective in protecting against membership inference attacks while preserving the utility of target models. ", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2207.05801/code)", "_bibtex": "@inproceedings{\nchen2022relaxloss,\ntitle={RelaxLoss: Defending Membership Inference Attacks without Losing Utility},\nauthor={Dingfan Chen and Ning Yu and Mario Fritz},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FEDfGWVZYIn}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "eBS-3YiaIL-", "original": "R9I37doQyE", "number": 1800, "cdate": 1632875542201, "mdate": null, "ddate": null, "tcdate": 1632875542201, "tmdate": 1676330593663, "tddate": null, "forum": "eBS-3YiaIL-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation", "authorids": ["~Bingbin_Liu1", "~Elan_Rosenfeld1", "~Pradeep_Kumar_Ravikumar1", "~Andrej_Risteski2"], "authors": ["Bingbin Liu", "Elan Rosenfeld", "Pradeep Kumar Ravikumar", "Andrej Risteski"], "keywords": ["noise contrastive estimation", "contrastive learning", "unsupervised learning", "theory"], "abstract": "Noise-contrastive estimation (NCE) is a statistically consistent method for learning unnormalized probabilistic models. It has been empirically observed that the choice of the noise distribution is crucial for NCE\u2019s performance. However, such observation has never been made formal or quantitative. In fact, it is not even clear whether the difficulties arising from a poorly chosen noise distribution are statistical or algorithmic in nature.\nIn this work, we formally pinpoint reasons for NCE\u2019s poor performance when an inappropriate noise distribution is used. Namely, we prove these challenges arise due to an ill-behaved (more precisely, flat) loss landscape.\nTo address this, we introduce a variant of NCE called \\emph{eNCE} which uses an exponential loss and for which \\emph{normalized gradient descent} addresses the landscape issues \\emph{provably} when the target and noise distributions are in a given exponential family. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|analyzing_and_improving_the_optimization_landscape_of_noisecontrastive_estimation", "pdf": "/pdf/6fece6afb2eea04bf01bd8ff7ec9da4a780eb660.pdf", "one-sentence_summary": "This work theoretically explains the difficulty of optimizing the NCE loss when the noise distribution is poor, and provides a provably efficient solution consisting of normalized gradient descent (NGD) combined with the proposed \\emph{eNCE} loss.", "supplementary_material": "/attachment/7676e3ed2e2e43b2dfee0c9bf18e331e82568113.zip", "_bibtex": "@inproceedings{\nliu2022analyzing,\ntitle={Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation},\nauthor={Bingbin Liu and Elan Rosenfeld and Pradeep Kumar Ravikumar and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=eBS-3YiaIL-}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "mmUA7_O9mjY", "original": "Qh0-BaOIgCn", "number": 1797, "cdate": 1632875542006, "mdate": null, "ddate": null, "tcdate": 1632875542006, "tmdate": 1676330593911, "tddate": null, "forum": "mmUA7_O9mjY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics", "authorids": ["~Sizhe_Li1", "~Zhiao_Huang1", "~Tao_Du1", "~Hao_Su1", "~Joshua_B._Tenenbaum1", "~Chuang_Gan1"], "authors": ["Sizhe Li", "Zhiao Huang", "Tao Du", "Hao Su", "Joshua B. Tenenbaum", "Chuang Gan"], "keywords": ["differentiable physics", "soft body manipulation"], "abstract": "Differentiable physics has recently been shown as a powerful tool for solving soft-body manipulation tasks. However, the differentiable physics solver often gets stuck when the initial contact points of the end effectors are sub-optimal or when performing multi-stage tasks that require contact point switching, which often leads to local minima.\nTo address this challenge, we propose a  contact point discovery approach (CPDeform) that guides the stand-alone differentiable physics solver to deform various soft-body plasticines. The key idea of our approach is to integrate optimal transport-based contact points discovery into the differentiable physics solver to overcome the local minima from initial contact points or contact switching.\nOn single-stage tasks, our method can automatically find suitable initial contact points based on transport priorities. On complex multi-stage tasks, we can iteratively switch the contact points of end-effectors based on transport priorities. To evaluate the effectiveness of our method, we introduce PlasticineLab-M that extends the existing differentiable physics benchmark PlasticineLab to seven new challenging multi-stage soft-body manipulation tasks. Extensive experimental results suggest that: 1) on multi-stage tasks that are infeasible for the vanilla differentiable physics solver, our approach discovers contact points that efficiently guide the solver to completion; 2) on tasks where the vanilla solver performs sub-optimally or near-optimally, our contact point discovery method performs better than or on par with the manipulation performance obtained with handcrafted contact points.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|contact_points_discovery_for_softbody_manipulations_with_differentiable_physics", "pdf": "/pdf/21c3d98f02351425ae7c7201ec763c603b24b4c7.pdf", "one-sentence_summary": "We propose a contact pose discovery method that guides the stand-alone differentiable physics solver to complete various soft-body manipulation tasks.", "data": "", "_bibtex": "@inproceedings{\nli2022contact,\ntitle={Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics},\nauthor={Sizhe Li and Zhiao Huang and Tao Du and Hao Su and Joshua B. Tenenbaum and Chuang Gan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=mmUA7_O9mjY}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "cmt-6KtR4c4", "original": "UTS8yHGVet1", "number": 1793, "cdate": 1632875541742, "mdate": null, "ddate": null, "tcdate": 1632875541742, "tmdate": 1676330593997, "tddate": null, "forum": "cmt-6KtR4c4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Leveraging Automated Unit Tests for Unsupervised Code Translation", "authorids": ["~Baptiste_Roziere1", "~Jie_Zhang21", "~Francois_Charton1", "~Mark_Harman1", "~Gabriel_Synnaeve1", "~Guillaume_Lample1"], "authors": ["Baptiste Roziere", "Jie Zhang", "Francois Charton", "Mark Harman", "Gabriel Synnaeve", "Guillaume Lample"], "keywords": ["unsupervised", "translation", "code", "self-training", "pseudo-labelling", "unit tests", "programming languages", "deep learning", "transformer"], "abstract": "With little to no parallel data available for programming languages, unsupervised methods are well-suited to source code translation. However, the majority of unsupervised machine translation approaches rely on back-translation, a method developed in the context of natural language translation and one that inherently involves training on noisy inputs. Unfortunately, source code is highly sensitive to small changes; a single token can result in compilation failures or erroneous programs, unlike natural languages where small inaccuracies may not change the meaning of a sentence. To address this issue, we propose to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus. We found that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state-of-the-art for all language pairs studied. In particular, for Java\u2192Python and Python\u2192C++ we outperform the best previous methods by more than 16% and 24% respectively, reducing the error rate by more than 35%.", "one-sentence_summary": "We leverage automatically created multilingual unit tests to improve unsupervised machine translation methods for source code and substantially outperform the state-of-the-art on all the language pairs we consider.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "roziere|leveraging_automated_unit_tests_for_unsupervised_code_translation", "pdf": "/pdf/5afda866e17de3287b9281461435fdc488309beb.pdf", "supplementary_material": "/attachment/200f116f8517f802f4c65a6b62fb95224f70b3cc.zip", "_bibtex": "@inproceedings{\nroziere2022leveraging,\ntitle={Leveraging Automated Unit Tests for Unsupervised Code Translation},\nauthor={Baptiste Roziere and Jie Zhang and Francois Charton and Mark Harman and Gabriel Synnaeve and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=cmt-6KtR4c4}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "BB4e8Atc1eR", "original": "q8-VUYO9EZG", "number": 1785, "cdate": 1632875541194, "mdate": null, "ddate": null, "tcdate": 1632875541194, "tmdate": 1697934786840, "tddate": null, "forum": "BB4e8Atc1eR", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Scalable Sampling for Nonsymmetric Determinantal Point Processes", "authorids": ["~Insu_Han1", "~Mike_Gartrell1", "~Jennifer_Gillenwater1", "~Elvis_Dohmatob1", "~amin_karbasi1"], "authors": ["Insu Han", "Mike Gartrell", "Jennifer Gillenwater", "Elvis Dohmatob", "amin karbasi"], "keywords": ["determinantal point processes", "sampling"], "abstract": "A determinantal point process (DPP) on a collection of $M$ items is a model, parameterized by a symmetric kernel matrix, that assigns a probability to every subset of those items.  Recent work shows that removing the kernel symmetry constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant predictive performance gains for machine learning applications. However, existing work leaves open the question of scalable NDPP sampling. There is only one known DPP sampling algorithm, based on Cholesky decomposition, that can directly apply to NDPPs as well. Unfortunately, its runtime is cubic in $M$, and thus does not scale to large item collections. In this work, we first note that this algorithm can be transformed into a linear-time one for kernels with low-rank structure.  Furthermore, we develop a scalable sublinear-time rejection sampling algorithm by constructing a novel proposal distribution.  Additionally, we show that imposing certain structural constraints on the NDPP kernel enables us to bound the rejection rate in a way that depends only on the kernel rank. In our experiments we compare the speed of all of these samplers for a variety of real-world tasks.", "one-sentence_summary": "We propose the first scalable linear-time and sublinear-time sampling algorithms for nonsymmetric determinantal point processes.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "han|scalable_sampling_for_nonsymmetric_determinantal_point_processes", "pdf": "/pdf/b38ff838b862c1f5918c345f4322281132fa0715.pdf", "supplementary_material": "/attachment/7b1db943c21151769505bd80e41b0039fb0352e8.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2201.08417/code)", "_bibtex": "@inproceedings{\nhan2022scalable,\ntitle={Scalable Sampling for Nonsymmetric Determinantal Point Processes},\nauthor={Insu Han and Mike Gartrell and Jennifer Gillenwater and Elvis Dohmatob and amin karbasi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BB4e8Atc1eR}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "FRxhHdnxt1", "original": "hqR-OtB4hx6", "number": 1737, "cdate": 1632875538142, "mdate": null, "ddate": null, "tcdate": 1632875538142, "tmdate": 1676330596873, "tddate": null, "forum": "FRxhHdnxt1", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design", "authorids": ["~Wenhao_Gao1", "~Roc\u00edo_Mercado1", "~Connor_W._Coley1"], "authors": ["Wenhao Gao", "Roc\u00edo Mercado", "Connor W. Coley"], "keywords": ["molecular design", "synthesis planning", "tree generation", "graph generation"], "abstract": "Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously. The approach leverages neural networks to probabilistically model the synthetic trees, one reaction step at a time, according to reactivity rules encoded in a discrete action space of reaction templates. We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates. We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to bioactivity and drug discovery.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gao|amortized_tree_generation_for_bottomup_synthesis_planning_and_synthesizable_molecular_design", "pdf": "/pdf/9a1a1ce4faf67f9a0f649866c9158c2f0f055db1.pdf", "one-sentence_summary": "We propose a model that address synthesis planning and synthesizable molecular design simultaneously.", "supplementary_material": "/attachment/1eae0d5d5f7f74a3ff02aeb14e519820d38a5072.zip", "_bibtex": "@inproceedings{\ngao2022amortized,\ntitle={Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design},\nauthor={Wenhao Gao and Roc{\\'\\i}o Mercado and Connor W. Coley},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FRxhHdnxt1}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 23}}, {"id": "LI2bhrE_2A", "original": "d7mcgg2Eir", "number": 1727, "cdate": 1632875537465, "mdate": null, "ddate": null, "tcdate": 1632875537465, "tmdate": 1676330597860, "tddate": null, "forum": "LI2bhrE_2A", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design", "authorids": ["~Wengong_Jin1", "jwohlwend@csail.mit.edu", "~Regina_Barzilay1", "~Tommi_S._Jaakkola1"], "authors": ["Wengong Jin", "Jeremy Wohlwend", "Regina Barzilay", "Tommi S. Jaakkola"], "keywords": ["Drug Discovery", "Antibody Design", "Generative Models", "Graph Generation"], "abstract": "Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune system. The specificity of antibody binding is determined by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a generative model to automatically design the CDRs of antibodies with enhanced binding specificity or neutralization capabilities. Previous generative approaches formulate protein design as a structure-conditioned sequence generation task, assuming the desired 3D structure is given a priori. In contrast, we propose to co-design the sequence and 3D structure of CDRs as graphs. Our model unravels a sequence autoregressively while iteratively refining its predicted global structure. The inferred structure in turn guides subsequent residue choices. For efficiency, we model the conditional dependence between residues inside and outside of a CDR in a coarse-grained manner. Our method achieves superior log-likelihood on the test set and outperforms previous baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus.\n", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jin|iterative_refinement_graph_neural_network_for_antibody_sequencestructure_codesign", "pdf": "/pdf/f85e516c2ab137179adf7bda106ae34944694b9d.pdf", "one-sentence_summary": "We propose a new graph-based generative model for antibody design", "_bibtex": "@inproceedings{\njin2022iterative,\ntitle={Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design},\nauthor={Wengong Jin and Jeremy Wohlwend and Regina Barzilay and Tommi S. Jaakkola},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LI2bhrE_2A}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "HbtFCX2PLq0", "original": "MjWohhTjj1L", "number": 1708, "cdate": 1632875536310, "mdate": null, "ddate": null, "tcdate": 1632875536310, "tmdate": 1697934794018, "tddate": null, "forum": "HbtFCX2PLq0", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Churn Reduction via Distillation", "authorids": ["~Heinrich_Jiang1", "~Harikrishna_Narasimhan1", "~Dara_Bahri1", "~Andrew_Cotter1", "~Afshin_Rostamizadeh1"], "authors": ["Heinrich Jiang", "Harikrishna Narasimhan", "Dara Bahri", "Andrew Cotter", "Afshin Rostamizadeh"], "keywords": ["distillation", "churn", "constraints"], "abstract": "In real-world systems, models are frequently updated as more data becomes available, and in addition to achieving high accuracy, the goal is to also maintain a low difference in predictions compared to the base model (i.e. predictive churn). If model retraining results in vastly different behavior, then it could cause negative effects in downstream systems, especially if this churn can be avoided with limited impact on model accuracy. In this paper, we show an equivalence between training with distillation using the base model as the teacher and training with an explicit constraint on the predictive churn. We then show that distillation performs strongly for low churn training against a number of recent baselines on a wide range of datasets and model architectures, including fully-connected networks, convolutional networks, and transformers.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|churn_reduction_via_distillation", "pdf": "/pdf/d4c4b0da2bc7b1427e642ebdfc966ac7b142ecd0.pdf", "one-sentence_summary": "We show distillation is a principled and practical solution to churn reduction.", "data": "", "code": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2106.02654/code)", "_bibtex": "@inproceedings{\njiang2022churn,\ntitle={Churn Reduction via Distillation},\nauthor={Heinrich Jiang and Harikrishna Narasimhan and Dara Bahri and Andrew Cotter and Afshin Rostamizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HbtFCX2PLq0}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 8}}, {"id": "7twQI5VnC8", "original": "zoSLn1_YEvd", "number": 1707, "cdate": 1632875536243, "mdate": null, "ddate": null, "tcdate": 1632875536243, "tmdate": 1676330598664, "tddate": null, "forum": "7twQI5VnC8", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Causal Models from Conditional Moment Restrictions by Importance Weighting", "authorids": ["~Masahiro_Kato1", "~Masaaki_Imaizumi1", "~Kenichiro_McAlinn2", "~Shota_Yasui1", "~Haruo_Kakehi2"], "authors": ["Masahiro Kato", "Masaaki Imaizumi", "Kenichiro McAlinn", "Shota Yasui", "Haruo Kakehi"], "keywords": ["Causal inference", "Conditional moment restrictions"], "abstract": "We consider learning causal relationships under conditional moment restrictions. Unlike causal inference under unconditional moment restrictions, conditional moment restrictions pose serious challenges for causal inference. To address this issue, we propose a method that transforms conditional moment restrictions to unconditional moment restrictions through importance weighting using a conditional density ratio estimator. Then, using this transformation, we propose a method that successfully estimate a parametric or nonparametric functions defined under the conditional moment restrictions. We analyze the estimation error and provide a bound on the structural function, providing theoretical support for our proposed method. In experiments, we confirm the soundness of our proposed method.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "kato|learning_causal_models_from_conditional_moment_restrictions_by_importance_weighting", "pdf": "/pdf/84e1728a94b82499ea64a0d48409de2dfc9115ee.pdf", "one-sentence_summary": "Learning causal relationships under conditional moment restrictions by importance weighting using the conditional density ratio function.", "_bibtex": "@inproceedings{\nkato2022learning,\ntitle={Learning Causal Models from Conditional Moment Restrictions by Importance Weighting},\nauthor={Masahiro Kato and Masaaki Imaizumi and Kenichiro McAlinn and Shota Yasui and Haruo Kakehi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7twQI5VnC8}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "hfU7Ka5cfrC", "original": "bcLFQrXfPfg", "number": 1635, "cdate": 1632875531347, "mdate": null, "ddate": null, "tcdate": 1632875531347, "tmdate": 1676330602319, "tddate": null, "forum": "hfU7Ka5cfrC", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation", "authorids": ["~Ross_M_Clarke1", "~Elre_Talea_Oldewage1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1"], "authors": ["Ross M Clarke", "Elre Talea Oldewage", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "keywords": ["Hyperparameter Optimisation"], "abstract": "Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser hyperparameters (such as learning rates and momenta) or take several times longer to train than their base models. We extend these existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuous hyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts. We also provide a motivating argument for convergence to the true hypergradient, and perform tractable gradient-based optimisation of independent learning rates for each model parameter. Our method performs competitively from varied random hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a ResNet-18), in time only 2-3x greater than vanilla training.", "one-sentence_summary": "We develop a gradient-based hyperparameter optimisation algorithm, applicable to a wide range of continuous hyperparameters, and scaling to large numbers of hyperparameters, without dramatically increasing training time from the non-HPO baseline.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "clarke|scalable_onepass_optimisation_of_highdimensional_weightupdate_hyperparameters_by_implicit_differentiation", "pdf": "/pdf/a7f35bc772a0804247c8631982741afe42ec790e.pdf", "supplementary_material": "/attachment/f57e3b46ebbdb3ef82fb39cc04d569d0db8ee8e7.zip", "_bibtex": "@inproceedings{\nclarke2022scalable,\ntitle={Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation},\nauthor={Ross M Clarke and Elre Talea Oldewage and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=hfU7Ka5cfrC}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "vrW3tvDfOJQ", "original": "TMqbyrhhxdG", "number": 1632, "cdate": 1632875531137, "mdate": null, "ddate": null, "tcdate": 1632875531137, "tmdate": 1697934801139, "tddate": null, "forum": "vrW3tvDfOJQ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation", "authorids": ["~Vincent_Mai1", "~Kaustubh_Mani1", "~Liam_Paull1"], "authors": ["Vincent Mai", "Kaustubh Mani", "Liam Paull"], "keywords": ["Deep reinforcement learning", "uncertainty estimation", "inverse-variance", "heteroscedastic"], "abstract": "In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efficiency. As this noise is heteroscedastic, its effects can be mitigated using uncertainty-based weights in the optimization process. Previous methods rely on sampled ensembles, which do not capture all aspects of uncertainty. We provide a systematic analysis of the sources of uncertainty in the noisy supervision that occurs in RL, and introduce inverse-variance RL, a Bayesian framework which combines probabilistic ensembles and Batch Inverse Variance weighting. We propose a method whereby two complementary uncertainty estimation methods account for both the Q-value and the environment stochasticity to better mitigate the negative impacts of noisy supervision. Our results show significant improvement in terms of sample efficiency on discrete and continuous control tasks.", "one-sentence_summary": "The sample efficiency and performance of model-free DRL is improved by estimating the predictive uncertainty of the targets using probabilistic ensembles and down-weighting the uncertain samples using batch inverse-variance weighting.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mai|sample_efficient_deep_reinforcement_learning_via_uncertainty_estimation", "pdf": "/pdf/2957fd1597c9d0c85f628e1d53b0aba1a7aa45b1.pdf", "supplementary_material": "/attachment/14bed71546a875f6a3d6d55814d9818b4816aa8f.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2201.01666/code)", "_bibtex": "@inproceedings{\nmai2022sample,\ntitle={Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation},\nauthor={Vincent Mai and Kaustubh Mani and Liam Paull},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vrW3tvDfOJQ}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "O1DEtITim__", "original": "cv__Oc2GF2H", "number": 1627, "cdate": 1632875530791, "mdate": null, "ddate": null, "tcdate": 1632875530791, "tmdate": 1698697307370, "tddate": null, "forum": "O1DEtITim__", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining", "authorids": ["~Miao_Lu3", "~Xiaolong_Luo3", "~Tianlong_Chen1", "~Wuyang_Chen1", "~Dong_Liu6", "~Zhangyang_Wang1"], "authors": ["Miao Lu", "Xiaolong Luo", "Tianlong Chen", "Wuyang Chen", "Dong Liu", "Zhangyang Wang"], "keywords": ["Pruning", "Frank-Wolfe"], "abstract": "We present a novel framework to train a large deep neural network (DNN) for only $\\textit{once}$, which can then be pruned to $\\textit{any sparsity ratio}$ to preserve competitive accuracy $\\textit{without any re-training}$. Conventional methods often require (iterative) pruning followed by re-training, which not only incurs large overhead beyond the original DNN training but also can be sensitive to retraining hyperparameters. Our core idea is to re-cast the DNN training as an explicit $\\textit{pruning-aware}$ process: that is formulated with an auxiliary $K$-sparse polytope constraint, to encourage network weights to lie in a convex hull spanned by $K$-sparse vectors, potentially resulting in more sparse weight matrices. We then leverage a stochastic Frank-Wolfe (SFW) algorithm to solve this new constrained optimization, which naturally leads to sparse weight updates each time. We further note an overlooked fact that existing DNN initializations were derived to enhance SGD training (e.g., avoid gradient explosion or collapse), but was unaligned with the challenges of training with SFW. We hence also present the first learning-based initialization scheme specifically for boosting SFW-based DNN training. Experiments on CIFAR-10 and Tiny-ImageNet datasets demonstrate that our new framework named $\\textbf{SFW-pruning}$ consistently achieves the state-of-the-art performance on various benchmark DNNs over a wide range of pruning ratios. Moreover, SFW-pruning only needs to train once on the same model and dataset, for obtaining arbitrary ratios, while requiring neither iterative pruning nor retraining. All codes will be released to the public. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lu|learning_pruningfriendly_networks_via_frankwolfe_oneshot_anysparsity_and_no_retraining", "pdf": "/pdf/c893710fa491c04dc86547df19635fae45a567c7.pdf", "one-sentence_summary": "We propose a novel and state-of-the-art one-shot pruning method, which can generate sparse networks at any pruning ratio in one pruning and without any retraining.", "data": "", "_bibtex": "@inproceedings{\nmiao2022learning,\ntitle={Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining},\nauthor={Miao Lu and Xiaolong Luo and Tianlong Chen and Wuyang Chen and Dong Liu and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=O1DEtITim__}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "qTHBE7E9iej", "original": "hreez6ow4-k", "number": 1618, "cdate": 1632875530236, "mdate": null, "ddate": null, "tcdate": 1632875530236, "tmdate": 1676330603557, "tddate": null, "forum": "qTHBE7E9iej", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning transferable motor skills with hierarchical latent mixture policies", "authorids": ["~Dushyant_Rao1", "~Fereshteh_Sadeghi3", "~Leonard_Hasenclever1", "~Markus_Wulfmeier1", "~Martina_Zambelli2", "~Giulia_Vezzani1", "~Dhruva_Tirumala1", "~Yusuf_Aytar1", "~Josh_Merel1", "~Nicolas_Heess1", "~raia_hadsell1"], "authors": ["Dushyant Rao", "Fereshteh Sadeghi", "Leonard Hasenclever", "Markus Wulfmeier", "Martina Zambelli", "Giulia Vezzani", "Dhruva Tirumala", "Yusuf Aytar", "Josh Merel", "Nicolas Heess", "raia hadsell"], "keywords": ["Robotics", "Reinforcement Learning", "Hierarchical", "Latent Variable Models", "Skills", "Transfer"], "abstract": "For robots operating in the real world, it is desirable to learn reusable abstract behaviours that can effectively be transferred across numerous tasks and scenarios.\nWe propose an approach to learn skills from data using a hierarchical mixture latent variable model.\nOur method exploits a multi-level hierarchy of both discrete and continuous latent variables, to model a discrete set of abstract high-level behaviours while allowing for variance in how they are executed.\nWe demonstrate in manipulation domains that the method can effectively cluster offline data into distinct, executable behaviours, while retaining the flexibility of a continuous latent variable model.\nThe resulting skills can be transferred to new tasks, unseen objects, and from state to vision-based policies, yielding significantly better sample efficiency and asymptotic performance compared to existing skill- and imitation-based methods.\nWe also perform further analysis showing how and when the skills are most beneficial: they encourage directed exploration to cover large regions of the state space relevant to the task, making them most effective in challenging sparse-reward settings.", "one-sentence_summary": "An approach to learn reusable and transferable skills from data via a hierarchical latent mixture policy, which can significantly improve sample efficiency and asymptotic performance on downstream RL tasks", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rao|learning_transferable_motor_skills_with_hierarchical_latent_mixture_policies", "pdf": "/pdf/da585a69d336f46f18b80d4a026fd3a7dcb40eae.pdf", "_bibtex": "@inproceedings{\nrao2022learning,\ntitle={Learning transferable motor skills with hierarchical latent mixture policies},\nauthor={Dushyant Rao and Fereshteh Sadeghi and Leonard Hasenclever and Markus Wulfmeier and Martina Zambelli and Giulia Vezzani and Dhruva Tirumala and Yusuf Aytar and Josh Merel and Nicolas Heess and raia hadsell},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qTHBE7E9iej}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "gPvB4pdu_Z", "original": "qVtp2W3oOAJ", "number": 1606, "cdate": 1632875529464, "mdate": null, "ddate": null, "tcdate": 1632875529464, "tmdate": 1676330603926, "tddate": null, "forum": "gPvB4pdu_Z", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Compositional Training for End-to-End Deep AUC Maximization", "authorids": ["~Zhuoning_Yuan1", "~Zhishuai_Guo1", "~Nitesh_Chawla1", "~Tianbao_Yang1"], "authors": ["Zhuoning Yuan", "Zhishuai Guo", "Nitesh Chawla", "Tianbao Yang"], "keywords": ["Compositional Training", "Imbalanced Losses", "AUC optimization", "Deep Learning"], "abstract": "Recently, deep AUC maximization (DAM) has achieved great success in different domains (e.g., medical image classification). However, the end-to-end training for deep AUC maximization still remains a challenging problem. Previous studies employ an ad-hoc  two-stage approach that first trains the network by optimizing a traditional  loss (e.g., cross-entropy loss) and then finetunes the network by optimizing an AUC loss. This is because that training a deep neural network from scratch by maximizing an AUC loss usually does not yield a satisfactory performance. This phenomenon can be attributed to the degraded feature representations learned by maximizing the AUC loss from scratch. To address this issue, we propose a novel compositional training framework for end-to-end DAM, namely compositional DAM. The key idea of compositional training is to minimize a compositional objective function, where the outer function corresponds to an AUC loss and the inner function represents  a gradient descent step for minimizing a traditional loss, e.g., the cross-entropy (CE) loss. To optimize the non-standard compositional objective, we propose an efficient and provable stochastic optimization algorithm. The proposed algorithm enhances the capabilities  of  both robust feature learning and robust classifier learning  by alternatively taking a gradient descent step for the CE loss and for the AUC loss in a systematic way.  We conduct extensive empirical studies on imbalanced benchmark and medical image datasets, which unanimously verify the effectiveness of the proposed method.  Our results show that the compositional training approach dramatically improves both the feature representations and the testing AUC score compared with traditional deep learning approaches, and yields better performance than the two-stage approaches for DAM as well. The proposed method is implemented in our open-sourced library LibAUC (https://www.libauc.org) and code is available at https://github.com/Optimization-AI/LibAUC.", "one-sentence_summary": "We propose a novel end-to-end training framework with a provable stochastic algorithm for deep AUC maximization. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "yuan|compositional_training_for_endtoend_deep_auc_maximization", "pdf": "/pdf/a32a2790db0163f6e6f71daa98631818c4713912.pdf", "supplementary_material": "", "data": "", "_bibtex": "@inproceedings{\nyuan2022compositional,\ntitle={Compositional Training for End-to-End Deep {AUC} Maximization},\nauthor={Zhuoning Yuan and Zhishuai Guo and Nitesh Chawla and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gPvB4pdu_Z}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "45Mr7LeKR9", "original": "0sy6kSpC9P", "number": 1604, "cdate": 1632875529328, "mdate": null, "ddate": null, "tcdate": 1632875529328, "tmdate": 1676330604149, "tddate": null, "forum": "45Mr7LeKR9", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Explanations of Black-Box Models based on Directional Feature Interactions", "authorids": ["~Aria_Masoomi1", "~Davin_Hill1", "~Zhonghui_Xu1", "~Craig_P_Hersh1", "~Edwin_K._Silverman1", "~Peter_J._Castaldi1", "~Stratis_Ioannidis1", "~Jennifer_Dy1"], "authors": ["Aria Masoomi", "Davin Hill", "Zhonghui Xu", "Craig P Hersh", "Edwin K. Silverman", "Peter J. Castaldi", "Stratis Ioannidis", "Jennifer Dy"], "keywords": ["Explainability", "Shapley values", "Interpretability", "Directional interaction", "feature interaction"], "abstract": "As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent.  Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature.  We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph.  Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features.  We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. We show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census, Divorce, Drug, and gene data.  ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "masoomi|explanations_of_blackbox_models_based_on_directional_feature_interactions", "pdf": "/pdf/861d5f0b89fc65fc7bd1c9d4a41c92e697f76061.pdf", "one-sentence_summary": "We introduce a bivariate explainer to explain directional feature interactions in black box models. ", "supplementary_material": "/attachment/18c104288296fe3b066533e675c6996c37bcc392.zip", "_bibtex": "@inproceedings{\nmasoomi2022explanations,\ntitle={Explanations of Black-Box Models based on Directional Feature Interactions},\nauthor={Aria Masoomi and Davin Hill and Zhonghui Xu and Craig P Hersh and Edwin K. Silverman and Peter J. Castaldi and Stratis Ioannidis and Jennifer Dy},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=45Mr7LeKR9}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "Fl3Mg_MZR-", "original": "staTW4qeekh", "number": 1589, "cdate": 1632875528340, "mdate": null, "ddate": null, "tcdate": 1632875528340, "tmdate": 1676330605085, "tddate": null, "forum": "Fl3Mg_MZR-", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning", "authorids": ["~Marc_Vischer1", "~Robert_Tjarko_Lange1", "h.sprekeler@tu-berlin.de"], "authors": ["Marc Vischer", "Robert Tjarko Lange", "Henning Sprekeler"], "keywords": ["Reinforcement Learning", "Sparsity", "Pruning", "Lottery Ticket Hypothesis"], "abstract": "The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets affected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the exploration-exploitation problem with supervised agents trained to imitate an expert. We show that feed-forward networks trained with behavioural cloning compared to reinforcement learning can be pruned to higher levels of sparsity without performance degradation. This suggests that in order to solve the RL-specific distributional shift agents require more degrees of freedom. Using a set of carefully designed baseline conditions, we find that the majority of the lottery ticket effect in both learning paradigms can be attributed to the identified mask rather than the weight initialization. The input layer mask selectively prunes entire input dimensions that turn out to be irrelevant for the task at hand. At a moderate level of sparsity the mask identified by iterative magnitude pruning yields minimal task-relevant representations, i.e., an interpretable inductive bias. Finally, we propose a simple initialization rescaling which promotes the robust identification of sparse task representations in low-dimensional control tasks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "vischer|on_lottery_tickets_and_minimal_task_representations_in_deep_reinforcement_learning", "pdf": "/pdf/8e8fd56ca3b46bba9d6db9d68f6fc7df8c828705.pdf", "one-sentence_summary": "We investigate the mechanisms underlying the lottery ticket effect in Deep RL and show that the derived mask extracts minimal task representations.", "_bibtex": "@inproceedings{\nvischer2022on,\ntitle={On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning},\nauthor={Marc Vischer and Robert Tjarko Lange and Henning Sprekeler},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Fl3Mg_MZR-}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "4AZz9osqrar", "original": "Hj16ZsRq01U", "number": 1578, "cdate": 1632875527579, "mdate": null, "ddate": null, "tcdate": 1632875527579, "tmdate": 1697934806956, "tddate": null, "forum": "4AZz9osqrar", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Self-supervised Learning is More Robust to Dataset Imbalance", "authorids": ["~Hong_Liu5", "~Jeff_Z._HaoChen1", "~Adrien_Gaidon1", "~Tengyu_Ma1"], "authors": ["Hong Liu", "Jeff Z. HaoChen", "Adrien Gaidon", "Tengyu Ma"], "keywords": ["self-supervised learning", "dataset imbalance", "representation learning", "long-tailed recognition"], "abstract": "Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we find via extensive experiments that off-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations. The performance gap between balanced and imbalanced pre-training with SSL is significantly smaller than the gap with supervised learning, across sample sizes, for both in-domain and, especially, out-of-domain evaluation. Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks. In contrast, supervised learning has no incentive to learn features irrelevant to the labels from frequent examples. We validate this hypothesis with semi-synthetic experiments as well as rigorous mathematical analyses on a simplified setting. Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that  consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liu|selfsupervised_learning_is_more_robust_to_dataset_imbalance", "pdf": "/pdf/8dbaf8d4a30f70cb8b4967ee6b1814c513bc92e6.pdf", "one-sentence_summary": "We show that self-supervised pre-training yields representations more robust to dataset imbalance, because it captures more diverse features from the frequent classes, and can be improved further by re-weighting regularization.", "supplementary_material": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/arxiv:2110.05025/code)", "_bibtex": "@inproceedings{\nliu2022selfsupervised,\ntitle={Self-supervised Learning is More Robust to Dataset Imbalance},\nauthor={Hong Liu and Jeff Z. HaoChen and Adrien Gaidon and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=4AZz9osqrar}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "apv504XsysP", "original": "ibNfD7HtSPF", "number": 1541, "cdate": 1632875525253, "mdate": null, "ddate": null, "tcdate": 1632875525253, "tmdate": 1676330608231, "tddate": null, "forum": "apv504XsysP", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions", "authorids": ["~Nicholas_Gao1", "~Stephan_G\u00fcnnemann1"], "authors": ["Nicholas Gao", "Stephan G\u00fcnnemann"], "keywords": ["Graph Neural Networks", "Computational Physics", "Self-Generative Learning", "Machine Learning for Science"], "abstract": "Solving the Schr\u00f6dinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at modelling wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this led to solutions on par with the best known classical methods. Still, these neural methods require tremendous amounts of computational resources as one has to train a separate model for each molecular geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to simultaneously solve the Schr\u00f6dinger equation for multiple geometries via VMC. This enables us to model continuous subsets of the potential energy surface with a single training pass. Compared to existing state-of-the-art networks, our Potential Energy Surface Network (PESNet) speeds up training for multiple geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to accurate and orders of magnitude cheaper quantum mechanical calculations.", "one-sentence_summary": "We introduce a PESNet, a new network architecture that solves the Schr\u00f6dinger equation for multiple geometries simultaneously.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gao|abinitio_potential_energy_surfaces_by_pairing_gnns_with_neural_wave_functions", "pdf": "/pdf/c55015744159581849683b350d34f68681b90315.pdf", "_bibtex": "@inproceedings{\ngao2022abinitio,\ntitle={Ab-Initio Potential Energy Surfaces by Pairing {GNN}s with Neural Wave Functions},\nauthor={Nicholas Gao and Stephan G{\\\"u}nnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=apv504XsysP}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "SidzxAb9k30", "original": "9CA56KuMki4", "number": 1485, "cdate": 1632875521607, "mdate": null, "ddate": null, "tcdate": 1632875521607, "tmdate": 1676330611178, "tddate": null, "forum": "SidzxAb9k30", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver", "authorids": ["~Xiaoyu_Chen2", "~Jiachen_Hu1", "~Lin_Yang12", "~Liwei_Wang1"], "authors": ["Xiaoyu Chen", "Jiachen Hu", "Lin Yang", "Liwei Wang"], "keywords": ["reward-free exploration", "model-based reinforcement learning", "learning theory"], "abstract": "Although model-based reinforcement learning (RL) approaches are considered more sample efficient, existing algorithms are usually relying on sophisticated planning algorithm to couple tightly with the model-learning procedure. Hence the learned models may lack the ability of being re-used with more specialized planners. In this paper we address this issue and provide approaches to learn an RL model efficiently without the guidance of a reward signal. In particular, we take a plug-in solver approach, where we focus on learning a model in the exploration phase and demand that \\emph{any planning algorithm} on the learned model can give a near-optimal policy. Specicially, we focus on the linear mixture MDP setting, where the probability transition matrix is a (unknown) convex combination of a set of existing models. We show that, by establishing a novel exploration algorithm, the plug-in approach learns a model by taking $\\tilde{O}(d^2H^3/\\epsilon^2)$ interactions with the environment and \\emph{any} $\\epsilon$-optimal planner on the model gives an $O(\\epsilon)$-optimal policy on the original model. This sample complexity matches lower bounds for non-plug-in approaches and is \\emph{statistically optimal}. We achieve this result by leveraging a careful maximum total-variance bound using Bernstein inequality and properties specified to linear mixture MDP.", "one-sentence_summary": "We propose near-optimal exploration algorithms for reward-free exploration with plug-in solver.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|nearoptimal_rewardfree_exploration_for_linear_mixture_mdps_with_plugin_solver", "pdf": "/pdf/91c23b5592d358e7283f6fdfe3f0cf6890b65e1c.pdf", "_bibtex": "@inproceedings{\nchen2022nearoptimal,\ntitle={Near-Optimal Reward-Free Exploration for Linear Mixture {MDP}s with Plug-in Solver},\nauthor={Xiaoyu Chen and Jiachen Hu and Lin Yang and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=SidzxAb9k30}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "MEpKGLsY8f", "original": "ijRXcyrDofc", "number": 1474, "cdate": 1632875520831, "mdate": null, "ddate": null, "tcdate": 1632875520831, "tmdate": 1676330611861, "tddate": null, "forum": "MEpKGLsY8f", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Meta Discovery: Learning to Discover Novel Classes given Very Limited Data", "authorids": ["~Haoang_Chi1", "~Feng_Liu2", "~Wenjing_Yang1", "~Long_Lan2", "~Tongliang_Liu1", "~Bo_Han1", "~Gang_Niu1", "~Mingyuan_Zhou1", "~Masashi_Sugiyama1"], "authors": ["Haoang Chi", "Feng Liu", "Wenjing Yang", "Long Lan", "Tongliang Liu", "Bo Han", "Gang Niu", "Mingyuan Zhou", "Masashi Sugiyama"], "keywords": [], "abstract": "In novel class discovery (NCD), we are given labeled data from seen classes and unlabeled data from unseen classes, and we train clustering models for the unseen classes. However, the implicit assumptions behind NCD are still unclear. In this paper, we demystify assumptions behind NCD and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, NCD is theoretically solvable under certain assumptions and can be naturally linked to meta-learning that has exactly the same assumption as NCD. Thus, we can empirically solve the NCD problem by meta-learning algorithms after slight modifications. This meta-learning-based methodology significantly reduces the amount of unlabeled data needed for training and makes it more practical, as demonstrated in experiments. The use of very limited data is also justified by the application scenario of NCD: since it is unnatural to label only seen-class data, NCD is sampling instead of labeling in causality. Therefore, unseen-class data should be collected on the way of collecting seen-class data, which is why they are novel and first need to be clustered.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chi|meta_discovery_learning_to_discover_novel_classes_given_very_limited_data", "pdf": "/pdf/ce36270eda861ce89f8998343017db1dff96ed19.pdf", "data": "", "_bibtex": "@inproceedings{\nchi2022meta,\ntitle={Meta Discovery: Learning to Discover Novel Classes given Very Limited Data},\nauthor={Haoang Chi and Feng Liu and Wenjing Yang and Long Lan and Tongliang Liu and Bo Han and Gang Niu and Mingyuan Zhou and Masashi Sugiyama},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MEpKGLsY8f}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "PRZoSmCinhf", "original": "84kUH34aDHi", "number": 1470, "cdate": 1632875520627, "mdate": null, "ddate": null, "tcdate": 1632875520627, "tmdate": 1697934819100, "tddate": null, "forum": "PRZoSmCinhf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Constrained Policy Optimization via Bayesian World Models", "authorids": ["~Yarden_As1", "~Ilnura_Usmanova1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["Yarden As", "Ilnura Usmanova", "Sebastian Curi", "Andreas Krause"], "keywords": ["Reinforcement learning", "Constrained Markov decision processes", "Constrained policy optimization", "Bayesian model-based RL"], "abstract": "Improving sample-efficiency and safety are crucial challenges when deploying reinforcement learning in high-stakes real world applications. We propose LAMBDA, a novel model-based approach for policy optimization in safety critical tasks modeled via constrained Markov decision processes. Our approach utilizes Bayesian world models, and harnesses the resulting uncertainty to maximize optimistic upper bounds on the task objective, as well as pessimistic upper bounds on the safety constraints. We demonstrate LAMBDA's state of the art performance on the Safety-Gym benchmark suite in terms of sample efficiency and constraint violation.", "one-sentence_summary": "Solving constrained Markov decision processes with Bayesian model-based reinforcement learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "as|constrained_policy_optimization_via_bayesian_world_models", "pdf": "/pdf/649d2990399ada19288169dd3031ecbb109a02aa.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2201.09802/code)", "_bibtex": "@inproceedings{\nas2022constrained,\ntitle={Constrained Policy Optimization via Bayesian World Models},\nauthor={Yarden As and Ilnura Usmanova and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PRZoSmCinhf}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "OIs3SxU5Ynl", "original": "d-lVAnfyLft", "number": 1451, "cdate": 1632875519388, "mdate": null, "ddate": null, "tcdate": 1632875519388, "tmdate": 1676330613247, "tddate": null, "forum": "OIs3SxU5Ynl", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "VAE Approximation Error: ELBO and Exponential Families", "authorids": ["~Alexander_Shekhovtsov1", "~Dmitrij_Schlesinger1", "~Boris_Flach1"], "authors": ["Alexander Shekhovtsov", "Dmitrij Schlesinger", "Boris Flach"], "keywords": [], "abstract": "The importance of Variational Autoencoders reaches far beyond standalone generative models -- the approach is also used for learning latent representations and can be generalized to semi-supervised learning. This requires a thorough analysis of their commonly known shortcomings: posterior collapse and approximation errors. This paper analyzes VAE approximation errors caused by the combination of the ELBO objective and encoder models from conditional exponential families, including, but not limited to, commonly used conditionally independent discrete and continuous models.\nWe characterize subclasses of generative models consistent with these encoder families. We show that the ELBO optimizer is pulled away from the likelihood optimizer towards the consistent subset and study this effect experimentally. Importantly, this subset can not be enlarged, and the respective error cannot be decreased, by considering deeper encoder/decoder networks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shekhovtsov|vae_approximation_error_elbo_and_exponential_families", "pdf": "/pdf/a5374002f74f6bc2b38c0470b1886b02536c628f.pdf", "one-sentence_summary": "VAEs have an inductive bias towards RBMs and generalized linear models", "_bibtex": "@inproceedings{\nshekhovtsov2022vae,\ntitle={{VAE} Approximation Error: {ELBO} and Exponential Families},\nauthor={Alexander Shekhovtsov and Dmitrij Schlesinger and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=OIs3SxU5Ynl}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 7}}, {"id": "CAjxVodl_v", "original": "r-uSNy75Qx", "number": 1408, "cdate": 1632875516555, "mdate": null, "ddate": null, "tcdate": 1632875516555, "tmdate": 1676330615855, "tddate": null, "forum": "CAjxVodl_v", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generalized Decision Transformer for Offline Hindsight Information Matching", "authorids": ["~Hiroki_Furuta1", "~Yutaka_Matsuo1", "~Shixiang_Shane_Gu1"], "authors": ["Hiroki Furuta", "Yutaka Matsuo", "Shixiang Shane Gu"], "keywords": ["Hindsight Information Matching", "Decision Transformer", "State-Marginal Matching", "Hindsight Experience Replay", "Reinforcement Learning"], "abstract": "How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications. Recent works have shown that using expressive policy function approximators and conditioning on future trajectory information -- such as future states in hindsight experience replay (HER) or returns-to-go in Decision Transformer (DT) -- enables efficient learning of multi-task policies, where at times online RL is fully replaced by offline behavioral cloning (BC), e.g. sequence modeling. We demonstrate that all these approaches are doing hindsight information matching (HIM) -- training policies that can output the rest of trajectory that matches some statistics of future state information. We present Generalized Decision Transformer (GDT) for solving any HIM problem, and show how different choices for the feature function and the anti-causal aggregator not only recover DT as a special case, but also lead to novel Categorical DT (CDT) and Bi-directional DT (BDT) for matching different statistics of the future. For evaluating CDT and BDT, we define offline multi-task state-marginal matching (SMM) and imitation learning (IL) as two generic HIM problems, propose a Wasserstein distance loss as a metric for both, and empirically study them on MuJoCo continuous control benchmarks. Categorical DT, which simply replaces anti-causal summation with anti-causal binning in DT, enables arguably the first effective offline multi-task SMM algorithm that generalizes well to unseen (and even synthetic) multi-modal reward or state-feature distributions. Bi-directional DT, which uses an anti-causal second transformer as the aggregator, can learn to model any statistics of the future and outperforms DT variants in offline multi-task IL, i.e. one-shot IL. Our generalized formulations from HIM and GDT greatly expand the role of powerful sequence modeling architectures in modern RL.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "furuta|generalized_decision_transformer_for_offline_hindsight_information_matching", "pdf": "/pdf/86d7058e78842b10462a9f0e0311ca3040adfe97.pdf", "one-sentence_summary": "We generalize hindsight algorithms in RL, and propose Distributional Decision Transformer for information matching.", "supplementary_material": "/attachment/f9a4c7bcae4543507a50a18992eba14bd3d7eab0.zip", "data": "", "_bibtex": "@inproceedings{\nfuruta2022generalized,\ntitle={Generalized Decision Transformer for Offline Hindsight Information Matching},\nauthor={Hiroki Furuta and Yutaka Matsuo and Shixiang Shane Gu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=CAjxVodl_v}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "1HxTO6CTkz", "original": "gvtmZ2jcFfJ", "number": 1370, "cdate": 1632875514021, "mdate": null, "ddate": null, "tcdate": 1632875514021, "tmdate": 1676330618173, "tddate": null, "forum": "1HxTO6CTkz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Unifying Likelihood-free Inference with Black-box Optimization and Beyond", "authorids": ["~Dinghuai_Zhang1", "~Jie_Fu2", "~Yoshua_Bengio1", "~Aaron_Courville3"], "authors": ["Dinghuai Zhang", "Jie Fu", "Yoshua Bengio", "Aaron Courville"], "keywords": ["biological sequence design", "black-box optimization", "likelihood-free inference", "Bayesian inference"], "abstract": "Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be \"reinvented\" in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design application illustrate the benefits of the proposed methodology.", "pdf": "/pdf/e2ec346ff6de5e9270bf7e826ba6ff87f1b8055b.pdf", "one-sentence_summary": "We propose a framework to unify likelihood-free inference and black-box sequence design and further propose novel sequence design algorithms based on the framework.", "supplementary_material": "/attachment/621cd50b3f74abec8bcd856fb3e5add524e31a94.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|unifying_likelihoodfree_inference_with_blackbox_optimization_and_beyond", "_bibtex": "@inproceedings{\nzhang2022unifying,\ntitle={Unifying Likelihood-free Inference with Black-box Sequence Design and Beyond},\nauthor={Dinghuai Zhang and Jie Fu and Yoshua Bengio and Aaron Courville},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1HxTO6CTkz}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "AJAR-JgNw__", "original": "PEuu3mFInv9", "number": 1367, "cdate": 1632875513820, "mdate": null, "ddate": null, "tcdate": 1632875513820, "tmdate": 1697934830828, "tddate": null, "forum": "AJAR-JgNw__", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting", "authorids": ["~Wei_Fan6", "~Shun_Zheng1", "xiaohan.yi@microsoft.com", "~Wei_Cao1", "~Yanjie_Fu2", "~Jiang_Bian1", "~Tie-Yan_Liu1"], "authors": ["Wei Fan", "Shun Zheng", "Xiaohan Yi", "Wei Cao", "Yanjie Fu", "Jiang Bian", "Tie-Yan Liu"], "keywords": [], "abstract": "Periodic time series (PTS) forecasting plays a crucial role in a variety of industries to foster critical tasks, such as early warning, pre-planning, resource scheduling, etc. However, the complicated dependencies of the PTS signal on its inherent periodicity as well as the sophisticated composition of various periods hinder the performance of PTS forecasting. In this paper, we introduce a deep expansion learning framework, DEPTS, for PTS forecasting. DEPTS starts with a decoupled formulation by introducing the periodic state as a hidden variable, which stimulates us to make two dedicated modules to tackle the aforementioned two challenges. First, we develop an expansion module on top of residual learning to perform a layer-by-layer expansion of those complicated dependencies. Second, we introduce a periodicity module with a parameterized periodic function that holds sufficient capacity to capture diversified periods. Moreover, our two customized modules also have certain interpretable capabilities, such as attributing the forecasts to either local momenta or global periodicity and characterizing certain core periodic properties, e.g., amplitudes and frequencies. Extensive experiments on both synthetic data and real-world data demonstrate the effectiveness of DEPTS on handling PTS. In most cases, DEPTS achieves significant improvements over the best baseline. Specifically, the error reduction can even reach up to 20% for a few cases. All codes for this paper are publicly available.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "fan|depts_deep_expansion_learning_for_periodic_time_series_forecasting", "pdf": "/pdf/cd132957a26c075bcbe5f26a96995eea829b38e0.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2203.07681/code)", "_bibtex": "@inproceedings{\nfan2022depts,\ntitle={{DEPTS}: Deep Expansion Learning for Periodic Time Series Forecasting},\nauthor={Wei Fan and Shun Zheng and Xiaohan Yi and Wei Cao and Yanjie Fu and Jiang Bian and Tie-Yan Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AJAR-JgNw__}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 13}}, {"id": "tBtoZYKd9n", "original": "4l2danJgDxy", "number": 1335, "cdate": 1632875511738, "mdate": null, "ddate": null, "tcdate": 1632875511738, "tmdate": 1676330620222, "tddate": null, "forum": "tBtoZYKd9n", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions", "authorids": ["~Leslie_O'Bray1", "~Max_Horn1", "~Bastian_Rieck1", "~Karsten_Borgwardt2"], "authors": ["Leslie O'Bray", "Max Horn", "Bastian Rieck", "Karsten Borgwardt"], "keywords": ["graph generative models", "model evaluation"], "abstract": "Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to evaluate and compare them. In this paper, we enumerate the desirable criteria for such a comparison metric and provide an overview of the status quo of graph generative model comparison in use today, which predominantly relies on the maximum mean discrepancy (MMD). We perform a systematic evaluation of MMD in the context of graph generative model comparison, highlighting some of the challenges and pitfalls researchers inadvertently may encounter. After conducting a thorough analysis of the behaviour of MMD on synthetically-generated perturbed graphs as well as on recently-proposed graph generative models, we are able to provide a suitable procedure to mitigate these challenges and pitfalls. We aggregate our findings into a list of practical recommendations for researchers to use when evaluating graph generative models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "obray|evaluation_metrics_for_graph_generative_models_problems_pitfalls_and_practical_solutions", "pdf": "/pdf/3af218144851b57d7d59c78ea79729bed4f8adba.pdf", "one-sentence_summary": "We investigate the potential pitfalls of using MMD to evaluate graph generative models and propose recommendations for the practitioner on how to mitigate those challenges.", "supplementary_material": "", "_bibtex": "@inproceedings{\no'bray2022evaluation,\ntitle={Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions},\nauthor={Leslie O'Bray and Max Horn and Bastian Rieck and Karsten Borgwardt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tBtoZYKd9n}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "tV3N0DWMxCg", "original": "E7QcIQTUSyE", "number": 1328, "cdate": 1632875511264, "mdate": null, "ddate": null, "tcdate": 1632875511264, "tmdate": 1676330620408, "tddate": null, "forum": "tV3N0DWMxCg", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions", "authorids": ["~Bertrand_Charpentier2", "~Oliver_Borchert1", "~Daniel_Z\u00fcgner1", "~Simon_Geisler1", "~Stephan_G\u00fcnnemann1"], "authors": ["Bertrand Charpentier", "Oliver Borchert", "Daniel Z\u00fcgner", "Simon Geisler", "Stephan G\u00fcnnemann"], "keywords": ["Uncertainty", "Exponential Family", "Bayesian Update", "Conjugate Prior"], "abstract": "Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, NatPN finds application for both classification and general regression settings. Unlike many previous approaches, NatPN does not require out-of-distribution (OOD) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and task-dependent latent space. For any input sample, NatPN uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, NatPN assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and OOD detection show that NatPN delivers highly competitive performance for classification, regression and count prediction tasks.", "pdf": "/pdf/c3ffe01a3bb574ad88e06692cb426a681a7c0f54.pdf", "supplementary_material": "", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "charpentier|natural_posterior_network_deep_bayesian_predictive_uncertainty_for_exponential_family_distributions", "data": "", "code": "", "_bibtex": "@inproceedings{\ncharpentier2022natural,\ntitle={Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions},\nauthor={Bertrand Charpentier and Oliver Borchert and Daniel Z{\\\"u}gner and Simon Geisler and Stephan G{\\\"u}nnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tV3N0DWMxCg}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}, {"id": "KxbhdyiPHE", "original": "-Dbynu2zD7z", "number": 1287, "cdate": 1632875508753, "mdate": null, "ddate": null, "tcdate": 1632875508753, "tmdate": 1676330622515, "tddate": null, "forum": "KxbhdyiPHE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards", "authorids": ["~Tim_Franzmeyer1", "~Mateusz_Malinowski1", "~Joao_F._Henriques1"], "authors": ["Tim Franzmeyer", "Mateusz Malinowski", "Joao F. Henriques"], "keywords": ["reinforcement learning", "altruistic behavior in AI", "multi-agent systems"], "abstract": "Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents' goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully; they might be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and allowing them to achieve their goals better. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach in three different multi-agent environments where another agent's success depends on altruistic behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "franzmeyer|learning_altruistic_behaviours_in_reinforcement_learning_without_external_rewards", "pdf": "/pdf/1af1674dc962e470709ff0dba09d61b75acd8daa.pdf", "one-sentence_summary": "We propose and investigate unsupervised training of agents to behave altruistically towards others by actively maximizing others' choice.", "supplementary_material": "/attachment/115544af1b40f9e23fc84107abf92e0f88c3a399.zip", "_bibtex": "@inproceedings{\nfranzmeyer2022learning,\ntitle={Learning Altruistic Behaviours in Reinforcement Learning without External Rewards},\nauthor={Tim Franzmeyer and Mateusz Malinowski and Joao F. Henriques},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KxbhdyiPHE}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "wQfgfb8VKTn", "original": "GDmbADSSlG", "number": 1272, "cdate": 1632875507791, "mdate": null, "ddate": null, "tcdate": 1632875507791, "tmdate": 1697934839384, "tddate": null, "forum": "wQfgfb8VKTn", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Context-Aware Sparse Deep Coordination Graphs", "authorids": ["~Tonghan_Wang1", "~Liang_Zeng1", "~Weijun_Dong1", "~Qianlan_Yang1", "~Yang_Yu5", "~Chongjie_Zhang1"], "authors": ["Tonghan Wang", "Liang Zeng", "Weijun Dong", "Qianlan Yang", "Yang Yu", "Chongjie Zhang"], "keywords": ["Multi-agent reinforcement learning", "Sparse coordination graphs", "Deep coordination graphs"], "abstract": "Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies. We theoretically consolidate our method by proving that the smaller the variance of payoff functions is, the less likely action selection will change after removing the corresponding edge. Moreover, we propose to learn action representations to effectively reduce the influence of payoff functions' estimation errors on graph construction. To empirically evaluate our method, we present the Multi-Agent COordination (MACO) benchmark by collecting classic coordination problems in the literature, increasing their difficulty, and classifying them into different types. We carry out a case study and experiments on the MACO and StarCraft II micromanagement benchmark to demonstrate the dynamics of sparse graph learning, the influence of graph sparseness, and the learning performance of our method.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|contextaware_sparse_deep_coordination_graphs", "pdf": "/pdf/a4e94260f9a234c7a8d59eb139b8f31b32a87673.pdf", "one-sentence_summary": "We propose a novel method for learning sparse coordination graphs that can be theoretically justified and can significantly reduce communication overhead and improve learning performance of deep coordination graphs.", "supplementary_material": "/attachment/4ae39108a78e9b258a9cdfc44a1ad7ccf281cf63.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2106.02886/code)", "_bibtex": "@inproceedings{\nwang2022contextaware,\ntitle={Context-Aware Sparse Deep Coordination Graphs},\nauthor={Tonghan Wang and Liang Zeng and Weijun Dong and Qianlan Yang and Yang Yu and Chongjie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wQfgfb8VKTn}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "xDIvIqQ3DXD", "original": "OsV914RutyP", "number": 1267, "cdate": 1632875507512, "mdate": null, "ddate": null, "tcdate": 1632875507512, "tmdate": 1676330623654, "tddate": null, "forum": "xDIvIqQ3DXD", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the approximation properties of recurrent encoder-decoder architectures", "authorids": ["~Zhong_Li2", "~Haotian_Jiang1", "~Qianxiao_Li1"], "authors": ["Zhong Li", "Haotian Jiang", "Qianxiao Li"], "keywords": ["encoder-decoder", "recurrent neural networks", "approximation", "temporal product"], "abstract": "Encoder-decoder architectures have recently gained popularity in sequence to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working principles still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established theoretical results for RNNs in the linear setting, where approximation capabilities can be related to smoothness and memory of target temporal relationships. Here, we uncover that the encoder and decoder together form a particular \u201ctemporal product structure\u201d which determines the approximation efficiency. Moreover, the encoder-decoder architecture generalises RNNs with the capability to learn time-inhomogeneous relationships. Our results provide the theoretical understanding of approximation properties of the recurrent encoder-decoder architecture, which precisely characterises, in the considered setting, the types of temporal relationships that can be efficiently learned.", "one-sentence_summary": "Approximation properties of recurrent encoder-decoder architectures are given, where the formed temporal product structure further characterises temporal relationships able to be efficiently learned.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|on_the_approximation_properties_of_recurrent_encoderdecoder_architectures", "pdf": "/pdf/93c3702cdbb8429d512ae64ed57daf520f84137f.pdf", "_bibtex": "@inproceedings{\nli2022on,\ntitle={On the approximation properties of recurrent encoder-decoder architectures},\nauthor={Zhong Li and Haotian Jiang and Qianxiao Li},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=xDIvIqQ3DXD}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "Nfl-iXa-y7R", "original": "kr31vBz5LaZ", "number": 1260, "cdate": 1632875507039, "mdate": null, "ddate": null, "tcdate": 1632875507039, "tmdate": 1676330624033, "tddate": null, "forum": "Nfl-iXa-y7R", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models", "authorids": ["~Beidi_Chen1", "~Tri_Dao1", "~Kaizhao_Liang1", "~Jiaming_Yang1", "~Zhao_Song6", "~Atri_Rudra1", "~Christopher_Re1"], "authors": ["Beidi Chen", "Tri Dao", "Kaizhao Liang", "Jiaming Yang", "Zhao Song", "Atri Rudra", "Christopher Re"], "keywords": ["Sparse training", "butterfly", "low-rank", "Lottery Tickets", "Block sparsity", "Hashing", "Transformer", "ViT", "MLP-Mixer"], "abstract": "Overparameterized neural networks generalize well but are expensive to train. Ideally one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is $3\\times$ faster than Butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.3$\\times$ faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 small with no drop in accuracy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|pixelated_butterfly_simple_and_efficient_sparse_training_for_neural_network_models", "pdf": "/pdf/ee0e47a9502622a5bc9e044424d6f3217c00bdf4.pdf", "one-sentence_summary": "We propose a simple sparse training method, which can speed up model training in wall-clock time with no drop in accuracy.", "supplementary_material": "/attachment/275ea7734810800046956d024c7f07459c71cc48.zip", "_bibtex": "@inproceedings{\nchen2022pixelated,\ntitle={Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},\nauthor={Beidi Chen and Tri Dao and Kaizhao Liang and Jiaming Yang and Zhao Song and Atri Rudra and Christopher Re},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Nfl-iXa-y7R}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "shpkpVXzo3h", "original": "E3eNvC5kBmb", "number": 1231, "cdate": 1632875505271, "mdate": null, "ddate": null, "tcdate": 1632875505271, "tmdate": 1697934843995, "tddate": null, "forum": "shpkpVXzo3h", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "8-bit Optimizers via Block-wise Quantization", "authorids": ["~Tim_Dettmers2", "~Mike_Lewis1", "~Sam_Shleifer1", "~Luke_Zettlemoyer1"], "authors": ["Tim Dettmers", "Mike Lewis", "Sam Shleifer", "Luke Zettlemoyer"], "keywords": ["language models", "pretraining", "finetuning", "GPU memory"], "abstract": "Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization significantly, compared to plain stochastic gradient descent, but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change.", "one-sentence_summary": "We develop 8-bit optimizers reduce the memory footprint of training and maintain 32-bit optimizer performance across NLP/CV benchmarks.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dettmers|8bit_optimizers_via_blockwise_quantization", "pdf": "/pdf/eae16788bf15e102fb9f104d044c6dff582683f4.pdf", "supplementary_material": "/attachment/831b28094d0b2ef84c6e475195348166159bd984.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.02861/code)", "_bibtex": "@inproceedings{\ndettmers2022bit,\ntitle={8-bit Optimizers via Block-wise Quantization},\nauthor={Tim Dettmers and Mike Lewis and Sam Shleifer and Luke Zettlemoyer},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=shpkpVXzo3h}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "yeP_zx9vqNm", "original": "WJlqU9on2d", "number": 1197, "cdate": 1632875503045, "mdate": null, "ddate": null, "tcdate": 1632875503045, "tmdate": 1676330627703, "tddate": null, "forum": "yeP_zx9vqNm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks", "authorids": ["annekh@mit.edu", "~Arturo_Deza1"], "authors": ["Anne Harrington", "Arturo Deza"], "keywords": ["Peripheral Computation", "Adversarial Robustness", "Perceptual Invariance", "Metamerism", "Texture", "Psychophysics"], "abstract": "Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such adversarially robust networks have also been shown to be more human perceptually-aligned than non-robust networks via image manipulations (Santurkar et al., 2019, Engstrom et al., 2019). Despite appearing closer to human visual perception, it is unclear if the constraints in robust DNN representations match biological constraints found in human vision. Human vision seems to rely on texture-based/summary statistic representations in the periphery, which have been shown to explain phenomena such as crowding (Balas et al., 2009) and performance on visual search tasks (Rosenholtz et al., 2012). To understand how adversarially robust optimizations/representations compare to human vision, we performed a psychophysics experiment using a metamer task similar to Freeman \\& Simoncelli, 2011, Wallis et al., 2016 and Deza et al., 2019 where we evaluated how well human observers could distinguish between images synthesized to match adversarially robust representations compared to non-robust representations and a texture synthesis model of peripheral vision (Texforms a la Long et al., 2018).  We found that the discriminability of robust representation and texture model images decreased to near chance performance as stimuli were presented farther in the periphery.  Moreover, performance on robust and texture-model images showed similar trends within participants, while performance on non-robust representations changed minimally across the visual field.  These results together suggest that (1) adversarially robust representations capture peripheral computation better than non-robust representations and (2) robust representations capture peripheral computation similar to current state-of-the-art texture peripheral vision models. More broadly, our findings support the idea that localized texture summary statistic representations may drive human invariance to adversarial perturbations and that the incorporation of such representations in DNNs could give rise to useful properties like adversarial robustness.", "pdf": "/pdf/7f2e10fe0e775d6b9a7ac2d2d46206fcefd3f1ca.pdf", "one-sentence_summary": "We suggest that the representations learned by an Adversarially Trained Network are aligned with Human Peripheral Computation", "supplementary_material": "/attachment/1811a2ca899eae0183454e1177acfcdadf2f587e.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "harrington|finding_biological_plausibility_for_adversarially_robust_features_via_metameric_tasks", "_bibtex": "@inproceedings{\nharrington2022finding,\ntitle={Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks},\nauthor={Anne Harrington and Arturo Deza},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=yeP_zx9vqNm}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "DmpCfq6Mg39", "original": "iSCkMm45Nmo", "number": 1181, "cdate": 1632875501956, "mdate": null, "ddate": null, "tcdate": 1632875501956, "tmdate": 1697934850129, "tddate": null, "forum": "DmpCfq6Mg39", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Omni-Dimensional Dynamic Convolution", "authorids": ["~Chao_Li16", "~Aojun_Zhou2", "~Anbang_Yao1"], "authors": ["Chao Li", "Aojun Zhou", "Anbang Yao"], "keywords": ["Convolutional Neural Networks", "Dynamic Convolution", "Attention", "Image Classification"], "abstract": "Learning a single static convolutional kernel in each convolutional layer is the common training paradigm of modern Convolutional Neural Networks (CNNs). Instead, recent research in dynamic convolution shows that learning a linear combination of n convolutional kernels weighted with their input-dependent attentions can significantly improve the accuracy of light-weight CNNs, while maintaining efficient inference. However, we observe that existing works endow convolutional kernels with the dynamic property through one dimension (regarding the convolutional kernel number) of the kernel space, but the other three dimensions (regarding the spatial size, the input channel number and the output channel number for each convolutional kernel) are overlooked. Inspired by this, we present Omni-dimensional Dynamic Convolution (ODConv), a more generalized yet elegant dynamic convolution design, to advance this line of research. ODConv leverages a novel multi-dimensional attention mechanism with a parallel strategy to learn complementary attentions for convolutional kernels along all four dimensions of the kernel space at any convolutional layer. As a drop-in replacement of regular convolutions, ODConv can be plugged into many CNN architectures. Extensive experiments on the ImageNet and MS-COCO datasets show that ODConv brings solid accuracy boosts for various prevailing CNN backbones including both light-weight and large ones, e.g., 3.77%~5.71%|1.86%~3.72% absolute top-1 improvements to MobivleNetV2|ResNet family on the ImageNet dataset. Intriguingly, thanks to its improved feature learning ability, ODConv with even one single kernel can compete with or outperform existing dynamic convolution counterparts with multiple kernels, substantially reducing extra parameters. Furthermore, ODConv is also superior to other attention modules for modulating the output features or the convolutional weights. Code and models will be available at https://github.com/OSVAI/ODConv.", "one-sentence_summary": "This paper presents Omni-dimensional Dynamic Convolution (ODConv) to advance the research in dynamic convolution.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "li|omnidimensional_dynamic_convolution", "pdf": "/pdf/7b2dd41d0729d79f0f22fac00e8ac757b46ff5a9.pdf", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2209.07947/code)", "_bibtex": "@inproceedings{\nli2022omnidimensional,\ntitle={Omni-Dimensional Dynamic Convolution},\nauthor={Chao Li and Aojun Zhou and Anbang Yao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=DmpCfq6Mg39}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "BjyvwnXXVn_", "original": "Advv2jAI2Op", "number": 1167, "cdate": 1632875500902, "mdate": null, "ddate": null, "tcdate": 1632875500902, "tmdate": 1676330629793, "tddate": null, "forum": "BjyvwnXXVn_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "EViT: Expediting Vision Transformers via Token Reorganizations", "authorids": ["~Youwei_Liang1", "~Chongjian_GE1", "~Zhan_Tong1", "~Yibing_Song1", "~Jue_Wang2", "~Pengtao_Xie3"], "authors": ["Youwei Liang", "Chongjian GE", "Zhan Tong", "Yibing Song", "Jue Wang", "Pengtao Xie"], "keywords": ["Vision Transformers", "multi-head self-attention", "efficient inference"], "abstract": "Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50% while its recognition accuracy is decreased by only 0.3% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit", "one-sentence_summary": "We propose to reorganize attentive tokens in Vision Transformers to expedite inference speed.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "liang|evit_expediting_vision_transformers_via_token_reorganizations", "pdf": "/pdf/feb0c5a2e1c1fc63509c2e528ca07aa95aea2d5e.pdf", "data": "", "_bibtex": "@inproceedings{\nliang2022evit,\ntitle={{EV}iT: Expediting Vision Transformers via Token Reorganizations},\nauthor={Youwei Liang and Chongjian GE and Zhan Tong and Yibing Song and Jue Wang and Pengtao Xie},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BjyvwnXXVn_}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "wENMvIsxNN", "original": "uDAaiCvENWW", "number": 1109, "cdate": 1632875497213, "mdate": null, "ddate": null, "tcdate": 1632875497213, "tmdate": 1676330633359, "tddate": null, "forum": "wENMvIsxNN", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "D-CODE: Discovering Closed-form ODEs from Observed Trajectories", "authorids": ["~Zhaozhi_Qian1", "~Krzysztof_Kacprzyk1", "~Mihaela_van_der_Schaar2"], "authors": ["Zhaozhi Qian", "Krzysztof Kacprzyk", "Mihaela van der Schaar"], "keywords": ["Symbolic Regression", "Ordinary Differential Equation"], "abstract": "For centuries, scientists have manually designed closed-form ordinary differential equations (ODEs) to model dynamical systems. An automated tool to distill closed-form ODEs from observed trajectories would accelerate the modeling process. Traditionally, symbolic regression is used to uncover a closed-form prediction function $a=f(b)$ with label-feature pairs $(a_i, b_i)$ as training examples. However, an ODE models the time derivative $\\dot{x}(t)$ of a dynamical system, e.g. $\\dot{x}(t) = f(x(t),t)$, and the \"label\" $\\dot{x}(t)$ is usually *not* observed. The existing ways to bridge this gap only perform well for a narrow range of settings with low measurement noise, frequent sampling, and non-chaotic dynamics. In this work, we propose the Discovery of Closed-form ODE framework (D-CODE), which advances symbolic regression beyond the paradigm of supervised learning. D-CODE leverages a novel objective function based on the variational formulation of ODEs to bypass the unobserved time derivative. For formal justification, we prove that this objective is a valid proxy for the estimation error of the true (but unknown) ODE. In the experiments, D-CODE successfully discovered the governing equations of a diverse range of dynamical systems under challenging measurement settings with high noise and infrequent sampling.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "qian|dcode_discovering_closedform_odes_from_observed_trajectories", "pdf": "/pdf/3a0bdc107d197bd18aa7299f8d8f198db2225d03.pdf", "_bibtex": "@inproceedings{\nqian2022dcode,\ntitle={D-{CODE}: Discovering Closed-form {ODE}s from Observed Trajectories},\nauthor={Zhaozhi Qian and Krzysztof Kacprzyk and Mihaela van der Schaar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wENMvIsxNN}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "w60btE_8T2m", "original": "-uyGPPYdZT", "number": 1092, "cdate": 1632875496063, "mdate": null, "ddate": null, "tcdate": 1632875496063, "tmdate": 1676330633890, "tddate": null, "forum": "w60btE_8T2m", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Spanning Tree-based Graph Generation for Molecules", "authorids": ["~Sungsoo_Ahn1", "~Binghong_Chen1", "~Tianzhe_Wang1", "~Le_Song1"], "authors": ["Sungsoo Ahn", "Binghong Chen", "Tianzhe Wang", "Le Song"], "keywords": ["molecule generation", "tree generation", "graph generation", "deep generative model", "de novo drug design"], "abstract": "In this paper, we explore the problem of generating molecules using deep neural networks, which has recently gained much interest in chemistry. To this end, we propose a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning tree and the residual edges. Such a formulation exploits the sparsity of molecular graphs and allows using compact tree-constructive operations to define the molecular graph connectivity. Based on the intermediate graph structure of the construction process, our framework can constrain its generation to molecular graphs that satisfy the chemical valence rules. We also newly design a Transformer architecture with tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework in metrics such as validity, Frechet ChemNet distance, and fragment similarity. We also demonstrate the usefulness of STGG in maximizing penalized LogP value of molecules.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ahn|spanning_treebased_graph_generation_for_molecules", "pdf": "/pdf/dcb1134d836d26dc8ef7d83683aa9f5b35964eae.pdf", "one-sentence_summary": "We propose a new molecular graph generative model based on compact tree constructive operators.", "supplementary_material": "/attachment/8391d3e3f9a797ffc025777a45388ccf498bb2c9.zip", "data": "", "_bibtex": "@inproceedings{\nahn2022spanning,\ntitle={Spanning Tree-based Graph Generation for Molecules},\nauthor={Sungsoo Ahn and Binghong Chen and Tianzhe Wang and Le Song},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=w60btE_8T2m}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "bERaNdoegnO", "original": "JAj57LmzRBs", "number": 1088, "cdate": 1632875495793, "mdate": null, "ddate": null, "tcdate": 1632875495793, "tmdate": 1676330634156, "tddate": null, "forum": "bERaNdoegnO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Policy improvement by planning with Gumbel", "authorids": ["~Ivo_Danihelka1", "~Arthur_Guez1", "~Julian_Schrittwieser1", "~David_Silver1"], "authors": ["Ivo Danihelka", "Arthur Guez", "Julian Schrittwieser", "David Silver"], "keywords": ["AlphaZero", "MuZero", "reinforcement learning"], "abstract": "AlphaZero is a powerful reinforcement learning algorithm based on approximate policy iteration and tree search. However, AlphaZero can fail to improve its policy network, if not visiting all actions at the root of a search tree. To address this issue, we propose a policy improvement algorithm based on sampling actions without replacement. Furthermore, we use the idea of policy improvement to replace the more heuristic mechanisms by which AlphaZero selects and uses actions, both at root nodes and at non-root nodes. Our new algorithms, Gumbel AlphaZero and Gumbel MuZero, respectively without and with model-learning, match the state of the art on Go, chess, and Atari, and significantly improve prior performance when planning with few simulations.", "one-sentence_summary": "We redesign AlphaZero to keep improving even when training with a small number of simulations.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "danihelka|policy_improvement_by_planning_with_gumbel", "pdf": "/pdf/4f2c0c813d0fbe127329c69b1ba216fbcd95d52c.pdf", "data": "", "_bibtex": "@inproceedings{\ndanihelka2022policy,\ntitle={Policy improvement by planning with Gumbel},\nauthor={Ivo Danihelka and Arthur Guez and Julian Schrittwieser and David Silver},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bERaNdoegnO}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "t8O-4LKFVx", "original": "Uz62Oxgj_H", "number": 1078, "cdate": 1632875495183, "mdate": null, "ddate": null, "tcdate": 1632875495183, "tmdate": 1697934860698, "tddate": null, "forum": "t8O-4LKFVx", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning Optimal Conformal Classifiers", "authorids": ["~David_Stutz1", "~Krishnamurthy_Dj_Dvijotham1", "~Ali_Taylan_Cemgil2", "~Arnaud_Doucet2"], "authors": ["David Stutz", "Krishnamurthy Dj Dvijotham", "Ali Taylan Cemgil", "Arnaud Doucet"], "keywords": ["conformal prediction", "conformal classification", "uncertainty estimation"], "abstract": "Modern deep learning based classifiers show very high accuracy on test data but this does not provide sufficient guarantees for safe deployment, especially in high-stake AI applications such as medical diagnosis. Usually, predictions are obtained without a reliable uncertainty estimate or a formal guarantee. Conformal prediction (CP) addresses these issues by using the classifier's predictions, e.g., its probability estimates, to predict confidence sets containing the true class with a user-specified probability. However, using CP as a separate processing step after training prevents the underlying model from adapting to the prediction of confidence sets. Thus, this paper explores strategies to differentiate through CP during training with the goal of training model with the conformal wrapper end-to-end. In our approach, conformal training (ConfTr), we specifically \"simulate\" conformalization on mini-batches during training. Compared to standard training, ConfTr reduces the average confidence set size (inefficiency) of state-of-the-art CP methods applied after training. Moreover, it allows to \"shape\" the confidence sets predicted at test time, which is difficult for standard CP. On experiments with several datasets, we show ConfTr can influence how inefficiency is distributed across classes, or guide the composition of confidence sets in terms of the included classes, while retaining the guarantees offered by CP.", "one-sentence_summary": "Conformal training allows to train classifier and conformal predictor end-to-end, optimizing average confidence set size (inefficiency) or other application-specific losses defined on confidence sets.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "stutz|learning_optimal_conformal_classifiers", "pdf": "/pdf/9fab4fe6695fabd7230072d001a43466d0e92499.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2110.09192/code)", "_bibtex": "@inproceedings{\nstutz2022learning,\ntitle={Learning Optimal Conformal Classifiers},\nauthor={David Stutz and Krishnamurthy Dj Dvijotham and Ali Taylan Cemgil and Arnaud Doucet},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=t8O-4LKFVx}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "9Vrb9D0WI4", "original": "UBz-IyxXYmu", "number": 1066, "cdate": 1632875494358, "mdate": null, "ddate": null, "tcdate": 1632875494358, "tmdate": 1697934862332, "tddate": null, "forum": "9Vrb9D0WI4", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "authorids": ["~Victor_Sanh1", "~Albert_Webson1", "~Colin_Raffel1", "~Stephen_Bach1", "~Lintang_Sutawika1", "~Zaid_Alyafeai1", "~Antoine_Chaffin1", "~Arnaud_Stiegler1", "~Arun_Raja1", "~Manan_Dey3", "~M_Saiful_Bari2", "~Canwen_Xu1", "~Urmish_Thakker1", "~Shanya_Sharma_Sharma1", "~Eliza_Szczechla1", "~Taewoon_Kim1", "~Gunjan_Chhablani1", "~Nihal_Nayak1", "~Debajyoti_Datta1", "~Jonathan_Chang2", "~Mike_Tian-Jian_Jiang1", "~Han_Wang9", "~Matteo_Manica1", "~Sheng_Shen2", "~Zheng_Xin_Yong1", "~Harshit_Pandey1", "~Rachel_Bawden1", "~Thomas_Wang1", "~Trishala_Neeraj1", "~Jos_Rozen1", "~Abheesht_Sharma1", "~Andrea_Santilli1", "~Thibault_Fevry1", "~Jason_Alan_Fries1", "~Ryan_Teehan1", "~Teven_Le_Scao1", "~Stella_Biderman1", "~Leo_Gao1", "~Thomas_Wolf1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Albert Webson", "Colin Raffel", "Stephen Bach", "Lintang Sutawika", "Zaid Alyafeai", "Antoine Chaffin", "Arnaud Stiegler", "Arun Raja", "Manan Dey", "M Saiful Bari", "Canwen Xu", "Urmish Thakker", "Shanya Sharma Sharma", "Eliza Szczechla", "Taewoon Kim", "Gunjan Chhablani", "Nihal Nayak", "Debajyoti Datta", "Jonathan Chang", "Mike Tian-Jian Jiang", "Han Wang", "Matteo Manica", "Sheng Shen", "Zheng Xin Yong", "Harshit Pandey", "Rachel Bawden", "Thomas Wang", "Trishala Neeraj", "Jos Rozen", "Abheesht Sharma", "Andrea Santilli", "Thibault Fevry", "Jason Alan Fries", "Ryan Teehan", "Teven Le Scao", "Stella Biderman", "Leo Gao", "Thomas Wolf", "Alexander M Rush"], "keywords": [], "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models\u2019 pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several datasets, often outperforming models 16\u00d7 its size. Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6\u00d7 its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "sanh|multitask_prompted_training_enables_zeroshot_task_generalization", "pdf": "/pdf/bec425b93713482f8e2de5d1d15b66ff95a47026.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/arxiv:2110.08207/code)", "_bibtex": "@inproceedings{\nsanh2022multitask,\ntitle={Multitask Prompted Training Enables Zero-Shot Task Generalization},\nauthor={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9Vrb9D0WI4}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "57PipS27Km", "original": "uKyGHqnpsa", "number": 1057, "cdate": 1632875493777, "mdate": null, "ddate": null, "tcdate": 1632875493777, "tmdate": 1697934863121, "tddate": null, "forum": "57PipS27Km", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Continuous-Time Meta-Learning with Forward Mode Differentiation", "authorids": ["~Tristan_Deleu1", "~David_Kanaa1", "~Leo_Feng1", "~Giancarlo_Kerg1", "~Yoshua_Bengio1", "~Guillaume_Lajoie1", "~Pierre-Luc_Bacon1"], "authors": ["Tristan Deleu", "David Kanaa", "Leo Feng", "Giancarlo Kerg", "Yoshua Bengio", "Guillaume Lajoie", "Pierre-Luc Bacon"], "keywords": ["meta-learning", "few-shot learning", "dynamical systems"], "abstract": "Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we  devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems.", "one-sentence_summary": "COMLN is a new meta-learning algorithm, where adaptation follows a gradient flow. It enables learning the amount of adaptation using SGD. We devise a novel efficient algorithm to compute the meta-gradients of COMLN, based on forward-mode diff.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "deleu|continuoustime_metalearning_with_forward_mode_differentiation", "pdf": "/pdf/68b03565ea73b881a10643d2e81e4ace23821ef2.pdf", "supplementary_material": "/attachment/85675731a0dbac5f83cca025b5b88c23416c44e3.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/arxiv:2203.01443/code)", "_bibtex": "@inproceedings{\ndeleu2022continuoustime,\ntitle={Continuous-Time Meta-Learning with Forward Mode Differentiation},\nauthor={Tristan Deleu and David Kanaa and Leo Feng and Giancarlo Kerg and Yoshua Bengio and Guillaume Lajoie and Pierre-Luc Bacon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=57PipS27Km}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "zXM0b4hi5_B", "original": "6VZZGa-sNZ2", "number": 1035, "cdate": 1632875492259, "mdate": null, "ddate": null, "tcdate": 1632875492259, "tmdate": 1697934865561, "tddate": null, "forum": "zXM0b4hi5_B", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the relation between statistical learning and perceptual distances", "authorids": ["~Alexander_Hepburn2", "~Valero_Laparra1", "~Raul_Santos-Rodriguez1", "~Johannes_Ball\u00e91", "~Jesus_Malo1"], "authors": ["Alexander Hepburn", "Valero Laparra", "Raul Santos-Rodriguez", "Johannes Ball\u00e9", "Jesus Malo"], "keywords": [], "abstract": "It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we find perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "hepburn|on_the_relation_between_statistical_learning_and_perceptual_distances", "pdf": "/pdf/12c717193deff83fed4cdbbc207d6c4ffebad63e.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2106.04427/code)", "_bibtex": "@inproceedings{\nhepburn2022on,\ntitle={On the relation between statistical learning and perceptual distances},\nauthor={Alexander Hepburn and Valero Laparra and Raul Santos-Rodriguez and Johannes Ball{\\'e} and Jesus Malo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zXM0b4hi5_B}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "vA7doMdgi75", "original": "1rJIjALi2Xe", "number": 1016, "cdate": 1632875491144, "mdate": null, "ddate": null, "tcdate": 1632875491144, "tmdate": 1676330638162, "tddate": null, "forum": "vA7doMdgi75", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension", "authorids": ["~Paris_Giampouras1", "~Benjamin_David_Haeffele1", "~Rene_Vidal1"], "authors": ["Paris Giampouras", "Benjamin David Haeffele", "Rene Vidal"], "keywords": ["representation learning", "robust subspace recovery", "dual principals component pursuit", "outliers", "model selection"], "abstract": "Robust subspace recovery (RSR) is the problem of learning a subspace from sample data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is a robust subspace recovery method that aims to find a basis for the orthogonal complement of the subspace by minimizing the sum of the distances of the points to the subspaces subject to orthogonality constraints on the basis. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers as long as the true dimension of the subspace is known. In this paper, we show that if the orthogonality constraints --adopted in previous DPCP formulations-- are relaxed and random initialization is used instead of spectral one, DPCP can provably recover a subspace of \\emph{unknown dimension}. Specifically, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach succeeds with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of the PSGM algorithm that allows us to perform RSR without knowing the subspace dimension", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "giampouras|implicit_bias_of_projected_subgradient_method_gives_provable_robust_recovery_of_subspaces_of_unknown_codimension", "pdf": "/pdf/4590e3755b5109113b2a58fd038313d6a8b091ec.pdf", "one-sentence_summary": "We study the robust subspace recovery problem when subspace codimension is unknown.", "_bibtex": "@inproceedings{\ngiampouras2022implicit,\ntitle={Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension},\nauthor={Paris Giampouras and Benjamin David Haeffele and Rene Vidal},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vA7doMdgi75}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 10}}, {"id": "MkTPtnjeYTV", "original": "ovgxxzYWk9X", "number": 1003, "cdate": 1632875490500, "mdate": null, "ddate": null, "tcdate": 1632875490500, "tmdate": 1676330638922, "tddate": null, "forum": "MkTPtnjeYTV", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "On the Optimal Memorization Power of ReLU Neural Networks", "authorids": ["~Gal_Vardi1", "~Gilad_Yehudai2", "~Ohad_Shamir1"], "authors": ["Gal Vardi", "Gilad Yehudai", "Ohad Shamir"], "keywords": ["Expressivness", "Memorization", "Theory", "VC-dimension", "Deep learning theory"], "abstract": "We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any $N$ points that satisfy a mild separability assumption using $\\tilde{O}\\left(\\sqrt{N}\\right)$ parameters. Known VC-dimension upper bounds imply that memorizing $N$ samples requires $\\Omega(\\sqrt{N})$ parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by $1 \\leq L \\leq \\sqrt{N}$, for memorizing $N$ samples using $\\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "vardi|on_the_optimal_memorization_power_of_relu_neural_networks", "pdf": "/pdf/6278a5d471f74abf6613e3926eac6ce535669473.pdf", "one-sentence_summary": "We show that ReLU neural networks can memorize N samples using \\sqrt{N} parameters, and prove that up to logarithmic terms this is the optimal solution.", "_bibtex": "@inproceedings{\nvardi2022on,\ntitle={On the Optimal Memorization Power of Re{LU} Neural Networks},\nauthor={Gal Vardi and Gilad Yehudai and Ohad Shamir},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=MkTPtnjeYTV}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "6Tk2noBdvxt", "original": "l5dcby7QCJ-", "number": 904, "cdate": 1632875483797, "mdate": null, "ddate": null, "tcdate": 1632875483797, "tmdate": 1676330644638, "tddate": null, "forum": "6Tk2noBdvxt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Programmatic Reinforcement Learning without Oracles", "authorids": ["~Wenjie_Qiu1", "~He_Zhu4"], "authors": ["Wenjie Qiu", "He Zhu"], "keywords": ["Reinforcement Learning", "Programmatic Reinforcement Learning", "Compositional Reinforcement Learning", "Program Synthesis", "Differentiable Architecture Search"], "abstract": "Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable. The code of this work is available at https://github.com/RU-Automated-Reasoning-Group/pi-PRL.", "one-sentence_summary": "We present a differentiable program architecture search framework to synthesize interpretable, generalizable, and compositional programs for controlling reinforcement learning applications.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "qiu|programmatic_reinforcement_learning_without_oracles", "pdf": "/pdf/92dbdb48fe9a64f9e46e509762a9443b84450f68.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nqiu2022programmatic,\ntitle={Programmatic Reinforcement Learning without Oracles},\nauthor={Wenjie Qiu and He Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=6Tk2noBdvxt}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 19}}, {"id": "LtKcMgGOeLt", "original": "uivFlADAGJp", "number": 869, "cdate": 1632875481487, "mdate": null, "ddate": null, "tcdate": 1632875481487, "tmdate": 1676330646543, "tddate": null, "forum": "LtKcMgGOeLt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations", "authorids": ["~Xiangning_Chen1", "~Cho-Jui_Hsieh1", "~Boqing_Gong1"], "authors": ["Xiangning Chen", "Cho-Jui Hsieh", "Boqing Gong"], "keywords": ["Vision Transformers", "Optimization"], "abstract": "Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\\% and +11.0\\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at \\url{https://github.com/google-research/vision_transformer}.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chen|when_vision_transformers_outperform_resnets_without_pretraining_or_strong_data_augmentations", "pdf": "/pdf/97b8505eb7034c4bfaee9c7d480a9f605be6fea8.pdf", "code": "", "data": "", "_bibtex": "@inproceedings{\nchen2022when,\ntitle={When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations},\nauthor={Xiangning Chen and Cho-Jui Hsieh and Boqing Gong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LtKcMgGOeLt}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "01AMRlen9wJ", "original": "72IIqJn3RIQ", "number": 846, "cdate": 1632875479861, "mdate": null, "ddate": null, "tcdate": 1632875479861, "tmdate": 1676330647950, "tddate": null, "forum": "01AMRlen9wJ", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Online Hyperparameter Meta-Learning with Hypergradient Distillation", "authorids": ["~Hae_Beom_Lee1", "~Hayeon_Lee1", "~JaeWoong_Shin1", "~Eunho_Yang1", "~Timothy_Hospedales1", "~Sung_Ju_Hwang1"], "authors": ["Hae Beom Lee", "Hayeon Lee", "JaeWoong Shin", "Eunho Yang", "Timothy Hospedales", "Sung Ju Hwang"], "keywords": ["Hyperparameter Optimization", "Meta-learning"], "abstract": "Many gradient-based meta-learning methods assume a set of parameters that do not participate in inner-optimization, which can be considered as hyperparameters. Although such hyperparameters can be optimized using the existing gradient-based hyperparameter optimization (HO) methods, they suffer from the following issues. Unrolled differentiation methods do not scale well to high-dimensional hyperparameters or horizon length, Implicit Function Theorem (IFT) based methods are restrictive for online optimization, and short horizon approximations suffer from short horizon bias. In this work, we propose a novel HO method that can overcome these limitations, by approximating the second-order term with knowledge distillation. Specifically, we parameterize a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second-order term. Our method allows online optimization and also is scalable to the hyperparameter dimension and the horizon length. We demonstrate the effectiveness of our method on three different meta-learning methods and two benchmark datasets.", "pdf": "/pdf/e1a0b8a3fc38e7d1b03adaf5cd0f110f66568bab.pdf", "one-sentence_summary": "We propose a gradient-based hyperparameter optimization method based on the idea of knowledge distillation, which is fully online and applicable to high-dimensional hyperparameters.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lee|online_hyperparameter_metalearning_with_hypergradient_distillation", "supplementary_material": "/attachment/ddb5f727c5a4d99a2696b10da7fb220146875b4a.zip", "data": "", "_bibtex": "@inproceedings{\nlee2022online,\ntitle={Online Hyperparameter Meta-Learning with Hypergradient Distillation},\nauthor={Hae Beom Lee and Hayeon Lee and JaeWoong Shin and Eunho Yang and Timothy Hospedales and Sung Ju Hwang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=01AMRlen9wJ}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "LBvk4QWIUpm", "original": "OmItyQYr97l", "number": 833, "cdate": 1632875479110, "mdate": null, "ddate": null, "tcdate": 1632875479110, "tmdate": 1676330648546, "tddate": null, "forum": "LBvk4QWIUpm", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Tighter Sparse Approximation Bounds for ReLU Neural Networks", "authorids": ["~Carles_Domingo-Enrich1", "~Youssef_Mroueh1"], "authors": ["Carles Domingo-Enrich", "Youssef Mroueh"], "keywords": ["neural network", "two-layer", "infinite-width", "approximation", "sparse", "Radon transform", "Fourier transform", "ReLU"], "abstract": "A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski & Barron, 2018) provides bounds on the width $n$ of a ReLU two-layer neural network needed to approximate a function $f$ over the ball $\\mathcal{B}_R(\\mathbb{R}^d)$ up to error $\\epsilon$, when the Fourier based quantity $C_f = \\int_{\\mathbb{R}^d} \\|\\xi\\|^2 |\\hat{f}(\\xi)| \\ d\\xi$ is finite. More recently Ongie et al. (2019) used the Radon transform as a tool for analysis of infinite-width ReLU two-layer networks. In particular, they introduce the concept of Radon-based $\\mathcal{R}$-norms and show that a function defined on $\\mathbb{R}^d$ can be represented as an infinite-width two-layer neural network if and only if its $\\mathcal{R}$-norm is finite. In this work, we extend the framework of Ongie et al. (2019) and define similar Radon-based semi-norms ($\\mathcal{R}, \\mathcal{U}$-norms) such that a function admits an infinite-width neural network representation on a bounded open set $\\mathcal{U} \\subseteq \\mathbb{R}^d$ when its $\\mathcal{R}, \\mathcal{U}$-norm is finite. Building on this, we derive sparse (finite-width) neural network approximation bounds that refine those of Breiman (1993); Klusowski & Barron (2018). Finally, we show that infinite-width neural network representations on bounded open sets are not unique and study their structure, providing a functional view of mode connectivity.", "one-sentence_summary": "We show conditions under which a function can be represented by an infinite-width neural network on a bounded set, and refine sparse neural network approximation bounds.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "domingoenrich|tighter_sparse_approximation_bounds_for_relu_neural_networks", "pdf": "/pdf/26cbfd341c2fcbea04f1b531de54a26548f1ec2d.pdf", "supplementary_material": "/attachment/bbacd3a015771636301a0741071ec82b47787fc8.zip", "_bibtex": "@inproceedings{\ndomingo-enrich2022tighter,\ntitle={Tighter Sparse Approximation Bounds for Re{LU} Neural Networks},\nauthor={Carles Domingo-Enrich and Youssef Mroueh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LBvk4QWIUpm}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "vwj6aUeocyf", "original": "9I_MmXEEtVe", "number": 831, "cdate": 1632875478975, "mdate": null, "ddate": null, "tcdate": 1632875478975, "tmdate": 1697934885801, "tddate": null, "forum": "vwj6aUeocyf", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Long Expressive Memory for Sequence Modeling", "authorids": ["~T._Konstantin_Rusch1", "~Siddhartha_Mishra1", "~N._Benjamin_Erichson1", "~Michael_W._Mahoney1"], "authors": ["T. Konstantin Rusch", "Siddhartha Mishra", "N. Benjamin Erichson", "Michael W. Mahoney"], "keywords": ["sequence modeling", "long-term dependencies", "multiscale ordinary differential equations", "dynamical systems"], "abstract": "We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classification through dynamical systems prediction to speech recognition and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.", "one-sentence_summary": "A novel method for sequence modeling based on multiscale ODEs that is provably able to learn very long-term dependencies while being sufficiently expressive to outperform state-of-the-art recurrent sequence models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "rusch|long_expressive_memory_for_sequence_modeling", "pdf": "/pdf/af3e85f49d797e1d183908208759b1776c72eb5d.pdf", "supplementary_material": "/attachment/218cbc7f66a519c33253a5cb7fb586b057c9b1a5.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:2110.04744/code)", "_bibtex": "@inproceedings{\nrusch2022long,\ntitle={Long Expressive Memory for Sequence Modeling},\nauthor={T. Konstantin Rusch and Siddhartha Mishra and N. Benjamin Erichson and Michael W. Mahoney},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vwj6aUeocyf}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 17}}, {"id": "LzQQ89U1qm_", "original": "FsI_aGXXKVo", "number": 803, "cdate": 1632875477115, "mdate": null, "ddate": null, "tcdate": 1632875477115, "tmdate": 1676330650660, "tddate": null, "forum": "LzQQ89U1qm_", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy", "authorids": ["~Jiehui_Xu1", "~Haixu_Wu1", "~Jianmin_Wang1", "~Mingsheng_Long5"], "authors": ["Jiehui Xu", "Haixu Wu", "Jianmin Wang", "Mingsheng Long"], "keywords": ["Time series anomaly detection", "Transformers", "Anomaly attention", "Association discrepancy"], "abstract": "Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-the-art results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space & earth exploration, and water treatment.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xu|anomaly_transformer_time_series_anomaly_detection_with_association_discrepancy", "pdf": "/pdf/b3974b079de39a5b7e379db64e3fe6b27d3bc07f.pdf", "one-sentence_summary": "This paper detects time series anomalies from a new association-based dimension. We find an inherently normal-abnormal distinguishable evidence as Association Discrepancy. Co-designed with this evidence, our model achieves the SOTA on six benchmarks.", "_bibtex": "@inproceedings{\nxu2022anomaly,\ntitle={Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy},\nauthor={Jiehui Xu and Haixu Wu and Jianmin Wang and Mingsheng Long},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LzQQ89U1qm_}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 24}}, {"id": "TIdIXIpzhoI", "original": "zXemAkxQ0A4", "number": 769, "cdate": 1632875474828, "mdate": null, "ddate": null, "tcdate": 1632875474828, "tmdate": 1697934893309, "tddate": null, "forum": "TIdIXIpzhoI", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Progressive Distillation for Fast Sampling of Diffusion Models", "authorids": ["~Tim_Salimans1", "~Jonathan_Ho1"], "authors": ["Tim Salimans", "Jonathan Ho"], "keywords": ["Diffusion Models", "Generative Models", "fast sampling"], "abstract": "Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps, compared to models in the literature. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with (near) state-of-the-art samplers taking 1024 or 8192 steps, and are able to distill down to models taking as little as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.", "one-sentence_summary": "Diffusion models now need just 4 sampling steps to produce high quality samples.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "salimans|progressive_distillation_for_fast_sampling_of_diffusion_models", "pdf": "/pdf/3b30857a628099896b6123e85d6cf04c59abe77b.pdf", "supplementary_material": "/attachment/1cadccb57b83fb0be6ce5eedae57e5c115462506.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.00512/code)", "_bibtex": "@inproceedings{\nsalimans2022progressive,\ntitle={Progressive Distillation for Fast Sampling of Diffusion Models},\nauthor={Tim Salimans and Jonathan Ho},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=TIdIXIpzhoI}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "7gWSJrP3opB", "original": "XRQcVP2T8Nh", "number": 736, "cdate": 1632875472552, "mdate": null, "ddate": null, "tcdate": 1632875472552, "tmdate": 1676330654388, "tddate": null, "forum": "7gWSJrP3opB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "A General Analysis of Example-Selection for Stochastic Gradient Descent", "authorids": ["~Yucheng_Lu1", "~Si_Yi_Meng2", "~Christopher_De_Sa2"], "authors": ["Yucheng Lu", "Si Yi Meng", "Christopher De Sa"], "keywords": [], "abstract": "Training example order in SGD has long been known to affect convergence rate. Recent results show that accelerated rates are possible in a variety of cases for permutation-based sample orders, in which each example from the training set is used once before any example is reused. In this paper, we develop a broad condition on the sequence of examples used by SGD that is sufficient to prove tight convergence rates in both strongly convex and non-convex settings. We show that our approach suffices to recover, and in some cases improve upon, previous state-of-the-art analyses for four known example-selection schemes: (1) shuffle once, (2) random reshuffling, (3) random reshuffling with data echoing, and (4) Markov Chain Gradient Descent. Motivated by our theory, we propose two new example-selection approaches. First, using quasi-Monte-Carlo methods, we achieve unprecedented accelerated convergence rates for learning with data augmentation. Second, we greedily choose a fixed scan-order to minimize the metric used in our condition and show that we can obtain more accurate solutions from the same number of epochs of SGD. We conclude by empirically demonstrating the utility of our approach for both convex linear-model and deep learning tasks. Our code is available at: https://github.com/EugeneLYC/qmc-ordering.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lu|a_general_analysis_of_exampleselection_for_stochastic_gradient_descent", "pdf": "/pdf/f12e4d11665b4464f75c539a335e09880c3e3472.pdf", "_bibtex": "@inproceedings{\nlu2022a,\ntitle={A General Analysis of Example-Selection for Stochastic Gradient Descent},\nauthor={Yucheng Lu and Si Yi Meng and Christopher De Sa},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7gWSJrP3opB}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "WvOGCEAQhxl", "original": "oZA03Ae3tL", "number": 715, "cdate": 1632875471178, "mdate": null, "ddate": null, "tcdate": 1632875471178, "tmdate": 1676330655835, "tddate": null, "forum": "WvOGCEAQhxl", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Assessing Generalization of SGD via Disagreement", "authorids": ["~Yiding_Jiang2", "~Vaishnavh_Nagarajan3", "~Christina_Baek2", "~J_Zico_Kolter1"], "authors": ["Yiding Jiang", "Vaishnavh Nagarajan", "Christina Baek", "J Zico Kolter"], "keywords": ["Generalization", "Deep Learning", "Empirical Phenomenon", "Accuracy Estimation", "Stochastic Gradient Descent"], "abstract": "We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data. This builds on -- and is a stronger version of -- the observation in Nakkiran&Bansal 20, which requires the runs to be on separate training sets. We further theoretically show that this peculiar phenomenon arises from the well-calibrated nature of ensembles of SGD-trained models. This finding not only provides a simple empirical measure to directly predict the test error using unlabeled test data, but also establishes a new conceptual connection between generalization and calibration.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "jiang|assessing_generalization_of_sgd_via_disagreement", "pdf": "/pdf/c7e974d96a71d3095746891b113427e16a551bb3.pdf", "one-sentence_summary": "We provide a surprisingly simple technique to accurately estimate the test error of deep neural networks using unlabeled data and we prove that this works because SGD ensembles are naturally well-calibrated.", "data": "", "_bibtex": "@inproceedings{\njiang2022assessing,\ntitle={Assessing Generalization of {SGD} via Disagreement},\nauthor={Yiding Jiang and Vaishnavh Nagarajan and Christina Baek and J Zico Kolter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WvOGCEAQhxl}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 20}}, {"id": "YZHES8wIdE", "original": "1jUWEKROsx9", "number": 710, "cdate": 1632875470833, "mdate": null, "ddate": null, "tcdate": 1632875470833, "tmdate": 1676330656259, "tddate": null, "forum": "YZHES8wIdE", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning", "authorids": ["~Haichao_Zhang4", "~Wei_Xu13", "~Haonan_Yu5"], "authors": ["Haichao Zhang", "Wei Xu", "Haonan Yu"], "keywords": [], "abstract": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly,  since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.", "pdf": "/pdf/0e68ff1fa269567c6c6101685f2f721afcc5d0aa.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|generative_planning_for_temporally_coordinated_exploration_in_reinforcement_learning", "one-sentence_summary": "Temporally coordinated exploration in reinforcement learning using Generative Planning Method.", "_bibtex": "@inproceedings{\nzhang2022generative,\ntitle={Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning},\nauthor={Haichao Zhang and Wei Xu and Haonan Yu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YZHES8wIdE}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "Y4cs1Z3HnqL", "original": "YcJ8kSJYquH", "number": 694, "cdate": 1632875469677, "mdate": null, "ddate": null, "tcdate": 1632875469677, "tmdate": 1676330657588, "tddate": null, "forum": "Y4cs1Z3HnqL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning", "authorids": ["~Chenjia_Bai2", "~Lingxiao_Wang6", "~Zhuoran_Yang1", "~Zhi-Hong_Deng2", "~Animesh_Garg1", "~Peng_Liu5", "~Zhaoran_Wang1"], "authors": ["Chenjia Bai", "Lingxiao Wang", "Zhuoran Yang", "Zhi-Hong Deng", "Animesh Garg", "Peng Liu", "Zhaoran Wang"], "keywords": ["Pessimistic Bootstrapping", "Bootstrapped Q-functions", "Uncertainty Estimation", "Offline Reinforcement Learning"], "abstract": "Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic updates by penalizing the value function based on the estimated uncertainty. To tackle the extrapolating error, we further propose a novel OOD sampling method. We show that such OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms.", "one-sentence_summary": "We propose pessimistic bootstrapping as a purely uncertainty-driven algorithm for offline Reinforcement Learning.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bai|pessimistic_bootstrapping_for_uncertaintydriven_offline_reinforcement_learning", "pdf": "/pdf/cbca2d6ef1966129d7e95e65cae5bec0dc19f0ea.pdf", "data": "", "_bibtex": "@inproceedings{\nbai2022pessimistic,\ntitle={Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning},\nauthor={Chenjia Bai and Lingxiao Wang and Zhuoran Yang and Zhi-Hong Deng and Animesh Garg and Peng Liu and Zhaoran Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Y4cs1Z3HnqL}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 31}}, {"id": "dFbKQaRk15w", "original": "ApZ4rlbftzx", "number": 693, "cdate": 1632875469606, "mdate": null, "ddate": null, "tcdate": 1632875469606, "tmdate": 1697934904451, "tddate": null, "forum": "dFbKQaRk15w", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Equivariant Subgraph Aggregation Networks", "authorids": ["~Beatrice_Bevilacqua1", "~Fabrizio_Frasca1", "~Derek_Lim1", "~Balasubramaniam_Srinivasan1", "~Chen_Cai1", "~Gopinath_Balamurugan1", "~Michael_M._Bronstein1", "~Haggai_Maron1"], "authors": ["Beatrice Bevilacqua", "Fabrizio Frasca", "Derek Lim", "Balasubramaniam Srinivasan", "Chen Cai", "Gopinath Balamurugan", "Michael M. Bronstein", "Haggai Maron"], "keywords": ["Graph Neural Networks", "Expressive power", "Equivariance", "Weisfeiler-Leman"], "abstract": "Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "bevilacqua|equivariant_subgraph_aggregation_networks", "pdf": "/pdf/a9272500150ccf0f8fafbd6cb0a26e71c003663f.pdf", "one-sentence_summary": "We present a provably expressive graph learning framework based on representing graphs as multisets of subgraphs and processing them with an equivariant architecture.", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2110.02910/code)", "_bibtex": "@inproceedings{\nbevilacqua2022equivariant,\ntitle={Equivariant Subgraph Aggregation Networks},\nauthor={Beatrice Bevilacqua and Fabrizio Frasca and Derek Lim and Balasubramaniam Srinivasan and Chen Cai and Gopinath Balamurugan and Michael M. Bronstein and Haggai Maron},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=dFbKQaRk15w}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "D78Go4hVcxO", "original": "9froiBqaqxd", "number": 676, "cdate": 1632875468524, "mdate": null, "ddate": null, "tcdate": 1632875468524, "tmdate": 1697934906280, "tddate": null, "forum": "D78Go4hVcxO", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "How Do Vision Transformers Work?", "authorids": ["~Namuk_Park1", "~Songkuk_Kim1"], "authors": ["Namuk Park", "Songkuk Kim"], "keywords": ["vision transformer", "self-attention", "multi-head self-attention", "loss landscape"], "abstract": "The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.", "one-sentence_summary": "We show that (1) multi-head self-attentions (MSAs) for computer vision flatten the loss landscapes, (2) MSAs are low-pass filters as opposed to Convs, and (3) MSAs at the end of a stage significantly improve the accuracy.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "park|how_do_vision_transformers_work", "pdf": "/pdf/e293cbd49c33a4924e5b45932a342361dd9845cf.pdf", "supplementary_material": "/attachment/c639dac30096d33d22a89d8688679b49c5737d4e.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.06709/code)", "_bibtex": "@inproceedings{\npark2022how,\ntitle={How Do Vision Transformers Work?},\nauthor={Namuk Park and Songkuk Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=D78Go4hVcxO}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "kZ0UYdhqkNY", "original": "XAIMxpPihAS", "number": 670, "cdate": 1632875468163, "mdate": null, "ddate": null, "tcdate": 1632875468163, "tmdate": 1697934907030, "tddate": null, "forum": "kZ0UYdhqkNY", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Variational methods for simulation-based inference", "authorids": ["~Manuel_Gl\u00f6ckler1", "~Michael_Deistler1", "~Jakob_H._Macke1"], "authors": ["Manuel Gl\u00f6ckler", "Michael Deistler", "Jakob H. Macke"], "keywords": ["likelihood-free inference", "simulation-based inference", "variational inference", "neural density estimation"], "abstract": "We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efficient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulation-based inference while maintaining accuracy and flexibility, making it possible to tackle problems that were previously inaccessible.", "one-sentence_summary": "We combine likelihood-estimation with variational inference to achieve a scalable approach for simulation-based inference.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "gl\u00f6ckler|variational_methods_for_simulationbased_inference", "pdf": "/pdf/7f95763dfe526e3086de5dd3efa1b9b647be0253.pdf", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2203.04176/code)", "_bibtex": "@inproceedings{\ngl{\\\"o}ckler2022variational,\ntitle={Variational methods for simulation-based inference},\nauthor={Manuel Gl{\\\"o}ckler and Michael Deistler and Jakob H. Macke},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=kZ0UYdhqkNY}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "JprM0p-q0Co", "original": "WnSKkIAtdhj", "number": 590, "cdate": 1632875462593, "mdate": null, "ddate": null, "tcdate": 1632875462593, "tmdate": 1697934915157, "tddate": null, "forum": "JprM0p-q0Co", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Tackling the Generative Learning Trilemma with Denoising Diffusion GANs", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Arash Vahdat"], "keywords": [], "abstract": "A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$\\times$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "xiao|tackling_the_generative_learning_trilemma_with_denoising_diffusion_gans", "pdf": "/pdf/6570cfc5a990e77af23d8a4c6b934ac249ba4426.pdf", "one-sentence_summary": "To reduce the number of sampling steps in diffusion models, we propose to model the denoising distribution with conditional GANs. We show our model tackles the generative learning trilemma & achieves high sample quality, diversity & fast sampling.", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2112.07804/code)", "_bibtex": "@inproceedings{\nxiao2022tackling,\ntitle={Tackling the Generative Learning Trilemma with Denoising Diffusion {GAN}s},\nauthor={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=JprM0p-q0Co}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "yKIAXjkJc2F", "original": "G2ZMTO1VA3l", "number": 562, "cdate": 1632875460788, "mdate": null, "ddate": null, "tcdate": 1632875460788, "tmdate": 1697934918749, "tddate": null, "forum": "yKIAXjkJc2F", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Imbedding Deep Neural Networks", "authorids": ["~Andrew_Corbett1", "~Dmitry_Kangin1"], "authors": ["Andrew Corbett", "Dmitry Kangin"], "keywords": ["Neural ODEs", "Optimal Control", "Deep Neural Networks", "Invariant Imbedding"], "abstract": "Continuous-depth neural networks, such as Neural ODEs, have refashioned the understanding of residual neural networks in terms of non-linear vector-valued optimal control problems. The common solution is to use the adjoint sensitivity method to replicate a forward-backward pass optimisation problem. We propose a new approach which explicates the network's `depth' as a fundamental variable, thus reducing the problem to a system of forward-facing initial value problems. This new method is based on the principal of `Invariant Imbedding' for which we prove a general solution, applicable to all non-linear, vector-valued optimal control problems with both running and terminal loss.\nOur new architectures provide a tangible tool for inspecting the theoretical--and to a great extent unexplained--properties of network depth. They also constitute a resource of discrete implementations of Neural ODEs comparable to classes of imbedded residual neural networks. Through a series of experiments, we show the competitive performance of the proposed architectures for supervised learning and time series prediction. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "corbett|imbedding_deep_neural_networks", "pdf": "/pdf/2696810601b722fa25baf9dd6ed1280d43c1c474.pdf", "one-sentence_summary": "Invariant imbedding solution for (Bolza) optimal control problem derived and proved to yield new architectures of imbedded deep neural networks.", "supplementary_material": "/attachment/b7662cb006cd7b6ccc9cc25ba55205d37a43d8f2.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2202.00113/code)", "_bibtex": "@inproceedings{\ncorbett2022imbedding,\ntitle={Imbedding Deep Neural Networks},\nauthor={Andrew Corbett and Dmitry Kangin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=yKIAXjkJc2F}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "ZkC8wKoLbQ7", "original": "DdSp9LxAOI", "number": 560, "cdate": 1632875460639, "mdate": null, "ddate": null, "tcdate": 1632875460639, "tmdate": 1697934919011, "tddate": null, "forum": "ZkC8wKoLbQ7", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding and Preventing Capacity Loss in Reinforcement Learning", "authorids": ["~Clare_Lyle1", "~Mark_Rowland1", "~Will_Dabney1"], "authors": ["Clare Lyle", "Mark Rowland", "Will Dabney"], "keywords": ["Reinforcement learning", "representation learning"], "abstract": "The reinforcement learning (RL) problem is rife with sources of non-stationarity that can destabilize or inhibit learning progress.\nWe identify a key mechanism by which this occurs in agents using neural networks as function approximators: \\textit{capacity loss}, whereby networks trained to predict a sequence of target values lose their ability to quickly fit new functions over time.\nWe demonstrate that capacity loss occurs in a broad range of RL agents and environments, and is particularly damaging to learning progress in sparse-reward tasks. We then present a simple regularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon by regressing a subspace of features towards its value at initialization, improving performance over a state-of-the-art model-free algorithm in the Atari 2600 suite. Finally, we study how this regularization affects different notions of capacity and evaluate other mechanisms by which it may improve performance.", "one-sentence_summary": "We show that RL agents experience representation collapse in sparse reward environments and propose an auxiliary task that prevents this from happening and outperforms the state of the art on the Atari benchmark.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lyle|understanding_and_preventing_capacity_loss_in_reinforcement_learning", "pdf": "/pdf/4d5e54a26ae7558a8d6efeb0bcc7d5f6844aba2a.pdf", "supplementary_material": "/attachment/b137b21a1ef08a10027eb583d895e8a4e7300e3d.zip", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.09560/code)", "_bibtex": "@inproceedings{\nlyle2022understanding,\ntitle={Understanding and Preventing Capacity Loss in Reinforcement Learning},\nauthor={Clare Lyle and Mark Rowland and Will Dabney},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ZkC8wKoLbQ7}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "1JDiK_TbV4S", "original": "Y3Il565ocv", "number": 550, "cdate": 1632875459989, "mdate": null, "ddate": null, "tcdate": 1632875459989, "tmdate": 1676330665914, "tddate": null, "forum": "1JDiK_TbV4S", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration", "authorids": ["~Cian_Eastwood1", "~Ian_Mason1", "~Chris_Williams1", "~Bernhard_Sch\u00f6lkopf1"], "authors": ["Cian Eastwood", "Ian Mason", "Chris Williams", "Bernhard Sch\u00f6lkopf"], "keywords": ["Transfer learning", "dataset shift", "unsupervised domain adaptation", "source-free domain adaptation"], "abstract": "Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.\n", "one-sentence_summary": "We identify a type of domain shift which can be resolved by restoring the *same* features and address it in the source-free setting by using softly-binned histograms to cheaply and flexibly align the marginal feature distributions.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "eastwood|sourcefree_adaptation_to_measurement_shift_via_bottomup_feature_restoration", "pdf": "/pdf/aeb9d0c4c2be949d33ea83ddad8547e536405144.pdf", "supplementary_material": "/attachment/0c82fd39eca730bb05e176302ee6db836cf32787.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\neastwood2022sourcefree,\ntitle={Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration},\nauthor={Cian Eastwood and Ian Mason and Chris Williams and Bernhard Sch{\\\"o}lkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1JDiK_TbV4S}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "lnEaqbTJIRz", "original": "HqPu9XGQeXi", "number": 544, "cdate": 1632875459559, "mdate": null, "ddate": null, "tcdate": 1632875459559, "tmdate": 1676330666159, "tddate": null, "forum": "lnEaqbTJIRz", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design", "authorids": ["~Yoav_Levine1", "~Noam_Wies1", "~Daniel_Jannai1", "dan.nav@mail.huji.ac.il", "~Yedid_Hoshen3", "~Amnon_Shashua1"], "authors": ["Yoav Levine", "Noam Wies", "Daniel Jannai", "Dan Navon", "Yedid Hoshen", "Amnon Shashua"], "keywords": ["Language Modeling", "Pretraining", "Self-attention", "Transformers", "Expressivity", "Separation Rank", "Sentence Embeddings"], "abstract": "Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose ``kNN-Pretraining\": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities.\tThis theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations. ", "one-sentence_summary": "We prove that pertained LMs model stronger dependencies between sentences that were shown in same training example, thus indicating benefits of better informed \"pretraining example design\"", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "levine|the_inductive_bias_of_incontext_learning_rethinking_pretraining_example_design", "pdf": "/pdf/da15c2bd56f71f3d484f0da8f8b25aea19884b0a.pdf", "supplementary_material": "", "_bibtex": "@inproceedings{\nlevine2022the,\ntitle={The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design},\nauthor={Yoav Levine and Noam Wies and Daniel Jannai and Dan Navon and Yedid Hoshen and Amnon Shashua},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=lnEaqbTJIRz}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "AUGBfDIV9rL", "original": "sPBCkrkD8Ct", "number": 521, "cdate": 1632875458041, "mdate": null, "ddate": null, "tcdate": 1632875458041, "tmdate": 1676330667180, "tddate": null, "forum": "AUGBfDIV9rL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Emergent Communication at Scale", "authorids": ["~Rahma_Chaabouni1", "~Florian_Strub1", "~Florent_Altch\u00e91", "~Eugene_Tarassov1", "~Corentin_Tallec2", "~Elnaz_Davoodi2", "~Kory_Wallace_Mathewson1", "~Olivier_Tieleman1", "~Angeliki_Lazaridou1", "~Bilal_Piot1"], "authors": ["Rahma Chaabouni", "Florian Strub", "Florent Altch\u00e9", "Eugene Tarassov", "Corentin Tallec", "Elnaz Davoodi", "Kory Wallace Mathewson", "Olivier Tieleman", "Angeliki Lazaridou", "Bilal Piot"], "keywords": ["emergent communication", "multi-agent reinforcement learning", "representation learning"], "abstract": "Emergent communication aims for a better understanding of human language evolution and building more efficient representations. We posit that reaching these goals will require scaling up, in contrast to a significant amount of literature that focuses on setting up small-scale problems to tease out desired properties of the emergent languages. We focus on three independent aspects to scale up, namely the dataset, task complexity, and population size. We provide a first set of results for large populations solving complex tasks on realistic large-scale datasets, as well as an easy-to-use codebase to enable further experimentation. In more complex tasks and datasets, we find that RL training can become unstable, but responds well to established stabilization techniques.\nWe also identify the need for a different metric than topographic similarity, which does not correlate with the generalization performances when working with natural images. In this context, we probe ease-of-learnability and transfer methods to assess emergent languages. Finally, we observe that larger populations do not induce robust emergent protocols with high generalization performance, leading us to explore different ways to leverage population, through voting and imitation learning. ", "pdf": "/pdf/89135b1aa6a72139d3332a71501a3d5d792156a8.pdf", "one-sentence_summary": "This work argues the importance of scaling up the emergent communication framework and investigates the impact of three scaling up aspects, namely the dataset, task complexity, and population size.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "chaabouni|emergent_communication_at_scale", "data": "", "_bibtex": "@inproceedings{\nchaabouni2022emergent,\ntitle={Emergent Communication at Scale},\nauthor={Rahma Chaabouni and Florian Strub and Florent Altch{\\'e} and Eugene Tarassov and Corentin Tallec and Elnaz Davoodi and Kory Wallace Mathewson and Olivier Tieleman and Angeliki Lazaridou and Bilal Piot},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AUGBfDIV9rL}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 14}}, {"id": "vds4SNooOe", "original": "yxMg6dKdgk_", "number": 496, "cdate": 1632875456499, "mdate": null, "ddate": null, "tcdate": 1632875456499, "tmdate": 1676330668353, "tddate": null, "forum": "vds4SNooOe", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings", "authorids": ["~Jingchao_Ni1", "~Wei_Cheng1", "zchen@nec-labs.com", "takayoshi.asakura@nec.com", "tomoya-s@nec.com", "kato@renascience.co.jp", "~Haifeng_Chen1"], "authors": ["Jingchao Ni", "Wei Cheng", "Zhengzhang Chen", "Takayoshi Asakura", "Tomoya Soma", "Sho Kato", "Haifeng Chen"], "keywords": ["Deep learning", "represenation learning", "generative model"], "abstract": "Learning fine-grained embeddings is essential for extending the generalizability of models pre-trained on \"coarse\" labels (e.g., animals). It is crucial to fields for which fine-grained labeling (e.g., breeds of animals) is expensive, but fine-grained prediction is desirable, such as medicine. The dilemma necessitates adaptation of a \"coarsely\" pre-trained model to new tasks with a few \"finer-grained\" training labels. However, coarsely supervised pre-training tends to suppress intra-class variation, which is vital for cross-granularity adaptation. In this paper, we develop a training framework underlain by a novel superclass-conditional Gaussian mixture model (SCGM). SCGM imitates the generative process of samples from hierarchies of classes through latent variable modeling of the fine-grained subclasses. The framework is agnostic to the encoders and only adds a few distribution related parameters, thus is efficient, and flexible to different domains. The model parameters are learned end-to-end by maximum-likelihood estimation via a principled Expectation-Maximization algorithm. Extensive experiments on benchmark datasets and a real-life medical dataset indicate the effectiveness of our method.", "one-sentence_summary": "We propose a training framework characterized by a novel superclass conditional Gaussian mixture (SCGM) based generative model for learning fine-grained representations for cross-granularity adaptation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ni|superclassconditional_gaussian_mixture_model_for_learning_finegrained_embeddings", "pdf": "/pdf/6eed00839e8f02de0ecba5b42ac5ce892def5ac0.pdf", "supplementary_material": "/attachment/7a9cd85f955393d2fa93666f4430918796480e9e.zip", "data": "", "_bibtex": "@inproceedings{\nni2022superclassconditional,\ntitle={Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings},\nauthor={Jingchao Ni and Wei Cheng and Zhengzhang Chen and Takayoshi Asakura and Tomoya Soma and Sho Kato and Haifeng Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=vds4SNooOe}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "-ApAkox5mp", "original": "Z6rkT2uUFt", "number": 427, "cdate": 1632875451594, "mdate": null, "ddate": null, "tcdate": 1632875451594, "tmdate": 1676330671795, "tddate": null, "forum": "-ApAkox5mp", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models", "authorids": ["~Zaccharie_Ramzi1", "~Florian_Mannel1", "~Shaojie_Bai1", "~Jean-Luc_Starck1", "~Philippe_Ciuciu1", "~Thomas_Moreau2"], "authors": ["Zaccharie Ramzi", "Florian Mannel", "Shaojie Bai", "Jean-Luc Starck", "Philippe Ciuciu", "Thomas Moreau"], "keywords": ["implicit models", "bi-level optimization", "quasi-newton methods"], "abstract": "In recent years, implicit deep learning has emerged as a method to increase the depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models~(DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empirically study this approach in many settings, ranging from hyperparameter optimization to large Multiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the computational cost of the backward pass by up to two orders of magnitude. All this is achieved while retaining the excellent performance of the original models in hyperparameter optimization and on CIFAR, and giving encouraging and competitive results on ImageNet.", "one-sentence_summary": "Use the approximate Jacobian matrix computed in quasi-Newton methods to perform the inversion needed in the training of implicit models.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ramzi|shine_sharing_the_inverse_estimate_from_the_forward_pass_for_bilevel_optimization_and_implicit_models", "pdf": "/pdf/d20ddecab60635b82dae5e9ac637ccb24c8038fc.pdf", "supplementary_material": "/attachment/0a592ce985426dc970e28e82b2c98a393a0600eb.zip", "code": "", "data": "", "_bibtex": "@inproceedings{\nramzi2022shine,\ntitle={{SHINE}: {SH}aring the {IN}verse Estimate from the forward pass for bi-level optimization and implicit models},\nauthor={Zaccharie Ramzi and Florian Mannel and Shaojie Bai and Jean-Luc Starck and Philippe Ciuciu and Thomas Moreau},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-ApAkox5mp}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "ccWaPGl9Hq", "original": "R40rj26_Pn", "number": 411, "cdate": 1632875450559, "mdate": null, "ddate": null, "tcdate": 1632875450559, "tmdate": 1676330672573, "tddate": null, "forum": "ccWaPGl9Hq", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality", "authorids": ["~Jiawei_Huang3", "~Jinglin_Chen2", "~Li_Zhao1", "~Tao_Qin1", "~Nan_Jiang2", "~Tie-Yan_Liu1"], "authors": ["Jiawei Huang", "Jinglin Chen", "Li Zhao", "Tao Qin", "Nan Jiang", "Tie-Yan Liu"], "keywords": ["reinforcement learning theory", "deployment efficiency", "linear MDP"], "abstract": "Deployment efficiency is an important criterion for many real-world applications of reinforcement learning (RL). Despite the community's increasing interest, there lacks a formal theoretical formulation for the problem. In this paper, we propose such a formulation for deployment-efficient RL (DE-RL) from an ''optimization with constraints'' perspective: we are interested in exploring an MDP and obtaining a near-optimal policy within minimal \\emph{deployment complexity}, whereas in each deployment the policy can sample a large batch of data. Using finite-horizon linear MDPs as a concrete structural model, we reveal the fundamental limit in achieving deployment efficiency by establishing information-theoretic lower bounds, and provide algorithms that achieve the optimal deployment efficiency. Moreover, our formulation for DE-RL is flexible and can serve as a building block for other practically relevant settings; we give ''Safe DE-RL'' and ''Sample-Efficient DE-RL'' as two examples, which may be worth future investigation.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "huang|towards_deploymentefficient_reinforcement_learning_lower_bound_and_optimality", "pdf": "/pdf/a0405b77400aed6d75d6c20a20d9a5335d22d314.pdf", "one-sentence_summary": "We propose a formal theoretical formulation for depolyment-efficient reinforcement learning; establish lower bounds for deployment complexity and study near-optimal deployment-efficient algorithms in linear MDP setting.", "supplementary_material": "/attachment/d59eb9b4cb070a9c284e47968b83fb293a8da603.zip", "_bibtex": "@inproceedings{\nhuang2022towards,\ntitle={Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality},\nauthor={Jiawei Huang and Jinglin Chen and Li Zhao and Tao Qin and Nan Jiang and Tie-Yan Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ccWaPGl9Hq}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 12}}, {"id": "A3HHaEdqAJL", "original": "GAe4Lf8qENK", "number": 344, "cdate": 1632875445946, "mdate": null, "ddate": null, "tcdate": 1632875445946, "tmdate": 1676330676686, "tddate": null, "forum": "A3HHaEdqAJL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Task Relatedness-Based Generalization Bounds for Meta Learning", "authorids": ["~Jiechao_Guan2", "~Zhiwu_Lu1"], "authors": ["Jiechao Guan", "Zhiwu Lu"], "keywords": [], "abstract": "Supposing the $n$ training tasks and the new task are sampled from the same environment, traditional meta learning theory derives an error bound on the expected loss over the new task in terms of the empirical training loss, uniformly over the set of all hypothesis spaces. However, there is still little research on how the relatedness of these tasks can affect the full utilization of all $mn$ training data (with $m$ examples per task). In this paper, we propose to address this problem by defining a new notion of task relatedness according to the existence of the bijective transformation between two tasks. A novel generalization bound of $\\mathcal{O}(\\frac{1}{\\sqrt{mn}})$ for meta learning is thus derived by exploiting the proposed task relatedness. Moreover, when investigating a special branch of meta learning that involves representation learning with deep neural networks, we establish spectrally-normalized bounds for both classification and regression problems. Finally, we demonstrate that the relatedness requirement between two tasks is satisfied when the sample space possesses the completeness and separability properties, validating the rationality and applicability of our proposed task-relatedness measure.", "pdf": "/pdf/ebbc100be110ad3e6a5f5491100b967847d22082.pdf", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guan|task_relatednessbased_generalization_bounds_for_meta_learning", "_bibtex": "@inproceedings{\nguan2022task,\ntitle={Task Relatedness-Based Generalization Bounds for Meta Learning},\nauthor={Jiechao Guan and Zhiwu Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=A3HHaEdqAJL}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 5}}, {"id": "pFyXqxChZc", "original": "kGaB3qHMNPg", "number": 314, "cdate": 1632875443909, "mdate": null, "ddate": null, "tcdate": 1632875443909, "tmdate": 1676330677642, "tddate": null, "forum": "pFyXqxChZc", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "IntSGD: Adaptive Floatless Compression of Stochastic Gradients", "authorids": ["~Konstantin_Mishchenko1", "~Bokun_Wang2", "~Dmitry_Kovalev2", "~Peter_Richt\u00e1rik1"], "authors": ["Konstantin Mishchenko", "Bokun Wang", "Dmitry Kovalev", "Peter Richt\u00e1rik"], "keywords": ["optimization", "distributed optimization", "compression", "theory", "parallel training", "switchML"], "abstract": "We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by (Sapio et al., 2021), our IntSGD method is provably convergent and computationally cheaper as it estimates the scaling of vectors adaptively. Our theory shows that the iteration complexity of IntSGD matches that of SGD up to constant factors for both convex and non-convex, smooth and non-smooth functions, with and without overparameterization. Moreover, our algorithm can also be tailored for the popular all-reduce primitive and shows promising empirical performance.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "mishchenko|intsgd_adaptive_floatless_compression_of_stochastic_gradients", "pdf": "/pdf/fa05928174931d3f8e117dbba3da063cb29acbd2.pdf", "one-sentence_summary": "We propose the provably convergent and computationally cheap IntSGD algorithm for efficient distributed machine learning.", "supplementary_material": "/attachment/52186daf95dd017b7b6b9878730dd61a8bf0ebfc.zip", "_bibtex": "@inproceedings{\nmishchenko2022intsgd,\ntitle={Int{SGD}: Adaptive Floatless Compression of Stochastic Gradients},\nauthor={Konstantin Mishchenko and Bokun Wang and Dmitry Kovalev and Peter Richt{\\'a}rik},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pFyXqxChZc}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 11}}, {"id": "iLHOIDsPv1P", "original": "YLlJNjQUvxs", "number": 313, "cdate": 1632875443833, "mdate": null, "ddate": null, "tcdate": 1632875443833, "tmdate": 1697934943468, "tddate": null, "forum": "iLHOIDsPv1P", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "PAC-Bayes Information Bottleneck", "authorids": ["~Zifeng_Wang3", "~Shao-Lun_Huang3", "~Ercan_Engin_Kuruoglu1", "~Jimeng_Sun3", "~Xi_Chen21", "~Yefeng_Zheng2"], "authors": ["Zifeng Wang", "Shao-Lun Huang", "Ercan Engin Kuruoglu", "Jimeng Sun", "Xi Chen", "Yefeng Zheng"], "keywords": ["information bottleneck", "representation learning", "generalization"], "abstract": "Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem. However, no solution of IIW has ever been provided, which builds a barrier for further investigation of the IIW's property and its potential in practical deep learning. In this paper, we propose an algorithm for the efficient approximation of IIW. Then, we build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the fitting to compressing phase transition during NNs' training and the concrete connection between the IIW compression and the generalization. Besides, we verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, over-parameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "wang|pacbayes_information_bottleneck", "pdf": "/pdf/3c2adeb5d32bd783ea4cbafee0397d6e76f81ce7.pdf", "one-sentence_summary": "We propose a novel PAC-Bayes bound guided information bottleneck for understanding and enhancing deep representation learning.", "supplementary_material": "/attachment/6dd440805d8b2df78932fcf3fb4525bad404f5eb.zip", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2109.14509/code)", "_bibtex": "@inproceedings{\nwang2022pacbayes,\ntitle={{PAC}-Bayes Information Bottleneck},\nauthor={Zifeng Wang and Shao-Lun Huang and Ercan Engin Kuruoglu and Jimeng Sun and Xi Chen and Yefeng Zheng},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iLHOIDsPv1P}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "jXKKDEi5vJt", "original": "zC15qH8JeDc", "number": 249, "cdate": 1632875439250, "mdate": null, "ddate": null, "tcdate": 1632875439250, "tmdate": 1676330681038, "tddate": null, "forum": "jXKKDEi5vJt", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing", "authorids": ["~Sai_Praneeth_Karimireddy1", "~Lie_He1", "~Martin_Jaggi1"], "authors": ["Sai Praneeth Karimireddy", "Lie He", "Martin Jaggi"], "keywords": ["Federated learning", "Distributed learning", "Byzantine robust optimization", "Heterogeneity (Non-IID)"], "abstract": "In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to significant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the first to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.\n", "one-sentence_summary": "Byzantine-robust distributed learning with heterogeneous data distribution", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "karimireddy|byzantinerobust_learning_on_heterogeneous_datasets_via_bucketing", "pdf": "/pdf/def52b4b7685b406e3aa64994f6e04b41df63bdb.pdf", "supplementary_material": "/attachment/cc13add474cedff602c5984e31e80b1203484e96.zip", "data": "", "code": "", "_bibtex": "@inproceedings{\nkarimireddy2022byzantinerobust,\ntitle={Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing},\nauthor={Sai Praneeth Karimireddy and Lie He and Martin Jaggi},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=jXKKDEi5vJt}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 25}}, {"id": "AvcfxqRy4Y", "original": "E2bTMu2u2NI", "number": 227, "cdate": 1632875437968, "mdate": null, "ddate": null, "tcdate": 1632875437968, "tmdate": 1676330682360, "tddate": null, "forum": "AvcfxqRy4Y", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Understanding the Role of Self Attention for Efficient Speech Recognition", "authorids": ["~Kyuhong_Shim1", "~Jungwook_Choi1", "~Wonyong_Sung1"], "authors": ["Kyuhong Shim", "Jungwook Choi", "Wonyong Sung"], "keywords": ["transformer", "self attention", "speech recognition"], "abstract": "Self-attention (SA) is a critical component of Transformer neural networks that have succeeded in automatic speech recognition (ASR). In this paper, we analyze the role of SA in Transformer-based ASR models for not only understanding the mechanism of improved recognition accuracy but also lowering the computational complexity. We reveal that SA performs two distinct roles: phonetic and linguistic localization. Especially, we show by experiments that phonetic localization in the lower layers extracts phonologically meaningful features from speech and reduces the phonetic variance in the utterance for proper linguistic localization in the upper layers. From this understanding, we discover that attention maps can be reused as long as their localization capability is preserved. To evaluate this idea, we implement the layer-wise attention map reuse on real GPU platforms and achieve up to 1.96 times speedup in inference and 33% savings in training time with noticeably improved ASR performance for the challenging benchmark on LibriSpeech dev/test-other dataset.\n", "pdf": "/pdf/d785690241796686225be6fa4f299ba32712c574.pdf", "one-sentence_summary": "We analyze the role of self attention in Transformer-based speech recognition and present a practical technique to design a model that accelerates the inference and improve the performance.", "supplementary_material": "/attachment/e5bec4a4f19f93bf6a75046f866f76da9f215624.zip", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shim|understanding_the_role_of_self_attention_for_efficient_speech_recognition", "data": "", "_bibtex": "@inproceedings{\nshim2022understanding,\ntitle={Understanding the Role of Self Attention for Efficient Speech Recognition},\nauthor={Kyuhong Shim and Jungwook Choi and Wonyong Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=AvcfxqRy4Y}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 15}}, {"id": "8WawVDdKqlL", "original": "pXaAF5FM3h", "number": 215, "cdate": 1632875437132, "mdate": null, "ddate": null, "tcdate": 1632875437132, "tmdate": 1697934954468, "tddate": null, "forum": "8WawVDdKqlL", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Label Encoding for Regression Networks", "authorids": ["~Deval_Shah1", "~Zi_Yu_Xue1", "~Tor_Aamodt1"], "authors": ["Deval Shah", "Zi Yu Xue", "Tor Aamodt"], "keywords": ["Regression", "Label encoding", "Output codes"], "abstract": "Deep neural networks are used for a wide range of regression problems. However, there exists a significant gap in accuracy between specialized approaches and generic direct regression in which a network is trained by minimizing the squared or absolute error of output labels. Prior work has shown that solving a regression problem with a set of binary classifiers can improve accuracy by utilizing well-studied binary classification algorithms. We introduce binary-encoded labels (BEL), which generalizes the application of binary classification to regression by providing a framework for considering arbitrary multi-bit values when encoding target values. We identify desirable properties of suitable encoding and decoding functions used for the conversion between real-valued and binary-encoded labels based on theoretical and empirical study. These properties highlight a tradeoff between classification error probability and error-correction capabilities of label encodings. BEL can be combined with off-the-shelf task-specific feature extractors and trained end-to-end. We propose a series of sample encoding, decoding, and training loss functions for BEL and demonstrate they result in lower error than direct regression and specialized approaches while being suitable for a diverse set of regression problems, network architectures, and evaluation metrics. BEL achieves state-of-the-art accuracies for several regression benchmarks. Code is available at https://github.com/ubc-aamodt-group/BEL_regression.\n", "one-sentence_summary": "We propose binary-encoded labels (BEL) which improves regression by generalizing the application of binary classification. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "shah|label_encoding_for_regression_networks", "pdf": "/pdf/f9f19a0a8f967030ba6a2061f7e7c78524762ef7.pdf", "supplementary_material": "/attachment/19a93cadaf0cf76f826b30f92d720dbe009aa393.zip", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2212.01927/code)", "_bibtex": "@inproceedings{\nshah2022label,\ntitle={Label Encoding for Regression Networks},\nauthor={Deval Shah and Zi Yu Xue and Tor Aamodt},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8WawVDdKqlL}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 16}}, {"id": "zNHzqZ9wrRB", "original": "nCR-cl3vxGp", "number": 202, "cdate": 1632875436219, "mdate": null, "ddate": null, "tcdate": 1632875436219, "tmdate": 1676330683625, "tddate": null, "forum": "zNHzqZ9wrRB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Equivariant Transformers for Neural Network based Molecular Potentials", "authorids": ["~Philipp_Th\u00f6lke1", "~Gianni_De_Fabritiis1"], "authors": ["Philipp Th\u00f6lke", "Gianni De Fabritiis"], "keywords": ["Molecular Modeling", "Quantum Chemistry", "Attention", "Transformers"], "abstract": "The prediction of quantum mechanical properties is historically plagued by a trade-off between accuracy and speed. Machine learning potentials have previously shown great success in this domain, reaching increasingly better accuracy while maintaining computational efficiency comparable with classical force fields. In this work we propose TorchMD-NET, a novel equivariant Transformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1, and many QM9 targets in both accuracy and computational efficiency. Through an extensive attention weight analysis, we gain valuable insights into the black box predictor and show differences in the learned representation of conformers versus conformations sampled from molecular dynamics or normal modes. Furthermore, we highlight the importance of datasets including off-equilibrium conformations for the evaluation of molecular potentials.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "th\u00f6lke|equivariant_transformers_for_neural_network_based_molecular_potentials", "pdf": "/pdf/3567cf90fbb34ac3013bdb0c392a4d115421dec8.pdf", "one-sentence_summary": "We propose a novel equivariant Transformer architecture for the prediction of molecular potentials and provide insights into the molecular representation through extensive analysis of the model's attention weights.", "data": "", "_bibtex": "@inproceedings{\nth{\\\"o}lke2022equivariant,\ntitle={Equivariant Transformers for Neural Network based Molecular Potentials},\nauthor={Philipp Th{\\\"o}lke and Gianni De Fabritiis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zNHzqZ9wrRB}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 9}}, {"id": "9XhPLAjjRB", "original": "rZQp1vKvelM", "number": 172, "cdate": 1632875434007, "mdate": null, "ddate": null, "tcdate": 1632875434007, "tmdate": 1676330685190, "tddate": null, "forum": "9XhPLAjjRB", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "SGD Can Converge to Local Maxima", "authorids": ["~Liu_Ziyin1", "botao.li95@gmail.com", "~James_B_Simon1", "~Masahito_Ueda1"], "authors": ["Liu Ziyin", "Botao Li", "James B Simon", "Masahito Ueda"], "keywords": ["stochastic gradient descent", "saddle points", "convergence", "amsgrad", "deep learning"], "abstract": "Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning.", "one-sentence_summary": "We show that it can be common for SGD to converge to saddle points and maxima.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "ziyin|sgd_can_converge_to_local_maxima", "pdf": "/pdf/c054a998fd46a7c8498a497ba6b856c2e0532b6b.pdf", "_bibtex": "@inproceedings{\nziyin2022sgd,\ntitle={{SGD} Can Converge to Local Maxima},\nauthor={Liu Ziyin and Botao Li and James B Simon and Masahito Ueda},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9XhPLAjjRB}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 27}}, {"id": "H0oaWl6THa", "original": "a1eseyegUWI", "number": 158, "cdate": 1632875433092, "mdate": null, "ddate": null, "tcdate": 1632875433092, "tmdate": 1676330685781, "tddate": null, "forum": "H0oaWl6THa", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Hybrid Local SGD for Federated Learning with Heterogeneous Communications", "authorids": ["~Yuanxiong_Guo1", "~Ying_Sun5", "ruihu2017@gmail.com", "~Yanmin_Gong1"], "authors": ["Yuanxiong Guo", "Ying Sun", "Rui Hu", "Yanmin Gong"], "keywords": ["Federated Learning", "Communication Efficiency", "Heterogeneity", "Local SGD"], "abstract": "Communication is a key bottleneck in federated learning where a large number of edge devices collaboratively learn a model under the orchestration of a central server without sharing their own training data. While local SGD has been proposed to reduce the number of FL rounds and become the algorithm of choice for FL, its total communication cost is still prohibitive when each device needs to communicate with the remote server repeatedly for many times over bandwidth-limited networks. In light of both device-to-device (D2D) and device-to-server (D2S) cooperation opportunities in modern communication networks, this paper proposes a new federated optimization algorithm dubbed hybrid local SGD (HL-SGD) in FL settings where devices are grouped into a set of disjoint clusters with high D2D communication bandwidth. HL-SGD subsumes previous proposed algorithms such as local SGD and gossip SGD and enables us to strike the best balance between model accuracy and runtime. We analyze the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings. We also perform extensive experiments and show that the use of hybrid model aggregation via D2D and D2S communications in HL-SGD can largely speed up the training time of federated learning. ", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "guo|hybrid_local_sgd_for_federated_learning_with_heterogeneous_communications", "pdf": "/pdf/e765632eb816448e521a3fb0562e39c39651de07.pdf", "data": "", "_bibtex": "@inproceedings{\nguo2022hybrid,\ntitle={Hybrid Local {SGD} for Federated Learning with Heterogeneous Communications},\nauthor={Yuanxiong Guo and Ying Sun and Rui Hu and Yanmin Gong},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=H0oaWl6THa}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "EAy7C1cgE1L", "original": "ixbEe4Ra4Iv", "number": 64, "cdate": 1632875426095, "mdate": null, "ddate": null, "tcdate": 1632875426095, "tmdate": 1676330690621, "tddate": null, "forum": "EAy7C1cgE1L", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work", "authorids": ["~Adam_Dziedzic1", "~Muhammad_Ahmad_Kaleem1", "~Yu_Shen_Lu1", "~Nicolas_Papernot1"], "authors": ["Adam Dziedzic", "Muhammad Ahmad Kaleem", "Yu Shen Lu", "Nicolas Papernot"], "keywords": ["model extraction", "model stealing", "model functionality stealing", "proof-of-work", "adversarial machine learning", "trustworthy machine learning", "deep learning"], "abstract": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.", "one-sentence_summary": "We propose to make model extraction more difficult by requiring users to complete a callibrated proof-of-work before they can read predictions from a machine learning model exposed via a public API.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "dziedzic|increasing_the_cost_of_model_extraction_with_calibrated_proof_of_work", "pdf": "/pdf/c75fb6727a40e11e23ec143778cd6e178a0681f3.pdf", "supplementary_material": "/attachment/32bc06f0c03c140a3e8513c39087e341066fd727.zip", "_bibtex": "@inproceedings{\ndziedzic2022increasing,\ntitle={Increasing the Cost of Model Extraction with Calibrated Proof of Work},\nauthor={Adam Dziedzic and Muhammad Ahmad Kaleem and Yu Shen Lu and Nicolas Papernot},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EAy7C1cgE1L}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 26}}, {"id": "HFmAukZ-k-2", "original": "QaczyaAdgox", "number": 34, "cdate": 1632875423835, "mdate": null, "ddate": null, "tcdate": 1632875423835, "tmdate": 1676330691932, "tddate": null, "forum": "HFmAukZ-k-2", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks", "authorids": ["~Marten_Lienen1", "~Stephan_G\u00fcnnemann1"], "authors": ["Marten Lienen", "Stephan G\u00fcnnemann"], "keywords": ["spatio-temporal", "finite", "elements", "forecasting", "continuous", "partial", "differential", "equation", "PDE", "graph", "gnn", "time-series"], "abstract": "We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we derive a continuous-time model for the dynamics of the data via the finite element method. The resulting graph neural network estimates the instantaneous effects of the unknown dynamics on each cell in a meshing of the spatial domain. Our model can incorporate prior knowledge via assumptions on the form of the unknown PDE, which induce a structural bias towards learning specific processes. Through this mechanism, we derive a transport variant of our model from the convection equation and show that it improves the transfer performance to higher-resolution meshes on sea surface temperature and gas flow forecasting against baseline models representing a selection of spatio-temporal forecasting methods. A qualitative analysis shows that our model disentangles the data dynamics into their constituent parts, which makes it uniquely interpretable.", "one-sentence_summary": "A continuous-time graph neural network model for spatio-temporal forecasting that can structurally incorporate prior knowledge", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "lienen|learning_the_dynamics_of_physical_systems_from_sparse_observations_with_finite_element_networks", "pdf": "/pdf/ded4dea1523b16ecaee30b3bb0676b5556db6544.pdf", "_bibtex": "@inproceedings{\nlienen2022learning,\ntitle={Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks},\nauthor={Marten Lienen and Stephan G{\\\"u}nnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=HFmAukZ-k-2}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 18}}, {"id": "BnQhMqDfcKG", "original": "8d00f_s6PIk", "number": 4, "cdate": 1632875421715, "mdate": null, "ddate": null, "tcdate": 1632875421715, "tmdate": 1697934974595, "tddate": null, "forum": "BnQhMqDfcKG", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Probabilistic Implicit Scene Completion", "authorids": ["~Dongsu_Zhang1", "~Changwoon_Choi1", "~Inbum_Park1", "~Young_Min_Kim1"], "authors": ["Dongsu Zhang", "Changwoon Choi", "Inbum Park", "Young Min Kim"], "keywords": ["3D shape completion", "3D generative model"], "abstract": "We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible outcomes. We employ the Generative Cellular Automata that learns the multi-modal distribution and transform the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. We formally derive that our training objective for the sparse voxel embedding maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data. We also demonstrate that our approach outperforms deterministic models even in less ambiguous cases with a small amount of missing data, which infers that probabilistic formulation is crucial for high-quality geometry completion on input scans exhibiting any levels of completeness.", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "zhang|probabilistic_implicit_scene_completion", "pdf": "/pdf/d9aba3c3efb0ad334f7b6a755f29a0b39bc05867.pdf", "one-sentence_summary": "We propose a scalable generative model for multi-modal completion of 3D scenes in implicit representation.", "supplementary_material": "", "code": "", "data": "", "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2204.01264/code)", "_bibtex": "@inproceedings{\nzhang2022probabilistic,\ntitle={Probabilistic Implicit Scene Completion},\nauthor={Dongsu Zhang and Changwoon Choi and Inbum Park and Young Min Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=BnQhMqDfcKG}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 21}}, {"id": "7l1IjZVddDW", "original": "Athjrfng74k", "number": 1, "cdate": 1632875421364, "mdate": null, "ddate": null, "tcdate": 1632875421364, "tmdate": 1676330693128, "tddate": null, "forum": "7l1IjZVddDW", "replyto": null, "invitation": "ICLR.cc/2022/Conference/-/Blind_Submission", "content": {"title": "Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters", "authorids": ["~Qiang_Meng1", "~Feng_Zhou8", "~Hainan_Ren1", "~Tianshu_Feng1", "~Guochao_Liu1", "~Yuanqing_Lin3"], "authors": ["Qiang Meng", "Feng Zhou", "Hainan Ren", "Tianshu Feng", "Guochao Liu", "Yuanqing Lin"], "keywords": [], "abstract": "The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly  due to the particularity of the task, \\textit{i.e.},  broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo leads to more discriminative features. The proposed schemes are mathematically proved to be differential private, introduce a lightweight overhead as well as yield prominent performance boosts (\\textit{e.g.}, +9.63\\% and +10.26\\% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method.  ", "pdf": "/pdf/6e32e9b384f0d8b5260a2a95ad7645cebf50046f.pdf", "supplementary_material": "", "code_of_ethics": "", "submission_guidelines": "", "resubmission": "", "student_author": "", "serve_as_reviewer": "", "paperhash": "meng|improving_federated_learning_face_recognition_via_privacyagnostic_clusters", "_bibtex": "@inproceedings{\nmeng2022improving,\ntitle={Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters},\nauthor={Qiang Meng and Feng Zhou and Hainan Ren and Tianshu Feng and Guochao Liu and Yuanqing Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7l1IjZVddDW}\n}", "venue": "ICLR 2022 Spotlight", "venueid": "ICLR.cc/2022/Conference"}, "signatures": ["ICLR.cc/2022/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2022/Conference"], "pdate": 1643407560000, "odate": 1633539600000, "details": {"replyCount": 22}}], "count": 175}