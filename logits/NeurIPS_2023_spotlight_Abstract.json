{"notes": [{"id": "CAF4CnUblx", "forum": "CAF4CnUblx", "number": 15576, "cdate": 1683835114001, "tcdate": 1683835114001, "mdate": 1698949793410, "tmdate": 1698949793410, "signatures": ["NeurIPS.cc/2023/Conference/Submission15576/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15576/Authors"], "content": {"title": {"value": "Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable AI"}, "authors": {"value": ["Aditya Chattopadhyay", "Ryan Pilgrim", "Rene Vidal"]}, "authorids": {"value": ["~Aditya_Chattopadhyay1", "~Ryan_Pilgrim1", "~Rene_Vidal1"]}, "keywords": {"value": ["Information Maximization", "Sparse Coding", "Orthogonal Matching Pursuit", "Explainable AI", "Information Pursuit"]}, "abstract": {"value": "Information Pursuit (IP) is a classical active testing algorithm for predicting an output by sequentially and greedily querying the input in order of information gain. However, IP is computationally intensive since it involves estimating mutual information in high-dimensional spaces. This paper explores Orthogonal Matching Pursuit (OMP) as an alternative to IP for greedily selecting the queries. OMP is a classical signal processing algorithm for sequentially encoding a signal in terms of dictionary atoms chosen in order of correlation gain. In each iteration, OMP selects the atom that is most correlated with the signal residual (the signal minus its reconstruction thus far). Our first contribution is to establish a fundamental connection between IP and OMP, where we prove that IP with random projections of dictionary atoms as queries ``almost'' reduces to OMP, with the difference being that IP selects atoms in order of normalized correlation gain. We call this version IP-OMP and present simulations indicating that this difference does not have any appreciable effect on the sparse code recovery rate of IP-OMP compared to that of OMP for random Gaussian dictionaries. Inspired by this connection, our second contribution is to explore the utility of IP-OMP for generating explainable predictions, an area in which IP has recently gained traction. More specifically, we propose a simple explainable AI algorithm which encodes an image as a sparse combination of semantically meaningful dictionary atoms that are defined as text embeddings of interpretable concepts. The final prediction is made using the weights of this sparse combination, which serve as an explanation. Empirically, our proposed algorithm is not only competitive with existing explainability methods but also computationally less expensive."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/85c4c03ddf3bda422a872dc4110f7e55d0551002.pdf"}, "TLDR": {"value": "We show that the popular OMP algorithm can be derived from information-theoretic principles modulo a normalization factor. We then use this insight to design a computationally simple sparse-coding based explainable AI algorithm."}, "_bibtex": {"value": "@inproceedings{\nchattopadhyay2023information,\ntitle={Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable {AI}},\nauthor={Aditya Chattopadhyay and Ryan Pilgrim and Rene Vidal},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=CAF4CnUblx}\n}"}, "paperhash": {"value": "chattopadhyay|information_maximization_perspective_of_orthogonal_matching_pursuit_with_applications_to_explainable_ai"}}, "pdate": 1695326178872, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15576/-/Revision", "NeurIPS.cc/2023/Conference/Submission15576/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15576/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "odate": 1698949793398, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "YkBDJWerKg", "number": 15575, "cdate": 1683835108906, "tcdate": 1683835108906, "mdate": 1698949793411, "tmdate": 1698949793411, "signatures": ["NeurIPS.cc/2023/Conference/Submission15575/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15575/Authors"], "forum": "YkBDJWerKg", "content": {"title": {"value": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft"}, "authors": {"value": ["Shalev Lifshitz", "Keiran Paster", "Harris Chan", "Jimmy Ba", "Sheila A. McIlraith"]}, "authorids": {"value": ["shalev.lifshitz@mail.utoronto.ca", "~Keiran_Paster1", "~Harris_Chan1", "~Jimmy_Ba1", "~Sheila_A._McIlraith1"]}, "keywords": {"value": ["minecraft", "instruction following", "foundation models", "sequence models", "reinforcement learning", "sequential decision making", "goal conditioned reinforcement learning", "text conditioned reinforcement learning", "transformers", "deep learning"]}, "TLDR": {"value": "We train an instruction-following agent for Minecraft by finetuning VPT with both text and visual goals."}, "abstract": {"value": "Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL\u2022E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instruction following in Minecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We provide experimental evidence highlighting key factors for downstream performance, including pretraining, classifier-free guidance, and data scaling. All resources, including our model weights, training scripts, and evaluation tools are made available for further research."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3f03e161ea3e5af996e4365cd8c0a6254e8ec6c6.pdf"}, "_bibtex": {"value": "@inproceedings{\nlifshitz2023steve,\ntitle={{STEVE}-1: A Generative Model for Text-to-Behavior in Minecraft},\nauthor={Shalev Lifshitz and Keiran Paster and Harris Chan and Jimmy Ba and Sheila A. McIlraith},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=YkBDJWerKg}\n}"}, "paperhash": {"value": "lifshitz|steve1_a_generative_model_for_texttobehavior_in_minecraft"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15575/-/Revision", "NeurIPS.cc/2023/Conference/Submission15575/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15575/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326178821, "odate": 1698949793396, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "4KV2xLeqPN", "number": 15480, "cdate": 1683834582168, "tcdate": 1683834582168, "mdate": 1698949793194, "tmdate": 1698949793194, "signatures": ["NeurIPS.cc/2023/Conference/Submission15480/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15480/Authors"], "forum": "4KV2xLeqPN", "content": {"title": {"value": "On the Variance, Admissibility, and Stability of Empirical Risk Minimization"}, "authors": {"value": ["Gil Kur", "Eli Putterman", "Alexander Rakhlin"]}, "authorids": {"value": ["~Gil_Kur2", "~Eli_Putterman1", "~Alexander_Rakhlin1"]}, "keywords": {"value": ["empirical risk minimization", "bias-variance decomposition", "admissibility"]}, "TLDR": {"value": "We show that the variance of Empirical Risk Minimization enjoys the minimax rate in various settings."}, "abstract": {"value": "It is well known that Empirical Risk Minimization (ERM) may attain minimax suboptimal rates in terms of the mean squared error (Birg\u00e9 and Massart, 1993). In this paper, we prove that, under relatively mild assumptions, the suboptimality of ERM must be due to its bias. Namely, the variance error term of ERM (in terms of the bias and variance decomposition) enjoys the minimax rate. In the fixed design setting, we provide an elementary proof of this result using the probabilistic method. Then, we extend our proof to the random design setting for various models. In addition, we provide a simple proof of Chatterjee\u2019s admissibility theorem (Chatterjee, 2014, Theorem 1.4), which states that in the fixed design setting, ERM cannot be ruled out as an optimal method, and then we extend this result to the random design setting. We also show that our estimates imply stability of ERM, complementing the main result of Caponnetto and Rakhlin (2006) for non-Donsker classes. Finally, we highlight the somewhat irregular nature of the loss landscape of ERM in the non-Donsker regime, by showing that functions can be close to ERM, in terms of $L_2$ distance, while still being far from almost-minimizers of the empirical loss."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/23c02b08c192fae4fd2b6f452a0e2ef97a7e5baa.pdf"}, "supplementary_material": {"value": "/attachment/9b0fd40e5940bdb2d2fff768d706d3559c046494.pdf"}, "_bibtex": {"value": "@inproceedings{\nkur2023on,\ntitle={On the Variance, Admissibility, and Stability of Empirical Risk Minimization},\nauthor={Gil Kur and Eli Putterman and Alexander Rakhlin},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=4KV2xLeqPN}\n}"}, "paperhash": {"value": "kur|on_the_variance_admissibility_and_stability_of_empirical_risk_minimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15480/-/Revision", "NeurIPS.cc/2023/Conference/Submission15480/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15480/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326177410, "odate": 1698949793181, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "3PjCt4kmRx", "number": 15350, "cdate": 1683833656186, "tcdate": 1683833656186, "mdate": 1698954294537, "tmdate": 1698954294537, "signatures": ["NeurIPS.cc/2023/Conference/Submission15350/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15350/Authors"], "forum": "3PjCt4kmRx", "content": {"title": {"value": "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces"}, "authors": {"value": ["Peter Shaw", "Mandar Joshi", "James Cohan", "Jonathan Berant", "Panupong Pasupat", "Hexiang Hu", "Urvashi Khandelwal", "Kenton Lee", "Kristina Toutanova"]}, "authorids": {"value": ["~Peter_Shaw1", "~Mandar_Joshi1", "jamesfcohan@google.com", "~Jonathan_Berant1", "~Panupong_Pasupat1", "~Hexiang_Hu1", "~Urvashi_Khandelwal1", "~Kenton_Lee1", "~Kristina_Toutanova1"]}, "keywords": {"value": ["instruction following", "web tasks", "user interface tasks", "vision and language", "representation learning", "reinforcement learning", "imitation learning", "tree search", "language grounding", "web agents", "computer control"]}, "TLDR": {"value": "We study GUI-based instruction following with a general observation and action space consisting of pixel-based inputs and low-level actions."}, "abstract": {"value": "Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces.  This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use \u2014 via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a52a52e0e8e330a8e2a1416c429852ed72b1446a.pdf"}, "_bibtex": {"value": "@inproceedings{\nshaw2023from,\ntitle={From Pixels to {UI} Actions: Learning to Follow Instructions via Graphical User Interfaces},\nauthor={Peter Shaw and Mandar Joshi and James Cohan and Jonathan Berant and Panupong Pasupat and Hexiang Hu and Urvashi Khandelwal and Kenton Lee and Kristina Toutanova},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=3PjCt4kmRx}\n}"}, "paperhash": {"value": "shaw|from_pixels_to_ui_actions_learning_to_follow_instructions_via_graphical_user_interfaces"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15350/-/Revision", "NeurIPS.cc/2023/Conference/Submission15350/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15350/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326175228, "odate": 1698949792953, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "CSbGXyCswu", "number": 15213, "cdate": 1683832606392, "tcdate": 1683832606392, "mdate": 1698949792543, "tmdate": 1698949792543, "signatures": ["NeurIPS.cc/2023/Conference/Submission15213/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15213/Authors"], "forum": "CSbGXyCswu", "content": {"title": {"value": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"}, "authors": {"value": ["Zeqiu Wu", "Yushi Hu", "Weijia Shi", "Nouha Dziri", "Alane Suhr", "Prithviraj Ammanabrolu", "Noah A. Smith", "Mari Ostendorf", "Hannaneh Hajishirzi"]}, "authorids": {"value": ["~Zeqiu_Wu1", "~Yushi_Hu1", "~Weijia_Shi1", "~Nouha_Dziri2", "~Alane_Suhr1", "~Prithviraj_Ammanabrolu1", "~Noah_A._Smith2", "~Mari_Ostendorf1", "~Hannaneh_Hajishirzi1"]}, "keywords": {"value": ["Language Model", "Reinforcement Learning with Human Feedback", "Long-Form Text Generation"]}, "abstract": {"value": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. \nReinforcement learning from human feedback (RLHF)---where human preference judgments on LM outputs are transformed into a learning signal---has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with this reward function leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a9c6f119c447d0b45293ea94d1080f35ff06afcb.pdf"}, "TLDR": {"value": "We introduce Fine-Grained RLHF, a framework that enables training and learning from fine-grained rewards: (1) density, providing a reward after each granular segment; and (2) mixing multiple reward models associated with different feedback types."}, "_bibtex": {"value": "@inproceedings{\nwu2023finegrained,\ntitle={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training},\nauthor={Zeqiu Wu and Yushi Hu and Weijia Shi and Nouha Dziri and Alane Suhr and Prithviraj Ammanabrolu and Noah A. Smith and Mari Ostendorf and Hannaneh Hajishirzi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=CSbGXyCswu}\n}"}, "paperhash": {"value": "wu|finegrained_human_feedback_gives_better_rewards_for_language_model_training"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15213/-/Revision", "NeurIPS.cc/2023/Conference/Submission15213/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15213/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326172749, "odate": 1698949792530, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "OZ7aImD4uQ", "number": 15165, "cdate": 1683832189630, "tcdate": 1683832189630, "mdate": 1698949792312, "tmdate": 1698949792312, "signatures": ["NeurIPS.cc/2023/Conference/Submission15165/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15165/Authors"], "forum": "OZ7aImD4uQ", "content": {"title": {"value": "Scale Alone Does not Improve Mechanistic Interpretability in Vision Models"}, "authors": {"value": ["Roland S. Zimmermann", "Thomas Klein", "Wieland Brendel"]}, "authorids": {"value": ["~Roland_S._Zimmermann1", "~Thomas_Klein1", "~Wieland_Brendel1"]}, "keywords": {"value": ["feature visualization", "interpretability", "explainability", "deep learning", "neural networks", "analysis", "activation maximization", "psychophysics"]}, "abstract": {"value": "In light of the recent widespread adoption of AI systems, understanding the internal information processing of neural networks has become increasingly critical. Most recently, machine vision has seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model size. We here ask whether this extraordinary increase in scale also positively impacts the field of mechanistic interpretability. In other words, has our understanding of the inner workings of scaled neural networks improved as well? We use a psychophysical paradigm to quantify one form of mechanistic interpretability for a diverse suite of nine models and find no scaling effect for interpretability - neither for model nor dataset size. Specifically, none of the investigated state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago. Latest-generation vision models appear even less interpretable than older architectures, hinting at a regression rather than improvement, with modern models sacrificing interpretability for accuracy. These results highlight the need for models explicitly designed to be mechanistically interpretable and the need for more helpful interpretability methods to increase our understanding of networks at an atomic level. We release a dataset containing more than 130'000 human responses from our psychophysical evaluation of 767 units across nine models. This dataset facilitates research on automated instead of human-based interpretability evaluations, which can ultimately be leveraged to directly optimize the mechanistic interpretability of models."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We compare the mechanistic interpretability of vision models differing scale, architecture, training paradigm and dataset size and find that none of these design choices have any significant effect on the interpretability of individual units."}, "pdf": {"value": "/pdf/72b0a93789ca2e8c88d973e811d46f723eaa54d8.pdf"}, "_bibtex": {"value": "@inproceedings{\nzimmermann2023scale,\ntitle={Scale Alone Does not Improve Mechanistic Interpretability in Vision Models},\nauthor={Roland S. Zimmermann and Thomas Klein and Wieland Brendel},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=OZ7aImD4uQ}\n}"}, "paperhash": {"value": "zimmermann|scale_alone_does_not_improve_mechanistic_interpretability_in_vision_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15165/-/Revision", "NeurIPS.cc/2023/Conference/Submission15165/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15165/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326171884, "odate": 1698949792297, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cRGINXQWem", "number": 15155, "cdate": 1683832127263, "tcdate": 1683832127263, "mdate": 1698949792210, "tmdate": 1698949792210, "signatures": ["NeurIPS.cc/2023/Conference/Submission15155/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15155/Authors"], "forum": "cRGINXQWem", "content": {"title": {"value": "Precise asymptotic generalization for multiclass classification with overparameterized linear models"}, "authors": {"value": ["David Xing Wu", "Anant Sahai"]}, "authorids": {"value": ["~David_Xing_Wu1", "~Anant_Sahai1"]}, "keywords": {"value": ["overparameterized", "multiclass", "classification", "theory", "generalization", "interpolation", "bi-level", "Gaussian model"]}, "abstract": {"value": "We study the asymptotic generalization of an overparameterized linear model for multiclass classification under the Gaussian covariates bi-level model introduced in Subramanian et al. (NeurIPS'22), where the number of data points, features, and classes all grow together. We fully resolve the conjecture posed in Subramanian et al. '22, matching the predicted regimes for which the model does and does not generalize. Furthermore, our new lower bounds are akin to an information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1 asymptotically. One surprising consequence of our tight results is that the min-norm interpolating classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime where the min-norm interpolating regressor is known to be optimal. \n\nThe key to our tight analysis is a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with sparse labels. As an application, we show that the same type of analysis can be used to analyze the related multi-label classification problem under the same bi-level ensemble."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d3b31b7fcd9567f5a7281bf08240d304f12a34de.pdf"}, "supplementary_material": {"value": "/attachment/b0657d4150010235306e8d1be743f9a89faebace.zip"}, "_bibtex": {"value": "@inproceedings{\nwu2023precise,\ntitle={Precise asymptotic generalization for multiclass classification with overparameterized linear models},\nauthor={David Xing Wu and Anant Sahai},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cRGINXQWem}\n}"}, "paperhash": {"value": "wu|precise_asymptotic_generalization_for_multiclass_classification_with_overparameterized_linear_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15155/-/Revision", "NeurIPS.cc/2023/Conference/Submission15155/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15155/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326171595, "odate": 1698949792196, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "u39QQh5L8Q", "number": 15007, "cdate": 1683830901273, "tcdate": 1683830901273, "mdate": 1698949791596, "tmdate": 1698949791596, "signatures": ["NeurIPS.cc/2023/Conference/Submission15007/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission15007/Authors"], "forum": "u39QQh5L8Q", "content": {"title": {"value": "Uncovering motifs of concurrent signaling across multiple neuronal populations"}, "authors": {"value": ["Evren Gokcen", "Anna Ivic Jasper", "Alison Xu", "Adam Kohn", "Christian K. Machens", "Byron M. Yu"]}, "authorids": {"value": ["~Evren_Gokcen1", "~Anna_Ivic_Jasper1", "~Alison_Xu1", "~Adam_Kohn1", "~Christian_K._Machens1", "~Byron_M._Yu1"]}, "keywords": {"value": ["neuroscience", "multi-population neural recordings", "dimensionality reduction", "latent variable models", "Gaussian processes"]}, "TLDR": {"value": "We developed a dimensionality reduction framework for characterizing the multi-dimensional, concurrent flow of signals across multiple neuronal populations."}, "abstract": {"value": "Modern recording techniques now allow us to record from distinct neuronal populations in different brain networks. However, especially as we consider multiple (more than two) populations, new conceptual and statistical frameworks are needed to characterize the multi-dimensional, concurrent flow of signals among these populations. Here, we develop a dimensionality reduction framework that determines (1) the subset of populations described by each latent dimension, (2) the direction of signal flow among those populations, and (3) how those signals evolve over time within and across experimental trials. We illustrate these features in simulation, and further validate the method by applying it to previously studied recordings from neuronal populations in macaque visual areas V1 and V2. Then we study interactions across select laminar compartments of areas V1, V2, and V3d, recorded simultaneously with multiple Neuropixels probes. Our approach uncovered signatures of selective communication across these three areas that related to their retinotopic alignment. This work advances the study of concurrent signaling across multiple neuronal populations."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/47ea21f513fbe9259accad7336a186edf6201da4.pdf"}, "supplementary_material": {"value": "/attachment/7476fc5108a27797b3a5494917caeffed89c3a5e.pdf"}, "_bibtex": {"value": "@inproceedings{\ngokcen2023uncovering,\ntitle={Uncovering motifs of concurrent signaling across multiple neuronal populations},\nauthor={Evren Gokcen and Anna Ivic Jasper and Alison Xu and Adam Kohn and Christian K. Machens and Byron M. Yu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=u39QQh5L8Q}\n}"}, "paperhash": {"value": "gokcen|uncovering_motifs_of_concurrent_signaling_across_multiple_neuronal_populations"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission15007/-/Revision", "NeurIPS.cc/2023/Conference/Submission15007/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission15007/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326167973, "odate": 1698949791576, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "DkeeXVdQyu", "number": 14973, "cdate": 1683830579584, "tcdate": 1683830579584, "mdate": 1698949791516, "tmdate": 1698949791516, "signatures": ["NeurIPS.cc/2023/Conference/Submission14973/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14973/Authors"], "forum": "DkeeXVdQyu", "content": {"title": {"value": "How to Scale Your EMA"}, "authors": {"value": ["Dan Busbridge", "Jason Ramapuram", "Pierre Ablin", "Tatiana Likhomanenko", "Eeshan Gunesh Dhekane", "Xavier Suau", "Russell Webb"]}, "authorids": {"value": ["~Dan_Busbridge1", "~Jason_Ramapuram1", "~Pierre_Ablin2", "~Tatiana_Likhomanenko1", "~Eeshan_Gunesh_Dhekane1", "~Xavier_Suau1", "~Russell_Webb1"]}, "keywords": {"value": ["Optimization", "scaling rules", "EMA", "exponential moving average", "self-supervised learning", "pseudo-labelling", "semi-supervised learning", "BYOL", "distillation", "speech", "vision"]}, "TLDR": {"value": "We derive and verify a scaling law for model exponential moving averages, allowing the systematic scaling of model classes depending on EMA, like distillation self-supervised methods."}, "abstract": {"value": "Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important machine learning tool is the model EMA, a functional copy of a target model, whose parameters move towards those of its target model according to an Exponential Moving Average (EMA) at a rate parameterized by a momentum hyperparameter. This model EMA can improve the robustness and generalization of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have not considered the optimization of the model EMA when performing scaling, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of a model EMA and demonstrate the rule's validity across a range of architectures, optimizers, and data modalities.  We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, a 6$\\times$ wall-clock time reduction under idealized hardware settings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/16a19affda7192ef0d6ac439a9862af2180de41d.pdf"}, "supplementary_material": {"value": "/attachment/4a314510198d73d940b8484317e24b460e3d44d9.pdf"}, "_bibtex": {"value": "@inproceedings{\nbusbridge2023how,\ntitle={How to Scale Your {EMA}},\nauthor={Dan Busbridge and Jason Ramapuram and Pierre Ablin and Tatiana Likhomanenko and Eeshan Gunesh Dhekane and Xavier Suau and Russell Webb},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=DkeeXVdQyu}\n}"}, "paperhash": {"value": "busbridge|how_to_scale_your_ema"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14973/-/Revision", "NeurIPS.cc/2023/Conference/Submission14973/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14973/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326167410, "odate": 1698949791501, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "89ia77nZ8u", "number": 14912, "cdate": 1683830212330, "tcdate": 1683830212330, "mdate": 1698949791327, "tmdate": 1698949791327, "signatures": ["NeurIPS.cc/2023/Conference/Submission14912/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14912/Authors"], "forum": "89ia77nZ8u", "content": {"title": {"value": "Towards Automated Circuit Discovery for Mechanistic Interpretability"}, "authors": {"value": ["Arthur Conmy", "Augustine N. Mavor-Parker", "Aengus Lynch", "Stefan Heimersheim", "Adri\u00e0 Garriga-Alonso"]}, "authorids": {"value": ["~Arthur_Conmy1", "~Augustine_N._Mavor-Parker1", "~Aengus_Lynch1", "~Stefan_Heimersheim1", "~Adri\u00e0_Garriga-Alonso1"]}, "keywords": {"value": ["Mechanistic Interpretability", "Pruning", "Science of Deep Learning", "AI Safety"]}, "TLDR": {"value": "We identify the common workflow for mechanistic interpretability work, and automate its \u201csystematic ablations\u201d step with a new algorithm, ACDC."}, "abstract": {"value": "Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of\ntransformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers\nchoose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which\nabstract neural network units are involved in the behavior. By varying the dataset, metric, and units under\ninvestigation, researchers can understand the functionality of each component.\n\nWe automate one of the process' steps: finding the connections between the abstract neural network units that form a circuit. We propose several algorithms and reproduce previous interpretability results to validate them. For\nexample, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the\nGreater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by\nprevious work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ca04797a35df32989953386d3892ecdf3aedc869.pdf"}, "_bibtex": {"value": "@inproceedings{\nconmy2023towards,\ntitle={Towards Automated Circuit Discovery for Mechanistic Interpretability},\nauthor={Arthur Conmy and Augustine N. Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adri{\\`a} Garriga-Alonso},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=89ia77nZ8u}\n}"}, "paperhash": {"value": "conmy|towards_automated_circuit_discovery_for_mechanistic_interpretability"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14912/-/Revision", "NeurIPS.cc/2023/Conference/Submission14912/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14912/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326166171, "odate": 1698949791316, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "qjnl1QUnFA", "number": 14752, "cdate": 1683828976897, "tcdate": 1683828976897, "mdate": 1698949790530, "tmdate": 1698949790530, "signatures": ["NeurIPS.cc/2023/Conference/Submission14752/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14752/Authors"], "forum": "qjnl1QUnFA", "content": {"title": {"value": "High-Fidelity Audio Compression with Improved RVQGAN"}, "authors": {"value": ["Rithesh Kumar", "Prem Seetharaman", "Alejandro Luebs", "Ishaan Kumar", "Kundan Kumar"]}, "authorids": {"value": ["~Rithesh_Kumar1", "~Prem_Seetharaman1", "alejandro@descript.com", "~Ishaan_Kumar2", "~Kundan_Kumar1"]}, "keywords": {"value": ["audio generation", "audio compression", "GAN", "audio", "speech"]}, "TLDR": {"value": "A method for neural audio compression that outperforms competing approaches in terms of audio quality, at much higher compression rates."}, "abstract": {"value": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/816db6078fa12108762348dd137a90cca5ddd839.pdf"}, "supplementary_material": {"value": "/attachment/9dc3a69306fcabed0fe426ac50b9c29269ab1a48.zip"}, "_bibtex": {"value": "@inproceedings{\nkumar2023highfidelity,\ntitle={High-Fidelity Audio Compression with Improved {RVQGAN}},\nauthor={Rithesh Kumar and Prem Seetharaman and Alejandro Luebs and Ishaan Kumar and Kundan Kumar},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=qjnl1QUnFA}\n}"}, "paperhash": {"value": "kumar|highfidelity_audio_compression_with_improved_rvqgan"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14752/-/Revision", "NeurIPS.cc/2023/Conference/Submission14752/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14752/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326162566, "odate": 1698949790515, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "zaQ7wV9NOg", "number": 14663, "cdate": 1683828212146, "tcdate": 1683828212146, "mdate": 1698949790134, "tmdate": 1698949790134, "signatures": ["NeurIPS.cc/2023/Conference/Submission14663/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14663/Authors"], "forum": "zaQ7wV9NOg", "content": {"title": {"value": "Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework  for Online RL"}, "authors": {"value": ["Qinghua Liu", "Gell\u00e9rt Weisz", "Andr\u00e1s Gy\u00f6rgy", "Chi Jin", "Csaba Szepesvari"]}, "authorids": {"value": ["~Qinghua_Liu1", "~Gell\u00e9rt_Weisz2", "~Andr\u00e1s_Gy\u00f6rgy2", "~Chi_Jin1", "~Csaba_Szepesvari1"]}, "keywords": {"value": ["Theory of reinforcement learning", "policy optimization"]}, "abstract": {"value": "While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited---they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especial in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework---Optimistic NPG for online RL. Optimistic NPG can be viewed as simply combining of the classic natural policy gradient (NPG) algorithm [Kakade, 2001]  with optimistic policy evaluation subroutines to encourage exploration. For $d$-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an $\\epsilon$-optimal policy within  $\\tilde{\\mathcal{O}}(d^2/\\epsilon^3)$ samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence $\\tilde{\\Theta}(d^2)$. It also improves over state-of-the-art results of policy optimization algorithms [Zanette et al., 2021] by a factor of $d$. For general function approximation that subsumes linear MDPs, Optimistic NPG, to our best knowledge, is also the first policy optimization algorithm that achieves the polynomial sample complexity for learning near-optimal policies."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/da3bc6aaa475dddeb5ef58f47d49d414756b7238.pdf"}, "_bibtex": {"value": "@inproceedings{\nliu2023optimistic,\ntitle={Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework  for Online {RL}},\nauthor={Qinghua Liu and Gell{\\'e}rt Weisz and Andr{\\'a}s Gy{\\\"o}rgy and Chi Jin and Csaba Szepesvari},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=zaQ7wV9NOg}\n}"}, "paperhash": {"value": "liu|optimistic_natural_policy_gradient_a_simple_efficient_policy_optimization_framework_for_online_rl"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14663/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14663/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326160397, "odate": 1698949790117, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "917crxqJdA", "number": 14650, "cdate": 1683828145319, "tcdate": 1683828145319, "mdate": 1698949790053, "tmdate": 1698949790053, "signatures": ["NeurIPS.cc/2023/Conference/Submission14650/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14650/Authors"], "forum": "917crxqJdA", "content": {"title": {"value": "Distribution-Free Statistical Dispersion Control for Societal Applications"}, "authors": {"value": ["Zhun Deng", "Thomas P Zollo", "Jake Snell", "Toniann Pitassi", "Richard Zemel"]}, "authorids": {"value": ["~Zhun_Deng1", "~Thomas_P_Zollo1", "~Jake_Snell1", "~Toniann_Pitassi3", "~Richard_Zemel1"]}, "keywords": {"value": ["societal dispersion", "distribution-free uncertainty quantification"]}, "abstract": {"value": "Explicit finite-sample statistical guarantees on model performance are an important ingredient in responsible machine learning.  Previous work has focused mainly on bounding either the expected loss of a predictor or the probability that an individual prediction will incur a loss value in a specified range.  However, for many high-stakes applications it is crucial to understand and control the \\textit{dispersion} of a loss distribution, or the extent to which different members of a population experience unequal effects of algorithmic decisions. We initiate the study of distribution-free control of statistical dispersion measures with societal implications and propose a simple yet flexible framework that allows us to handle a much richer class of statistical functionals beyond previous work. Our methods are verified through experiments in toxic comment detection, medical imaging, and film recommendation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f1c2ba500331a60a852765cf9bd74ae638b01f4e.pdf"}, "supplementary_material": {"value": "/attachment/a43faf53295aeca27718e0eac0526373dd023ec7.zip"}, "_bibtex": {"value": "@inproceedings{\ndeng2023distributionfree,\ntitle={Distribution-Free Statistical Dispersion Control for Societal Applications},\nauthor={Zhun Deng and Thomas P Zollo and Jake Snell and Toniann Pitassi and Richard Zemel},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=917crxqJdA}\n}"}, "paperhash": {"value": "deng|distributionfree_statistical_dispersion_control_for_societal_applications"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14650/-/Revision", "NeurIPS.cc/2023/Conference/Submission14650/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14650/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326160063, "odate": 1698949790039, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "d0IEd3VgBh", "number": 14593, "cdate": 1683827681947, "tcdate": 1683827681947, "mdate": 1698949789676, "tmdate": 1698949789676, "signatures": ["NeurIPS.cc/2023/Conference/Submission14593/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14593/Authors"], "forum": "d0IEd3VgBh", "content": {"title": {"value": "On the Role of Randomization in Adversarially Robust Classification"}, "authors": {"value": ["Lucas Gnecco Heredia", "Muni Sreenivas Pydi", "Laurent Meunier", "benjamin negrevergne", "Yann Chevaleyre"]}, "authorids": {"value": ["~Lucas_Gnecco_Heredia2", "~Muni_Sreenivas_Pydi1", "~Laurent_Meunier1", "~benjamin_negrevergne1", "~Yann_Chevaleyre1"]}, "keywords": {"value": ["adversarial attacks", "robustness", "adversarial", "attacks", "deep learning", "randomization", "randomized ensembles"]}, "TLDR": {"value": "We study the conditions under which randomized classifiers offer improvements for adversarial robustness. We show that for any binary randomized classifiers, there exists a deterministic one that is at least as robust."}, "abstract": {"value": "Deep neural networks are known to be vulnerable to small adversarial perturbations in test data. To defend against adversarial attacks, probabilistic classifiers have been proposed as an alternative to deterministic ones. However, literature has conflicting findings on the effectiveness of probabilistic classifiers in comparison to deterministic ones. In this paper, we clarify the role of randomization in building adversarially robust classifiers.\nGiven a base hypothesis set of deterministic classifiers, we show the conditions under which a randomized ensemble outperforms the hypothesis set in adversarial risk, extending previous results.\nAdditionally, we show that for any probabilistic binary classifier (including randomized ensembles), there exists a deterministic classifier that outperforms it. Finally, we give an explicit description of the deterministic hypothesis set that contains such a deterministic classifier for many types of commonly used probabilistic classifiers, *i.e.* randomized ensembles and parametric/input noise injection."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/511c601fbea177f4a9a15c17ad04d529d27828c2.pdf"}, "_bibtex": {"value": "@inproceedings{\nheredia2023on,\ntitle={On the Role of Randomization in Adversarially Robust Classification},\nauthor={Lucas Gnecco Heredia and Muni Sreenivas Pydi and Laurent Meunier and benjamin negrevergne and Yann Chevaleyre},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=d0IEd3VgBh}\n}"}, "paperhash": {"value": "heredia|on_the_role_of_randomization_in_adversarially_robust_classification"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14593/-/Revision", "NeurIPS.cc/2023/Conference/Submission14593/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14593/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326158459, "odate": 1698949789664, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "uJ3qNIsDGF", "number": 14477, "cdate": 1683826852657, "tcdate": 1683826852657, "mdate": 1698949789221, "tmdate": 1698949789221, "signatures": ["NeurIPS.cc/2023/Conference/Submission14477/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14477/Authors"], "forum": "uJ3qNIsDGF", "content": {"title": {"value": "Exploring Geometry of Blind Spots in Vision models"}, "authors": {"value": ["Sriram Balasubramanian", "Gaurang Sriramanan", "Vinu Sankar Sadasivan", "Soheil Feizi"]}, "authorids": {"value": ["~Sriram_Balasubramanian2", "~Gaurang_Sriramanan1", "~Vinu_Sankar_Sadasivan1", "~Soheil_Feizi2"]}, "keywords": {"value": ["Neural networks", "Vision models", "blind spots", "undersensitivity", "invariance", "level set geometry", "input connectivity"]}, "abstract": {"value": "Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of \u201cequi-confidence\u201d level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary images from other classes. We further observe that the source image is linearly connected by a high-confidence path to these inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we attempt to identify and estimate the extent of these connected higher-dimensional regions over which the model maintains a high degree of confidence."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9105c6addfca1a8763647d20d7d6cf6dea31f0e5.pdf"}, "_bibtex": {"value": "@inproceedings{\nbalasubramanian2023exploring,\ntitle={Exploring Geometry of Blind Spots in Vision models},\nauthor={Sriram Balasubramanian and Gaurang Sriramanan and Vinu Sankar Sadasivan and Soheil Feizi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=uJ3qNIsDGF}\n}"}, "TLDR": {"value": "We discover blind spots of common vision models by exploring the geometry of their level sets"}, "paperhash": {"value": "balasubramanian|exploring_geometry_of_blind_spots_in_vision_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14477/-/Revision", "NeurIPS.cc/2023/Conference/Submission14477/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14477/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326156120, "odate": 1698949789207, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "RInTOCEL3l", "number": 14448, "cdate": 1683826675559, "tcdate": 1683826675559, "mdate": 1698949788981, "tmdate": 1698949788981, "signatures": ["NeurIPS.cc/2023/Conference/Submission14448/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14448/Authors"], "forum": "RInTOCEL3l", "content": {"title": {"value": "Relax, it doesn\u2019t matter how you get there: A new self-supervised approach for multi-timescale behavior analysis"}, "authors": {"value": ["Mehdi Azabou", "Michael Jacob Mendelson", "Nauman Ahad", "Maks Sorokin", "Shantanu Thakoor", "Carolina Urzay", "Eva L Dyer"]}, "authorids": {"value": ["~Mehdi_Azabou2", "~Michael_Jacob_Mendelson1", "~Nauman_Ahad1", "~Maks_Sorokin1", "~Shantanu_Thakoor5", "~Carolina_Urzay1", "~Eva_L_Dyer1"]}, "keywords": {"value": ["animal behavior", "behavioral neuroscience", "self-supervised learning", "multi-timescale"]}, "abstract": {"value": "Unconstrained and natural  behavior consists of dynamics that are complex and  unpredictable, especially when trying to predict what will happen  multiple steps into the future. While some success has been found in building representations of animal behavior under constrained or simplified task-based conditions, many of these models cannot be applied to free and naturalistic settings where behavior becomes increasingly hard to model. In this work, we develop a multi-task representation learning model for animal behavior that combines two novel components: (i) an action-prediction objective that aims to predict the  distribution of actions over future timesteps, and (ii) a multi-scale architecture that builds separate latent spaces to accommodate short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and global dynamics in robots in varying environments and terrains, we apply our method to the MABe 2022 Multi-Agent Behavior challenge, where our model ranks first overall on both mice and fly benchmarks. In all of these cases, we show that our model can build representations that capture the many different factors that drive behavior and solve a wide range of downstream tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "This paper presents a self-supervised learning model to analyze animal tracking data and to reveal the factors that underlie animal behavior across different timescales."}, "pdf": {"value": "/pdf/307c6dca91df2facd1757a88cc7c494fb5f468ac.pdf"}, "_bibtex": {"value": "@inproceedings{\nazabou2023relax,\ntitle={Relax, it doesn{\\textquoteright}t matter how you get there: A new self-supervised approach for multi-timescale behavior analysis},\nauthor={Mehdi Azabou and Michael Jacob Mendelson and Nauman Ahad and Maks Sorokin and Shantanu Thakoor and Carolina Urzay and Eva L Dyer},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=RInTOCEL3l}\n}"}, "paperhash": {"value": "azabou|relax_it_doesnt_matter_how_you_get_there_a_new_selfsupervised_approach_for_multitimescale_behavior_analysis"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14448/-/Revision", "NeurIPS.cc/2023/Conference/Submission14448/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14448/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326155414, "odate": 1698949788970, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "8aunGrXdkl", "number": 14279, "cdate": 1683825516793, "tcdate": 1683825516793, "mdate": 1698949788181, "tmdate": 1698949788181, "signatures": ["NeurIPS.cc/2023/Conference/Submission14279/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14279/Authors"], "forum": "8aunGrXdkl", "content": {"title": {"value": "Convex and Non-convex Optimization Under Generalized Smoothness"}, "authors": {"value": ["Haochuan Li", "Jian Qian", "Yi Tian", "Alexander Rakhlin", "Ali Jadbabaie"]}, "authorids": {"value": ["~Haochuan_Li2", "~Jian_Qian2", "~Yi_Tian1", "~Alexander_Rakhlin1", "~Ali_Jadbabaie1"]}, "keywords": {"value": ["Optimization", "Convergence", "Generalized smoothness"]}, "TLDR": {"value": "We generalize the classical smoothness condition in optimization, and develop a new analysis approach, which gives much better results for convergence of GD, SGD, and NAG in the convex and/or non-convex setting."}, "abstract": {"value": "Classical analysis of convex and non-convex optimization methods often requires the Lipschitz continuity of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm  bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to  stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with bounded variance in the stochastic setting."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8155b31c9cecd6c6119f248dbdcd5dcc083336ea.pdf"}, "_bibtex": {"value": "@inproceedings{\nli2023convex,\ntitle={Convex and Non-convex Optimization Under Generalized Smoothness},\nauthor={Haochuan Li and Jian Qian and Yi Tian and Alexander Rakhlin and Ali Jadbabaie},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=8aunGrXdkl}\n}"}, "paperhash": {"value": "li|convex_and_nonconvex_optimization_under_generalized_smoothness"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14279/-/Revision", "NeurIPS.cc/2023/Conference/Submission14279/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14279/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326150684, "odate": 1698949788158, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "yEewbkBNzi", "number": 14212, "cdate": 1683825075008, "tcdate": 1683825075008, "mdate": 1698949787887, "tmdate": 1698949787887, "signatures": ["NeurIPS.cc/2023/Conference/Submission14212/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14212/Authors"], "forum": "yEewbkBNzi", "content": {"title": {"value": "Convergence of Adam Under Relaxed Assumptions"}, "authors": {"value": ["Haochuan Li", "Alexander Rakhlin", "Ali Jadbabaie"]}, "authorids": {"value": ["~Haochuan_Li2", "~Alexander_Rakhlin1", "~Ali_Jadbabaie1"]}, "keywords": {"value": ["Non-convex optimization", "Adam", "Convergence", "Variance reduction"]}, "TLDR": {"value": "We provide a new analysis of Adam and prove its convergence without assuming boundedness of gradients, under a generalized smoothness condition. We also propose a variance-reduced version of Adam with convergence guarantees."}, "abstract": {"value": "In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\\epsilon$-stationary points with $\\mathcal{O}(\\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory of Adam, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradient complexity of $\\mathcal{O}(\\epsilon^{-3})$."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/862f2a4fb941567e8c797e2c153beb10dddaba5d.pdf"}, "_bibtex": {"value": "@inproceedings{\nli2023convergence,\ntitle={Convergence of Adam Under Relaxed Assumptions},\nauthor={Haochuan Li and Alexander Rakhlin and Ali Jadbabaie},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=yEewbkBNzi}\n}"}, "paperhash": {"value": "li|convergence_of_adam_under_relaxed_assumptions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14212/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14212/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326149145, "odate": 1698949787875, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Sxu7xlUJGx", "number": 14159, "cdate": 1683824562755, "tcdate": 1683824562755, "mdate": 1698949787695, "tmdate": 1698949787695, "signatures": ["NeurIPS.cc/2023/Conference/Submission14159/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14159/Authors"], "forum": "Sxu7xlUJGx", "content": {"title": {"value": "Implicit Variational Inference for High-Dimensional Posteriors"}, "authors": {"value": ["Anshuk Uppal", "Kristoffer Stensbo-Smidt", "Wouter Boomsma", "Jes Frellsen"]}, "authorids": {"value": ["~Anshuk_Uppal1", "~Kristoffer_Stensbo-Smidt1", "~Wouter_Boomsma1", "~Jes_Frellsen1"]}, "keywords": {"value": ["Implicit models", "Variational Inference", "Bayesian Deep Learning", "Bayesian Inference", "Generative Modelling"]}, "TLDR": {"value": "High dimensional expressive variational approximations that perform better than existing methods."}, "abstract": {"value": "In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach introduces novel bounds for approximate inference using implicit distributions by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over tens of millions of latent variables, addressing computational concerns by using differentiable numerical approximations. We empirically show that our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notoriously challenging to achieve. To the best of our knowledge, no other method has been shown to accomplish this task for such large models. Through experiments in downstream tasks, we demonstrate that our expressive posteriors outperform state-of-the-art uncertainty quantification methods, validating the effectiveness of our training algorithm and the quality of the learned implicit approximation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/299dc4381be31ff46d3a0c03fc2ee67b76ecdc52.pdf"}, "_bibtex": {"value": "@inproceedings{\nuppal2023implicit,\ntitle={Implicit Variational Inference for High-Dimensional Posteriors},\nauthor={Anshuk Uppal and Kristoffer Stensbo-Smidt and Wouter Boomsma and Jes Frellsen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Sxu7xlUJGx}\n}"}, "paperhash": {"value": "uppal|implicit_variational_inference_for_highdimensional_posteriors"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14159/-/Revision", "NeurIPS.cc/2023/Conference/Submission14159/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14159/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326148042, "odate": 1698949787616, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "AA1xrgAP5z", "number": 14157, "cdate": 1683824555527, "tcdate": 1683824555527, "mdate": 1698949787606, "tmdate": 1698949787606, "signatures": ["NeurIPS.cc/2023/Conference/Submission14157/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14157/Authors"], "forum": "AA1xrgAP5z", "content": {"title": {"value": "Universal Online Learning with Gradient Variations: A Multi-layer Online Ensemble Approach"}, "authors": {"value": ["Yu-Hu Yan", "Peng Zhao", "Zhi-Hua Zhou"]}, "authorids": {"value": ["~Yu-Hu_Yan1", "~Peng_Zhao1", "~Zhi-Hua_Zhou2"]}, "keywords": {"value": ["online learning"]}, "abstract": {"value": "In this paper, we propose an online convex optimization approach with two different levels of adaptivity. On a higher level, our approach is agnostic to the unknown types and curvatures of the online functions, while at a lower level, it can exploit the unknown niceness of the environments and attain problem-dependent guarantees. Specifically, we obtain $\\mathcal{O}(\\log V_T)$, $\\mathcal{O}(d \\log V_T)$ and $\\hat{\\mathcal{O}}(\\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and the $\\hat{\\mathcal{O}}(\\cdot)$-notation omits $\\log V_T$ factors. Our result not only safeguards the worst-case guarantees but also directly implies the small-loss bounds in analysis. Moreover, when applied to adversarial/stochastic convex optimization and game theory problems, our result enhances the existing universal guarantees. Our approach is based on a multi-layer online ensemble framework incorporating novel ingredients, including a carefully designed optimism for unifying diverse function types and cascaded corrections for algorithmic stability. Notably, despite its multi-layer structure, our algorithm necessitates only one gradient query per round, making it favorable when the gradient evaluation is time-consuming. This is facilitated by a novel regret decomposition equipped with carefully designed surrogate losses."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c67966d8690586837ae06cf479c27c89c6782585.pdf"}, "_bibtex": {"value": "@inproceedings{\nyan2023universal,\ntitle={Universal Online Learning with Gradient Variations: A Multi-layer Online Ensemble Approach},\nauthor={Yu-Hu Yan and Peng Zhao and Zhi-Hua Zhou},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=AA1xrgAP5z}\n}"}, "paperhash": {"value": "yan|universal_online_learning_with_gradient_variations_a_multilayer_online_ensemble_approach"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14157/-/Revision", "NeurIPS.cc/2023/Conference/Submission14157/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14157/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326147937, "odate": 1698949787594, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "z37ki6nqAY", "number": 14155, "cdate": 1683824542353, "tcdate": 1683824542353, "mdate": 1698949787574, "tmdate": 1698949787574, "signatures": ["NeurIPS.cc/2023/Conference/Submission14155/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14155/Authors"], "forum": "z37ki6nqAY", "content": {"title": {"value": "Online List Labeling with Predictions"}, "authors": {"value": ["Samuel McCauley", "Benjamin Moseley", "Aidin Niaparast", "Shikha Singh"]}, "authorids": {"value": ["~Samuel_McCauley1", "~Benjamin_Moseley1", "~Aidin_Niaparast1", "~Shikha_Singh2"]}, "keywords": {"value": ["Algorithms with Predictions", "Data Structures", "Learned Indices", "Online List Labeling", "Resource Allocation", "Beyond Worst Case Analysis"]}, "TLDR": {"value": "This paper gives a theoretical analysis of a list labeling data structure in the learning augmented algorithm competitive analysis model."}, "abstract": {"value": "A growing line of work shows how learned predictions can be used to break through worst-case barriers to improve the running time of an algorithm. However, incorporating predictions into data structures with strong theoretical guarantees remains underdeveloped.  This paper takes a step in this direction by showing that predictions can be leveraged in the fundamental online list labeling problem. In the problem, $n$ items arrive over time and must be stored in sorted order in an array of size $\\Theta(n)$.  The array slot of an element is its label and the goal is to maintain sorted order while minimizing the total number of elements moved (i.e., relabeled). We design a new list labeling data structure and bound its performance in two models.  In the worst-case learning-augmented model, we give guarantees in terms of the error in the predictions.  Our data structure provides strong guarantees: it is optimal for any prediction error and guarantees the best-known worst-case bound even when the predictions are entirely erroneous. We also consider a stochastic error model and bound the performance in terms of the expectation and variance of the error. Finally, the theoretical results are demonstrated empirically.  In particular, we show that our data structure has strong performance on real temporal data sets where predictions are constructed from elements that arrived in the past, as is typically done in a practical use case."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d70e991ce3d6bb1a4621047c117e525be9179d0e.pdf"}, "supplementary_material": {"value": "/attachment/f11120c7d1e28c3d0d30a0c4aca0d250b043f8e1.zip"}, "_bibtex": {"value": "@inproceedings{\nmccauley2023online,\ntitle={Online List Labeling with Predictions},\nauthor={Samuel McCauley and Benjamin Moseley and Aidin Niaparast and Shikha Singh},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=z37ki6nqAY}\n}"}, "paperhash": {"value": "mccauley|online_list_labeling_with_predictions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14155/-/Revision", "NeurIPS.cc/2023/Conference/Submission14155/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14155/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326147880, "odate": 1698949787561, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "CbsJ53LdKc", "number": 14051, "cdate": 1683823774300, "tcdate": 1683823774300, "mdate": 1698954293078, "tmdate": 1698954293078, "signatures": ["NeurIPS.cc/2023/Conference/Submission14051/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission14051/Authors"], "forum": "CbsJ53LdKc", "content": {"title": {"value": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases"}, "authors": {"value": ["Leonard Salewski", "Stephan Alaniz", "Isabel Rio-Torto", "Eric Schulz", "Zeynep Akata"]}, "authorids": {"value": ["~Leonard_Salewski1", "~Stephan_Alaniz1", "~Isabel_Rio-Torto1", "~Eric_Schulz1", "~Zeynep_Akata1"]}, "keywords": {"value": ["large language models", "impersonation", "vision language models", "reasoning"]}, "TLDR": {"value": "Large language models can impersonate different personas and it affects their performance in bandit, reasoning and vision and language tasks."}, "abstract": {"value": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their strengths and hidden biases. Our code is available at https://github.com/ExplainableML/in-context-impersonation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6ced925f002af339320a3d02e1ff8b7c7b0f2846.pdf"}, "supplementary_material": {"value": "/attachment/f23adc5558b4a4eee45c4cf3b9f60048626cad96.pdf"}, "_bibtex": {"value": "@inproceedings{\nsalewski2023incontext,\ntitle={In-Context Impersonation Reveals Large Language Models' Strengths and Biases},\nauthor={Leonard Salewski and Stephan Alaniz and Isabel Rio-Torto and Eric Schulz and Zeynep Akata},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=CbsJ53LdKc}\n}"}, "paperhash": {"value": "salewski|incontext_impersonation_reveals_large_language_models_strengths_and_biases"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission14051/-/Revision", "NeurIPS.cc/2023/Conference/Submission14051/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission14051/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326145011, "odate": 1698949786930, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "pjSzKhSrfs", "number": 13805, "cdate": 1683821988021, "tcdate": 1683821988021, "mdate": 1698949785735, "tmdate": 1698949785735, "signatures": ["NeurIPS.cc/2023/Conference/Submission13805/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13805/Authors"], "forum": "pjSzKhSrfs", "content": {"title": {"value": "Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\u00f6dinger Equation"}, "authors": {"value": ["Kirill Neklyudov", "Jannes Nys", "Luca Thiede", "Juan Felipe Carrasquilla Alvarez", "qiang liu", "Max Welling", "Alireza Makhzani"]}, "authorids": {"value": ["~Kirill_Neklyudov1", "jannes.nys@epfl.ch", "~Luca_Thiede1", "carrasqu@vectorinstitute.ai", "~qiang_liu4", "~Max_Welling1", "~Alireza_Makhzani1"]}, "keywords": {"value": ["Quantum Monte Carlo", "Schr\u00f6dinger equation", "Wasserstein Fisher-Rao gradient flow"]}, "abstract": {"value": "Solving the quantum many-body Schr\u00f6dinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this distributional space, followed by a projection step onto the variational manifold. This perspective provides us with a principled framework to derive new QMC algorithms, by endowing the distributional space with better metrics, and following the projected gradient flow induced by those metrics. More specifically, we propose ``Wasserstein Quantum Monte Carlo'' (WQMC), which uses the gradient flow induced by the Wasserstein metrics, rather than Fisher--Rao metric, and corresponds to *transporting* the probability mass, rather than *teleporting* it. We demonstrate empirically that the dynamics of WQMC results in faster convergence to the ground state of molecular systems."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We minimize the energy functional of quantum systems by following a projected Wasserstein gradient flow in the space of distributions, which results in faster convergence compared to the conventional Quantum Variational Monte Carlo."}, "pdf": {"value": "/pdf/7d1918193c1decd8f6ced43c4005f542c5c48639.pdf"}, "supplementary_material": {"value": "/attachment/bfc20543673ebe8a4f645c17157143a4d56255ed.zip"}, "_bibtex": {"value": "@inproceedings{\nneklyudov2023wasserstein,\ntitle={Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\\\"odinger Equation},\nauthor={Kirill Neklyudov and Jannes Nys and Luca Thiede and Juan Felipe Carrasquilla Alvarez and qiang liu and Max Welling and Alireza Makhzani},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=pjSzKhSrfs}\n}"}, "paperhash": {"value": "neklyudov|wasserstein_quantum_monte_carlo_a_novel_approach_for_solving_the_quantum_manybody_schr\u00f6dinger_equation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13805/-/Revision", "NeurIPS.cc/2023/Conference/Submission13805/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13805/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326138809, "odate": 1698949785720, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "utreNaM1VY", "number": 13797, "cdate": 1683821896160, "tcdate": 1683821896160, "mdate": 1698949785652, "tmdate": 1698949785652, "signatures": ["NeurIPS.cc/2023/Conference/Submission13797/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13797/Authors"], "forum": "utreNaM1VY", "content": {"title": {"value": "Can semi-supervised learning use all the data effectively? A lower bound perspective"}, "authors": {"value": ["Alexandru Tifrea", "Gizem Y\u00fcce", "Amartya Sanyal", "Fanny Yang"]}, "authorids": {"value": ["~Alexandru_Tifrea1", "~Gizem_Y\u00fcce1", "~Amartya_Sanyal1", "~Fanny_Yang1"]}, "keywords": {"value": ["semi-supervised learning", "statistical lower bound"]}, "TLDR": {"value": "Adaptive statistical lower bound for semi-supervised learning linear classification in 2-GMM data model."}, "abstract": {"value": "Prior theoretical and empirical works have established that semi-supervised learning algorithms can leverage the unlabeled data to improve over the labeled sample complexity of supervised learning (SL) algorithms. However, existing theoretical work focuses on regimes where the unlabeled data is sufficient to learn a good decision boundary using unsupervised learning (UL) alone. This begs the question: Can SSL algorithms simultaneously improve upon both UL and SL? To this end, we derive a tight lower bound for 2-Gaussian mixture models that explicitly depends on the labeled and the unlabeled dataset size as well as the signal-to-noise ratio of the mixture distribution. Surprisingly, our result implies that no SSL algorithm improves upon the minimax-optimal statistical error rates of SL or UL algorithms for these distributions. Nevertheless, in our real-world experiments, SSL algorithms can often outperform UL and SL algorithms. In summary, our work suggests that while it is possible to prove the performance gains of SSL algorithms, this would require careful tracking of constants in the theoretical analysis."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7f2a4a243732ab6f219eef0f5b44cf97697996e5.pdf"}, "supplementary_material": {"value": "/attachment/5a5f85f7d8d9dce4c739123f16f0aa009d37f0fe.pdf"}, "_bibtex": {"value": "@inproceedings{\ntifrea2023can,\ntitle={Can semi-supervised learning use all the data effectively? A lower bound perspective},\nauthor={Alexandru Tifrea and Gizem Y{\\\"u}ce and Amartya Sanyal and Fanny Yang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=utreNaM1VY}\n}"}, "paperhash": {"value": "tifrea|can_semisupervised_learning_use_all_the_data_effectively_a_lower_bound_perspective"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13797/-/Revision", "NeurIPS.cc/2023/Conference/Submission13797/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13797/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326138542, "odate": 1698949785637, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "FasIQqsJhe", "number": 13733, "cdate": 1683821308375, "tcdate": 1683821308375, "mdate": 1698949785103, "tmdate": 1698949785103, "signatures": ["NeurIPS.cc/2023/Conference/Submission13733/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13733/Authors"], "forum": "FasIQqsJhe", "content": {"title": {"value": "Towards In-context Scene Understanding"}, "authors": {"value": ["Ivana Balazevic", "David Steiner", "Nikhil Parthasarathy", "Relja Arandjelovic", "Olivier J Henaff"]}, "authorids": {"value": ["~Ivana_Balazevic1", "~David_Steiner1", "~Nikhil_Parthasarathy1", "~Relja_Arandjelovic1", "~Olivier_J_Henaff1"]}, "keywords": {"value": ["transfer learning", "adaptation", "self-supervised learning", "contrastive learning", "scene understanding", "representation learning", "in-context learning", "vision transformers"]}, "abstract": {"value": "In-context learning\u2013\u2013the ability to configure a model's behavior with different prompts\u2013\u2013has revolutionized the field of natural language processing, alleviating the need for task-specific models and paving the way for generalist models capable of assisting with any query. Computer vision, in contrast, has largely stayed in the former regime: specialized decoders and finetuning protocols are generally required to perform dense tasks such as semantic segmentation and depth estimation. In this work we explore a simple mechanism for in-context learning of such scene understanding tasks: nearest neighbor retrieval from a prompt of annotated features. We propose a new pretraining protocol\u2013\u2013leveraging attention within and across images\u2013\u2013which yields representations particularly useful in this regime. The resulting Hummingbird model, suitably prompted, performs various scene understanding tasks without modification while approaching the performance of specialists that have been finetuned for each task. Moreover, Hummingbird can be configured to perform new tasks much more efficiently than finetuned models, raising the possibility of scene understanding in the interactive assistant regime."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1806f0590774e914937a6435743d045a54145be5.pdf"}, "TLDR": {"value": "Retrieval-augmented self-supervised models display fast and data-efficient adaptation to a range of scene understanding tasks."}, "supplementary_material": {"value": "/attachment/f55a132689f8ad2cbdb16a9bcae0b0d467e88b96.pdf"}, "_bibtex": {"value": "@inproceedings{\nbalazevic2023towards,\ntitle={Towards In-context Scene Understanding},\nauthor={Ivana Balazevic and David Steiner and Nikhil Parthasarathy and Relja Arandjelovic and Olivier J Henaff},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=FasIQqsJhe}\n}"}, "paperhash": {"value": "balazevic|towards_incontext_scene_understanding"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13733/-/Revision", "NeurIPS.cc/2023/Conference/Submission13733/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13733/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326136933, "odate": 1698949785089, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "nN8TnHB5nw", "number": 13709, "cdate": 1683821133157, "tcdate": 1683821133157, "mdate": 1698949785029, "tmdate": 1698949785029, "signatures": ["NeurIPS.cc/2023/Conference/Submission13709/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13709/Authors"], "forum": "nN8TnHB5nw", "content": {"title": {"value": "Memory Efficient Optimizers with 4-bit States"}, "authors": {"value": ["Bingrui Li", "Jianfei Chen", "Jun Zhu"]}, "authorids": {"value": ["~Bingrui_Li1", "~Jianfei_Chen1", "~Jun_Zhu2"]}, "keywords": {"value": ["memory efficiency", "optimizer", "Adam", "quantization"]}, "abstract": {"value": "Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizers are evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/615a327f0d1560d008272c0f5c72429af8a853af.pdf"}, "supplementary_material": {"value": "/attachment/1f68049af56e9ad919599339264ef9c1249e5e49.zip"}, "_bibtex": {"value": "@inproceedings{\nli2023memory,\ntitle={Memory Efficient Optimizers with 4-bit States},\nauthor={Bingrui Li and Jianfei Chen and Jun Zhu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=nN8TnHB5nw}\n}"}, "paperhash": {"value": "li|memory_efficient_optimizers_with_4bit_states"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13709/-/Revision", "NeurIPS.cc/2023/Conference/Submission13709/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13709/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326136194, "odate": 1698949785009, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "b6FeLpKKjl", "number": 13657, "cdate": 1683820748777, "tcdate": 1683820748777, "mdate": 1698949784788, "tmdate": 1698949784788, "signatures": ["NeurIPS.cc/2023/Conference/Submission13657/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13657/Authors"], "forum": "b6FeLpKKjl", "content": {"title": {"value": "Convergence of Alternating Gradient Descent for Matrix Factorization"}, "authors": {"value": ["Rachel Ward", "Tamara G. Kolda"]}, "authorids": {"value": ["~Rachel_Ward1", "~Tamara_G._Kolda1"]}, "keywords": {"value": ["matrix factorization; gradient descent; global convergence; concentration; optimization"]}, "abstract": {"value": "We consider alternating gradient descent (AGD) with fixed step size applied to the asymmetric matrix factorization objective.\n  We show that, for a rank-$r$ matrix $A \\in \\mathbb{R}^{m \\times n}$,\n  $T = C ( \\frac{\\sigma_1(A)}{\\sigma_r(A)} )^2 \\log(1/\\epsilon)$\n  iterations of alternating gradient descent suffice to reach an $\\epsilon$-optimal factorization \n  $\\| A - X_{T} Y_{T}' \\|^2 \\leq \\epsilon \\| A \\|^2$   with high probability\n  starting from an atypical random initialization. The\n  factors have rank $d \\geq r$ so that $X_{T}\\in \\mathbb{R}^{m \\times d}$ and $Y_{T} \\in\\mathbb{R}^{n \\times d}$, and mild overparameterization suffices for the constant  $C$ in the iteration complexity $T$ to be an absolute constant. \n  Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves the convergence rate of gradient descent in practice. Our proof is conceptually simple: a uniform Polyak-Lojasiewicz (PL) inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization.  Our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ab07398355ff0cbfe73cedf8f419b99564dd94f9.pdf"}, "supplementary_material": {"value": "/attachment/448e5722d71427a347f403669397d66c45ec6b97.pdf"}, "_bibtex": {"value": "@inproceedings{\nward2023convergence,\ntitle={Convergence of Alternating Gradient Descent for Matrix Factorization},\nauthor={Rachel Ward and Tamara G. Kolda},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=b6FeLpKKjl}\n}"}, "paperhash": {"value": "ward|convergence_of_alternating_gradient_descent_for_matrix_factorization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13657/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13657/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326134673, "odate": 1698949784776, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "3FJaFElIVN", "number": 13617, "cdate": 1683820488530, "tcdate": 1683820488530, "mdate": 1698949784569, "tmdate": 1698949784569, "signatures": ["NeurIPS.cc/2023/Conference/Submission13617/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13617/Authors"], "forum": "3FJaFElIVN", "content": {"title": {"value": "GLIME: General, Stable and Local LIME Explanation"}, "authors": {"value": ["Zeren Tan", "Yang Tian", "Jian Li"]}, "authorids": {"value": ["~Zeren_Tan1", "~Yang_Tian2", "~Jian_Li2"]}, "keywords": {"value": ["Explanation", "LIME", "Stability", "Local fidelity", "Interpretability"]}, "TLDR": {"value": "A new framework, which allows flexible sampling distribution, improves stability and local fidelity of LIME both theoretically and empirically."}, "abstract": {"value": "As black-box machine learning models become more complex and are applied in high-stakes settings, the need for providing explanations for their predictions becomes crucial. Although Local Interpretable Model-agnostic Explanations (LIME) \\cite{ribeiro2016should} is a widely adopted method for understanding model behavior, it suffers from instability with respect to random seeds \\cite{zafar2019dlime, shankaranarayana2019alime, bansal2020sam} and exhibits low local fidelity (i.e., how the explanation explains model's local behaviors) \\cite{rahnama2019study, laugel2018defining}. Our study demonstrates that this instability is caused by small sample weights, resulting in the dominance of regularization and slow convergence. Additionally, LIME's sampling approach is non-local and biased towards the reference, leading to diminished local fidelity and instability to references. To address these challenges, we propose \\textsc{Glime}, an enhanced framework that extends LIME and unifies several previous methods. Within the \\textsc{Glime} framework, we derive an equivalent formulation of LIME that achieves significantly faster convergence and improved stability. By employing a local and unbiased sampling distribution, \\textsc{Glime} generates explanations with higher local fidelity compared to LIME, while being independent of the reference choice. Moreover, \\textsc{Glime} offers users the flexibility to choose sampling distribution based on their specific scenarios."}, "pdf": {"value": "/pdf/a238f5336a39b2513ab77af2602e16b527306f9d.pdf"}, "supplementary_material": {"value": "/attachment/85b1a8bf14bfe8b4eaff34ee5511db6d62da9814.zip"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\ntan2023glime,\ntitle={{GLIME}: General, Stable and Local {LIME} Explanation},\nauthor={Zeren Tan and Yang Tian and Jian Li},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=3FJaFElIVN}\n}"}, "paperhash": {"value": "tan|glime_general_stable_and_local_lime_explanation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13617/-/Revision", "NeurIPS.cc/2023/Conference/Submission13617/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13617/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326133771, "odate": 1698949784554, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "TczT2jiPT5", "number": 13571, "cdate": 1683820119226, "tcdate": 1683820119226, "mdate": 1698949784107, "tmdate": 1698949784107, "signatures": ["NeurIPS.cc/2023/Conference/Submission13571/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13571/Authors"], "forum": "TczT2jiPT5", "content": {"title": {"value": "The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance"}, "authors": {"value": ["Jon Donnelly", "Srikar Katta", "Cynthia Rudin", "Edward P Browne"]}, "authorids": {"value": ["~Jon_Donnelly1", "srikar.katta@duke.edu", "~Cynthia_Rudin1", "epbrowne@email.unc.edu"]}, "keywords": {"value": ["Rashomon Effect", "Variable Importance", "XAI", "Stability", "Interpretable Machine Learning"]}, "abstract": {"value": "Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We demonstrate through experiments that our framework recovers variable importance rankings for complex simulation setups where other methods fail. Further, we show that our framework accurately estimates the _true importance_ of a variable for the underlying data distribution. We provide theoretical guarantees on the consistency and finite sample error rates for our estimator. Finally, we demonstrate its utility with a real-world case study exploring which genes are important for predicting HIV load in persons with HIV, highlighting an important gene that has not previously been studied in connection with HIV."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/943d2cd611298f0943eca77555a91fd45873b03a.pdf"}, "supplementary_material": {"value": "/attachment/483c7aecb4d82a0a9adf14dd8cdcb236cc9845cd.pdf"}, "_bibtex": {"value": "@inproceedings{\ndonnelly2023the,\ntitle={The Rashomon Importance Distribution: Getting {RID} of Unstable, Single Model-based Variable Importance},\nauthor={Jon Donnelly and Srikar Katta and Cynthia Rudin and Edward P Browne},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=TczT2jiPT5}\n}"}, "paperhash": {"value": "donnelly|the_rashomon_importance_distribution_getting_rid_of_unstable_single_modelbased_variable_importance"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13571/-/Revision", "NeurIPS.cc/2023/Conference/Submission13571/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13571/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326132462, "odate": 1698949784089, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "UdaTyy0BNB", "number": 13491, "cdate": 1683819555236, "tcdate": 1683819555236, "mdate": 1698949783572, "tmdate": 1698949783572, "signatures": ["NeurIPS.cc/2023/Conference/Submission13491/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13491/Authors"], "forum": "UdaTyy0BNB", "content": {"title": {"value": "Double Gumbel Q-Learning"}, "authors": {"value": ["David Yu-Tung Hui", "Aaron Courville", "Pierre-Luc Bacon"]}, "authorids": {"value": ["~David_Yu-Tung_Hui1", "~Aaron_Courville3", "~Pierre-Luc_Bacon1"]}, "keywords": {"value": ["deep reinforcement learning", "Q-Learning", "TD-Learning with function approximation", "extreme value theory", "maximum-likelihood estimation", "moment-matching"]}, "abstract": {"value": "We show that Deep Neural Networks introduce two heteroscedastic Gumbel noise sources into Q-Learning.  To account for these noise sources, we propose Double Gumbel Q-Learning, a Deep Q-Learning algorithm applicable for both discrete and continuous control.  In discrete control, we derive a closed-form expression for the loss function of our algorithm.  In continuous control, this loss function is intractable and we therefore derive an approximation with a hyperparameter whose value regulates pessimism in Q-Learning.  We present a default value for our pessimism hyperparameter that enables DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D and show that tuning this hyperparameter may further improve sample efficiency."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/09af4d6c40114fed8acdb1fe7e610507d023441c.pdf"}, "supplementary_material": {"value": "/attachment/0fb2e7a941d10f9a0bd9e6e8c4a47fd9c2dacd0c.pdf"}, "_bibtex": {"value": "@inproceedings{\nhui2023double,\ntitle={Double Gumbel Q-Learning},\nauthor={David Yu-Tung Hui and Aaron Courville and Pierre-Luc Bacon},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=UdaTyy0BNB}\n}"}, "TLDR": {"value": "DoubleGum is a well-performing Q-Learning algorithm that models noise with two heteroscedastic Gumbel distributions."}, "paperhash": {"value": "hui|double_gumbel_qlearning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13491/-/Revision", "NeurIPS.cc/2023/Conference/Submission13491/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13491/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326130542, "odate": 1698949783561, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "FAZ3i0hvm0", "number": 13441, "cdate": 1683819242777, "tcdate": 1683819242777, "mdate": 1698949783350, "tmdate": 1698949783350, "signatures": ["NeurIPS.cc/2023/Conference/Submission13441/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13441/Authors"], "forum": "FAZ3i0hvm0", "content": {"title": {"value": "A Privacy-Friendly Approach to Data Valuation"}, "authors": {"value": ["Jiachen T. Wang", "Yuqing Zhu", "Yu-Xiang Wang", "Ruoxi Jia", "Prateek Mittal"]}, "authorids": {"value": ["~Jiachen_T._Wang1", "~Yuqing_Zhu1", "~Yu-Xiang_Wang1", "~Ruoxi_Jia1", "~Prateek_Mittal1"]}, "keywords": {"value": ["Data Valuation", "Differential Privacy"]}, "TLDR": {"value": "We develop an alternative of KNN-Shapley with improved efficiency and can be easily extended to incorporate differential privacy."}, "abstract": {"value": "Data valuation, a growing field that aims at quantifying the usefulness of individual data sources for training machine learning (ML) models, faces notable yet often overlooked privacy challenges. This paper studies these challenges with a focus on KNN-Shapley, one of the most practical data valuation methods nowadays. We first emphasize the inherent privacy risks of KNN-Shapley, and demonstrate the significant technical challenges in adapting KNN-Shapley to accommodate differential privacy (DP). To overcome these challenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is privacy-friendly, allowing for straightforward modifications to incorporate DP guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several advantages and offers a superior privacy-utility tradeoff compared to naively privatized KNN-Shapley. Moreover, even non-private TKNN-Shapley matches KNN-Shapley's performance in discerning data quality. Overall, our findings suggest that TKNN-Shapley is a promising alternative to KNN-Shapley, particularly for real-world applications involving sensitive data."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/99dc885562e6ddec532c44571208d78774c99d28.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023a,\ntitle={A Privacy-Friendly Approach to Data Valuation},\nauthor={Jiachen T. Wang and Yuqing Zhu and Yu-Xiang Wang and Ruoxi Jia and Prateek Mittal},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=FAZ3i0hvm0}\n}"}, "paperhash": {"value": "wang|a_privacyfriendly_approach_to_data_valuation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13441/-/Revision", "NeurIPS.cc/2023/Conference/Submission13441/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13441/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326129031, "odate": 1698949783335, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5AMa9fiyJq", "number": 13435, "cdate": 1683819227660, "tcdate": 1683819227660, "mdate": 1698949783277, "tmdate": 1698949783277, "signatures": ["NeurIPS.cc/2023/Conference/Submission13435/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13435/Authors"], "forum": "5AMa9fiyJq", "content": {"title": {"value": "Common Ground in Cooperative Communication"}, "authors": {"value": ["Xiaoran Hao", "Yash Jhaveri", "Patrick Shafto"]}, "authorids": {"value": ["~Xiaoran_Hao1", "~Yash_Jhaveri1", "~Patrick_Shafto2"]}, "keywords": {"value": ["Cooperative Communication", "Common Ground", "Bayesian Theory"]}, "abstract": {"value": "Cooperative communication plays a fundamental role in theories of human-human interaction--cognition, culture, development, language, etc.--as well as human-robot interaction. The core challenge in cooperative communication is the problem of common ground: having enough shared knowledge and understanding to successfully communicate. Prior models of cooperative communication, however, uniformly assume the strongest form of common ground, perfect and complete knowledge sharing, and, therefore, fail to capture the core challenge of cooperative communication. We propose a general theory of cooperative communication that is mathematically principled and explicitly defines a spectrum of common ground possibilities, going well beyond that of perfect and complete knowledge sharing, on spaces that permit arbitrary representations of data and hypotheses. Our framework is a strict generalization of prior models of cooperative communication. After considering a parametric form of common ground and viewing the data selection and hypothesis inference processes of communication as encoding and decoding, we establish a connection to variational autoencoding, a powerful model in modern machine learning. Finally, we carry out a series of empirical simulations to support and elaborate on our theoretical results."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/97c898cd59add7a1c150dc64d356275451021f94.pdf"}, "supplementary_material": {"value": "/attachment/e4d16a61755e5df41f31d8614daab79762ad4f0e.pdf"}, "_bibtex": {"value": "@inproceedings{\nhao2023common,\ntitle={Common Ground in Cooperative Communication},\nauthor={Xiaoran Hao and Yash Jhaveri and Patrick Shafto},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5AMa9fiyJq}\n}"}, "paperhash": {"value": "hao|common_ground_in_cooperative_communication"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13435/-/Revision", "NeurIPS.cc/2023/Conference/Submission13435/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13435/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326128755, "odate": 1698949783262, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Ki6DqBXss4", "number": 13401, "cdate": 1683818919673, "tcdate": 1683818919673, "mdate": 1698949783131, "tmdate": 1698949783131, "signatures": ["NeurIPS.cc/2023/Conference/Submission13401/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13401/Authors"], "forum": "Ki6DqBXss4", "content": {"title": {"value": "Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms"}, "authors": {"value": ["Dheeraj Baby", "Saurabh Garg", "Tzu-Ching Yen", "Sivaraman Balakrishnan", "Zachary Chase Lipton", "Yu-Xiang Wang"]}, "authorids": {"value": ["~Dheeraj_Baby1", "~Saurabh_Garg3", "~Tzu-Ching_Yen1", "~Sivaraman_Balakrishnan1", "~Zachary_Chase_Lipton1", "~Yu-Xiang_Wang1"]}, "keywords": {"value": ["online learning", "label shift", "distribution shift", "unsupervised domain adaptation"]}, "TLDR": {"value": "In this work, we focused on unsupervised and supervised online label shift settings. For both settings, we developed algorithms with minimax optimal dynamic regret. Experimental results on numerous datasets highlight the effectiveness of our methods."}, "abstract": {"value": "This paper focuses on supervised and unsupervised online label shift,\nwhere the class marginals $Q(y)$ varies\nbut the class-conditionals $Q(x|y)$ remain invariant. In the unsupervised setting, our goal is to adapt a learner, trained on some offline labeled data, to changing label distributions given unlabeled online data. In the supervised setting, we must both learn a classifier and adapt to the dynamically evolving class marginals given only labeled online data. We develop novel algorithms that reduce the adaptation problem to online regression and guarantee optimal dynamic regret without any prior knowledge of the extent of drift in the label distribution. Our solution is based on bootstrapping the estimates of *online regression oracles* that track the drifting proportions. Experiments across numerous simulated and real-world online label shift scenarios demonstrate the superior performance of our proposed approaches, often achieving 1-3% improvement in accuracy while being sample and computationally efficient. Code is publicly available at https://github.com/Anon-djiwh/OnlineLabelShift"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/93721eccd5fa3073da317dc83fa2faf5fdd95b21.pdf"}, "_bibtex": {"value": "@inproceedings{\nbaby2023online,\ntitle={Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms},\nauthor={Dheeraj Baby and Saurabh Garg and Tzu-Ching Yen and Sivaraman Balakrishnan and Zachary Chase Lipton and Yu-Xiang Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Ki6DqBXss4}\n}"}, "paperhash": {"value": "baby|online_label_shift_optimal_dynamic_regret_meets_practical_algorithms"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13401/-/Revision", "NeurIPS.cc/2023/Conference/Submission13401/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13401/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326127783, "odate": 1698949783113, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "rUf0GV5CuU", "number": 13356, "cdate": 1683818631578, "tcdate": 1683818631578, "mdate": 1698949782897, "tmdate": 1698949782897, "signatures": ["NeurIPS.cc/2023/Conference/Submission13356/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13356/Authors"], "forum": "rUf0GV5CuU", "content": {"title": {"value": "Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search"}, "authors": {"value": ["Indradyumna Roy", "Rishi Agarwal", "Soumen Chakrabarti", "Anirban Dasgupta", "Abir De"]}, "authorids": {"value": ["~Indradyumna_Roy1", "~Rishi_Agarwal1", "~Soumen_Chakrabarti1", "~Anirban_Dasgupta1", "~Abir_De1"]}, "keywords": {"value": ["Locality sensitive hashing", "Fourier transform", "Order embeddings"]}, "TLDR": {"value": "We design an asymmetric LSH in the Fourier frequency domain, for soft set containment search."}, "abstract": {"value": "In many search applications related to passage retrieval, text entailment, and subgraph search, the query and each 'document' is a set of elements, with a document being relevant if it contains the query. These elements are not represented by atomic IDs, but by  embedded representations, thereby extending set containment to *soft* set containment. Recent applications address soft set containment by encoding sets into fixed-size vectors and checking for elementwise *vector* *dominance*. This 0/1 property can be relaxed to an asymmetric *hinge* *distance* for scoring and ranking candidate documents. Here we focus on data-sensitive, trainable indices for fast retrieval of relevant documents. Existing LSH methods are designed for mostly symmetric or few  simple asymmetric distance functions, which are not suitable for hinge distance. Instead, we transform hinge distance into a proposed *dominance* *similarity* measure, to which we then apply a Fourier transform, thereby expressing dominance similarity as an expectation of inner products of functions in the frequency domain. Next, we approximate the expectation with an importance-sampled estimate. The overall consequence is that now we can use a traditional LSH, but in the frequency domain. To ensure that the LSH uses hash bits efficiently, we learn hash functions that are sensitive to both corpus and query distributions, mapped to the frequency domain. Our experiments show that the proposed asymmetric dominance similarity is critical to the targeted applications, and that our LSH, which we call FourierHashNet, provides a better query time vs. retrieval quality trade-off, compared to several baselines. Both the Fourier transform and the trainable hash codes contribute to performance gains."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0656b09ccc3b06354f8129c8dbf444ed88c767d3.pdf"}, "_bibtex": {"value": "@inproceedings{\nroy2023locality,\ntitle={Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search},\nauthor={Indradyumna Roy and Rishi Agarwal and Soumen Chakrabarti and Anirban Dasgupta and Abir De},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=rUf0GV5CuU}\n}"}, "paperhash": {"value": "roy|locality_sensitive_hashing_in_fourier_frequency_domain_for_soft_set_containment_search"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13356/-/Revision", "NeurIPS.cc/2023/Conference/Submission13356/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13356/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326126663, "odate": 1698949782882, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "EldbUlZtbd", "number": 13353, "cdate": 1683818619009, "tcdate": 1683818619009, "mdate": 1698949782848, "tmdate": 1698949782848, "signatures": ["NeurIPS.cc/2023/Conference/Submission13353/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13353/Authors"], "forum": "EldbUlZtbd", "content": {"title": {"value": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models"}, "authors": {"value": ["Peter Hase", "Mohit Bansal", "Been Kim", "Asma Ghandeharioun"]}, "authorids": {"value": ["~Peter_Hase1", "~Mohit_Bansal2", "~Been_Kim1", "~Asma_Ghandeharioun1"]}, "keywords": {"value": ["localization", "model editing", "mechanistic interpretability", "language models"]}, "abstract": {"value": "Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing problem, including erasing and amplifying facts. For one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/873c2db4f2fd3c87bfcfbdfef51d9d4bcebc464e.pdf"}, "supplementary_material": {"value": "/attachment/3ce71714041af1a8fbce554c6c6de928f3e5dd28.zip"}, "_bibtex": {"value": "@inproceedings{\nhase2023does,\ntitle={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},\nauthor={Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=EldbUlZtbd}\n}"}, "paperhash": {"value": "hase|does_localization_inform_editing_surprising_differences_in_causalitybased_localization_vs_knowledge_editing_in_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13353/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13353/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326126578, "odate": 1698949782834, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cpUuSV8kRw", "number": 13343, "cdate": 1683818573281, "tcdate": 1683818573281, "mdate": 1698954291608, "tmdate": 1698954291608, "signatures": ["NeurIPS.cc/2023/Conference/Submission13343/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13343/Authors"], "forum": "cpUuSV8kRw", "content": {"title": {"value": "Group Fairness in Peer Review"}, "authors": {"value": ["Haris Aziz", "Evi Micha", "Nisarg Shah"]}, "authorids": {"value": ["~Haris_Aziz1", "~Evi_Micha1", "~Nisarg_Shah1"]}, "keywords": {"value": ["peer review; group fairness; core; stable"]}, "TLDR": {"value": "We define group fairness in a peer review assignment problem using the concept of core stability and show that a review assignment in the core can be efficiently found when each submission is single-authored."}, "abstract": {"value": "Large conferences such as NeurIPS and AAAI serve as crossroads  of various AI fields, since they attract submissions from a vast number of communities. However, in some cases, this has resulted in a poor reviewing experience for some communities, whose submissions get assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up any such large conference into smaller conferences, but this can lead to isolation of communities and harm interdisciplinary research. We tackle this challenge by introducing a  notion of group fairness, called the core, which requires that every possible community (subset of researchers) to be treated in a way that prevents them from unilaterally benefiting by  withdrawing from a large conference. \n\nWe study a simple peer review model, prove that it always admits a reviewing assignment in the core, and design an efficient algorithm to find one such assignment. \nWe use real data from CVPR and ICLR conferences to compare our algorithm to existing reviewing assignment algorithms on a number of metrics."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/022bd1d7901dd30a148659e843058bf7c3137e5c.pdf"}, "supplementary_material": {"value": "/attachment/d4fd6df46b9fb4094c09fa399fb30952c4d12359.zip"}, "_bibtex": {"value": "@inproceedings{\naziz2023group,\ntitle={Group Fairness in Peer Review},\nauthor={Haris Aziz and Evi Micha and Nisarg Shah},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cpUuSV8kRw}\n}"}, "paperhash": {"value": "aziz|group_fairness_in_peer_review"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13343/-/Revision", "NeurIPS.cc/2023/Conference/Submission13343/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13343/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326126138, "odate": 1698949782754, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "0VcvYQ3uPh", "number": 13335, "cdate": 1683818526320, "tcdate": 1683818526320, "mdate": 1698949782721, "tmdate": 1698949782721, "signatures": ["NeurIPS.cc/2023/Conference/Submission13335/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission13335/Authors"], "forum": "0VcvYQ3uPh", "content": {"title": {"value": "Improved Frequency Estimation Algorithms with and without Predictions"}, "authors": {"value": ["Anders Aamand", "Justin Y. Chen", "Huy Nguyen", "Sandeep Silwal", "Ali Vakilian"]}, "authorids": {"value": ["~Anders_Aamand1", "~Justin_Y._Chen1", "~Huy_Nguyen1", "~Sandeep_Silwal1", "~Ali_Vakilian1"]}, "keywords": {"value": ["learning-augmented algorithms", "algorithms with predictions", "data-driven algorithms", "sublinear", "streaming", "frequency estimation", "sketching"]}, "TLDR": {"value": "We give improved learning-augmented algorithms for frequency estimation."}, "abstract": {"value": "Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7bf950cb1db5c09d69ee007e41bd1d9739866768.pdf"}, "supplementary_material": {"value": "/attachment/7433548be99855c66d61f01ff751d1fc044ddf47.zip"}, "_bibtex": {"value": "@inproceedings{\naamand2023improved,\ntitle={Improved Frequency Estimation Algorithms with and without Predictions},\nauthor={Anders Aamand and Justin Y. Chen and Huy Nguyen and Sandeep Silwal and Ali Vakilian},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=0VcvYQ3uPh}\n}"}, "paperhash": {"value": "aamand|improved_frequency_estimation_algorithms_with_and_without_predictions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission13335/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission13335/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326125974, "odate": 1698949782705, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "75v88kyyko", "number": 12822, "cdate": 1683814689530, "tcdate": 1683814689530, "mdate": 1698949780280, "tmdate": 1698949780280, "signatures": ["NeurIPS.cc/2023/Conference/Submission12822/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12822/Authors"], "forum": "75v88kyyko", "content": {"title": {"value": "Hierarchical clustering with dot products recovers hidden tree structure"}, "authors": {"value": ["Annie Gray", "Alexander Modell", "Patrick Rubin-Delanchy", "Nick Whiteley"]}, "authorids": {"value": ["~Annie_Gray1", "~Alexander_Modell1", "~Patrick_Rubin-Delanchy1", "~Nick_Whiteley1"]}, "keywords": {"value": ["agglomerative clustering", "generative model", "graphical model", "hierarchical clustering", "high-dimensional data"]}, "TLDR": {"value": "A new perspective on recovery of tree structure from data."}, "abstract": {"value": "In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a69d30d7df545a2d0b06855f3b26b302a1b65a4b.pdf"}, "supplementary_material": {"value": "/attachment/7906e817651c17b377cb4ae41437eed0e52a362c.zip"}, "_bibtex": {"value": "@inproceedings{\ngray2023hierarchical,\ntitle={Hierarchical clustering with dot products recovers hidden tree structure},\nauthor={Annie Gray and Alexander Modell and Patrick Rubin-Delanchy and Nick Whiteley},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=75v88kyyko}\n}"}, "paperhash": {"value": "gray|hierarchical_clustering_with_dot_products_recovers_hidden_tree_structure"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12822/-/Revision", "NeurIPS.cc/2023/Conference/Submission12822/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12822/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326113147, "odate": 1698949780266, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5bWW9Eop7l", "number": 12821, "cdate": 1683814685420, "tcdate": 1683814685420, "mdate": 1698949780237, "tmdate": 1698949780237, "signatures": ["NeurIPS.cc/2023/Conference/Submission12821/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12821/Authors"], "forum": "5bWW9Eop7l", "content": {"title": {"value": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs"}, "authors": {"value": ["Laura Eline Ruis", "Akbir Khan", "Stella Biderman", "Sara Hooker", "Tim Rockt\u00e4schel", "Edward Grefenstette"]}, "authorids": {"value": ["~Laura_Eline_Ruis1", "~Akbir_Khan1", "~Stella_Biderman1", "~Sara_Hooker2", "~Tim_Rockt\u00e4schel1", "~Edward_Grefenstette1"]}, "keywords": {"value": ["large language models", "pragmatics", "natural language processing", "communication", "conversation", "implicature", "language model fine-tuning"]}, "TLDR": {"value": "Of four categories of large language models evaluated the only type of model that understands implicatures is those fine-tuned on example-level instruction data."}, "abstract": {"value": "Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context---incorporating its pragmatics. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response \"I wore gloves\" to the question \"Did you leave fingerprints?\" as meaning \"No\". To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate four categories of widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), models in three of these categories perform close to random. However, LLMs instruction-tuned at the example-level perform significantly better. These results suggest that certain fine-tuning strategies are far better at inducing pragmatic understanding in models. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9a2d92e21913cbee388de4eb8893f78a3d612459.pdf"}, "supplementary_material": {"value": "/attachment/84714cc99c2ae53153c15c045bc7d5cfcbe771d2.zip"}, "_bibtex": {"value": "@inproceedings{\nruis2023the,\ntitle={The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by {LLM}s},\nauthor={Laura Eline Ruis and Akbir Khan and Stella Biderman and Sara Hooker and Tim Rockt{\\\"a}schel and Edward Grefenstette},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5bWW9Eop7l}\n}"}, "paperhash": {"value": "ruis|the_goldilocks_of_pragmatic_understanding_finetuning_strategy_matters_for_implicature_resolution_by_llms"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12821/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12821/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326113094, "odate": 1698949780224, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "P1TCHxJwLB", "number": 12787, "cdate": 1683814382599, "tcdate": 1683814382599, "mdate": 1698949780087, "tmdate": 1698949780087, "signatures": ["NeurIPS.cc/2023/Conference/Submission12787/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12787/Authors"], "forum": "P1TCHxJwLB", "content": {"title": {"value": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling"}, "authors": {"value": ["Zhen Qin", "Songlin Yang", "Yiran Zhong"]}, "authorids": {"value": ["~Zhen_Qin6", "~Songlin_Yang1", "~Yiran_Zhong1"]}, "keywords": {"value": ["RNN", "Sequence Modeling", "NLP"]}, "abstract": {"value": "Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling.\nRecently, there has been a renewed interest in using linear RNNs for efficient sequence modeling.\nThese linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling"}, "pdf": {"value": "/pdf/2ca76587cfc8094ef7217d592459e0951accbd12.pdf"}, "supplementary_material": {"value": "/attachment/58f3f3c16b8907a09b3306617c6850e78de0e99e.pdf"}, "_bibtex": {"value": "@inproceedings{\nqin2023hierarchically,\ntitle={Hierarchically Gated Recurrent Neural Network for Sequence Modeling},\nauthor={Zhen Qin and Songlin Yang and Yiran Zhong},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=P1TCHxJwLB}\n}"}, "paperhash": {"value": "qin|hierarchically_gated_recurrent_neural_network_for_sequence_modeling"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12787/-/Revision", "NeurIPS.cc/2023/Conference/Submission12787/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12787/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326112256, "odate": 1698949780072, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "phnN1eu5AX", "number": 12717, "cdate": 1683813795353, "tcdate": 1683813795353, "mdate": 1698949779747, "tmdate": 1698949779747, "signatures": ["NeurIPS.cc/2023/Conference/Submission12717/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12717/Authors"], "forum": "phnN1eu5AX", "content": {"title": {"value": "Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance"}, "authors": {"value": ["Jinwoo Kim", "Dat Tien Nguyen", "Ayhan Suleymanzade", "Hyeokjun An", "Seunghoon Hong"]}, "authorids": {"value": ["~Jinwoo_Kim4", "~Dat_Tien_Nguyen2", "~Ayhan_Suleymanzade1", "~Hyeokjun_An1", "~Seunghoon_Hong2"]}, "keywords": {"value": ["equivariant machine learning", "transformers", "graphs", "general-purpose architectures"]}, "TLDR": {"value": "A framework for equivariant learning with non-equivariant architectures, e.g., vision transformer for graph recognition."}, "abstract": {"value": "We present a novel framework to overcome the limitations of equivariant architectures in learning functions with group symmetries. In contrary to equivariant architectures, the framework uses an arbitrary backbone (such as an MLP or a transformer) and symmetrizes it to be equivariant to given group by employing a small equivariant network that parameterizes the probabilistic distribution underlying the symmetrization. The distribution is end-to-end trained with the backbone which can maximize performance while reducing sample complexity of symmetrization. We show that this approach ensures not only equivariance to the given group but also universal approximation ability in expectation. We implement our method on a simple patch-based transformer backbone initialized from pretrained vision transformer, and test it for a wide range of symmetry groups including permutation and Euclidean groups and their combinations. Empirical tests show competitive results against tailored equivariant architectures, suggesting the potential for learning equivariant functions for diverse groups using a non-equivariant universal backbone. We further show evidence of enhanced learning in symmetric modalities, like graphs, when pretrained from non-symmetric modalities, like vision."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b8f1e430987850a8ebb13acc4c0096b3d5436ce0.pdf"}, "supplementary_material": {"value": "/attachment/8a0bbdfc31d79b4942800e5d829e8833aa54896b.pdf"}, "_bibtex": {"value": "@inproceedings{\nkim2023learning,\ntitle={Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance},\nauthor={Jinwoo Kim and Dat Tien Nguyen and Ayhan Suleymanzade and Hyeokjun An and Seunghoon Hong},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=phnN1eu5AX}\n}"}, "paperhash": {"value": "kim|learning_probabilistic_symmetrization_for_architecture_agnostic_equivariance"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12717/-/Revision", "NeurIPS.cc/2023/Conference/Submission12717/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12717/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326110448, "odate": 1698949779730, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ZQMlfNijY5", "number": 12705, "cdate": 1683813708355, "tcdate": 1683813708355, "mdate": 1698949779707, "tmdate": 1698949779707, "signatures": ["NeurIPS.cc/2023/Conference/Submission12705/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12705/Authors"], "forum": "ZQMlfNijY5", "content": {"title": {"value": "Normalizing flow neural networks by JKO scheme"}, "authors": {"value": ["Chen Xu", "Xiuyuan Cheng", "Yao Xie"]}, "authorids": {"value": ["~Chen_Xu12", "~Xiuyuan_Cheng1", "~Yao_Xie2"]}, "keywords": {"value": ["Normalizing flow", "invertible neural networks", "JKO scheme"]}, "TLDR": {"value": "Neural ODE model which learns the deterministic transport equation that solves the Fokker-Planck equation of the diffusion process, allowing block-wise training inspired by the JKO scheme at reduced computation and memory cost."}, "abstract": {"value": "Normalizing flow is a class of deep generative models for efficient sampling and likelihood estimation, which achieves attractive performance, particularly in high dimensions. The flow is often implemented using a sequence of invertible residual blocks. Existing works adopt special network architectures and regularization of flow trajectories. In this paper, we develop a neural ODE flow network called JKO-iFlow, inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which unfolds the discrete-time dynamic of the Wasserstein gradient flow. The proposed method stacks residual blocks one after another, allowing efficient block-wise training of the residual blocks, avoiding sampling SDE trajectories and score matching or variational learning, thus reducing the memory load and difficulty in end-to-end training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the induced trajectory in probability space to improve the model accuracy further. Experiments with synthetic and real data show that the proposed JKO-iFlow network achieves competitive performance compared with existing flow and diffusion models at a significantly reduced computational and memory cost."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b386bcf2d78b531757087ac468d4cc1a1a024e6f.pdf"}, "_bibtex": {"value": "@inproceedings{\nxu2023normalizing,\ntitle={Normalizing flow neural networks by {JKO} scheme},\nauthor={Chen Xu and Xiuyuan Cheng and Yao Xie},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ZQMlfNijY5}\n}"}, "paperhash": {"value": "xu|normalizing_flow_neural_networks_by_jko_scheme"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12705/-/Revision", "NeurIPS.cc/2023/Conference/Submission12705/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12705/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326110201, "odate": 1698949779692, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "LTbIUkN95h", "number": 12687, "cdate": 1683813562353, "tcdate": 1683813562353, "mdate": 1698949779578, "tmdate": 1698949779578, "signatures": ["NeurIPS.cc/2023/Conference/Submission12687/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12687/Authors"], "forum": "LTbIUkN95h", "content": {"title": {"value": "Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks"}, "authors": {"value": ["Honghao Wei", "Xin Liu", "Weina Wang", "Lei Ying"]}, "authorids": {"value": ["~Honghao_Wei2", "~Xin_Liu14", "~Weina_Wang1", "~Lei_Ying1"]}, "keywords": {"value": ["Reinforcement Learning", "Mixed Systems", "Queueing Network", "Sample Efficient"]}, "TLDR": {"value": "We propose a sample efficient RL method that accelerates learning in mixed systems."}, "abstract": {"value": "This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic {\\em given} the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including Manufacturing systems, communication networks, and queueing networks. We propose a sample-efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven (model-free), but it learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the optimality gap decreases as  $O\\left(\\sqrt{\\frac{1}{n}}+\\sqrt{\\frac{1}{m}}\\right),$ where $n$ represents the number of real samples, and $m$ is the number of augmented samples per real sample. It is important to note that without augmented samples, the optimality gap is $O(1)$ due to the insufficient data coverage of the pseudo-stochastic states. Our experimental results on multiple queueing network applications confirm that the proposed method indeed significantly accelerates both deep Q-learning and deep policy gradient."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/69edfe02e4a5907f1172ddcb774d1138aee62e60.pdf"}, "supplementary_material": {"value": "/attachment/26a5b58edf7d428193249415efa4ea74e8c1fadf.zip"}, "_bibtex": {"value": "@inproceedings{\nwei2023sample,\ntitle={Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks},\nauthor={Honghao Wei and Xin Liu and Weina Wang and Lei Ying},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=LTbIUkN95h}\n}"}, "paperhash": {"value": "wei|sample_efficient_reinforcement_learning_in_mixed_systems_through_augmented_samples_and_its_applications_to_queueing_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12687/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12687/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326109735, "odate": 1698949779564, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "uvdJgFFzby", "number": 12679, "cdate": 1683813457582, "tcdate": 1683813457582, "mdate": 1698949779516, "tmdate": 1698949779516, "signatures": ["NeurIPS.cc/2023/Conference/Submission12679/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12679/Authors"], "forum": "uvdJgFFzby", "content": {"title": {"value": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, "authors": {"value": ["Sotiris Anagnostidis", "Dario Pavllo", "Luca Biggio", "Lorenzo Noci", "Aurelien Lucchi", "Thomas Hofmann"]}, "authorids": {"value": ["~Sotiris_Anagnostidis1", "~Dario_Pavllo2", "~Luca_Biggio1", "~Lorenzo_Noci1", "~Aurelien_Lucchi1", "~Thomas_Hofmann1"]}, "keywords": {"value": ["Transformers", "Context-pruning", "Efficient Transformer"]}, "abstract": {"value": "Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/315993e807c69e009985eff2b928999b73968cd9.pdf"}, "_bibtex": {"value": "@inproceedings{\nanagnostidis2023dynamic,\ntitle={Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers},\nauthor={Sotiris Anagnostidis and Dario Pavllo and Luca Biggio and Lorenzo Noci and Aurelien Lucchi and Thomas Hofmann},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=uvdJgFFzby}\n}"}, "paperhash": {"value": "anagnostidis|dynamic_context_pruning_for_efficient_and_interpretable_autoregressive_transformers"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12679/-/Revision", "NeurIPS.cc/2023/Conference/Submission12679/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12679/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326109583, "odate": 1698949779501, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Ex3oJEKS53", "number": 12578, "cdate": 1683812459664, "tcdate": 1683812459664, "mdate": 1698949778981, "tmdate": 1698949778981, "signatures": ["NeurIPS.cc/2023/Conference/Submission12578/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12578/Authors"], "forum": "Ex3oJEKS53", "content": {"title": {"value": "Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures"}, "authors": {"value": ["Runa Eschenhagen", "Alexander Immer", "Richard E Turner", "Frank Schneider", "Philipp Hennig"]}, "authorids": {"value": ["~Runa_Eschenhagen1", "~Alexander_Immer1", "~Richard_E_Turner1", "~Frank_Schneider1", "~Philipp_Hennig1"]}, "keywords": {"value": ["deep learning", "second-order", "optimization", "natural gradient", "fisher", "gauss-newton", "k-fac", "weight-sharing"]}, "TLDR": {"value": "We generalise Kronecker-Factored Approximate Curvature to modern deep neural network architectures like transformers, graph, and convolutional neural networks."}, "abstract": {"value": "The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with *weight-sharing*. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- *expand* and *reduce*. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer. However, both variations are able to reach a fixed validation metric target in $50$-$75$\\% of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. This highlights the potential of applying K-FAC to modern neural network architectures."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5febc0ba20588fc6f27f5eb50943e20738d8fe17.pdf"}, "supplementary_material": {"value": "/attachment/f2763127d9a50c4a81d1ac66ca8e75037d0db653.zip"}, "_bibtex": {"value": "@inproceedings{\neschenhagen2023kroneckerfactored,\ntitle={Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures},\nauthor={Runa Eschenhagen and Alexander Immer and Richard E Turner and Frank Schneider and Philipp Hennig},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Ex3oJEKS53}\n}"}, "paperhash": {"value": "eschenhagen|kroneckerfactored_approximate_curvature_for_modern_neural_network_architectures"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12578/-/Revision", "NeurIPS.cc/2023/Conference/Submission12578/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12578/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326106953, "odate": 1698949778967, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "eVrmcOvJV4", "number": 12530, "cdate": 1683812040774, "tcdate": 1683812040774, "mdate": 1698949778808, "tmdate": 1698949778808, "signatures": ["NeurIPS.cc/2023/Conference/Submission12530/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12530/Authors"], "forum": "eVrmcOvJV4", "content": {"title": {"value": "Inferring the Future by Imagining the Past"}, "authors": {"value": ["Kartik Chandra", "Tony Chen", "Tzu-Mao Li", "Jonathan Ragan-Kelley", "Joshua B. Tenenbaum"]}, "authorids": {"value": ["~Kartik_Chandra2", "~Tony_Chen1", "~Tzu-Mao_Li1", "~Jonathan_Ragan-Kelley1", "~Joshua_B._Tenenbaum1"]}, "keywords": {"value": ["cognitive science", "cogsci", "inverse planning", "Bayesian inference", "theory of mind", "Monte Carlo", "inverse reinforcement learning"]}, "TLDR": {"value": "We model how humans infer an agent's goal from a snapshot of its current state. We frame the problem as Monte Carlo path tracing, which allows us to apply ideas from computer graphics to design a cognitively-plausible sample-efficient algorithm."}, "abstract": {"value": "A single panel of a comic book can say a lot: it can depict not only where the characters currently are, but also their motions, their motivations, their emotions, and what they might do next. More generally, humans routinely infer complex sequences of past and future events from a *static snapshot* of a *dynamic scene*, even in situations they have never seen before.\n\nIn this paper, we model how humans make such rapid and flexible inferences. Building on a long line of work in cognitive science, we offer a Monte Carlo algorithm whose inferences correlate well with human intuitions in a wide variety of domains, while only using a small, cognitively-plausible number of samples. Our key technical insight is a surprising connection between our inference problem and Monte Carlo path tracing, which allows us to apply decades of ideas from the computer graphics community to this seemingly-unrelated theory of mind task."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f2170d4c196d8d9ab7355bbb0745acf526324d5b.pdf"}, "supplementary_material": {"value": "/attachment/c5b3cc6b4dc704bf47e4d68934c43e94c3308503.zip"}, "_bibtex": {"value": "@inproceedings{\nchandra2023inferring,\ntitle={Inferring the Future by Imagining the Past},\nauthor={Kartik Chandra and Tony Chen and Tzu-Mao Li and Jonathan Ragan-Kelley and Joshua B. Tenenbaum},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=eVrmcOvJV4}\n}"}, "paperhash": {"value": "chandra|inferring_the_future_by_imagining_the_past"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12530/-/Revision", "NeurIPS.cc/2023/Conference/Submission12530/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12530/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326105791, "odate": 1698949778793, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "nCLdsEzZBV", "number": 12442, "cdate": 1683811287865, "tcdate": 1683811287865, "mdate": 1698949778381, "tmdate": 1698949778381, "signatures": ["NeurIPS.cc/2023/Conference/Submission12442/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12442/Authors"], "forum": "nCLdsEzZBV", "content": {"title": {"value": "The Equivalence of Dynamic and Strategic Stability under Regularized Learning in Games"}, "authors": {"value": ["Victor Boone", "Panayotis Mertikopoulos"]}, "authorids": {"value": ["~Victor_Boone1", "~Panayotis_Mertikopoulos1"]}, "keywords": {"value": ["Regularized learning", "dynamic stability", "strategic stability", "Nash equilibrium"]}, "abstract": {"value": "In this paper, we examine the long-run behavior of regularized, no-regret learning in finite N-player games. A well-known result in the field states that the empirical frequencies of play under no-regret learning converge to the game\u2019s set of coarse correlated equilibria; however, our understanding of how the players' _actual strategies_ evolve over time is much more limited \u2013 and, in many cases, non-existent. This issue is exacerbated further by a series of recent results showing that _only_ strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and _pointwise_ solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the _setwise_ rationality properties of the players' day-to-day trajectory of play. To do so, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator \u2013 a property known as _closedness under better replies_ (club). In so doing, we obtain a remarkable equivalence between strategic and dynamic stability: _a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning._ In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a finite number of iterations, even with bandit, payoff-based feedback."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1d544402bfdd0dcd8791277c8fd7945f97284a9d.pdf"}, "supplementary_material": {"value": "/attachment/ff055ed9385c11c9f3d5eab88b6c80e779402653.pdf"}, "_bibtex": {"value": "@inproceedings{\nboone2023the,\ntitle={The Equivalence of Dynamic and Strategic Stability under Regularized Learning in Games},\nauthor={Victor Boone and Panayotis Mertikopoulos},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=nCLdsEzZBV}\n}"}, "paperhash": {"value": "boone|the_equivalence_of_dynamic_and_strategic_stability_under_regularized_learning_in_games"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12442/-/Revision", "NeurIPS.cc/2023/Conference/Submission12442/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12442/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326103320, "odate": 1698949778301, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "HwhRehMr4a", "number": 12388, "cdate": 1683810673664, "tcdate": 1683810673664, "mdate": 1699983039445, "tmdate": 1699983039445, "signatures": ["NeurIPS.cc/2023/Conference/Submission12388/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12388/Authors"], "forum": "HwhRehMr4a", "content": {"title": {"value": "Future-Dependent Value-Based Off-Policy Evaluation in POMDPs"}, "authors": {"value": ["Masatoshi Uehara", "Haruka Kiyohara", "Andrew Bennett", "Victor Chernozhukov", "Nan Jiang", "Nathan Kallus", "Chengchun Shi", "Wen Sun"]}, "authorids": {"value": ["~Masatoshi_Uehara1", "~Haruka_Kiyohara1", "~Andrew_Bennett5", "~Victor_Chernozhukov1", "~Nan_Jiang2", "~Nathan_Kallus1", "~Chengchun_Shi1", "~Wen_Sun1"]}, "keywords": {"value": ["Reinforcement learning theory", "POMDP", "PAC RL", "Off-policy evaluation", "Offilne reinforcement learning"]}, "abstract": {"value": "We study off-policy evaluation (OPE) for partially observable MDPs (POMDPs) with general function approximation. Existing methods such as sequential importance sampling estimators and fitted-Q evaluation suffer from the curse of horizon in POMDPs. To circumvent this problem, we develop a novel model-free OPE method by introducing future-dependent value functions that take future proxies as inputs. Future-dependent value functions play similar roles as classical value functions in fully-observable MDPs. We derive a new off-policy Bellman equation for future-dependent value functions as conditional moment equations that use history proxies as instrumental variables. We further propose a minimax learning method to learn future-dependent value functions using the new Bellman equation. We obtain the PAC result, which implies our OPE estimator is close to the true policy value as long as futures and histories contain sufficient information about latent states, and the Bellman completeness. Our code is available at https://github.com/aiueola/neurips2023-future-dependent-ope"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/628445c008e43934746c23a1344a18dcf30f6451.pdf"}, "TLDR": {"value": "Model-free off-policy evaluation in POMDPs without a curse of horizon"}, "supplementary_material": {"value": "/attachment/fa610c0bebf7174ea70e7f28fd83f65b9d349ae1.zip"}, "_bibtex": {"value": "@inproceedings{\nuehara2023futuredependent,\ntitle={Future-Dependent Value-Based Off-Policy Evaluation in {POMDP}s},\nauthor={Masatoshi Uehara and Haruka Kiyohara and Andrew Bennett and Victor Chernozhukov and Nan Jiang and Nathan Kallus and Chengchun Shi and Wen Sun},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=HwhRehMr4a}\n}"}, "paperhash": {"value": "uehara|futuredependent_valuebased_offpolicy_evaluation_in_pomdps"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12388/-/Revision", "NeurIPS.cc/2023/Conference/Submission12388/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12388/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326101731, "odate": 1698949777972, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "y08bkEtNBK", "number": 12301, "cdate": 1683809769962, "tcdate": 1683809769962, "mdate": 1698949777736, "tmdate": 1698949777736, "signatures": ["NeurIPS.cc/2023/Conference/Submission12301/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12301/Authors"], "forum": "y08bkEtNBK", "content": {"title": {"value": "WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting"}, "authors": {"value": ["Yuxin Jia", "Youfang Lin", "Xinyan Hao", "Yan Lin", "Shengnan Guo", "Huaiyu Wan"]}, "authorids": {"value": ["~Yuxin_Jia1", "~Youfang_Lin1", "~Xinyan_Hao1", "~Yan_Lin1", "~Shengnan_Guo1", "~Huaiyu_Wan1"]}, "keywords": {"value": ["long-range time series forecasting", "information transmission", "long- and short-term repetitive patterns", "global and local correlations"]}, "TLDR": {"value": "We propose a novel WITRAN model, with the best performance and the lowest complexity for long-range time series forecasting."}, "abstract": {"value": "Capturing semantic information is crucial for accurate long-range time series forecasting, which involves modeling global and local correlations, as well as discovering long- and short-term repetitive patterns. Previous works have partially addressed these issues separately, but have not been able to address all of them simultaneously. Meanwhile, their time and memory complexities are still not sufficiently low for long-range forecasting. To address the challenge of capturing different types of semantic information, we propose a novel Water-wave Information Transmission (WIT) framework. This framework captures both long- and short-term repetitive patterns through bi-granular information transmission. It also models global and local correlations by recursively fusing and selecting information using Horizontal Vertical Gated Selective Unit (HVGSU). In addition, to improve the computing efficiency, we propose a generic Recurrent Acceleration Network (RAN) which reduces the time complexity to $\\mathcal{O}(\\sqrt{L})$ while maintaining the memory complexity at $\\mathcal{O}(L)$. Our proposed method, called Water-wave Information Transmission and Recurrent Acceleration Network (WITRAN), outperforms the state-of-the-art methods by 5.80% and 14.28% on long-range and ultra-long-range time series forecasting tasks respectively, as demonstrated by experiments on four benchmark datasets. The code is available at: https://github.com/Water2sea/WITRAN."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/aa8048e3c5e6bf5ed06e363ae34e19596e54bc20.pdf"}, "_bibtex": {"value": "@inproceedings{\njia2023witran,\ntitle={{WITRAN}: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting},\nauthor={Yuxin Jia and Youfang Lin and Xinyan Hao and Yan Lin and Shengnan Guo and Huaiyu Wan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=y08bkEtNBK}\n}"}, "paperhash": {"value": "jia|witran_waterwave_information_transmission_and_recurrent_acceleration_network_for_longrange_time_series_forecasting"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12301/-/Revision", "NeurIPS.cc/2023/Conference/Submission12301/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12301/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326099396, "odate": 1698949777723, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "htM8yp2EwX", "number": 12290, "cdate": 1683809594894, "tcdate": 1683809594894, "mdate": 1698949777662, "tmdate": 1698949777662, "signatures": ["NeurIPS.cc/2023/Conference/Submission12290/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12290/Authors"], "forum": "htM8yp2EwX", "content": {"title": {"value": "AMDP: An Adaptive Detection Procedure for False Discovery Rate Control in High-Dimensional Mediation Analysis"}, "authors": {"value": ["Jiarong Ding", "Xuehu Zhu"]}, "authorids": {"value": ["djr9901@stu.xjtu.edu.cn", "~Xuehu_Zhu1"]}, "keywords": {"value": ["Mediation analysis", "Composite null hypothesis", "Local false discovery rate", "Optimal ranking rule", "High-dimensional"]}, "abstract": {"value": "High-dimensional mediation analysis is often associated with a multiple testing problem for detecting significant mediators. Assessing the uncertainty of this detecting process via false discovery rate (FDR) has garnered great interest. To control the FDR in multiple testing, two essential steps are involved: ranking and selection. Existing approaches either construct p-values without calibration or disregard the joint information across tests, leading to conservation in FDR control or non-optimal ranking rules for multiple hypotheses. In this paper, we develop an adaptive mediation detection procedure (referred to as \"AMDP\") to identify relevant mediators while asymptotically controlling the FDR in high-dimensional mediation analysis. AMDP produces the optimal rule for ranking hypotheses and proposes a data-driven strategy to determine the threshold for mediator selection. This novel method captures information from the proportions of composite null hypotheses and the distribution of p-values, which turns the high dimensionality into an advantage instead of a limitation. The numerical studies on synthetic and real data sets illustrate the performances of AMDP compared with existing approaches."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/cd6d250d84e4259b0e4ad34d1bca7bb04ba2d7a5.pdf"}, "supplementary_material": {"value": "/attachment/c6dd55755f6ede22b10e39c44139ff5e142c35ca.pdf"}, "_bibtex": {"value": "@inproceedings{\nding2023amdp,\ntitle={{AMDP}: An Adaptive Detection Procedure for False Discovery Rate Control in High-Dimensional Mediation Analysis},\nauthor={Jiarong Ding and Xuehu Zhu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=htM8yp2EwX}\n}"}, "paperhash": {"value": "ding|amdp_an_adaptive_detection_procedure_for_false_discovery_rate_control_in_highdimensional_mediation_analysis"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12290/-/Revision", "NeurIPS.cc/2023/Conference/Submission12290/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12290/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326099051, "odate": 1698949777647, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "yEfmhgwslQ", "number": 12286, "cdate": 1683809511774, "tcdate": 1683809511774, "mdate": 1698949777712, "tmdate": 1698949777712, "signatures": ["NeurIPS.cc/2023/Conference/Submission12286/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12286/Authors"], "forum": "yEfmhgwslQ", "content": {"title": {"value": "Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency"}, "authors": {"value": ["Owen Queen", "Thomas Hartvigsen", "Teddy Koker", "Huan He", "Theodoros Tsiligkaridis", "Marinka Zitnik"]}, "authorids": {"value": ["~Owen_Queen1", "~Thomas_Hartvigsen1", "~Teddy_Koker1", "~Huan_He2", "~Theodoros_Tsiligkaridis1", "~Marinka_Zitnik1"]}, "keywords": {"value": ["Explainability", "Interpretability", "Time Series", "Explanations", "Temporal patterns", "Model Understanding", "Latent space", "Self-supervised learning"]}, "abstract": {"value": "Interpreting time series models is uniquely challenging because it requires identifying both the location of time series signals that drive model predictions and their matching to an interpretable temporal pattern. While explainers from other modalities can be applied to time series, their inductive biases do not transfer well to the inherently challenging interpretation of time series. We present TimeX, a time series consistency model for training explainers. TimeX trains an interpretable surrogate to mimic the behavior of a pretrained time series model. It addresses the issue of model faithfulness by introducing model behavior consistency, a novel formulation that preserves relations in the latent space induced by the pretrained model with relations in the latent space induced by TimeX. TimeX provides discrete attribution maps and, unlike existing interpretability methods, it learns a latent space of explanations that can be used in various ways, such as to provide landmarks to visually aggregate similar explanations and easily recognize temporal patterns. We evaluate TimeX on eight synthetic and real-world datasets and compare its performance against state-of-the-art interpretability methods. We also conduct case studies using physiological time series. Quantitative evaluations demonstrate that TimeX achieves the highest or second-highest performance in every metric compared to baselines across all datasets. Through case studies, we show that the novel components of TimeX show potential for training faithful, interpretable models that capture the behavior of pretrained time series models."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "TimeX is an in-hoc explainer for time series models that generates interpretable attributions and also learns a latent space of explanations, along with landmarks, to summarize groups of informative temporal patterns."}, "pdf": {"value": "/pdf/6c929f4720cece2dbcfdb739210fda7a6e365afb.pdf"}, "_bibtex": {"value": "@inproceedings{\nqueen2023encoding,\ntitle={Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency},\nauthor={Owen Queen and Thomas Hartvigsen and Teddy Koker and Huan He and Theodoros Tsiligkaridis and Marinka Zitnik},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=yEfmhgwslQ}\n}"}, "paperhash": {"value": "queen|encoding_timeseries_explanations_through_selfsupervised_model_behavior_consistency"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12286/-/Revision", "NeurIPS.cc/2023/Conference/Submission12286/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12286/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326098983, "odate": 1698949777631, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "b1JPBGJhUi", "number": 12267, "cdate": 1683809296976, "tcdate": 1683809296976, "mdate": 1698949777477, "tmdate": 1698949777477, "signatures": ["NeurIPS.cc/2023/Conference/Submission12267/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12267/Authors"], "forum": "b1JPBGJhUi", "content": {"title": {"value": "Stable Nonconvex-Nonconcave Training via Linear Interpolation"}, "authors": {"value": ["Thomas Pethick", "Wanyun Xie", "Volkan Cevher"]}, "authorids": {"value": ["~Thomas_Pethick1", "~Wanyun_Xie1", "~Volkan_Cevher1"]}, "keywords": {"value": ["Minimax optimization", "Lookahead", "Generative adversarial networks", "Stability", "Nonconvex-nonconcave", "Cohypomonotone"]}, "abstract": {"value": "This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of the base optimizer. We corroborate the results with experiments on generative adversarial networks which demonstrates the benefits of the linear interpolation present in both RAPP and Lookahead."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f7129e4dbe08114e66763fc14c2ad8b779a56a1b.pdf"}, "supplementary_material": {"value": "/attachment/4ec35b5132975128b5a6af3b55b552a8e516f2be.pdf"}, "_bibtex": {"value": "@inproceedings{\npethick2023stable,\ntitle={Stable Nonconvex-Nonconcave Training via Linear Interpolation},\nauthor={Thomas Pethick and Wanyun Xie and Volkan Cevher},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=b1JPBGJhUi}\n}"}, "paperhash": {"value": "pethick|stable_nonconvexnonconcave_training_via_linear_interpolation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12267/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12267/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326098189, "odate": 1698949777462, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "6EaLIw3W7c", "number": 12223, "cdate": 1683808789335, "tcdate": 1683808789335, "mdate": 1698949777029, "tmdate": 1698949777029, "signatures": ["NeurIPS.cc/2023/Conference/Submission12223/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12223/Authors"], "forum": "6EaLIw3W7c", "content": {"title": {"value": "LinkerNet: Fragment Poses and Linker Co-Design with 3D Equivariant Diffusion"}, "authors": {"value": ["Jiaqi Guan", "Xingang Peng", "PeiQi Jiang", "Yunan Luo", "Jian Peng", "Jianzhu Ma"]}, "authorids": {"value": ["~Jiaqi_Guan1", "~Xingang_Peng1", "~PeiQi_Jiang2", "~Yunan_Luo1", "~Jian_Peng1", "~Jianzhu_Ma2"]}, "keywords": {"value": ["Linker design", "generative models"]}, "abstract": {"value": "Targeted protein degradation techniques, such as PROteolysis TArgeting Chimeras (PROTACs), have emerged as powerful tools for selectively removing disease-causing proteins. One challenging problem in this field is designing a linker to connect different molecular fragments to form a stable drug-candidate molecule. Existing models for linker design assume that the relative positions of the fragments are known, which may not be the case in real scenarios. In this work, we address a more general problem where the poses of the fragments are *unknown* in 3D space. We develop a 3D equivariant diffusion model that jointly learns the generative process of both fragment poses and the 3D structure of the linker. By viewing fragments as rigid bodies, we design a fragment pose prediction module inspired by the Newton-Euler equations in rigid body mechanics. Empirical studies on ZINC and PROTAC-DB datasets demonstrate that our model can generate chemically valid, synthetically-accessible,  and low-energy molecules under both unconstrained and constrained generation settings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/18c4ca5ade0f47732cd4a81cb37e997773ca7d85.pdf"}, "_bibtex": {"value": "@inproceedings{\nguan2023linkernet,\ntitle={LinkerNet: Fragment Poses and Linker Co-Design with 3D Equivariant Diffusion},\nauthor={Jiaqi Guan and Xingang Peng and PeiQi Jiang and Yunan Luo and Jian Peng and Jianzhu Ma},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=6EaLIw3W7c}\n}"}, "paperhash": {"value": "guan|linkernet_fragment_poses_and_linker_codesign_with_3d_equivariant_diffusion"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12223/-/Revision", "NeurIPS.cc/2023/Conference/Submission12223/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12223/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326096722, "odate": 1698949777011, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Pya0kCEpDk", "number": 12104, "cdate": 1683807377700, "tcdate": 1683807377700, "mdate": 1698949776235, "tmdate": 1698949776235, "signatures": ["NeurIPS.cc/2023/Conference/Submission12104/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12104/Authors"], "forum": "Pya0kCEpDk", "content": {"title": {"value": "Private estimation algorithms for stochastic block models and mixture models"}, "authors": {"value": ["Hongjie Chen", "Vincent Cohen-Addad", "Tommaso d'Orsi", "Alessandro Epasto", "Jacob Imola", "David Steurer", "Stefan Tiegel"]}, "authorids": {"value": ["~Hongjie_Chen2", "~Vincent_Cohen-Addad1", "~Tommaso_d'Orsi1", "~Alessandro_Epasto3", "~Jacob_Imola1", "~David_Steurer1", "~Stefan_Tiegel1"]}, "keywords": {"value": ["differential privacy", "stochastic block model", "Gaussian mixture model", "sum of squares"]}, "abstract": {"value": "We introduce general tools for designing efficient private estimation algorithms, in the high-dimensional settings, whose statistical guarantees almost match those of the best known non-private algorithms.\nTo illustrate our techniques, we consider two problems: recovery of stochastic block models and learning mixtures of spherical Gaussians.\n\nFor the former, we present the first efficient $(\\epsilon, \\delta)$-differentially private algorithm for both weak recovery and exact recovery. Previously known algorithms achieving comparable guarantees required quasi-polynomial time. \n\nFor the latter, we design an  $(\\epsilon, \\delta)$-differentially private algorithm that recovers the centers of the $k$-mixture when the minimum separation is at least $\tO(k^{1/t}\\sqrt{t})$. For all choices of $t$, this algorithm requires sample complexity $n\\geq k^{O(1)}d^{O(t)}$ and time complexity $(nd)^{O(t)}$. Prior work required either an additional additive $\\Omega(\\sqrt{\\log n})$ term in the minimum separation or an explicit upper bound on the Euclidean norm of the centers."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c8c1ba0bc6b796708cb382477247d97fd3dc847a.pdf"}, "_bibtex": {"value": "@inproceedings{\nchen2023private,\ntitle={Private estimation algorithms for stochastic block models and mixture models},\nauthor={Hongjie Chen and Vincent Cohen-Addad and Tommaso d'Orsi and Alessandro Epasto and Jacob Imola and David Steurer and Stefan Tiegel},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Pya0kCEpDk}\n}"}, "paperhash": {"value": "chen|private_estimation_algorithms_for_stochastic_block_models_and_mixture_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12104/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12104/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326092857, "odate": 1698949776222, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "tbbId8u7nP", "number": 12093, "cdate": 1683807282291, "tcdate": 1683807282291, "mdate": 1698949776210, "tmdate": 1698949776210, "signatures": ["NeurIPS.cc/2023/Conference/Submission12093/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12093/Authors"], "forum": "tbbId8u7nP", "content": {"title": {"value": "Tracr: Compiled Transformers as a Laboratory for Interpretability"}, "authors": {"value": ["David Lindner", "Janos Kramar", "Sebastian Farquhar", "Matthew Rahtz", "Thomas McGrath", "Vladimir Mikulik"]}, "authorids": {"value": ["~David_Lindner1", "~Janos_Kramar1", "~Sebastian_Farquhar1", "~Matthew_Rahtz1", "~Thomas_McGrath1", "~Vladimir_Mikulik1"]}, "keywords": {"value": ["interpretability", "transformers", "language models", "RASP", "Tracr", "mechanistic interpretability"]}, "TLDR": {"value": "Compiling human-readable programs into weights of a transformer model to accelerate interpretability research."}, "abstract": {"value": "We show how to \"compile\" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study \"superposition\" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as _ground-truth_ for evaluating interpretability methods. Commonly, because the \"programs\" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/google-deepmind/tracr."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/888a92fcd9a7e5066bdb92133048522ac37e4c4b.pdf"}, "supplementary_material": {"value": "/attachment/b7946c993bff496badb72d6bd10883f69114cf40.zip"}, "_bibtex": {"value": "@inproceedings{\nlindner2023tracr,\ntitle={Tracr: Compiled Transformers as a Laboratory for Interpretability},\nauthor={David Lindner and Janos Kramar and Sebastian Farquhar and Matthew Rahtz and Thomas McGrath and Vladimir Mikulik},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=tbbId8u7nP}\n}"}, "paperhash": {"value": "lindner|tracr_compiled_transformers_as_a_laboratory_for_interpretability"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12093/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12093/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326092621, "odate": 1698949776194, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "nijJN0LHqM", "number": 12020, "cdate": 1683806415256, "tcdate": 1683806415256, "mdate": 1698949775744, "tmdate": 1698949775744, "signatures": ["NeurIPS.cc/2023/Conference/Submission12020/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12020/Authors"], "forum": "nijJN0LHqM", "content": {"title": {"value": "Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima"}, "authors": {"value": ["Dongkuk Si", "Chulhee Yun"]}, "authorids": {"value": ["~Dongkuk_Si1", "~Chulhee_Yun1"]}, "keywords": {"value": ["Sharpness-Aware Minimization", "convex optimization"]}, "TLDR": {"value": "We analyze the convergence guarantee and provide non-convergence examples for Sharpness-Aware Minimization (SAM) with constant perturbation size."}, "abstract": {"value": "Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \\rho \\frac{\\nabla f(x_t)}{\\lVert \\nabla f(x_t) \\rVert}$ of the current point $x_t$. \nExisting studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions.\nPerhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points.\nFor smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\\tilde \\Theta(\\frac{1}{T^2})$, the convergence bound of stochastic SAM suffers an inevitable additive term $\\mathcal O(\\rho^2)$, indicating convergence only up to neighborhoods of optima.\nIn fact, such $\\mathcal O(\\rho^2)$ factors arise for stochastic SAM in all the settings we consider, and also for deterministic SAM in nonconvex cases; importantly, we prove by examples that such terms are unavoidable.\nOur results highlight vastly different characteristics of SAM with vs. without decaying perturbation size or gradient normalization, and suggest that the intuitions gained from one version may not apply to the other."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0bd60eef55973d73af962f4d567266081aec8d9d.pdf"}, "_bibtex": {"value": "@inproceedings{\nsi2023practical,\ntitle={Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima},\nauthor={Dongkuk Si and Chulhee Yun},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=nijJN0LHqM}\n}"}, "paperhash": {"value": "si|practical_sharpnessaware_minimization_cannot_converge_all_the_way_to_optima"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12020/-/Revision", "NeurIPS.cc/2023/Conference/Submission12020/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12020/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326090697, "odate": 1698949775728, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "FQGRkwmRzm", "number": 12013, "cdate": 1683806293985, "tcdate": 1683806293985, "mdate": 1698949775737, "tmdate": 1698949775737, "signatures": ["NeurIPS.cc/2023/Conference/Submission12013/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission12013/Authors"], "forum": "FQGRkwmRzm", "content": {"title": {"value": "Streaming PCA for Markovian Data"}, "authors": {"value": ["Syamantak Kumar", "Purnamrita Sarkar"]}, "authorids": {"value": ["~Syamantak_Kumar1", "~Purnamrita_Sarkar1"]}, "keywords": {"value": ["Streaming PCA", "Markov Chain", "Mixing", "Oja's algorithm"]}, "TLDR": {"value": "We prove near-optimal rate of convergence of Oja's Streaming PCA algorithm for data sampled from a Markov chain and show that it is better than discarding data to reduce dependence at finding the leading eigenvector."}, "abstract": {"value": "Since its inception in 1982, Oja's algorithm has become an established method for streaming principle component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain starting in stationarity. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in scenarios where data can solely be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the objective is to perform inference on parameters of the stationary distribution. Most convergence guarantees for Oja's algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a \"nearly\" independent data stream. In this paper, we obtain the first near-optimal rate for Oja's algorithm on the entire data, where we remove the logarithmic dependence on the sample size, $n$, resulting from throwing data away in downsampling strategies."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/47bd4d0fb252c2f78942a9cfadc6d1d89299c2d9.pdf"}, "supplementary_material": {"value": "/attachment/9688ba166438558129b5794c74657f33aa42eed2.zip"}, "_bibtex": {"value": "@inproceedings{\nkumar2023streaming,\ntitle={Streaming {PCA} for Markovian Data},\nauthor={Syamantak Kumar and Purnamrita Sarkar},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=FQGRkwmRzm}\n}"}, "paperhash": {"value": "kumar|streaming_pca_for_markovian_data"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission12013/-/Revision", "NeurIPS.cc/2023/Conference/Submission12013/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission12013/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326090536, "odate": 1698949775722, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Xasl21tSOf", "number": 11911, "cdate": 1683804634184, "tcdate": 1683804634184, "mdate": 1698949775382, "tmdate": 1698949775382, "signatures": ["NeurIPS.cc/2023/Conference/Submission11911/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11911/Authors"], "forum": "Xasl21tSOf", "content": {"title": {"value": "Provable Training for Graph Contrastive Learning"}, "authors": {"value": ["Yue Yu", "Xiao Wang", "Mengmei Zhang", "Nian Liu", "Chuan Shi"]}, "authorids": {"value": ["~Yue_Yu7", "~Xiao_Wang2", "~Mengmei_Zhang1", "~Nian_Liu3", "~Chuan_Shi1"]}, "keywords": {"value": ["Graph Contrastive Learning", "Graph Neural Networks", "Bound Propagation"]}, "abstract": {"value": "Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node embeddings from augmented graphs without labels. Despite the key principle that maximizing the similarity between positive node pairs while minimizing it between negative node pairs is well established, some fundamental problems are still unclear. Considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? Or are there some nodes more likely to be untrained across graph augmentations and violate the principle? How to distinguish these nodes and further guide the training of GCL? To answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. To address this problem, we propose the metric \"node compactness\", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. We further derive the form of node compactness theoretically through bound propagation, which can be integrated into binary cross-entropy as a regularization. To this end, we propose the PrOvable Training (POT) for GCL, which regularizes the training of GCL to encode node embeddings that follows the GCL principle better. Through extensive experiments on various benchmarks, POT consistently improves the existing GCL approaches, serving as a friendly plugin."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/29e1723f504de017c2ddbf141fe3d17f7e38c11c.pdf"}, "supplementary_material": {"value": "/attachment/77cd316b52b9b13dff713376eaff7e999fe111c7.pdf"}, "_bibtex": {"value": "@inproceedings{\nyu2023provable,\ntitle={Provable Training for Graph Contrastive Learning},\nauthor={Yue Yu and Xiao Wang and Mengmei Zhang and Nian Liu and Chuan Shi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Xasl21tSOf}\n}"}, "TLDR": {"value": "We investigate the intrinsic property of nodes in GCL and improve the training of GCL provably upon that property."}, "paperhash": {"value": "yu|provable_training_for_graph_contrastive_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11911/-/Revision", "NeurIPS.cc/2023/Conference/Submission11911/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11911/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326087730, "odate": 1698949775360, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "bNIHdyunFC", "number": 11909, "cdate": 1683804623882, "tcdate": 1683804623882, "mdate": 1699950386558, "tmdate": 1699950386558, "signatures": ["NeurIPS.cc/2023/Conference/Submission11909/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11909/Authors"], "forum": "bNIHdyunFC", "content": {"title": {"value": "Learning Layer-wise Equivariances Automatically using Gradients"}, "authors": {"value": ["Tycho F.A. van der Ouderaa", "Alexander Immer", "Mark van der Wilk"]}, "authorids": {"value": ["~Tycho_F.A._van_der_Ouderaa1", "~Alexander_Immer1", "~Mark_van_der_Wilk1"]}, "keywords": {"value": ["learning layer-wise relaxed equivariances bayesian symmetry discovery marginal likelihood"]}, "abstract": {"value": "Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in deep networks. We demonstrate the ability to automatically learn layer-wise equivariances on image classification tasks, achieving equivalent or improved performance over baselines with hard-coded symmetry."}, "pdf": {"value": "/pdf/146ed1d25f3a8b951ce67b720713f1146e24ed5f.pdf"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "supplementary_material": {"value": "/attachment/e862118ef9e288e6c63c80bd734b5ac97fa95887.pdf"}, "_bibtex": {"value": "@inproceedings{\nouderaa2023learning,\ntitle={Learning Layer-wise Equivariances Automatically using Gradients},\nauthor={Tycho F.A. van der Ouderaa and Alexander Immer and Mark van der Wilk},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=bNIHdyunFC}\n}"}, "paperhash": {"value": "ouderaa|learning_layerwise_equivariances_automatically_using_gradients"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11909/-/Revision", "NeurIPS.cc/2023/Conference/Submission11909/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11909/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326087680, "odate": 1698949775324, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "I50HbChk3U", "number": 11833, "cdate": 1683803549281, "tcdate": 1683803549281, "mdate": 1698949774922, "tmdate": 1698949774922, "signatures": ["NeurIPS.cc/2023/Conference/Submission11833/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11833/Authors"], "forum": "I50HbChk3U", "content": {"title": {"value": "Provably Bounding Neural Network Preimages"}, "authors": {"value": ["Suhas Kotha", "Christopher Brix", "J Zico Kolter", "Krishnamurthy Dj Dvijotham", "Huan Zhang"]}, "authorids": {"value": ["~Suhas_Kotha1", "~Christopher_Brix1", "~J_Zico_Kolter1", "~Krishnamurthy_Dj_Dvijotham1", "~Huan_Zhang1"]}, "keywords": {"value": ["Trustworthy ML", "Formal Verification", "Safe Control", "OOD Detection"]}, "TLDR": {"value": "We present an efficient algorithm to over-approximate the preimage of a neural network."}, "abstract": {"value": "Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-approximations over $2500\\times$ tighter than prior work while being $2.5\\times$ faster. By strengthening robustness verification with output constraints, we consistently verify more properties than the previous state-of-the-art on multiple benchmarks, including a large model with 167k neurons in VNN-COMP 2023. Our algorithm has been incorporated into the $\\alpha,\\beta$-CROWN verifier, available at https://abcrown.org."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d4e8f71ea0f12c7050e5d590fddff649f7280206.pdf"}, "_bibtex": {"value": "@inproceedings{\nkotha2023provably,\ntitle={Provably Bounding Neural Network Preimages},\nauthor={Suhas Kotha and Christopher Brix and J Zico Kolter and Krishnamurthy Dj Dvijotham and Huan Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=I50HbChk3U}\n}"}, "paperhash": {"value": "kotha|provably_bounding_neural_network_preimages"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11833/-/Revision", "NeurIPS.cc/2023/Conference/Submission11833/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11833/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326085932, "odate": 1698949774910, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "JOkgEY9os2", "number": 11814, "cdate": 1683803340036, "tcdate": 1683803340036, "mdate": 1698949774843, "tmdate": 1698949774843, "signatures": ["NeurIPS.cc/2023/Conference/Submission11814/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11814/Authors"], "forum": "JOkgEY9os2", "content": {"title": {"value": "MMD-Fuse: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting"}, "authors": {"value": ["Felix Biggs", "Antonin Schrab", "Arthur Gretton"]}, "authorids": {"value": ["~Felix_Biggs1", "~Antonin_Schrab1", "~Arthur_Gretton1"]}, "keywords": {"value": ["Testing", "MMD", "Kernel Methods", "Two-sample testing"]}, "abstract": {"value": "We propose novel statistics which maximise the power  of a two-sample test based on the Maximum Mean Discrepancy (MMD), by\nadapting over the set of kernels used in defining it.\nFor finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum.\nExponential concentration bounds are proved for our proposed statistics under the null and alternative.\nWe further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting.\nThis technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders.\nWe highlight the applicability of our MMD-Fuse tests on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/bae8c384e04844e2c6dcdbf7e92fed8b2628058e.pdf"}, "TLDR": {"value": "Adaptive MMD two-sample tests without data splitting, by fusing kernels."}, "_bibtex": {"value": "@inproceedings{\nbiggs2023mmdfuse,\ntitle={{MMD}-Fuse: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting},\nauthor={Felix Biggs and Antonin Schrab and Arthur Gretton},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=JOkgEY9os2}\n}"}, "paperhash": {"value": "biggs|mmdfuse_learning_and_combining_kernels_for_twosample_testing_without_data_splitting"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11814/-/Revision", "NeurIPS.cc/2023/Conference/Submission11814/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11814/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326085443, "odate": 1698949774828, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "9STYRIVx6u", "number": 11801, "cdate": 1683803001898, "tcdate": 1683803001898, "mdate": 1698949774817, "tmdate": 1698949774817, "signatures": ["NeurIPS.cc/2023/Conference/Submission11801/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11801/Authors"], "forum": "9STYRIVx6u", "content": {"title": {"value": "Mean-field Langevin dynamics: Time-space discretization, stochastic gradient, and variance reduction"}, "authors": {"value": ["Taiji Suzuki", "Denny Wu", "Atsushi Nitanda"]}, "authorids": {"value": ["~Taiji_Suzuki1", "~Denny_Wu2", "~Atsushi_Nitanda1"]}, "keywords": {"value": ["mean-field regime", "interacting particle system", "propagation of chaos", "Neural network optimization", "MMD minimization", "kernel stein discrepancy"]}, "abstract": {"value": "The mean-field Langevin dynamics (MFLD) is a nonlinear generalization of the Langevin dynamics that incorporates a distribution-dependent drift, and it naturally arises from the optimization of two-layer neural networks via (noisy) gradient descent. Recent works have shown that MFLD globally minimizes an entropy-regularized convex functional in the space of measures. However, all prior analyses assumed the infinite-particle or continuous-time limit, and cannot handle stochastic gradient updates. We provide a general framework to prove a uniform-in-time propagation of chaos for MFLD that takes into account the errors due to finite-particle approximation, time-discretization, and stochastic gradient. To demonstrate the wide applicability of our framework, we establish quantitative convergence rate guarantees to the regularized global optimal solution for $(i)$ a wide range of learning problems such as mean-field neural network and MMD minimization, and $(ii)$ different gradient estimators including SGD and SVRG. Despite the generality of our results, we achieve an improved convergence rate in both the SGD and SVRG settings when specialized to the standard Langevin dynamics."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/bb5cd59a04383cf43617ee120a3aa89494acacca.pdf"}, "_bibtex": {"value": "@inproceedings{\nsuzuki2023meanfield,\ntitle={Mean-field Langevin dynamics: Time-space discretization, stochastic gradient, and variance reduction},\nauthor={Taiji Suzuki and Denny Wu and Atsushi Nitanda},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=9STYRIVx6u}\n}"}, "paperhash": {"value": "suzuki|meanfield_langevin_dynamics_timespace_discretization_stochastic_gradient_and_variance_reduction"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11801/-/Revision", "NeurIPS.cc/2023/Conference/Submission11801/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11801/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326085157, "odate": 1698949774803, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "B4G87Bq5wA", "number": 11741, "cdate": 1683801572752, "tcdate": 1683801572752, "mdate": 1698949774527, "tmdate": 1698949774527, "signatures": ["NeurIPS.cc/2023/Conference/Submission11741/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11741/Authors"], "forum": "B4G87Bq5wA", "content": {"title": {"value": "Fast Approximation of Similarity Graphs with Kernel Density Estimation"}, "authors": {"value": ["Peter Macgregor", "He Sun"]}, "authorids": {"value": ["~Peter_Macgregor1", "~He_Sun5"]}, "keywords": {"value": ["similarity graphs", "spectral clustering"]}, "abstract": {"value": "Constructing a similarity graph from a set $X$ of data points in $ \\mathbb{R}^d$ is the first step of many modern clustering algorithms. However, typical constructions of a similarity graph have high time complexity, and a quadratic space dependency with respect to $|X|$. We address this limitation and present a new algorithmic framework that constructs a sparse approximation of the fully connected similarity graph while preserving its cluster structure. Our presented algorithm is based on the kernel density estimation problem, and is applicable for arbitrary kernel functions. We compare our designed algorithm with the  well-known implementations from the scikit-learn library and the FAISS library,  and find that our method significantly outperforms the implementation from both libraries on a variety of datasets."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f29096d088e5aa90465f4efc837f3aee82d5deea.pdf"}, "supplementary_material": {"value": "/attachment/05af410d6356796af690ca832bc020d445ce2e22.zip"}, "_bibtex": {"value": "@inproceedings{\nmacgregor2023fast,\ntitle={Fast Approximation of Similarity Graphs with Kernel Density Estimation},\nauthor={Peter Macgregor and He Sun},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=B4G87Bq5wA}\n}"}, "paperhash": {"value": "macgregor|fast_approximation_of_similarity_graphs_with_kernel_density_estimation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11741/-/Revision", "NeurIPS.cc/2023/Conference/Submission11741/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11741/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326083692, "odate": 1698949774515, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "iWGC0Nsq9i", "number": 11694, "cdate": 1683800835703, "tcdate": 1683800835703, "mdate": 1698949774284, "tmdate": 1698949774284, "signatures": ["NeurIPS.cc/2023/Conference/Submission11694/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11694/Authors"], "forum": "iWGC0Nsq9i", "content": {"title": {"value": "Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond"}, "authors": {"value": ["Omar Chehab", "Aapo Hyvarinen", "Andrej Risteski"]}, "authorids": {"value": ["~Omar_Chehab1", "~Aapo_Hyvarinen1", "~Andrej_Risteski2"]}, "keywords": {"value": ["noise-contrastive estimation", "annealed importance sampling"]}, "abstract": {"value": "Recent research has developed several Monte Carlo methods for estimating the normalization constant (partition function) based on the idea of annealing. This means sampling successively from a path of distributions which interpolate between a tractable \"proposal\" distribution and the unnormalized \"target\" distribution. Prominent estimators in this family include annealed importance sampling and annealed noise-contrastive estimation (NCE). Such methods hinge on a number of design choices: which estimator to use, which path of distributions to use and whether to use a path at all; so far, there is no definitive theory on which choices are efficient. Here, we evaluate each design choice by the asymptotic estimation error it produces. First, we show that using NCE is more efficient than the importance sampling estimator, but in the limit of infinitesimal path steps, the difference vanishes. Second, we find that using the geometric path brings down the estimation error from an exponential to a polynomial function of the parameter distance between the target and proposal distributions. Third, we find that the arithmetic path, while rarely used, can offer optimality properties over the universally-used geometric path. In fact, in a particular limit, the optimal path is arithmetic. Based on this theory, we finally propose a two-step estimator to approximate the optimal path in an efficient way."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/4b460f14a8bfe280b8fc21bd90dc6a51e3762469.pdf"}, "supplementary_material": {"value": "/attachment/09061c11b23ead1fe381c0c1f1f847595f60a4a0.pdf"}, "_bibtex": {"value": "@inproceedings{\nchehab2023provable,\ntitle={Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond},\nauthor={Omar Chehab and Aapo Hyvarinen and Andrej Risteski},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=iWGC0Nsq9i}\n}"}, "paperhash": {"value": "chehab|provable_benefits_of_annealing_for_estimating_normalizing_constants_importance_sampling_noisecontrastive_estimation_and_beyond"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11694/-/Revision", "NeurIPS.cc/2023/Conference/Submission11694/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11694/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326082897, "odate": 1698949774269, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "gGl0n7Onug", "number": 11614, "cdate": 1683799374634, "tcdate": 1683799374634, "mdate": 1698949773845, "tmdate": 1698949773845, "signatures": ["NeurIPS.cc/2023/Conference/Submission11614/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11614/Authors"], "forum": "gGl0n7Onug", "content": {"title": {"value": "Theoretical and Practical Perspectives on what Influence Functions Do"}, "authors": {"value": ["Andrea Schioppa", "Katja Filippova", "Ivan Titov", "Polina Zablotskaia"]}, "authorids": {"value": ["~Andrea_Schioppa1", "~Katja_Filippova1", "~Ivan_Titov1", "~Polina_Zablotskaia1"]}, "keywords": {"value": ["Explainable AI", "Influence Functions", "Training Data Attribution"]}, "TLDR": {"value": "We identify the problematic assumptions made by Influence Functions methods and clarify what can be expected theoretically from them; based on this we propose to correct mis-predictions by taking a few fine-tuning steps on influential examples."}, "abstract": {"value": "Influence functions (IF) have been seen as a technique for explaining model predictions through the lens of the training data. Their utility is assumed to be in identifying training examples \"responsible\" for a prediction so that, for example, correcting a prediction is possible by intervening on those examples (removing or editing them) and retraining the model. However, recent empirical studies have shown that the existing methods of estimating IF predict the leave-one-out-and-retrain effect poorly. \nIn order to understand the mismatch between the theoretical promise and the practical results, we analyse five assumptions made by IF methods which are problematic for modern-scale deep neural networks and which concern convexity, numeric stability, training trajectory and parameter divergence. This allows us to clarify what can be expected theoretically from IF. We show that while most assumptions can be addressed successfully, the parameter divergence poses a clear limitation on the predictive power of IF: influence fades over training time even with deterministic training. We illustrate this theoretical result with BERT and ResNet models.\nAnother conclusion from the theoretical analysis is that IF are still useful for model debugging and correcting even though some of the assumptions made in prior work do not hold: using natural language processing and computer vision tasks, we verify that mis-predictions can be successfully corrected by taking only a few fine-tuning steps on influential examples."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/34caf567fde5797c18facef93cd01b7528443fd8.pdf"}, "_bibtex": {"value": "@inproceedings{\nschioppa2023theoretical,\ntitle={Theoretical and Practical Perspectives on what Influence Functions Do},\nauthor={Andrea Schioppa and Katja Filippova and Ivan Titov and Polina Zablotskaia},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=gGl0n7Onug}\n}"}, "paperhash": {"value": "schioppa|theoretical_and_practical_perspectives_on_what_influence_functions_do"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11614/-/Revision", "NeurIPS.cc/2023/Conference/Submission11614/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11614/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326080914, "odate": 1698949773833, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "4L9g1jUDtO", "number": 11598, "cdate": 1683799153539, "tcdate": 1683799153539, "mdate": 1699011100325, "tmdate": 1699011100325, "signatures": ["NeurIPS.cc/2023/Conference/Submission11598/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11598/Authors"], "forum": "4L9g1jUDtO", "content": {"title": {"value": "Generalization in the Face of Adaptivity: A Bayesian Perspective"}, "authors": {"value": ["Moshe Shenfeld", "Katrina Ligett"]}, "authorids": {"value": ["~Moshe_Shenfeld1", "~Katrina_Ligett1"]}, "keywords": {"value": ["Differential Privacy", "Adaptive Data Analysis"]}, "TLDR": {"value": "We provide generalization guarantees under adaptivity that scale with the variance, rather than the range"}, "abstract": {"value": "Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the empirical evaluation of queries on the sample significantly deviates from their mean with respect to the underlying data distribution. It turns out that simple noise addition algorithms suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows that they can handle an asymptotically optimal number of queries.  However, differential privacy's worst-case nature entails scaling such noise to the range of the queries even for highly-concentrated queries, or introducing more complex algorithms.\n\nIn this paper, we prove that straightforward noise-addition algorithms already provide variance-dependent guarantees that also extend to unbounded queries. This improvement stems from a novel characterization that illuminates the core problem of adaptive data analysis. We show that the harm of adaptivity results from the covariance between the new query and a Bayes factor-based measure of how much information about the data sample was encoded in the responses given to past queries. We then leverage this characterization to introduce a new data-dependent stability notion that can bound this covariance."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0c4b43fa039c69dfff8d88d784341e99544e2cf6.pdf"}, "supplementary_material": {"value": "/attachment/ce067a60d585c4f79c7cd6b510b4e4d2ac423dc8.pdf"}, "_bibtex": {"value": "@inproceedings{\nshenfeld2023generalization,\ntitle={Generalization in the Face of Adaptivity: A Bayesian Perspective},\nauthor={Moshe Shenfeld and Katrina Ligett},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=4L9g1jUDtO}\n}"}, "paperhash": {"value": "shenfeld|generalization_in_the_face_of_adaptivity_a_bayesian_perspective"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11598/-/Revision", "NeurIPS.cc/2023/Conference/Submission11598/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11598/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326080336, "odate": 1698949773691, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "yvqqkOn9Pi", "number": 11567, "cdate": 1683798776498, "tcdate": 1683798776498, "mdate": 1698949773604, "tmdate": 1698949773604, "signatures": ["NeurIPS.cc/2023/Conference/Submission11567/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11567/Authors"], "forum": "yvqqkOn9Pi", "content": {"title": {"value": "Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis"}, "authors": {"value": ["Alexander Meulemans", "Simon Schug", "Seijin Kobayashi", "Nathaniel Daw", "Greg Wayne"]}, "authorids": {"value": ["~Alexander_Meulemans1", "~Simon_Schug1", "~Seijin_Kobayashi1", "~Nathaniel_Daw1", "~Greg_Wayne1"]}, "keywords": {"value": ["Reinforcement learning", "Long-term credit assignment", "contribution analysis", "hindsight credit assignment", "policy gradient methods"]}, "abstract": {"value": "To make reinforcement learning more sample efficient, we need better credit assignment methods that measure an action\u2019s influence on future rewards. Building upon Hindsight Credit Assignment (HCA), we introduce Counterfactual Contribution Analysis (COCOA), a new family of model-based credit assignment algorithms. Our algorithms achieve precise credit assignment by measuring the contribution of actions upon obtaining subsequent rewards, by quantifying a counterfactual query: \u2018Would the agent still have reached this reward if it had taken another action?\u2019. We show that measuring contributions w.r.t. rewarding _states_, as is done in HCA, results in spurious estimates of contributions, causing HCA to degrade towards the high-variance REINFORCE estimator in many relevant environments. Instead, we measure contributions w.r.t. rewards or learned representations of the rewarding objects, resulting in gradient estimates with lower variance. We run experiments on a suite of problems specifically designed to evaluate long-term credit assignment capabilities. By using dynamic programming, we measure ground-truth policy gradients and show that the improved performance of our new model-based credit assignment methods is due to lower bias and variance compared to HCA and common baselines. Our results demonstrate how modeling action contributions towards rewarding outcomes can be leveraged for credit assignment, opening a new path towards sample-efficient reinforcement learning."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/dc0b198685e8d48c445a5261d5f9350255e8d2f8.pdf"}, "supplementary_material": {"value": "/attachment/5585ba9f90866284630f68021fa48ada9dfd20e1.zip"}, "_bibtex": {"value": "@inproceedings{\nmeulemans2023would,\ntitle={Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis},\nauthor={Alexander Meulemans and Simon Schug and Seijin Kobayashi and Nathaniel Daw and Greg Wayne},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=yvqqkOn9Pi}\n}"}, "paperhash": {"value": "meulemans|would_i_have_gotten_that_reward_longterm_credit_assignment_by_counterfactual_contribution_analysis"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11567/-/Revision", "NeurIPS.cc/2023/Conference/Submission11567/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11567/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326079547, "odate": 1698949773590, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "XpmJNP8BVA", "number": 11539, "cdate": 1683798321996, "tcdate": 1683798321996, "mdate": 1698949773501, "tmdate": 1698949773501, "signatures": ["NeurIPS.cc/2023/Conference/Submission11539/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11539/Authors"], "forum": "XpmJNP8BVA", "content": {"title": {"value": "Regularized Behavior Cloning for Blocking the Leakage of Past Action Information"}, "authors": {"value": ["Seokin Seo", "HyeongJoo Hwang", "Hongseok Yang", "Kee-Eung Kim"]}, "authorids": {"value": ["~Seokin_Seo1", "~HyeongJoo_Hwang1", "~Hongseok_Yang2", "~Kee-Eung_Kim2"]}, "keywords": {"value": ["Imitation learning", "Information leakage", "Causal Confusion"]}, "abstract": {"value": "For partially observable environments, imitation learning with observation histories (ILOH) assumes that control-relevant information is sufficiently captured in the observation histories for imitating the expert actions. In the offline setting wherethe agent is required to learn to imitate without interaction with the environment, behavior cloning (BC) has been shown to be a simple yet effective method for imitation learning. However, when the information about the actions executed in the past timesteps leaks into the observation histories, ILOH via BC often ends up imitating its own past actions. In this paper, we address this catastrophic failure by proposing a principled regularization for BC, which we name Past Action Leakage Regularization (PALR). The main idea behind our approach is to leverage the classical notion of conditional independence to mitigate the leakage. We compare different instances of our framework with natural choices of conditional independence metric and its estimator. The result of our comparison advocates the use of a particular kernel-based estimator for the conditional independence metric. We conduct an extensive set of experiments on benchmark datasets in order to assess the effectiveness of our regularization method. The experimental results show that our method significantly outperforms prior related approaches, highlighting its potential to successfully imitate expert actions when the past action information leaks into the observation histories."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8e2de94b831c59c45e334156bda335d21d176eb4.pdf"}, "_bibtex": {"value": "@inproceedings{\nseo2023regularized,\ntitle={Regularized Behavior Cloning for Blocking the Leakage of Past Action Information},\nauthor={Seokin Seo and HyeongJoo Hwang and Hongseok Yang and Kee-Eung Kim},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=XpmJNP8BVA}\n}"}, "paperhash": {"value": "seo|regularized_behavior_cloning_for_blocking_the_leakage_of_past_action_information"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11539/-/Revision", "NeurIPS.cc/2023/Conference/Submission11539/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11539/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326078889, "odate": 1698949773490, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "fY7dShbtmo", "number": 11430, "cdate": 1683796897974, "tcdate": 1683796897974, "mdate": 1698949773132, "tmdate": 1698949773132, "signatures": ["NeurIPS.cc/2023/Conference/Submission11430/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11430/Authors"], "forum": "fY7dShbtmo", "content": {"title": {"value": "Multi Time Scale World Models"}, "authors": {"value": ["Vaisakh Shaj", "Saleh GHOLAM ZADEH", "Ozan Demir", "Luiz Ricardo Douat", "Gerhard Neumann"]}, "authorids": {"value": ["~Vaisakh_Shaj1", "~Saleh_GHOLAM_ZADEH1", "~Ozan_Demir1", "~Luiz_Ricardo_Douat1", "~Gerhard_Neumann2"]}, "keywords": {"value": ["Hierarchical Models; Multi Time Scale Learning; World Models"]}, "abstract": {"value": "Intelligent agents use internal world models to reason and make predictions about different courses of their actions at many scales. Devising learning paradigms and architectures that allow machines to learn world models that operate at multiple levels of temporal abstractions while dealing with complex uncertainty predictions is a major technical hurdle. In this work, we propose a probabilistic formalism to learn multi-time scale world models which we call the Multi Time Scale State Space (MTS3) model. Our model uses a  computationally efficient inference scheme on multiple time scales for highly accurate long-horizon predictions and uncertainty estimates over several seconds into the future. Our experiments, which focus on action conditional long horizon future predictions, show that MTS3 outperforms recent methods on several system identification benchmarks including complex simulated and real-world dynamical systems."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f7d283d68f926552ec27a680e30338887899b54f.pdf"}, "supplementary_material": {"value": "/attachment/d7c6213119e7875e8434e663541533ace64025ba.pdf"}, "_bibtex": {"value": "@inproceedings{\nshaj2023multi,\ntitle={Multi Time Scale World Models},\nauthor={Vaisakh Shaj and Saleh GHOLAM ZADEH and Ozan Demir and Luiz Ricardo Douat and Gerhard Neumann},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=fY7dShbtmo}\n}"}, "TLDR": {"value": "Propose a principled framework for learing world models at multiple time scales/temporal abstractions."}, "paperhash": {"value": "shaj|multi_time_scale_world_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11430/-/Revision", "NeurIPS.cc/2023/Conference/Submission11430/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11430/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326076241, "odate": 1698949773118, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "1PnSOKQKvq", "number": 11423, "cdate": 1683796782076, "tcdate": 1683796782076, "mdate": 1698949773031, "tmdate": 1698949773031, "signatures": ["NeurIPS.cc/2023/Conference/Submission11423/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11423/Authors"], "forum": "1PnSOKQKvq", "content": {"title": {"value": "Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data"}, "authors": {"value": ["Praveen Venkatesh", "Corbett Bennett", "Sam Gale", "Tamina K. Ramirez", "Greggory Heller", "Severine Durand", "Shawn R Olsen", "Stefan Mihalas"]}, "authorids": {"value": ["~Praveen_Venkatesh1", "corbettb@alleninstitute.org", "samg@alleninstitute.org", "tkr2116@cumc.columbia.edu", "greggh@mit.edu", "severined@alleninstitute.org", "~Shawn_R_Olsen1", "~Stefan_Mihalas1"]}, "keywords": {"value": ["partial information decomposition", "estimation", "bias", "inter-area interaction", "neuroscience"]}, "abstract": {"value": "Recent advances in neuroscientific experimental techniques have enabled us to simultaneously record the activity of thousands of neurons across multiple brain regions. This has led to a growing need for computational tools capable of analyzing how task-relevant information is represented and communicated between several brain regions. Partial information decompositions (PIDs) have emerged as one such tool, quantifying how much unique, redundant and synergistic information two or more brain regions carry about a task-relevant message. However, computing PIDs is computationally challenging in practice, and statistical issues such as the bias and variance of estimates remain largely unexplored. In this paper, we propose a new method for efficiently computing and estimating a PID definition on multivariate Gaussian distributions. We show empirically that our method satisfies an intuitive additivity property, and recovers the ground truth in a battery of canonical examples, even at high dimensionality. We also propose and evaluate, for the first time, a method to correct the bias in PID estimates at finite sample sizes. Finally, we demonstrate that our Gaussian PID effectively characterizes inter-areal interactions in the mouse brain, revealing higher redundancy between visual areas when a stimulus is behaviorally relevant."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We present a new bias-corrected estimator of the BROJA-PID for multivariate Gaussian distributions, show that it agrees with ground truth even at high dimensionalities, and use it to characterize inter-areal interactions in neural data"}, "pdf": {"value": "/pdf/78f2afa80fb8ef8009b340db859291a8ebf1b4b0.pdf"}, "_bibtex": {"value": "@inproceedings{\nvenkatesh2023gaussian,\ntitle={Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data},\nauthor={Praveen Venkatesh and Corbett Bennett and Sam Gale and Tamina K. Ramirez and Greggory Heller and Severine Durand and Shawn R Olsen and Stefan Mihalas},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=1PnSOKQKvq}\n}"}, "paperhash": {"value": "venkatesh|gaussian_partial_information_decomposition_bias_correction_and_application_to_highdimensional_data"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11423/-/Revision", "NeurIPS.cc/2023/Conference/Submission11423/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11423/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326075986, "odate": 1698949773015, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Cs9ea2Gbgx", "number": 11385, "cdate": 1683796318075, "tcdate": 1683796318075, "mdate": 1698949772828, "tmdate": 1698949772828, "signatures": ["NeurIPS.cc/2023/Conference/Submission11385/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11385/Authors"], "forum": "Cs9ea2Gbgx", "content": {"title": {"value": "List and Certificate Complexities in Replicable Learning"}, "authors": {"value": ["Peter Dixon", "A. Pavan", "Jason Vander Woude", "N V Vinodchandran"]}, "authorids": {"value": ["~Peter_Dixon1", "~A._Pavan1", "~Jason_Vander_Woude1", "~N_V_Vinodchandran1"]}, "keywords": {"value": ["Replicability", "learning algorithms", "sample complexity", "PAC Learning"]}, "abstract": {"value": "We investigate replicable learning algorithms. Informally  a learning algorithm is replicable if the algorithm outputs the same canonical hypothesis over multiple runs with high probability, even when different runs observe a different set of samples from the unknown data distribution. In general, such a strong notion of replicability is not achievable. \nThus we consider two feasible notions of replicability called {\\em list replicability} and {\\em certificate replicability}. \nIntuitively, these notions capture the degree of (non) replicability. The goal is to design learning algorithms with optimal list and certificate complexities while minimizing the sample complexity.  Our contributions are the following.\n\n1. We first study the learning task of estimating the biases of $d$ coins, up to an additive error of $\\varepsilon$, by observing samples. For this task, we design a $(d+1)$-list replicable algorithm. To complement this result, we establish that the list complexity is optimal, i.e there are no learning algorithms with a list size smaller than $d+1$ for this task. We also design learning algorithms with certificate complexity $\\tilde{O}(\\log d)$.   The sample complexity of both these algorithms is $\\tilde{O}(\\frac{d^2}{\\varepsilon^2})$ where $\\varepsilon$ is the approximation error parameter (for a constant error probability).  \n\n2. In the PAC model, we show that any hypothesis class that is learnable with $d$-nonadaptive statistical queries can be learned via a $(d+1)$-list replicable algorithm and also via a $\\tilde{O}(\\log d)$-certificate replicable algorithm. The sample complexity of both these algorithms is $\\tilde{O}(\\frac{d^2}{\\nu^2})$ where $\\nu$ is the approximation error of the statistical query. We also show that for the concept class \\dtep, the list complexity is exactly $d+1$ with respect to the uniform distribution.   \n\nTo establish our upper bound results we use rounding schemes induced by geometric partitions with certain properties. We use Sperner/KKM Lemma to establish the lower bound results."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We investigate list and certificate complexities in the context of replicable learning algorithms."}, "pdf": {"value": "/pdf/87b4eddb63611c4fda0b11e9bd024be53bfbb00c.pdf"}, "supplementary_material": {"value": "/attachment/c532486cc55ec7adee466eaf92df62b8b0d07f56.pdf"}, "_bibtex": {"value": "@inproceedings{\ndixon2023list,\ntitle={List and Certificate Complexities in Replicable Learning},\nauthor={Peter Dixon and A. Pavan and Jason Vander Woude and N V Vinodchandran},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Cs9ea2Gbgx}\n}"}, "paperhash": {"value": "dixon|list_and_certificate_complexities_in_replicable_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11385/-/Revision", "NeurIPS.cc/2023/Conference/Submission11385/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11385/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326074837, "odate": 1698949772816, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "geLARFEK8O", "number": 11308, "cdate": 1683795249702, "tcdate": 1683795249702, "mdate": 1698949772234, "tmdate": 1698949772234, "signatures": ["NeurIPS.cc/2023/Conference/Submission11308/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11308/Authors"], "forum": "geLARFEK8O", "content": {"title": {"value": "Combating Representation Learning Disparity with Geometric Harmonization"}, "authors": {"value": ["Zhihan Zhou", "Jiangchao Yao", "Feng Hong", "Ya Zhang", "Bo Han", "Yanfeng Wang"]}, "authorids": {"value": ["~Zhihan_Zhou2", "~Jiangchao_Yao1", "~Feng_Hong1", "~Ya_Zhang1", "~Bo_Han1", "~Yanfeng_Wang1"]}, "keywords": {"value": ["Long-tailed learning", "self-supervised learning"]}, "abstract": {"value": "Self-supervised learning (SSL) as an effective paradigm of representation learning has achieved tremendous success on various curated datasets in diverse scenarios. Nevertheless, when facing the long-tailed distribution in real-world applications, it is still hard for existing methods to capture transferable and robust representation. The attribution is that the vanilla SSL methods that pursue the sample-level uniformity easily leads to representation learning disparity, where head classes with the huge sample number dominate the feature regime but tail classes with the small sample number passively collapse. To address this problem, we propose a novel Geometric Harmonization (GH) method to encourage the category-level uniformity in representation learning, which is more benign to the minority and almost does not hurt the majority under long-tailed distribution. Specially, GH measures the population statistics of the embedding space on top of self-supervised learning, and then infer an fine-grained instance-wise calibration to constrain the space expansion of head classes and avoid the passive collapse of tail classes. Our proposal does not alter the setting of SSL and can be easily integrated into existing methods in a low-cost manner. Extensive results on a range of benchmark datasets show the effectiveness of \\methodspace with high tolerance to the distribution skewness."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d6dc8d8b93a08e18de4da7357830b20e06cfc742.pdf"}, "TLDR": {"value": "We propose the novel Geometric Harmonization for self-supervised long-tailed learning, which overcomes the intrinsic limitation of the contrastive learning, i.e., sample-level uniformity, and progressively approaches the category-level uniformity."}, "supplementary_material": {"value": "/attachment/b8192be41af790ee55193fd5c634371b15e7569a.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhou2023combating,\ntitle={Combating Representation Learning Disparity with Geometric Harmonization},\nauthor={Zhihan Zhou and Jiangchao Yao and Feng Hong and Ya Zhang and Bo Han and Yanfeng Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=geLARFEK8O}\n}"}, "paperhash": {"value": "zhou|combating_representation_learning_disparity_with_geometric_harmonization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11308/-/Revision", "NeurIPS.cc/2023/Conference/Submission11308/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11308/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326072273, "odate": 1698949772214, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "OIJ3VXDy6s", "forum": "OIJ3VXDy6s", "number": 11290, "cdate": 1683795086026, "tcdate": 1683795086026, "mdate": 1698949772144, "tmdate": 1698949772144, "signatures": ["NeurIPS.cc/2023/Conference/Submission11290/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11290/Authors"], "content": {"title": {"value": "RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability"}, "authors": {"value": ["Chuning Zhu", "Max Simchowitz", "Siri Gadipudi", "Abhishek Gupta"]}, "authorids": {"value": ["~Chuning_Zhu1", "~Max_Simchowitz1", "siri.gadipudi9@gmail.com", "~Abhishek_Gupta1"]}, "keywords": {"value": ["Model-Based Reinforcement Learning", "Deep Reinforcement Learning"]}, "abstract": {"value": "Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is able to operate in dynamic environments, it is not invariant under significant distribution shift. To address this, we propose a simple reward-free alignment procedure that enables test time adaptation of the encoder. This allows for quick adaptation to widely differing environments without having to relearn the dynamics and policy. Our effort is a step towards making model-based RL a practical and useful tool for dynamic, diverse domains and we show its effectiveness in simulation tasks with significant spurious variations."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9ad73a11a6aa840f2d36238ed83005bb5c3f6768.pdf"}, "supplementary_material": {"value": "/attachment/c71e3e4a8572df12f0e2e0f1438e8e66cd741e56.zip"}, "_bibtex": {"value": "@inproceedings{\nzhu2023repo,\ntitle={RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability},\nauthor={Chuning Zhu and Max Simchowitz and Siri Gadipudi and Abhishek Gupta},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=OIJ3VXDy6s}\n}"}, "paperhash": {"value": "zhu|repo_resilient_modelbased_reinforcement_learning_by_regularizing_posterior_predictability"}}, "pdate": 1695326071791, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11290/-/Revision", "NeurIPS.cc/2023/Conference/Submission11290/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11290/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "odate": 1698949772132, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "VGLXjbTSYa", "number": 11149, "cdate": 1683793254093, "tcdate": 1683793254093, "mdate": 1698949771409, "tmdate": 1698949771409, "signatures": ["NeurIPS.cc/2023/Conference/Submission11149/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission11149/Authors"], "forum": "VGLXjbTSYa", "content": {"title": {"value": "Delegated Classification"}, "authors": {"value": ["Eden Saig", "Inbal Talgam-Cohen", "Nir Rosenfeld"]}, "authorids": {"value": ["~Eden_Saig1", "~Inbal_Talgam-Cohen2", "~Nir_Rosenfeld2"]}, "keywords": {"value": ["Delegation", "Algorithmic Contract Design", "Moral Hazard", "Learning Curves"]}, "TLDR": {"value": "We develop a theoretical framework for incentive-aware delegation of machine learning tasks."}, "abstract": {"value": "When machine learning is outsourced to a rational agent, conflicts of interest might arise and severely impact predictive performance. In this work, we propose a theoretical framework for incentive-aware delegation of machine learning tasks. We model delegation as a principal-agent game, in which accurate learning can be incentivized by the principal using performance-based contracts. Adapting the economic theory of contract design to this setting, we define budget-optimal contracts and prove they take a simple threshold form under reasonable assumptions. In the binary-action case, the optimality of such contracts is shown to be equivalent to the classic Neyman-Pearson lemma, establishing a formal connection between contract design and statistical hypothesis testing. Empirically, we demonstrate that budget-optimal contracts can be constructed using small-scale data, leveraging recent advances in the study of learning curves and scaling laws. Performance and economic outcomes are evaluated using synthetic and real-world classification tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9b359321b993e185b5b398a10d1bade57755a3e3.pdf"}, "supplementary_material": {"value": "/attachment/01e48e6d9bf3a8fb78836f91f8c41dc830f22930.zip"}, "_bibtex": {"value": "@inproceedings{\nsaig2023delegated,\ntitle={Delegated Classification},\nauthor={Eden Saig and Inbal Talgam-Cohen and Nir Rosenfeld},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=VGLXjbTSYa}\n}"}, "paperhash": {"value": "saig|delegated_classification"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission11149/-/Revision", "NeurIPS.cc/2023/Conference/Submission11149/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission11149/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326068031, "odate": 1698949771396, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "QmPf29EHyI", "number": 10976, "cdate": 1683790994200, "tcdate": 1683790994200, "mdate": 1698949770475, "tmdate": 1698949770475, "signatures": ["NeurIPS.cc/2023/Conference/Submission10976/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10976/Authors"], "forum": "QmPf29EHyI", "content": {"title": {"value": "Bifurcations and loss jumps in RNN training"}, "authors": {"value": ["Lukas Eisenmann", "Zahra Monfared", "Niclas Alexander G\u00f6ring", "Daniel Durstewitz"]}, "authorids": {"value": ["~Lukas_Eisenmann1", "~Zahra_Monfared1", "~Niclas_Alexander_G\u00f6ring1", "~Daniel_Durstewitz1"]}, "keywords": {"value": ["dynamical systems", "bifurcations", "Recurrent Neural Networks", "attractors", "training algorithm", "BPTT", "exploding and vanishing gradient problem", "nonlinear dynamics", "time series"]}, "abstract": {"value": "Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associated with loss gradients tending toward infinity or zero. We then introduce a novel heuristic algorithm for detecting all fixed points and $k$-cycles in ReLU-based RNNs and their existence and stability regions, hence bifurcation manifolds in parameter space. In contrast to previous numerical algorithms for finding fixed points and common continuation methods, our algorithm provides $\\textit{exact}$ results and returns fixed points and cycles up to high orders with surprisingly good scaling behavior. We exemplify the algorithm on the analysis of the training process of RNNs, and find that the recently introduced technique of generalized teacher forcing completely avoids certain types of bifurcations in training. Thus, besides facilitating the DST analysis of trained RNNs, our algorithm provides a powerful instrument for analyzing the training process itself."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6017de10b4b5b7bfbcf378f2b24f3f971af8cbfb.pdf"}, "supplementary_material": {"value": "/attachment/c62b1ea2edcd6bd0062bdcd37b86e68c7837e816.pdf"}, "_bibtex": {"value": "@inproceedings{\neisenmann2023bifurcations,\ntitle={Bifurcations and loss jumps in {RNN} training},\nauthor={Lukas Eisenmann and Zahra Monfared and Niclas Alexander G{\\\"o}ring and Daniel Durstewitz},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=QmPf29EHyI}\n}"}, "TLDR": {"value": "We show that bifurcations cause loss jumps in RNN training and introduce an efficient algorithm for locating fixed points, cycles, and bifurcation manifolds in RNNs"}, "paperhash": {"value": "eisenmann|bifurcations_and_loss_jumps_in_rnn_training"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10976/-/Revision", "NeurIPS.cc/2023/Conference/Submission10976/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10976/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326063165, "odate": 1698949770463, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "dzqKAM2sKa", "number": 10921, "cdate": 1683790279098, "tcdate": 1683790279098, "mdate": 1698949770269, "tmdate": 1698949770269, "signatures": ["NeurIPS.cc/2023/Conference/Submission10921/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10921/Authors"], "forum": "dzqKAM2sKa", "content": {"title": {"value": "Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks"}, "authors": {"value": ["Woojin Cho", "Kookjin Lee", "Donsub Rim", "Noseong Park"]}, "authorids": {"value": ["~Woojin_Cho1", "~Kookjin_Lee1", "~Donsub_Rim1", "~Noseong_Park1"]}, "keywords": {"value": ["Scientific machine learning", "Physics-informed neural networks", "Meta learning", "Hypernetworks"]}, "abstract": {"value": "In various engineering and applied science applications, repetitive numerical simulations of partial differential equations (PDEs) for varying input parameters are often required (e.g., aircraft shape optimization over many design parameters) and solvers are required to perform rapid execution. In this study, we suggest a path that potentially opens up a possibility for physics-informed neural networks (PINNs), emerging deep-learning-based solvers, to be considered as one such solver. Although PINNs have pioneered a proper integration of deep-learning and scientific computing, they require repetitive time-consuming training of neural networks, which is not suitable for many-query scenarios. To address this issue, we propose a lightweight low-rank PINNs containing only hundreds of model parameters and an associated hypernetwork-based meta-learning algorithm, which allows efficient approximation of solutions of PDEs for varying ranges of PDE input parameters. Moreover, we show that the proposed method is effective in overcoming a challenging issue, known as \"failure modes\" of PINNs."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/72c09c64339ba10dc6bc248d29a531348943d5a9.pdf"}, "supplementary_material": {"value": "/attachment/94eaaaa931a58f73786440538c9d672008431757.zip"}, "_bibtex": {"value": "@inproceedings{\ncho2023hypernetworkbased,\ntitle={Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks},\nauthor={Woojin Cho and Kookjin Lee and Donsub Rim and Noseong Park},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=dzqKAM2sKa}\n}"}, "paperhash": {"value": "cho|hypernetworkbased_metalearning_for_lowrank_physicsinformed_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10921/-/Revision", "NeurIPS.cc/2023/Conference/Submission10921/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10921/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326061801, "odate": 1698949770249, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "3AreDQZ8eO", "number": 10917, "cdate": 1683790246556, "tcdate": 1683790246556, "mdate": 1698949770215, "tmdate": 1698949770215, "signatures": ["NeurIPS.cc/2023/Conference/Submission10917/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10917/Authors"], "forum": "3AreDQZ8eO", "content": {"title": {"value": "Schema-learning and rebinding as mechanisms of in-context learning and emergence"}, "authors": {"value": ["Sivaramakrishnan Swaminathan", "Antoine Dedieu", "Rajkumar Vasudeva Raju", "Murray Shanahan", "Miguel Lazaro-Gredilla", "Dileep George"]}, "authorids": {"value": ["~Sivaramakrishnan_Swaminathan1", "~Antoine_Dedieu1", "~Rajkumar_Vasudeva_Raju2", "~Murray_Shanahan1", "~Miguel_Lazaro-Gredilla1", "~Dileep_George1"]}, "keywords": {"value": ["mechanistic interpretability", "in-context learning", "emergence", "large language models"]}, "TLDR": {"value": "Interpretable mechanistic model explaining emergent behavior and in-context learning in large language models."}, "abstract": {"value": "In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at different levels of overparameterization, suggesting that overparameterization helps in learning more complex template (schema) circuits. By showing how ICL can be achieved with small models and datasets, we open up a path to novel architectures, and take a vital step towards a more general understanding of the mechanics behind this important capability."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/def6988589b42bd84ea063c84a72368cd56cbbe2.pdf"}, "supplementary_material": {"value": "/attachment/80224f38022c5058d024b8ac4d096a23f1185fb0.pdf"}, "_bibtex": {"value": "@inproceedings{\nswaminathan2023schemalearning,\ntitle={Schema-learning and rebinding as mechanisms of in-context learning and emergence},\nauthor={Sivaramakrishnan Swaminathan and Antoine Dedieu and Rajkumar Vasudeva Raju and Murray Shanahan and Miguel Lazaro-Gredilla and Dileep George},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=3AreDQZ8eO}\n}"}, "paperhash": {"value": "swaminathan|schemalearning_and_rebinding_as_mechanisms_of_incontext_learning_and_emergence"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10917/-/Revision", "NeurIPS.cc/2023/Conference/Submission10917/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10917/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326061656, "odate": 1698949770199, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "MvoMDD6emT", "number": 10909, "cdate": 1683790107863, "tcdate": 1683790107863, "mdate": 1698949770109, "tmdate": 1698949770109, "signatures": ["NeurIPS.cc/2023/Conference/Submission10909/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10909/Authors"], "forum": "MvoMDD6emT", "content": {"title": {"value": "State Sequences Prediction via Fourier Transform for Representation Learning"}, "authors": {"value": ["Mingxuan Ye", "Yufei Kuang", "Jie Wang", "Rui Yang", "Wengang Zhou", "Houqiang Li", "Feng Wu"]}, "authorids": {"value": ["~Mingxuan_Ye1", "~Yufei_Kuang1", "~Jie_Wang1", "~Rui_Yang9", "~Wengang_Zhou1", "~Houqiang_Li1", "~Feng_Wu1"]}, "keywords": {"value": ["Reinforcement learning", "Representation learning", "State sequences prediction", "Fourier transform"]}, "abstract": {"value": "While deep reinforcement learning (RL) has been demonstrated effective in solving complex control tasks, sample efficiency remains a key challenge due to the large amounts of data required for remarkable performance. Existing research explores the application of representation learning for data-efficient RL, e.g., learning predictive representations by predicting long-term future states. However, many existing methods do not fully exploit the structural information inherent in sequential state signals, which can potentially improve the quality of long-term decision-making but is difficult to discern in the time domain. To tackle this problem, we propose State Sequences Prediction via Fourier Transform (SPF), a novel method that exploits the frequency domain of state sequences to extract the underlying patterns in time series data for learning expressive representations efficiently. Specifically, we theoretically analyze the existence of structural information in state sequences, which is closely related to policy performance and signal regularity, and then propose to predict the Fourier transform of infinite-step future state sequences to extract such information. One of the appealing features of SPF is that it is simple to implement while not requiring storage of infinite-step future states as prediction targets. Experiments demonstrate that the proposed method outperforms several state-of-the-art algorithms in terms of both sample efficiency and performance."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/422d934bc6f3a6bdf81eea7c4c4dd721f9a78152.pdf"}, "supplementary_material": {"value": "/attachment/a007c0dc31255cd357625ab314b59de7dfa16ad5.zip"}, "_bibtex": {"value": "@inproceedings{\nye2023state,\ntitle={State Sequences Prediction via Fourier Transform for Representation Learning},\nauthor={Mingxuan Ye and Yufei Kuang and Jie Wang and Rui Yang and Wengang Zhou and Houqiang Li and Feng Wu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=MvoMDD6emT}\n}"}, "TLDR": {"value": "A representation learning method that predicts the Fourier transform of state sequences to improve sample efficiency of RL algorithms."}, "paperhash": {"value": "ye|state_sequences_prediction_via_fourier_transform_for_representation_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10909/-/Revision", "NeurIPS.cc/2023/Conference/Submission10909/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10909/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326061400, "odate": 1698949770097, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "rhIfzCZoXG", "number": 10830, "cdate": 1683789138472, "tcdate": 1683789138472, "mdate": 1698949769721, "tmdate": 1698949769721, "signatures": ["NeurIPS.cc/2023/Conference/Submission10830/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10830/Authors"], "forum": "rhIfzCZoXG", "content": {"title": {"value": "Counterfactual Evaluation of Peer-Review Assignment Policies"}, "authors": {"value": ["Martin Saveski", "Steven Jecmen", "Nihar B Shah", "Johan Ugander"]}, "authorids": {"value": ["~Martin_Saveski1", "~Steven_Jecmen1", "~Nihar_B_Shah1", "~Johan_Ugander1"]}, "keywords": {"value": ["peer review", "causal inference", "counterfactual policy evaluation"]}, "TLDR": {"value": "We propose methods for off-policy evaluation in peer review and apply them to data from the TPDP\u201921 workshop and the AAAI\u201922 conference."}, "abstract": {"value": "Peer review assignment algorithms aim to match research papers to suitable expert reviewers, working to maximize the quality of the resulting reviews. A key challenge in designing effective assignment policies is evaluating how changes to the assignment algorithm map to changes in review quality. In this work, we leverage recently proposed policies that introduce randomness in peer-review assignment\u2014in order to mitigate fraud\u2014as a valuable opportunity to evaluate counterfactual assignment policies. Specifically, we exploit how such randomized assignments provide a positive probability of observing the reviews of many assignment policies of interest. To address challenges in applying standard off-policy evaluation methods, such as violations of positivity, we introduce novel methods for partial identification based on monotonicity and Lipschitz smoothness assumptions for the mapping between reviewer-paper covariates and outcomes. We apply our methods to peer-review data from two computer science venues: the TPDP'21 workshop (95 papers and 35 reviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We consider estimates of (i) the effect on review quality when changing weights in the assignment algorithm, e.g., weighting reviewers' bids vs. textual similarity (between the review's past papers and the submission), and (ii) the \"cost of randomization\", capturing the difference in expected quality between the perturbed and unperturbed optimal match. We find that placing higher weight on text similarity results in higher review quality and that introducing randomization in the reviewer-paper assignment only marginally reduces the review quality. Our methods for partial identification may be of independent interest, while our off-policy approach can likely find use in evaluating a broad class of algorithmic matching systems."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/41d9b741c9a1b3539488348cf2f3840ba4b2bb79.pdf"}, "_bibtex": {"value": "@inproceedings{\nsaveski2023counterfactual,\ntitle={Counterfactual Evaluation of Peer-Review Assignment Policies},\nauthor={Martin Saveski and Steven Jecmen and Nihar B Shah and Johan Ugander},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=rhIfzCZoXG}\n}"}, "supplementary_material": {"value": "/attachment/ba1c808e7b364e6b9493db2fdd3864a06afee5b6.pdf"}, "paperhash": {"value": "saveski|counterfactual_evaluation_of_peerreview_assignment_policies"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10830/-/Revision", "NeurIPS.cc/2023/Conference/Submission10830/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10830/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326059162, "odate": 1698949769709, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "2SScUiWUbn", "number": 10811, "cdate": 1683788854221, "tcdate": 1683788854221, "mdate": 1698949769642, "tmdate": 1698949769642, "signatures": ["NeurIPS.cc/2023/Conference/Submission10811/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10811/Authors"], "forum": "2SScUiWUbn", "content": {"title": {"value": "On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"}, "authors": {"value": ["Vivek Ramanujan", "Thao Nguyen", "Sewoong Oh", "Ali Farhadi", "Ludwig Schmidt"]}, "authorids": {"value": ["~Vivek_Ramanujan1", "~Thao_Nguyen3", "~Sewoong_Oh1", "~Ali_Farhadi3", "~Ludwig_Schmidt1"]}, "keywords": {"value": ["robustness", "out-of-distribution shifts", "finetuning", "pretraining"]}, "abstract": {"value": "Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness (Taori et al., 2020) is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for robustness."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c7cf28bbf8c9a2d7186940e7790123eb7e5ddb5b.pdf"}, "_bibtex": {"value": "@inproceedings{\nramanujan2023on,\ntitle={On the Connection between Pre-training Data Diversity and Fine-tuning Robustness},\nauthor={Vivek Ramanujan and Thao Nguyen and Sewoong Oh and Ali Farhadi and Ludwig Schmidt},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=2SScUiWUbn}\n}"}, "supplementary_material": {"value": "/attachment/a0b51c714ae9b2cde88fc0a053ad646c8d08e605.pdf"}, "paperhash": {"value": "ramanujan|on_the_connection_between_pretraining_data_diversity_and_finetuning_robustness"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10811/-/Revision", "NeurIPS.cc/2023/Conference/Submission10811/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10811/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326058665, "odate": 1698949769626, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "KbqQMoqfLQ", "number": 10771, "cdate": 1683788292489, "tcdate": 1683788292489, "mdate": 1698949769341, "tmdate": 1698949769341, "signatures": ["NeurIPS.cc/2023/Conference/Submission10771/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10771/Authors"], "forum": "KbqQMoqfLQ", "content": {"title": {"value": "Blockwise Parallel Transformers for Large Context Models"}, "authors": {"value": ["Hao Liu", "Pieter Abbeel"]}, "authorids": {"value": ["~Hao_Liu1", "~Pieter_Abbeel2"]}, "keywords": {"value": ["Language Model", "Long Context Modeling", "Reinforcement Learning"]}, "abstract": {"value": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We present an efficient method of computing the standard transformer architecture, enabling effective processing of long contextual information."}, "pdf": {"value": "/pdf/a3209fad193743016063390eb56841b9b24b7bea.pdf"}, "_bibtex": {"value": "@inproceedings{\nliu2023blockwise,\ntitle={Blockwise Parallel Transformers for Large Context Models},\nauthor={Hao Liu and Pieter Abbeel},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=KbqQMoqfLQ}\n}"}, "paperhash": {"value": "liu|blockwise_parallel_transformers_for_large_context_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10771/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10771/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326057502, "odate": 1698949769328, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "LnZuxp3Tx7", "number": 10763, "cdate": 1683788190134, "tcdate": 1683788190134, "mdate": 1698949769207, "tmdate": 1698949769207, "signatures": ["NeurIPS.cc/2023/Conference/Submission10763/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10763/Authors"], "forum": "LnZuxp3Tx7", "content": {"title": {"value": "From Tempered to Benign Overfitting in ReLU Neural Networks"}, "authors": {"value": ["Guy Kornowski", "Gilad Yehudai", "Ohad Shamir"]}, "authorids": {"value": ["~Guy_Kornowski1", "~Gilad_Yehudai2", "~Ohad_Shamir1"]}, "keywords": {"value": ["benign overfitting", "implicit bias", "interpolating predictors", "neural networks", "theory"]}, "abstract": {"value": "Overparameterized neural networks (NNs) are observed to generalize well even when trained to perfectly fit noisy data. This phenomenon motivated a large body of work on \"benign overfitting\", where interpolating predictors achieve near-optimal performance. Recently, it was conjectured and empirically observed that the behavior of NNs is often better described as \"tempered overfitting\", where the performance is non-optimal yet also non-trivial, and degrades as a function of the noise level. However, a theoretical justification of this claim for non-linear NNs has been lacking so far. In this work, we provide several results that aim at bridging these complementing views. We study a simple classification setting with 2-layer ReLU NNs, and prove that under various assumptions, the type of overfitting transitions from tempered in the extreme case of one-dimensional data, to benign in high dimensions. Thus, we show that the input dimension has a crucial role on the overfitting profile in this setting, which we also validate empirically for intermediate dimensions. Overall, our results shed light on the intricate connections between the dimension, sample size, architecture and training algorithm on the one hand, and the type of resulting overfitting on the other hand."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We prove tempered and benign overfitting for ReLU NNs in various settings"}, "pdf": {"value": "/pdf/b3260899e85cc07d549244bab0200c47c466be9c.pdf"}, "supplementary_material": {"value": "/attachment/45de866b62513abd1c6299b144510e7a43749e8b.pdf"}, "_bibtex": {"value": "@inproceedings{\nkornowski2023from,\ntitle={From Tempered to Benign Overfitting in Re{LU} Neural Networks},\nauthor={Guy Kornowski and Gilad Yehudai and Ohad Shamir},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=LnZuxp3Tx7}\n}"}, "paperhash": {"value": "kornowski|from_tempered_to_benign_overfitting_in_relu_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10763/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10763/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326057177, "odate": 1698949769191, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ofa1U5BJVJ", "number": 10718, "cdate": 1683787556867, "tcdate": 1683787556867, "mdate": 1698949769011, "tmdate": 1698949769011, "signatures": ["NeurIPS.cc/2023/Conference/Submission10718/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10718/Authors"], "forum": "ofa1U5BJVJ", "content": {"title": {"value": "Online (Multinomial) Logistic Bandit: Improved Regret and Constant Computation Cost"}, "authors": {"value": ["Yu-Jie Zhang", "Masashi Sugiyama"]}, "authorids": {"value": ["~Yu-Jie_Zhang1", "~Masashi_Sugiyama1"]}, "keywords": {"value": ["Logistic Bandit", "Generalized Linear Bandit", "Regret Bound", "Computation Cost"]}, "abstract": {"value": "This paper investigates the logistic bandit problem, a variant of the generalized linear bandit model that utilizes a logistic model to depict the feedback from an action. While most existing research focuses on the binary logistic bandit problem, the multinomial case, which considers more than two possible feedback values, offers increased practical relevance and adaptability for use in complex decision-making problems such as reinforcement learning. In this paper, we provide an algorithm that enjoys both statistical and computational efficiency for the logistic bandit problem. In the binary case, our method improves the state-of-the-art binary logistic bandit method by reducing the per-round computation cost from $\\mathcal{O}(\\log T)$ to $\\mathcal{O}(1)$ with respect to the time horizon $T$, while still preserving the minimax optimal guarantee up to logarithmic factors. In the multinomial case, with $K+1$ potential feedback values, our algorithm achieves an $\\tilde{\\mathcal{O}}(K\\sqrt{T})$ regret bound with $\\mathcal{O}(1)$ computational cost per round. The result not only improves the $\\tilde{\\mathcal{O}}(K\\sqrt{\\kappa T})$ bound for the best-known tractable algorithm\u2014where the large constant $\\kappa$ increases exponentially with the diameter of the parameter domain\u2014but also reduces the $\\mathcal{O}(T)$ computational complexity demanded by the previous method."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8017f8a310f01c16e8d659af6e80582348475d93.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023online,\ntitle={Online (Multinomial) Logistic Bandit: Improved Regret and Constant Computation Cost},\nauthor={Yu-Jie Zhang and Masashi Sugiyama},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ofa1U5BJVJ}\n}"}, "paperhash": {"value": "zhang|online_multinomial_logistic_bandit_improved_regret_and_constant_computation_cost"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10718/-/Revision", "NeurIPS.cc/2023/Conference/Submission10718/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10718/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326056279, "odate": 1698949768996, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "r8snfquzs3", "number": 10678, "cdate": 1683787055800, "tcdate": 1683787055800, "mdate": 1698949768671, "tmdate": 1698949768671, "signatures": ["NeurIPS.cc/2023/Conference/Submission10678/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10678/Authors"], "forum": "r8snfquzs3", "content": {"title": {"value": "Grounding Neural Inference with Satisfiability Modulo Theories"}, "authors": {"value": ["Zifan Wang", "Saranya Vijayakumar", "Kaiji Lu", "Vijay Ganesh", "Somesh Jha", "Matt Fredrikson"]}, "authorids": {"value": ["~Zifan_Wang1", "~Saranya_Vijayakumar1", "~Kaiji_Lu1", "~Vijay_Ganesh1", "~Somesh_Jha1", "~Matt_Fredrikson1"]}, "keywords": {"value": ["Satisfiability Modulo Theories", "Solver Layer", "Combinatorial Problem", "MAXSAT", "SAT"]}, "TLDR": {"value": "We present a set of technique for integrating Satisfiability Modulo Theories (SMT) solvers into the forward and backward passes of a deep network layer, called SMTLayer"}, "abstract": {"value": "Recent techniques that integrate solver layers into Deep Neural Networks (DNNs) have shown promise in bridging a long-standing gap between inductive learning and symbolic reasoning techniques. In this paper we present a set of techniques for integrating Satisfiability Modulo Theories (SMT) solvers into the forward and backward passes of a deep network layer, called SMTLayer.\nUsing this approach, one can encode rich domain knowledge into the network in the form of mathematical formulas.\nIn the forward pass, the solver uses symbols produced by prior layers, along with these formulas, to construct inferences; in the backward pass, the solver informs updates to the network, driving it towards representations that are compatible with the solver's theory.\nNotably, the solver need not be differentiable. We implement SMTLayer as a Pytorch module, and our empirical results show that it leads to models that 1) require fewer training samples than conventional models, 2) that are robust to certain types of covariate shift, and 3) that ultimately learn representations that are consistent with symbolic knowledge, and thus naturally interpretable."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/989016825230fe2e13cea49c89ff3029ac4c1f1f.pdf"}, "supplementary_material": {"value": "/attachment/9e98da082b1caf84fd08aaaf18efb1d5c65df37f.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023grounding,\ntitle={Grounding Neural Inference with Satisfiability Modulo Theories},\nauthor={Zifan Wang and Saranya Vijayakumar and Kaiji Lu and Vijay Ganesh and Somesh Jha and Matt Fredrikson},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=r8snfquzs3}\n}"}, "paperhash": {"value": "wang|grounding_neural_inference_with_satisfiability_modulo_theories"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10678/-/Revision", "NeurIPS.cc/2023/Conference/Submission10678/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10678/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326055209, "odate": 1698949768655, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cLQCCtVDuW", "number": 10573, "cdate": 1683785564050, "tcdate": 1683785564050, "mdate": 1698949768230, "tmdate": 1698949768230, "signatures": ["NeurIPS.cc/2023/Conference/Submission10573/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10573/Authors"], "forum": "cLQCCtVDuW", "content": {"title": {"value": "HIQL: Offline Goal-Conditioned RL with Latent States as Actions"}, "authors": {"value": ["Seohong Park", "Dibya Ghosh", "Benjamin Eysenbach", "Sergey Levine"]}, "authorids": {"value": ["~Seohong_Park1", "~Dibya_Ghosh1", "~Benjamin_Eysenbach1", "~Sergey_Levine1"]}, "keywords": {"value": ["reinforcement learning"]}, "abstract": {"value": "Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a simple hierarchical offline goal-conditioned RL method that extracts \bboth high- and low-level policies from a single value function."}, "pdf": {"value": "/pdf/4bab40c657ab1b72d618bf8b7cd7865238a9b739.pdf"}, "supplementary_material": {"value": "/attachment/8850f2ee384b5ad4b1d069c03bd49d17a8c53b8c.pdf"}, "_bibtex": {"value": "@inproceedings{\npark2023hiql,\ntitle={{HIQL}: Offline Goal-Conditioned {RL} with Latent States as Actions},\nauthor={Seohong Park and Dibya Ghosh and Benjamin Eysenbach and Sergey Levine},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cLQCCtVDuW}\n}"}, "paperhash": {"value": "park|hiql_offline_goalconditioned_rl_with_latent_states_as_actions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10573/-/Revision", "NeurIPS.cc/2023/Conference/Submission10573/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10573/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326052694, "odate": 1698949768218, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "h1FhXVM0cB", "number": 10568, "cdate": 1683785507855, "tcdate": 1683785507855, "mdate": 1698949768274, "tmdate": 1698949768274, "signatures": ["NeurIPS.cc/2023/Conference/Submission10568/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10568/Authors"], "forum": "h1FhXVM0cB", "content": {"title": {"value": "Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tailed Noise"}, "authors": {"value": ["Ta Duy Nguyen", "Thien Hang Nguyen", "Alina Ene", "Huy Nguyen"]}, "authorids": {"value": ["~Ta_Duy_Nguyen1", "~Thien_Hang_Nguyen1", "~Alina_Ene1", "~Huy_Nguyen1"]}, "keywords": {"value": ["convex optimization", "non-convex optimization", "high probability convergence", "heavy-tailed noise", "clipped stochastic gradient descent", "clipped stochastic mirror descent"]}, "abstract": {"value": "In this work, we study the convergence in high probability of clipped gradient methods when the noise distribution has heavy tails, i.e., with bounded $p$th moments, for some $1<p\\le2$. Prior works in this setting follow the same recipe of using concentration inequalities and an inductive argument with union bound to bound the iterates across all iterations. This method results in an increase in the failure probability by a factor of $T$, where $T$ is the number of iterations. We instead propose a new analysis approach based on bounding the moment generating function of a well chosen supermartingale sequence. We improve the dependency on $T$ in the convergence guarantee for a wide range of algorithms with clipped gradients, including stochastic (accelerated) mirror descent for convex objectives and stochastic gradient descent for nonconvex objectives. Our high probability bounds achieve the optimal convergence rates and match the best currently known in-expectation bounds. Our approach naturally allows the algorithms to use time-varying step sizes and clipping parameters when the time horizon is unknown, which appears difficult or even impossible using the techniques from prior works. Furthermore, we show that in the case of clipped stochastic mirror descent, several problem constants, including the initial distance to the optimum, are not required when setting step sizes and clipping parameters."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/18fc8c30fef5664ec3a78419b273989924e2efa8.pdf"}, "TLDR": {"value": "We provide optimal convergence rate in high probability for clipped gradient methods under heavy-tailed noise, closing the logarithmic suboptimality gap and addressing several open questions in this setting left by previous works."}, "supplementary_material": {"value": "/attachment/95c171b89f6303da3287c749929bfb8beee9e00b.pdf"}, "_bibtex": {"value": "@inproceedings{\nnguyen2023improved,\ntitle={Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tailed Noise},\nauthor={Ta Duy Nguyen and Thien Hang Nguyen and Alina Ene and Huy Nguyen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=h1FhXVM0cB}\n}"}, "paperhash": {"value": "nguyen|improved_convergence_in_high_probability_of_clipped_gradient_methods_with_heavy_tailed_noise"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10568/-/Revision", "NeurIPS.cc/2023/Conference/Submission10568/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10568/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326052514, "odate": 1698949768185, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "8aDG51pxFc", "number": 10494, "cdate": 1683784413700, "tcdate": 1683784413700, "mdate": 1698949767879, "tmdate": 1698949767879, "signatures": ["NeurIPS.cc/2023/Conference/Submission10494/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10494/Authors"], "forum": "8aDG51pxFc", "content": {"title": {"value": "No Change, No Gain: Empowering Graph Neural Networks with Expected Model Change Maximization for Active Learning"}, "authors": {"value": ["Zixing Song", "Yifei Zhang", "Irwin King"]}, "authorids": {"value": ["~Zixing_Song2", "~Yifei_Zhang6", "~Irwin_King1"]}, "keywords": {"value": ["Graph Neural Networks", "Expected Model Change Maximization"]}, "abstract": {"value": "Graph Neural Networks (GNNs) are crucial for machine learning applications with graph-structured data, but their success depends on sufficient labeled data. We present a novel active learning (AL) method for GNNs, extending the Expected Model Change Maximization (EMCM) principle to improve prediction performance on unlabeled data. By presenting a Bayesian interpretation for the node embeddings generated by GNNs under the semi-supervised setting, we efficiently compute the closed-form EMCM acquisition function as the selection criterion for AL without re-training. Our method establishes a direct connection with expected prediction error minimization, offering theoretical guarantees for AL performance. Experiments demonstrate our method's effectiveness compared to existing approaches, in terms of both accuracy and efficiency."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/769f991cdae676bd5d3dad00a4baba6acbb7d43b.pdf"}, "supplementary_material": {"value": "/attachment/d87252ee93b1de12868b16c7eaf52af52bdeda29.pdf"}, "_bibtex": {"value": "@inproceedings{\nsong2023no,\ntitle={No Change, No Gain: Empowering Graph Neural Networks with Expected Model Change Maximization for Active Learning},\nauthor={Zixing Song and Yifei Zhang and Irwin King},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=8aDG51pxFc}\n}"}, "paperhash": {"value": "song|no_change_no_gain_empowering_graph_neural_networks_with_expected_model_change_maximization_for_active_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10494/-/Revision", "NeurIPS.cc/2023/Conference/Submission10494/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10494/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326049987, "odate": 1698949767656, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "BfQJrIiOZC", "number": 10465, "cdate": 1683783949508, "tcdate": 1683783949508, "mdate": 1698949767545, "tmdate": 1698949767545, "signatures": ["NeurIPS.cc/2023/Conference/Submission10465/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10465/Authors"], "forum": "BfQJrIiOZC", "content": {"title": {"value": "Zero-shot causal learning"}, "authors": {"value": ["Hamed Nilforoshan", "Michael Moor", "Yusuf H Roohani", "Yining Chen", "Anja \u0160urina", "Michihiro Yasunaga", "Sara Oblak", "Jure Leskovec"]}, "authorids": {"value": ["~Hamed_Nilforoshan1", "~Michael_Moor1", "~Yusuf_H_Roohani1", "~Yining_Chen1", "~Anja_\u0160urina1", "~Michihiro_Yasunaga1", "~Sara_Oblak1", "~Jure_Leskovec1"]}, "keywords": {"value": ["causal inference", "CATE", "CATE estimation", "causal machine learning", "causal ML", "heterogenous treatment effects", "causality", "potential outcomes", "treatment effect"]}, "TLDR": {"value": "A personalized causal effect prediction framework that can generalize to novel treatments that did not exist during training (e.g. a newly invented drug that has never been prescribed before)"}, "abstract": {"value": "Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. \nHowever, in many settings it is important to predict the effects of novel interventions (e.g., a newly invented drug), which these methods do not address.\nHere, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, its recipients, and its nonrecipients. By leveraging both intervention information (e.g., a drug's attributes) and individual features (e.g., a patient's history), CaML is able to predict the personalized effects of novel interventions that do not exist at the time of training. Experimental results on real world datasets in large-scale medical claims and cell-line perturbations demonstrate the effectiveness of our approach. Most strikingly, CaML's zero-shot predictions outperform even strong baselines trained directly on data from the test interventions."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/02548d76de9d343cef60f4eef0c6a8758b1ffc06.pdf"}, "supplementary_material": {"value": "/attachment/f485ab8b117b3957496b3201e42ecfe1a1f92365.zip"}, "_bibtex": {"value": "@inproceedings{\nnilforoshan2023zeroshot,\ntitle={Zero-shot causal learning},\nauthor={Hamed Nilforoshan and Michael Moor and Yusuf H Roohani and Yining Chen and Anja {\\v{S}}urina and Michihiro Yasunaga and Sara Oblak and Jure Leskovec},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BfQJrIiOZC}\n}"}, "paperhash": {"value": "nilforoshan|zeroshot_causal_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10465/-/Revision", "NeurIPS.cc/2023/Conference/Submission10465/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10465/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326048992, "odate": 1698949767530, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "gbOukzirpK", "number": 10448, "cdate": 1683783799938, "tcdate": 1683783799938, "mdate": 1698954287240, "tmdate": 1698954287240, "signatures": ["NeurIPS.cc/2023/Conference/Submission10448/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10448/Authors"], "forum": "gbOukzirpK", "content": {"title": {"value": "Object-Centric Slot Diffusion"}, "authors": {"value": ["Jindong Jiang", "Fei Deng", "Gautam Singh", "Sungjin Ahn"]}, "authorids": {"value": ["~Jindong_Jiang1", "~Fei_Deng1", "~Gautam_Singh3", "~Sungjin_Ahn1"]}, "keywords": {"value": ["Object-Centric Representation Learning", "Diffusion Models", "Unsupervised Representation Learning"]}, "TLDR": {"value": "We propose the Latent Slot Diffusion which combines an object-centric encoder with diffusion decoders. It achieves unsupervised learning of object segmentation, compositional generation, and image editing, surpassing the state-of-the-art models."}, "abstract": {"value": "The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. In addition, we conduct a preliminary investigation into the integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world image segmentation and generation. Project page is available at https://latentslotdiffusion.github.io"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f106cbf4de47cd027e07e3d0e0bcf83bc12b250e.pdf"}, "_bibtex": {"value": "@inproceedings{\njiang2023objectcentric,\ntitle={Object-Centric Slot Diffusion},\nauthor={Jindong Jiang and Fei Deng and Gautam Singh and Sungjin Ahn},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=gbOukzirpK}\n}"}, "paperhash": {"value": "jiang|objectcentric_slot_diffusion"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10448/-/Revision", "NeurIPS.cc/2023/Conference/Submission10448/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10448/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326048512, "odate": 1698949767432, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "zkfyOkBVpz", "number": 10441, "cdate": 1683783734526, "tcdate": 1683783734526, "mdate": 1698954287230, "tmdate": 1698954287230, "signatures": ["NeurIPS.cc/2023/Conference/Submission10441/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10441/Authors"], "forum": "zkfyOkBVpz", "content": {"title": {"value": "Curriculum Learning With Infant Egocentric Videos"}, "authors": {"value": ["Saber Sheybani", "Himanshu Hansaria", "Justin Newell Wood", "Linda B. Smith", "Zoran Tiganj"]}, "authorids": {"value": ["~Saber_Sheybani1", "~Himanshu_Hansaria1", "~Justin_Newell_Wood1", "~Linda_B._Smith1", "~Zoran_Tiganj1"]}, "keywords": {"value": ["Curriculum learning", "Self-supervised learning", "Slow changes", "Infant development"]}, "abstract": {"value": "Infants possess a remarkable ability to rapidly learn and process visual inputs. As an infant's mobility increases, so does the variety and dynamics of their visual inputs. Is this change in the properties of the visual inputs beneficial or even critical for the proper development of the visual system? To address this question, we used video recordings from infants wearing head-mounted cameras to train a variety of self-supervised learning models. Critically, we separated the infant data by age group and evaluated the importance of training with a curriculum aligned with developmental order. We found that initiating learning with the data from the youngest age group provided the strongest learning signal and led to the best learning outcomes in terms of downstream task performance. We then showed that the benefits of the data from the youngest age group are due to the slowness and simplicity of the visual experience. The results provide strong empirical evidence for the importance of the properties of the early infant experience and developmental progression in training. More broadly, our approach and findings take a noteworthy step towards reverse engineering the learning mechanisms in newborn brains using image-computable models from artificial intelligence."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a6c6e41c9a33d3e919fed94789926263f0e86889.pdf"}, "_bibtex": {"value": "@inproceedings{\nsheybani2023curriculum,\ntitle={Curriculum Learning With Infant Egocentric Videos},\nauthor={Saber Sheybani and Himanshu Hansaria and Justin Newell Wood and Linda B. Smith and Zoran Tiganj},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=zkfyOkBVpz}\n}"}, "TLDR": {"value": "We show the advantage of training self-supervised learning models of infant egocentric videos using the natural developmental curriculum."}, "supplementary_material": {"value": "/attachment/13e7e3151aab4cf83852aad6fd980f8d95979665.pdf"}, "paperhash": {"value": "sheybani|curriculum_learning_with_infant_egocentric_videos"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission10441/-/Revision", "NeurIPS.cc/2023/Conference/Submission10441/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10441/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326048173, "odate": 1698949767378, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "X6dEqXIsEW", "number": 10422, "cdate": 1683783352787, "tcdate": 1683783352787, "mdate": 1698949767284, "tmdate": 1698949767284, "signatures": ["NeurIPS.cc/2023/Conference/Submission10422/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10422/Authors"], "forum": "X6dEqXIsEW", "content": {"title": {"value": "On the Planning Abilities of Large Language Models - A Critical Investigation"}, "authors": {"value": ["Karthik Valmeekam", "Matthew Marquez", "Sarath Sreedharan", "Subbarao Kambhampati"]}, "authorids": {"value": ["~Karthik_Valmeekam1", "~Matthew_Marquez1", "~Sarath_Sreedharan1", "~Subbarao_Kambhampati1"]}, "keywords": {"value": ["Large Language Models", "Planning", "LLMs for autonomous and heuristic planning guidance"]}, "abstract": {"value": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs\u2019 ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3d27fd3e3c66752e9bf0148cb210da1b38ac1256.pdf"}, "supplementary_material": {"value": "/attachment/311a163cf136ee33f0e7c71f6ef277c97d3abb15.pdf"}, "_bibtex": {"value": "@inproceedings{\nvalmeekam2023on,\ntitle={On the Planning Abilities of Large Language Models - A Critical Investigation},\nauthor={Karthik Valmeekam and Matthew Marquez and Sarath Sreedharan and Subbarao Kambhampati},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=X6dEqXIsEW}\n}"}, "paperhash": {"value": "valmeekam|on_the_planning_abilities_of_large_language_models_a_critical_investigation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10422/-/Revision", "NeurIPS.cc/2023/Conference/Submission10422/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10422/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326047681, "odate": 1698949767269, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "STqaMqhtDi", "number": 10397, "cdate": 1683783047042, "tcdate": 1683783047042, "mdate": 1698949767124, "tmdate": 1698949767124, "signatures": ["NeurIPS.cc/2023/Conference/Submission10397/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10397/Authors"], "forum": "STqaMqhtDi", "content": {"title": {"value": "Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication"}, "authors": {"value": ["Chaofei Fan", "Nick Hahn", "Foram Kamdar", "Donald Avansino", "Guy H Wilson", "Leigh Hochberg", "Krishna V. Shenoy", "Jaimie M. Henderson", "Francis R Willett"]}, "authorids": {"value": ["~Chaofei_Fan1", "~Nick_Hahn1", "fkamdar@stanford.edu", "~Donald_Avansino1", "~Guy_H_Wilson1", "~Leigh_Hochberg1", "~Krishna_V._Shenoy1", "~Jaimie_M._Henderson1", "~Francis_R_Willett1"]}, "keywords": {"value": ["brain-computer interface", "self-training", "continual online learning"]}, "abstract": {"value": "Intracortical brain-computer interfaces (iBCIs) have shown promise for restoring rapid communication to people with neurological disorders such as amyotrophic lateral sclerosis (ALS). \nHowever, to maintain high performance over time, iBCIs typically need frequent recalibration to combat changes in the neural recordings that accrue over days. \nThis requires iBCI users to stop using the iBCI and engage in supervised data collection, making the iBCI system hard to use. \nIn this paper, we propose a method that enables self-recalibration of communication iBCIs without interrupting the user. \nOur method leverages large language models (LMs) to automatically correct errors in iBCI outputs. \nThe self-recalibration process uses these corrected outputs (\"pseudo-labels\") to continually update the iBCI decoder online. \nOver a period of more than one year (403 days), we evaluated our Continual Online Recalibration with Pseudo-labels (CORP) framework with one clinical trial participant. \nCORP achieved  a stable decoding accuracy of 93.84% in an online handwriting iBCI task, significantly outperforming other baseline methods. \nNotably, this is the longest-running iBCI stability demonstration involving a human participant. \nOur results provide the first  evidence for long-term stabilization of a plug-and-play, high-performance communication iBCI, addressing a major barrier for the clinical translation of iBCIs."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5cd9270b4b18ed619ce0c5e47f57d64233e941dd.pdf"}, "supplementary_material": {"value": "/attachment/6bec12cf1b3661bc4133b134665fb4c2e8eb6b27.pdf"}, "_bibtex": {"value": "@inproceedings{\nfan2023plugandplay,\ntitle={Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication},\nauthor={Chaofei Fan and Nick Hahn and Foram Kamdar and Donald Avansino and Guy H Wilson and Leigh Hochberg and Krishna V. Shenoy and Jaimie M. Henderson and Francis R Willett},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=STqaMqhtDi}\n}"}, "paperhash": {"value": "fan|plugandplay_stability_for_intracortical_braincomputer_interfaces_a_oneyear_demonstration_of_seamless_braintotext_communication"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10397/-/Revision", "NeurIPS.cc/2023/Conference/Submission10397/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10397/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326047067, "odate": 1698949767104, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "7h1YaSGaHS", "number": 10356, "cdate": 1683782346244, "tcdate": 1683782346244, "mdate": 1698949766825, "tmdate": 1698949766825, "signatures": ["NeurIPS.cc/2023/Conference/Submission10356/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10356/Authors"], "forum": "7h1YaSGaHS", "content": {"title": {"value": "Physics-Driven ML-Based Modelling for Correcting Inverse Estimation"}, "authors": {"value": ["Ruiyuan Kang", "Tingting Mu", "Panos Liatsis", "Dimitrios Kyritsis"]}, "authorids": {"value": ["~Ruiyuan_Kang1", "~Tingting_Mu1", "~Panos_Liatsis1", "dimitrios.kyritsis@ku.ac.ae"]}, "keywords": {"value": ["Failure detection", "Physical evaluation", "Network-based optimization", "Generative model", "Hybrid surrogate model"]}, "abstract": {"value": "When deploying machine learning  estimators in science and engineering (SAE) domains, it is critical  to avoid failed estimations that can have disastrous consequences, e.g., in aero engine design. This work focuses on detecting and correcting  failed  state estimations before adopting them in SAE inverse problems, by  utilizing simulations and performance metrics guided by physical laws. We suggest to flag a machine learning estimation when its physical model error exceeds a feasible threshold, and propose a novel approach, GEESE, to correct it  through optimization, aiming at delivering both low error and high efficiency. The key designs of GEESE include (1) a hybrid surrogate error model to  provide fast  error estimations  to reduce simulation cost and to enable gradient based backpropagation of error feedback, and (2) two generative models to approximate the probability distributions of the candidate states for simulating the  exploitation and exploration behaviours. All three models are constructed as neural networks. GEESE is tested on three real-world SAE inverse problems and compared to a number of state-of-the-art optimization/search approaches. Results show that it fails the least number of times in terms of finding a feasible state correction, and requires physical evaluations less frequently in general."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7282a5754537184817a9d155d3d037567cb87e01.pdf"}, "supplementary_material": {"value": "/attachment/6233d41e409fe9fde524e287882fd8e1b2b8a049.zip"}, "_bibtex": {"value": "@inproceedings{\nkang2023physicsdriven,\ntitle={Physics-Driven {ML}-Based Modelling for Correcting Inverse Estimation},\nauthor={Ruiyuan Kang and Tingting Mu and Panos Liatsis and Dimitrios Kyritsis},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=7h1YaSGaHS}\n}"}, "paperhash": {"value": "kang|physicsdriven_mlbased_modelling_for_correcting_inverse_estimation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10356/-/Revision", "NeurIPS.cc/2023/Conference/Submission10356/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10356/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326045543, "odate": 1698949766810, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5NxJuc0T1P", "number": 10332, "cdate": 1683781933452, "tcdate": 1683781933452, "mdate": 1698949766652, "tmdate": 1698949766652, "signatures": ["NeurIPS.cc/2023/Conference/Submission10332/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10332/Authors"], "forum": "5NxJuc0T1P", "content": {"title": {"value": "Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models"}, "authors": {"value": ["Zhong Yi Wan", "Ricardo Baptista", "Anudhyan Boral", "Yi-Fan Chen", "John Anderson", "Fei Sha", "Leonardo Zepeda-Nunez"]}, "authorids": {"value": ["~Zhong_Yi_Wan1", "~Ricardo_Baptista1", "~Anudhyan_Boral1", "yifanchen@google.com", "janders@google.com", "~Fei_Sha3", "~Leonardo_Zepeda-Nunez1"]}, "keywords": {"value": ["optimal transport", "probabilistic diffusion models", "statistical downscaling"]}, "TLDR": {"value": "Two-stage framework for a probabilistic approach to statistical downscaling with unpaired data, using optimal transport for debiasing and probabilistic diffusion models for super-resolution."}, "abstract": {"value": "We introduce a two-stage probabilistic framework for statistical downscaling using unpaired data. Statistical downscaling seeks a probabilistic map to transform low-resolution data from a biased coarse-grained numerical scheme to high-resolution data that is consistent with a high-fidelity scheme. Our framework tackles the problem by\ncomposing two transformations: (i) a debiasing step via an optimal transport map, and (ii) an upsampling step achieved by a probabilistic diffusion model with a posteriori conditional sampling. This approach characterizes a conditional distribution without needing paired data, and faithfully recovers relevant physical statistics from biased samples. We demonstrate the utility of the proposed approach on one- and two-dimensional fluid flow problems, which are representative of the core difficulties present in numerical simulations of weather and climate. Our method produces realistic high-resolution outputs from low-resolution inputs, by upsampling resolutions of $8\\times$ and $16\\times$. Moreover, our procedure correctly matches the statistics of physical quantities, even when the low-frequency content of the inputs and outputs do not match, a crucial but difficult-to-satisfy assumption needed by current state-of-the-art alternatives. Code for this work is available at: https://github.com/google-research/swirl-dynamics/tree/main/swirl_dynamics/projects/probabilistic_diffusion."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e21ddf8f070e035e79faf89662d61793c17ca72b.pdf"}, "supplementary_material": {"value": "/attachment/514edf61b984651176cb95b865507033668857b4.pdf"}, "_bibtex": {"value": "@inproceedings{\nwan2023debias,\ntitle={Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models},\nauthor={Zhong Yi Wan and Ricardo Baptista and Anudhyan Boral and Yi-Fan Chen and John Anderson and Fei Sha and Leonardo Zepeda-Nunez},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5NxJuc0T1P}\n}"}, "paperhash": {"value": "wan|debias_coarsely_sample_conditionally_statistical_downscaling_through_optimal_transport_and_probabilistic_diffusion_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10332/-/Revision", "NeurIPS.cc/2023/Conference/Submission10332/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10332/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326044803, "odate": 1698949766639, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "LlERoXEKjh", "number": 10300, "cdate": 1683781525806, "tcdate": 1683781525806, "mdate": 1698949766542, "tmdate": 1698949766542, "signatures": ["NeurIPS.cc/2023/Conference/Submission10300/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10300/Authors"], "forum": "LlERoXEKjh", "content": {"title": {"value": "Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?"}, "authors": {"value": ["Erin George", "Michael Murray", "William Joseph Swartworth", "Deanna Needell"]}, "authorids": {"value": ["~Erin_George1", "~Michael_Murray3", "~William_Joseph_Swartworth1", "~Deanna_Needell2"]}, "keywords": {"value": ["benign overfitting", "neural networks", "relu", "hinge loss"]}, "abstract": {"value": "We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the boundary of zero loss while corrupt points either converge towards zero loss or are eventually zeroed by the network. We prove these results using a combinatorial approach that involves bounding the number of clean versus corrupt updates during these phases of training."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e0c54d3f02b4361ab7cbfcbf24b3af496b6142d3.pdf"}, "supplementary_material": {"value": "/attachment/b96401e1b456d7b1e1e906aa9268e231ee565bce.pdf"}, "_bibtex": {"value": "@inproceedings{\ngeorge2023training,\ntitle={Training shallow Re{LU} networks on noisy data using hinge loss: when do we overfit and is it benign?},\nauthor={Erin George and Michael Murray and William Joseph Swartworth and Deanna Needell},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=LlERoXEKjh}\n}"}, "paperhash": {"value": "george|training_shallow_relu_networks_on_noisy_data_using_hinge_loss_when_do_we_overfit_and_is_it_benign"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10300/-/Revision", "NeurIPS.cc/2023/Conference/Submission10300/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10300/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326044022, "odate": 1698949766527, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "sLr1sohnmo", "number": 10223, "cdate": 1683780229945, "tcdate": 1683780229945, "mdate": 1698949765360, "tmdate": 1698949765360, "signatures": ["NeurIPS.cc/2023/Conference/Submission10223/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10223/Authors"], "forum": "sLr1sohnmo", "content": {"title": {"value": "Error Bounds for Learning with Vector-Valued Random Features"}, "authors": {"value": ["Samuel Lanthaler", "Nicholas H. Nelsen"]}, "authorids": {"value": ["~Samuel_Lanthaler1", "~Nicholas_H._Nelsen1"]}, "keywords": {"value": ["random features", "random feature model", "operator learning", "vector-valued"]}, "abstract": {"value": "This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with Monte Carlo intuition and free from logarithmic factors."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/bcb25a6efeab68a4edd26c4f5d64d992792e24af.pdf"}, "_bibtex": {"value": "@inproceedings{\nlanthaler2023error,\ntitle={Error Bounds for Learning with Vector-Valued Random Features},\nauthor={Samuel Lanthaler and Nicholas H. Nelsen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=sLr1sohnmo}\n}"}, "paperhash": {"value": "lanthaler|error_bounds_for_learning_with_vectorvalued_random_features"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10223/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10223/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326041137, "odate": 1698949765320, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5Gw9YkJkFF", "number": 10208, "cdate": 1683780037162, "tcdate": 1683780037162, "mdate": 1698949764957, "tmdate": 1698949764957, "signatures": ["NeurIPS.cc/2023/Conference/Submission10208/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10208/Authors"], "forum": "5Gw9YkJkFF", "content": {"title": {"value": "PAC Learning Linear Thresholds from Label Proportions"}, "authors": {"value": ["Anand Paresh Brahmbhatt", "Rishi Saket", "Aravindan Raghuveer"]}, "authorids": {"value": ["~Anand_Paresh_Brahmbhatt1", "~Rishi_Saket1", "~Aravindan_Raghuveer1"]}, "keywords": {"value": ["PAC learning", "Learning from label proportions", "Linear thresholds"]}, "TLDR": {"value": "Algorithms for PAC learning linear threshold functions from label proportions of random bags of Gaussian feature-vectors"}, "abstract": {"value": "Learning from label proportions (LLP) is a generalization of supervised learning in which the training data is available as sets or bags of feature-vectors (instances) along with the average instance-label of each bag. The goal is to train a good instance classifier. While most previous works on LLP have focused on training models on such training data, computational learnability of LLP was only\nrecently explored by Saket (2021, 2022) who showed worst case intractability of properly learning linear threshold functions (LTFs) from label proportions. However, their work did not rule out efficient algorithms for this problem for natural distributions.\n\nIn this work we show that it is indeed possible to efficiently learn LTFs using LTFs when given access to random bags of some label proportion in which feature-vectors are, conditioned on their labels, independently sampled from a Gaussian distribution $N(\u00b5, \u03a3)$. Our work shows that a certain matrix \u2013 formed using covariances of the differences of feature-vectors sampled from the bags with and without replacement \u2013 necessarily has its principal component, after a transformation, in the direction of the normal vector of the LTF. Our algorithm estimates the means and covariance matrices using subgaussian concentration bounds which we show can be applied to efficiently sample bags for approximating the normal direction. Using this in conjunction with novel generalization error bounds in the bag setting, we show that a low error hypothesis LTF can be identified. For some special cases of the $N(0, I)$ distribution we provide a simpler mean estimation based algorithm. We include an experimental evaluation of our learning algorithms along with a comparison with those of Saket (2021, 2022) and random LTFs, demonstrating the effectiveness of our techniques."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6420c37b5a186322069774ae391be72f75ca8c05.pdf"}, "_bibtex": {"value": "@inproceedings{\nbrahmbhatt2023pac,\ntitle={{PAC} Learning Linear Thresholds from Label Proportions},\nauthor={Anand Paresh Brahmbhatt and Rishi Saket and Aravindan Raghuveer},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5Gw9YkJkFF}\n}"}, "paperhash": {"value": "brahmbhatt|pac_learning_linear_thresholds_from_label_proportions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10208/-/Revision", "NeurIPS.cc/2023/Conference/Submission10208/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10208/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326040654, "odate": 1698949764942, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "UDqHhbqYJV", "number": 10162, "cdate": 1683779319670, "tcdate": 1683779319670, "mdate": 1698949764176, "tmdate": 1698949764176, "signatures": ["NeurIPS.cc/2023/Conference/Submission10162/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10162/Authors"], "forum": "UDqHhbqYJV", "content": {"title": {"value": "Can Language Models Solve Graph Problems in Natural Language?"}, "authors": {"value": ["Heng Wang", "Shangbin Feng", "Tianxing He", "Zhaoxuan Tan", "Xiaochuang Han", "Yulia Tsvetkov"]}, "authorids": {"value": ["~Heng_Wang10", "~Shangbin_Feng1", "~Tianxing_He1", "~Zhaoxuan_Tan1", "~Xiaochuang_Han1", "~Yulia_Tsvetkov1"]}, "keywords": {"value": ["large language models", "graph reasoning", "structured reasoning"]}, "abstract": {"value": "Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f45012123118b579d979e19628dc697c0245483c.pdf"}, "supplementary_material": {"value": "/attachment/6e3b61522a39dda6fc5831a757f63263134d6280.zip"}, "_bibtex": {"value": "@inproceedings{\nwang2023can,\ntitle={Can Language Models Solve Graph Problems in Natural Language?},\nauthor={Heng Wang and Shangbin Feng and Tianxing He and Zhaoxuan Tan and Xiaochuang Han and Yulia Tsvetkov},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=UDqHhbqYJV}\n}"}, "paperhash": {"value": "wang|can_language_models_solve_graph_problems_in_natural_language"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10162/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10162/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326039217, "odate": 1698949764162, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "2Xqvk2KVAq", "number": 10160, "cdate": 1683779289463, "tcdate": 1683779289463, "mdate": 1698949764041, "tmdate": 1698949764041, "signatures": ["NeurIPS.cc/2023/Conference/Submission10160/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10160/Authors"], "forum": "2Xqvk2KVAq", "content": {"title": {"value": "CLIP-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments"}, "authors": {"value": ["Jessica Dai", "Paula Gradu", "Christopher Harshaw"]}, "authorids": {"value": ["~Jessica_Dai1", "~Paula_Gradu1", "~Christopher_Harshaw1"]}, "keywords": {"value": ["causal inference", "randomized experiments", "online optimization"]}, "TLDR": {"value": "By adaptively setting probability of treatment assignment based on observed outcomes, we can achieve provably increased precision of effect estimators."}, "abstract": {"value": "From clinical development of cancer therapies to investigations into partisan bias, adaptive sequential designs have become increasingly popular method for causal inference, as they offer the possibility of improved precision over their non-adaptive counterparts. However, even in simple settings (e.g. two treatments) the extent to which adaptive designs can improve precision is not sufficiently well understood. In this work, we study the problem of Adaptive Neyman Allocation in a design-based potential outcomes framework, where the experimenter seeks to construct an adaptive design which is nearly as efficient as the optimal (but infeasible) non-adaptive Neyman design, which has access to all potential outcomes. Motivated by connections to online optimization, we propose Neyman Ratio and Neyman Regret as two (equivalent) performance measures of adaptive designs for this problem. We present Clip-OGD, an adaptive design which achieves $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ expected Neyman regret and thereby recovers the optimal Neyman variance in large samples. Finally, we construct a conservative variance estimator which facilitates the development of asymptotically valid confidence intervals. To complement our theoretical results, we conduct simulations using data from a microeconomic experiment."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9633cd1e0ea4a05d8106bcbd17e92e6c8d5459c2.pdf"}, "supplementary_material": {"value": "/attachment/08104d8c6267737453d95e549fc7d084ea982c53.zip"}, "_bibtex": {"value": "@inproceedings{\ndai2023clipogd,\ntitle={{CLIP}-{OGD}: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments},\nauthor={Jessica Dai and Paula Gradu and Christopher Harshaw},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=2Xqvk2KVAq}\n}"}, "paperhash": {"value": "dai|clipogd_an_experimental_design_for_adaptive_neyman_allocation_in_sequential_experiments"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10160/-/Revision", "NeurIPS.cc/2023/Conference/Submission10160/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10160/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326039172, "odate": 1698949764028, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "dCYBAGQXLo", "number": 10142, "cdate": 1683779073243, "tcdate": 1683779073243, "mdate": 1698949763735, "tmdate": 1698949763735, "signatures": ["NeurIPS.cc/2023/Conference/Submission10142/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10142/Authors"], "forum": "dCYBAGQXLo", "content": {"title": {"value": "Supervised Pretraining Can Learn In-Context Reinforcement Learning"}, "authors": {"value": ["Jonathan Lee", "Annie Xie", "Aldo Pacchiano", "Yash Chandak", "Chelsea Finn", "Ofir Nachum", "Emma Brunskill"]}, "authorids": {"value": ["~Jonathan_Lee4", "~Annie_Xie1", "~Aldo_Pacchiano1", "~Yash_Chandak1", "~Chelsea_Finn1", "~Ofir_Nachum1", "~Emma_Brunskill2"]}, "keywords": {"value": ["decision making", "reinforcement learning", "in-context learning", "bandits", "transformers", "offline reinforcement learning", "exploration", "reinforcement learning theory"]}, "abstract": {"value": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study the Decision-Pretrained Transformer (DPT), a supervised pretraining method where a transformer predicts an optimal action given a query state and an in-context dataset of interactions from a diverse set of tasks. While simple, this procedure produces a model with several surprising capabilities. We find that the trained transformer can solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/97cf29e208f578c91c19c761557a73bca7dffaf2.pdf"}, "supplementary_material": {"value": "/attachment/cdcb5a081d3c271a6a020607846f79b68994029a.zip"}, "_bibtex": {"value": "@inproceedings{\nlee2023supervised,\ntitle={Supervised Pretraining Can Learn In-Context Reinforcement Learning},\nauthor={Jonathan Lee and Annie Xie and Aldo Pacchiano and Yash Chandak and Chelsea Finn and Ofir Nachum and Emma Brunskill},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=dCYBAGQXLo}\n}"}, "paperhash": {"value": "lee|supervised_pretraining_can_learn_incontext_reinforcement_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10142/-/Revision", "NeurIPS.cc/2023/Conference/Submission10142/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10142/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326038649, "odate": 1698949763721, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Fkckkr3ya8", "number": 10134, "cdate": 1683778994461, "tcdate": 1683778994461, "mdate": 1698949763645, "tmdate": 1698949763645, "signatures": ["NeurIPS.cc/2023/Conference/Submission10134/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10134/Authors"], "forum": "Fkckkr3ya8", "content": {"title": {"value": "Faith and Fate: Limits of Transformers on Compositionality"}, "authors": {"value": ["Nouha Dziri", "Ximing Lu", "Melanie Sclar", "Xiang Lorraine Li", "Liwei Jiang", "Bill Yuchen Lin", "Sean Welleck", "Peter West", "Chandra Bhagavatula", "Ronan Le Bras", "Jena D. Hwang", "Soumya Sanyal", "Xiang Ren", "Allyson Ettinger", "Zaid Harchaoui", "Yejin Choi"]}, "authorids": {"value": ["~Nouha_Dziri2", "~Ximing_Lu1", "~Melanie_Sclar1", "~Xiang_Lorraine_Li1", "~Liwei_Jiang2", "~Bill_Yuchen_Lin1", "~Sean_Welleck1", "~Peter_West1", "~Chandra_Bhagavatula1", "~Ronan_Le_Bras1", "~Jena_D._Hwang1", "~Soumya_Sanyal1", "~Xiang_Ren1", "~Allyson_Ettinger1", "~Zaid_Harchaoui1", "~Yejin_Choi1"]}, "keywords": {"value": ["Natural language processing", "large language models", "multi-step reasoning"]}, "TLDR": {"value": "We study the limits of Transformers confronted with compositional tasks that require multi-step reasoning to arrive at the solution."}, "abstract": {"value": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. \nThis begs the question: Are these errors incidental, or do they signal more substantial limitations?\nIn an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks---multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer.  We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. \nOur empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.  To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with increased task complexity."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/72b98e8513613de8dfdf6d21c8550d10d28afcb8.pdf"}, "supplementary_material": {"value": "/attachment/dc54a152fe3fce152608267d97af8b7175d9d213.zip"}, "_bibtex": {"value": "@inproceedings{\ndziri2023faith,\ntitle={Faith and Fate: Limits of Transformers on Compositionality},\nauthor={Nouha Dziri and Ximing Lu and Melanie Sclar and Xiang Lorraine Li and Liwei Jiang and Bill Yuchen Lin and Sean Welleck and Peter West and Chandra Bhagavatula and Ronan Le Bras and Jena D. Hwang and Soumya Sanyal and Xiang Ren and Allyson Ettinger and Zaid Harchaoui and Yejin Choi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Fkckkr3ya8}\n}"}, "paperhash": {"value": "dziri|faith_and_fate_limits_of_transformers_on_compositionality"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10134/-/Revision", "NeurIPS.cc/2023/Conference/Submission10134/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10134/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326038546, "odate": 1698949763630, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Kg65qieiuB", "number": 10034, "cdate": 1683777384143, "tcdate": 1683777384143, "mdate": 1698949761341, "tmdate": 1698949761341, "signatures": ["NeurIPS.cc/2023/Conference/Submission10034/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10034/Authors"], "forum": "Kg65qieiuB", "content": {"title": {"value": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks"}, "authors": {"value": ["Xinyi Wu", "Amir Ajorlou", "Zihui Wu", "Ali Jadbabaie"]}, "authorids": {"value": ["~Xinyi_Wu3", "~Amir_Ajorlou1", "~Zihui_Wu2", "~Ali_Jadbabaie1"]}, "keywords": {"value": ["graph neural networks", "attention mechanisms", "oversmoothing", "dynamical systems", "theory"]}, "TLDR": {"value": "We rigorously establish that oversmoothing happens exponentially as model depth increases for attention-based graph neural networks."}, "abstract": {"value": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9f8043de46f249092ee94b5af6ce3525032b115d.pdf"}, "supplementary_material": {"value": "/attachment/64a46171b5327e0f42b6e4233c23c2981b22996d.pdf"}, "_bibtex": {"value": "@inproceedings{\nwu2023demystifying,\ntitle={Demystifying Oversmoothing in Attention-Based Graph Neural Networks},\nauthor={Xinyi Wu and Amir Ajorlou and Zihui Wu and Ali Jadbabaie},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Kg65qieiuB}\n}"}, "paperhash": {"value": "wu|demystifying_oversmoothing_in_attentionbased_graph_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10034/-/Revision", "NeurIPS.cc/2023/Conference/Submission10034/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission10034/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326035484, "odate": 1698949761289, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Bj1QSgiBPP", "number": 10011, "cdate": 1683777009735, "tcdate": 1683777009735, "mdate": 1698954286501, "tmdate": 1698954286501, "signatures": ["NeurIPS.cc/2023/Conference/Submission10011/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission10011/Authors"], "forum": "Bj1QSgiBPP", "content": {"title": {"value": "Participatory Personalization in Classification"}, "authors": {"value": ["Hailey James", "Chirag Nagpal", "Katherine A Heller", "Berk Ustun"]}, "authorids": {"value": ["~Hailey_James1", "~Chirag_Nagpal1", "~Katherine_A_Heller1", "~Berk_Ustun1"]}, "keywords": {"value": ["healthcare", "algorithmic fairness", "data privacy", "classification", "interpretability"]}, "TLDR": {"value": "We introduce a new family of prediction models that let individuals opt into personalization at prediction time."}, "abstract": {"value": "Machine learning models are often personalized based on information that is protected, sensitive, self-reported, or costly to acquire. These models use information about people, but do not facilitate nor inform their *consent*. Individuals cannot opt out of reporting information that a model needs to personalize their predictions nor tell if they benefit from personalization in the first place. We introduce a new family of prediction models, called participatory systems, that let individuals opt into personalization at prediction time. We present a model-agnostic algorithm to learn participatory systems for supervised learning tasks where models are personalized with categorical group attributes. We conduct a comprehensive empirical study of participatory systems in clinical prediction tasks, comparing them to common approaches for personalization and imputation. Our results show that participatory systems can facilitate and inform consent in a way that improves performance and privacy across all groups who report personal data."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ebd98daa9b1d01a74f52ef5d98fce2ef1602d8f8.pdf"}, "supplementary_material": {"value": "/attachment/f55c04c41606df51ffaa9c28fb3c48bd78f60168.pdf"}, "_bibtex": {"value": "@inproceedings{\njames2023participatory,\ntitle={Participatory Personalization in Classification},\nauthor={Hailey James and Chirag Nagpal and Katherine A Heller and Berk Ustun},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Bj1QSgiBPP}\n}"}, "paperhash": {"value": "james|participatory_personalization_in_classification"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission10011/-/Revision", "NeurIPS.cc/2023/Conference/Submission10011/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/Submission10011/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326034568, "odate": 1698949760774, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "MWQjqtV1z4", "number": 9985, "cdate": 1683776762413, "tcdate": 1683776762413, "mdate": 1698949760501, "tmdate": 1698949760501, "signatures": ["NeurIPS.cc/2023/Conference/Submission9985/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9985/Authors"], "forum": "MWQjqtV1z4", "content": {"title": {"value": "Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption"}, "authors": {"value": ["Yige Hong", "Qiaomin Xie", "Yudong Chen", "Weina Wang"]}, "authorids": {"value": ["~Yige_Hong1", "~Qiaomin_Xie1", "~Yudong_Chen1", "~Weina_Wang1"]}, "keywords": {"value": ["restless bandits", "average reward MDP", "simulation-based method", "asymptotic optimality"]}, "TLDR": {"value": "We study the average reward restless bandits, and propose the first asymptotically optimal policy that does not make the uniform global attractor assumption."}, "abstract": {"value": "We study the infinite-horizon Restless Bandit problem with the average reward criterion, under both discrete-time and continuous-time settings.\nA fundamental goal is to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. \nExisting results on asymptotic optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. \nIn this paper, we propose a general, simulation-based framework, Follow-the-Virtual-Advice, that converts any single-armed policy into a policy for the original $N$-armed problem. \nThis is done by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. \nOur framework can be instantiated to produce a policy with an $O(1/\\sqrt{N})$ optimality gap. \nIn the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that violate UGAP. \nMore notably, in the continuous-time setting, we do not require \\emph{any} additional assumptions beyond the standard unichain condition. \nIn both settings, our work is the first asymptotic optimality result that does not require UGAP."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c2028181d29818198143384eb7324b3b38dd3346.pdf"}, "_bibtex": {"value": "@inproceedings{\nhong2023restless,\ntitle={Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption},\nauthor={Yige Hong and Qiaomin Xie and Yudong Chen and Weina Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=MWQjqtV1z4}\n}"}, "paperhash": {"value": "hong|restless_bandits_with_average_reward_breaking_the_uniform_global_attractor_assumption"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9985/-/Revision", "NeurIPS.cc/2023/Conference/Submission9985/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9985/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326033629, "odate": 1698949760490, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "wm5Ane9VRO", "number": 9976, "cdate": 1683776662571, "tcdate": 1683776662571, "mdate": 1698949760375, "tmdate": 1698949760375, "signatures": ["NeurIPS.cc/2023/Conference/Submission9976/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9976/Authors"], "forum": "wm5Ane9VRO", "content": {"title": {"value": "Maximization of Average Precision for Deep Learning with Adversarial Ranking Robustness"}, "authors": {"value": ["Gang Li", "Wei Tong", "Tianbao Yang"]}, "authorids": {"value": ["~Gang_Li17", "wei.tong@gm.com", "~Tianbao_Yang1"]}, "keywords": {"value": ["Adversarial Average Precision Maximization", "Robust Average Precision", "Adversarial Ranking Robustness", "Adversarial Training"]}, "TLDR": {"value": "This paper introduces a novel formulation to address a gap in optimizing Average Precision (AP) while ensuring adversarial ranking robustness."}, "abstract": {"value": "This paper seeks to address a gap in optimizing Average Precision (AP) while ensuring adversarial robustness, an area that has not been extensively explored to the best of our knowledge. AP maximization for deep learning has widespread applications, particularly when there is a significant imbalance between positive and negative examples. Although numerous studies have been conducted on adversarial training, they primarily focus on robustness concerning accuracy, ensuring that the average accuracy on adversarially perturbed examples is well maintained. However, this type of adversarial robustness is insufficient for many applications, as minor perturbations on a single example can significantly impact AP  while not greatly influencing the accuracy of the prediction system. To tackle this issue, we introduce a novel formulation that combines an AP surrogate loss with a regularization term representing adversarial ranking robustness, which maintains the consistency between ranking of clean data and that of perturbed data. We then devise an efficient stochastic optimization algorithm to optimize the resulting objective. Our empirical studies, which compare our method to current leading adversarial training baselines and other robust AP maximization strategies, demonstrate the effectiveness of the proposed approach. Notably, our methods outperform a state-of-the-art method (TRADES) by more than 4\\% in terms of robust AP against PGD attacks while achieving 7\\% higher AP on clean data simultaneously on CIFAR10 and CIFAR100."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/fd869b4d687ba7adc6751109da15534112544ad3.pdf"}, "supplementary_material": {"value": "/attachment/ecbe23298e563627376ef734de741ece17be6b70.zip"}, "_bibtex": {"value": "@inproceedings{\nli2023maximization,\ntitle={Maximization of Average Precision for Deep Learning with Adversarial Ranking Robustness},\nauthor={Gang Li and Wei Tong and Tianbao Yang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=wm5Ane9VRO}\n}"}, "paperhash": {"value": "li|maximization_of_average_precision_for_deep_learning_with_adversarial_ranking_robustness"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9976/-/Revision", "NeurIPS.cc/2023/Conference/Submission9976/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9976/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326033310, "odate": 1698949760298, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "V2yFumwo5B", "number": 9952, "cdate": 1683776375422, "tcdate": 1683776375422, "mdate": 1698949760090, "tmdate": 1698949760090, "signatures": ["NeurIPS.cc/2023/Conference/Submission9952/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9952/Authors"], "forum": "V2yFumwo5B", "content": {"title": {"value": "Effective Human-AI Teams via Learned Natural Language Rules and Onboarding"}, "authors": {"value": ["Hussein Mozannar", "Jimin J Lee", "Dennis Wei", "Prasanna Sattigeri", "Subhro Das", "David Sontag"]}, "authorids": {"value": ["~Hussein_Mozannar1", "~Jimin_J_Lee1", "~Dennis_Wei1", "~Prasanna_Sattigeri1", "~Subhro_Das1", "~David_Sontag1"]}, "keywords": {"value": ["human-ai", "collaboration", "onboarding", "region-discovery", "LLM", "data description"]}, "TLDR": {"value": "We introduce a novel method for teaching humans how to effectively collaborate with AI agents through natural language rules learned from data and evaluate on user studies."}, "abstract": {"value": "People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/eef9ad55597a7935192f1cabde6bbba318871a64.pdf"}, "_bibtex": {"value": "@inproceedings{\nmozannar2023effective,\ntitle={Effective Human-{AI} Teams via Learned Natural Language Rules and Onboarding},\nauthor={Hussein Mozannar and Jimin J Lee and Dennis Wei and Prasanna Sattigeri and Subhro Das and David Sontag},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=V2yFumwo5B}\n}"}, "paperhash": {"value": "mozannar|effective_humanai_teams_via_learned_natural_language_rules_and_onboarding"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9952/-/Revision", "NeurIPS.cc/2023/Conference/Submission9952/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9952/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326032621, "odate": 1698949760075, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "waXoG35kbb", "number": 9849, "cdate": 1683775042058, "tcdate": 1683775042058, "mdate": 1698949759516, "tmdate": 1698949759516, "signatures": ["NeurIPS.cc/2023/Conference/Submission9849/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9849/Authors"], "forum": "waXoG35kbb", "content": {"title": {"value": "Provable benefits of score matching"}, "authors": {"value": ["Chirag Pabbaraju", "Dhruv Rohatgi", "Anish Sevekari", "Holden Lee", "Ankur Moitra", "Andrej Risteski"]}, "authorids": {"value": ["~Chirag_Pabbaraju1", "~Dhruv_Rohatgi1", "~Anish_Sevekari1", "~Holden_Lee1", "~Ankur_Moitra1", "~Andrej_Risteski2"]}, "keywords": {"value": ["theory", "score matching", "exponential families", "sample complexity", "computational hardness"]}, "TLDR": {"value": "For natural exponential families, optimizing MLE using zero- and first-order methods can be NP-hard, while score matching is computationally efficient and statistically comparable."}, "abstract": {"value": "Score matching is an alternative to maximum likelihood (ML) for estimating a probability distribution parametrized up to a constant of proportionality. By fitting the ''score'' of the distribution, it sidesteps the need to compute this constant of proportionality (which is often intractable).\nWhile score matching and variants thereof are popular in practice, precise theoretical understanding of the benefits and tradeoffs with maximum likelihood---both computational and statistical---are not well understood. In this work, we give the first example of a natural exponential family of distributions such that the score matching loss is computationally efficient to optimize, and has a comparable statistical efficiency to ML, while the ML loss is intractable to optimize using a gradient-based method. The family consists of exponentials of polynomials of fixed degree, and our result can be viewed as a continuous analogue of recent developments in the discrete setting. Precisely, we show: (1) Designing a zeroth-order or first-order oracle for optimizing the maximum likelihood loss is NP-hard. (2) Maximum likelihood has a statistical efficiency polynomial in the ambient dimension and the radius of the parameters of the family. (3) \nMinimizing the score matching loss is both computationally and statistically efficient, with complexity polynomial in the ambient dimension."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/07de7f5bd1db9dc75e12b66d2cf3a4afa5c95a64.pdf"}, "supplementary_material": {"value": "/attachment/1f645469939fb9a771cc31ffc7f5cb2b830324ba.pdf"}, "_bibtex": {"value": "@inproceedings{\npabbaraju2023provable,\ntitle={Provable benefits of score matching},\nauthor={Chirag Pabbaraju and Dhruv Rohatgi and Anish Sevekari and Holden Lee and Ankur Moitra and Andrej Risteski},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=waXoG35kbb}\n}"}, "paperhash": {"value": "pabbaraju|provable_benefits_of_score_matching"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9849/-/Revision", "NeurIPS.cc/2023/Conference/Submission9849/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9849/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326029994, "odate": 1698949759490, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ubzNoJjOKj", "number": 9765, "cdate": 1683774054410, "tcdate": 1683774054410, "mdate": 1698949759008, "tmdate": 1698949759008, "signatures": ["NeurIPS.cc/2023/Conference/Submission9765/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9765/Authors"], "forum": "ubzNoJjOKj", "content": {"title": {"value": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution"}, "authors": {"value": ["Eric Nguyen", "Michael Poli", "Marjan Faizi", "Armin W Thomas", "Michael Wornow", "Callum Birch-Sykes", "Stefano Massaroli", "Aman Patel", "Clayton M. Rabideau", "Yoshua Bengio", "Stefano Ermon", "Christopher Re", "Stephen Baccus"]}, "authorids": {"value": ["~Eric_Nguyen1", "~Michael_Poli1", "~Marjan_Faizi1", "~Armin_W_Thomas1", "~Michael_Wornow1", "~Callum_Birch-Sykes1", "~Stefano_Massaroli1", "~Aman_Patel2", "~Clayton_M._Rabideau1", "~Yoshua_Bengio1", "~Stefano_Ermon1", "~Christopher_Re1", "~Stephen_Baccus2"]}, "keywords": {"value": ["genomics", "hyena", "foundation models", "large language models", "transformers"]}, "TLDR": {"value": "HyenaDNA is a genomic foundation model trained on the human genome using a context length of 450k tokens."}, "abstract": {"value": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution (i.e. DNA \"characters\") where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena\u2019s new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level \u2013 an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics for simple adaptation to novel tasks without updating pretrained model weights. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data.1 On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/876434250ec4f3bc7581ffa48a304cbc49067651.pdf"}, "_bibtex": {"value": "@inproceedings{\nnguyen2023hyenadna,\ntitle={Hyena{DNA}: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution},\nauthor={Eric Nguyen and Michael Poli and Marjan Faizi and Armin W Thomas and Michael Wornow and Callum Birch-Sykes and Stefano Massaroli and Aman Patel and Clayton M. Rabideau and Yoshua Bengio and Stefano Ermon and Christopher Re and Stephen Baccus},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ubzNoJjOKj}\n}"}, "paperhash": {"value": "nguyen|hyenadna_longrange_genomic_sequence_modeling_at_single_nucleotide_resolution"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission9765/-/Revision", "NeurIPS.cc/2023/Conference/Submission9765/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9765/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326027365, "odate": 1698949758997, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "2JtwuJtoa0", "number": 9760, "cdate": 1683773977432, "tcdate": 1683773977432, "mdate": 1698949759014, "tmdate": 1698949759014, "signatures": ["NeurIPS.cc/2023/Conference/Submission9760/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9760/Authors"], "forum": "2JtwuJtoa0", "content": {"title": {"value": "Unifying Predictions of Deterministic and Stochastic Physics in Mesh-reduced Space with Sequential Flow Generative Model"}, "authors": {"value": ["Luning Sun", "Xu Han", "Han Gao", "Jian-Xun Wang", "Liping Liu"]}, "authorids": {"value": ["~Luning_Sun1", "~Xu_Han9", "~Han_Gao3", "~Jian-Xun_Wang1", "~Liping_Liu1"]}, "keywords": {"value": ["AI4Science", "Fluid Dynamics", "Generative Models", "Graph Neural Network"]}, "abstract": {"value": "Accurate prediction of dynamical systems in unstructured meshes has recently shown successes in scientific simulations. Many dynamical systems have a nonnegligible level of stochasticity introduced by various factors (e.g. chaoticity), so there is a need for a unified framework that captures both deterministic and stochastic components in the rollouts of these systems. Inspired by regeneration learning, we propose a new model that combines generative and sequential networks to model dynamical systems. Specifically, we use an autoencoder to learn compact representations of full-space physical variables in a low-dimensional space. We then integrate a transformer with a conditional normalizing flow model to model the temporal sequence of latent representations. We evaluate the new model in both deterministic and stochastic systems. The model outperforms several competitive baseline models and makes more accurate predictions of deterministic systems. Its own prediction error is also reflected in its uncertainty estimations. When predicting stochastic systems, the proposed model generates high-quality rollout samples. The mean and variance of these samples well match the statistics of samples computed from expensive numerical simulations."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Propose a unified framework to predict the behavior of both deterministic and stochastic systems by generative model"}, "pdf": {"value": "/pdf/d9401844b95efc481a8743bb0c349eba58afa319.pdf"}, "_bibtex": {"value": "@inproceedings{\nsun2023unifying,\ntitle={Unifying Predictions of Deterministic and Stochastic Physics in Mesh-reduced Space with Sequential Flow Generative Model},\nauthor={Luning Sun and Xu Han and Han Gao and Jian-Xun Wang and Liping Liu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=2JtwuJtoa0}\n}"}, "paperhash": {"value": "sun|unifying_predictions_of_deterministic_and_stochastic_physics_in_meshreduced_space_with_sequential_flow_generative_model"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9760/-/Revision", "NeurIPS.cc/2023/Conference/Submission9760/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9760/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326027221, "odate": 1698949758994, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "blm1pqiOXe", "number": 9752, "cdate": 1683773892209, "tcdate": 1683773892209, "mdate": 1698949758926, "tmdate": 1698949758926, "signatures": ["NeurIPS.cc/2023/Conference/Submission9752/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9752/Authors"], "forum": "blm1pqiOXe", "content": {"title": {"value": "Paxion: Patching Action Knowledge in Video-Language Foundation Models"}, "authors": {"value": ["Zhenhailong Wang", "Ansel Blume", "Sha Li", "Genglin Liu", "Jaemin Cho", "Zineng Tang", "Mohit Bansal", "Heng Ji"]}, "authorids": {"value": ["~Zhenhailong_Wang1", "~Ansel_Blume1", "~Sha_Li1", "~Genglin_Liu1", "~Jaemin_Cho1", "~Zineng_Tang1", "~Mohit_Bansal2", "~Heng_Ji3"]}, "keywords": {"value": ["video-language model", "action knowledge benchmarking", "action understanding", "temporal understanding"]}, "TLDR": {"value": "Benchmarking and enhancing video-language foundation models with action knowledge"}, "abstract": {"value": "Action knowledge involves the understanding of textual, visual, and temporal aspects of actions. We introduce the **Action Dynamics Benchmark (ActionBench)** containing two carefully designed probing tasks: Action Antonym and Video Reversal, which targets multimodal alignment capabilities and temporal understanding skills of the model, respectively. Despite recent video-language models\u2019 (VidLM) impressive performance on various benchmark tasks, our diagnostic tasks reveal their surprising deficiency (near-random performance) in action knowledge, suggesting that current models rely on object recognition abilities as a shortcut for action understanding. To remedy this, we propose a novel framework, **Paxion**, along with a new **Discriminative Video Dynamics Modeling (DVDM)** objective. The Paxion framework utilizes a **Knowledge Patcher** network to encode new action knowledge and a **Knowledge Fuser** component to integrate the Patcher into frozen VidLMs without compromising their existing capabilities. Due to limitations of the widely-used Video-Text Contrastive (VTC) loss for learning action knowledge, we introduce the DVDM objective to train the Knowledge Patcher. DVDM forces the model to encode the correlation between the action text and the correct ordering of video frames. Our extensive analyses show that Paxion and DVDM together effectively fill the gap in action knowledge understanding (~50% \u2192 80%), while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/75f70ba6a193d347accace7069e781ae098ccee9.pdf"}, "supplementary_material": {"value": "/attachment/3e1ca2e010d52351ce0d1c23fe50b309c0627091.zip"}, "_bibtex": {"value": "@inproceedings{\nwang2023paxion,\ntitle={Paxion: Patching Action Knowledge in Video-Language Foundation Models},\nauthor={Zhenhailong Wang and Ansel Blume and Sha Li and Genglin Liu and Jaemin Cho and Zineng Tang and Mohit Bansal and Heng Ji},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=blm1pqiOXe}\n}"}, "paperhash": {"value": "wang|paxion_patching_action_knowledge_in_videolanguage_foundation_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9752/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9752/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326026978, "odate": 1698949758910, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "iMfPFPMsZo", "number": 9751, "cdate": 1683773882454, "tcdate": 1683773882454, "mdate": 1698949758924, "tmdate": 1698949758924, "signatures": ["NeurIPS.cc/2023/Conference/Submission9751/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9751/Authors"], "forum": "iMfPFPMsZo", "content": {"title": {"value": "Parallel Submodular Function Minimization"}, "authors": {"value": ["Deeparnab Chakrabarty", "Andrei Graur", "Haotian Jiang", "Aaron Sidford"]}, "authorids": {"value": ["~Deeparnab_Chakrabarty2", "~Andrei_Graur1", "~Haotian_Jiang2", "~Aaron_Sidford1"]}, "keywords": {"value": ["parallel computation", "convex optimization", "submodular function minimization"]}, "abstract": {"value": "We consider the parallel complexity of submodular function minimization (SFM). \n    We provide a pair of methods which obtain two new query versus depth trade-offs a submodular function defined on subsets of $n$ elements that has integer values between $-M$ and $M$. The first method has depth $2$ and query complexity $n^{O(M)}$ and the second method has depth $\\widetilde{O}(n^{1/3} M^{2/3})$ and query complexity $O(\\mathrm{poly}(n, M))$. Despite a line of work on improved parallel lower bounds for SFM, prior to our work the only known algorithms for parallel SFM either followed from more general methods for sequential SFM or highly-parallel minimization of convex $\\ell_2$-Lipschitz functions. Interestingly, to obtain our second result we provide the first highly-parallel algorithm for minimizing $\\ell_\\infty$-Lipschitz function over the hypercube which obtains near-optimal depth for obtaining constant accuracy."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/49da4141b1367dce2c614f671edb98d88d9ab218.pdf"}, "_bibtex": {"value": "@inproceedings{\nchakrabarty2023parallel,\ntitle={Parallel Submodular Function Minimization},\nauthor={Deeparnab Chakrabarty and Andrei Graur and Haotian Jiang and Aaron Sidford},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=iMfPFPMsZo}\n}"}, "paperhash": {"value": "chakrabarty|parallel_submodular_function_minimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9751/-/Revision", "NeurIPS.cc/2023/Conference/Submission9751/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9751/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326026906, "odate": 1698949758906, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "8kyIChWsAG", "number": 9543, "cdate": 1683771040006, "tcdate": 1683771040006, "mdate": 1698949757472, "tmdate": 1698949757472, "signatures": ["NeurIPS.cc/2023/Conference/Submission9543/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9543/Authors"], "forum": "8kyIChWsAG", "content": {"title": {"value": "When Does Optimizing a Proper Loss Yield Calibration?"}, "authors": {"value": ["Jaros\u0142aw B\u0142asiok", "Parikshit Gopalan", "Lunjia Hu", "Preetum Nakkiran"]}, "authorids": {"value": ["jaro3000@gmail.com", "~Parikshit_Gopalan1", "~Lunjia_Hu1", "~Preetum_Nakkiran1"]}, "keywords": {"value": ["calibration", "deep learning", "theory", "optimization"]}, "TLDR": {"value": "We theoretically characterize when (sub-optimally) optimizing a proper loss, as we do in DNNs, leads to a near-optimally calibrated predictor."}, "abstract": {"value": "Optimizing proper loss functions is popularly believed to yield predictors with good calibration properties; the intuition being that for such losses, the global optimum is to predict the ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are trained to approximately minimize loss over restricted families of predictors, that are unlikely to contain the ground truth. Under what circumstances does optimizing proper loss  over a restricted family yield calibrated models? What precise calibration guarantees does it give? In this work, we provide a rigorous answer to these questions. We replace the global optimality with a local optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by post-processing its predictions with a certain family of Lipschitz functions. We show that any predictor with this local optimality satisfies smooth calibration as defined in [Kakade and Foster, 2008, B\u0142asiok et al., 2023]. Local optimality is plausibly satisfied by well-trained DNNs, which suggests an explanation for why they are calibrated from proper loss minimization alone. Finally, we show that the connection between local optimality and calibration error goes both ways: nearly calibrated predictors are also nearly locally optimal."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a612951cf6a047ce113bd0125cfb50ae79c9a6c9.pdf"}, "_bibtex": {"value": "@inproceedings{\nb{\\l}asiok2023when,\ntitle={When Does Optimizing a Proper Loss Yield Calibration?},\nauthor={Jaros{\\l}aw B{\\l}asiok and Parikshit Gopalan and Lunjia Hu and Preetum Nakkiran},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=8kyIChWsAG}\n}"}, "paperhash": {"value": "basiok|when_does_optimizing_a_proper_loss_yield_calibration"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9543/-/Revision", "NeurIPS.cc/2023/Conference/Submission9543/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9543/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326021088, "odate": 1698949757459, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "PfpAQuyZCB", "number": 9410, "cdate": 1683768731110, "tcdate": 1683768731110, "mdate": 1698949756621, "tmdate": 1698949756621, "signatures": ["NeurIPS.cc/2023/Conference/Submission9410/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9410/Authors"], "forum": "PfpAQuyZCB", "content": {"title": {"value": "Behavior Alignment via Reward Function Optimization"}, "authors": {"value": ["Dhawal Gupta", "Yash Chandak", "Scott M. Jordan", "Philip S. Thomas", "Bruno Castro da Silva"]}, "authorids": {"value": ["~Dhawal_Gupta1", "~Yash_Chandak1", "~Scott_M._Jordan1", "~Philip_S._Thomas1", "~Bruno_Castro_da_Silva1"]}, "keywords": {"value": ["Reinforcement Learning", "Behavior Alignment", "Implicit Gradient", "Bi-level Optimization"]}, "abstract": {"value": "Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task.\nThis is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer's intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn \\emph{behavior alignment reward functions}. These functions integrate auxiliary rewards reflecting a designer's heuristics and domain knowledge with the environment's primary rewards. Our approach automatically determines the most effective way to blend these types of feedback, thereby enhancing robustness against heuristic reward misspecification. Remarkably, it can also adapt an agent's policy optimization process to mitigate suboptimalities resulting from limitations and biases inherent in the underlying RL algorithms. We evaluate our method's efficacy on a diverse set of tasks, from small-scale experiments to high-dimensional control challenges. We investigate heuristic auxiliary rewards of varying quality---some of which are beneficial and others detrimental to the learning process. Our results show that our framework offers a robust and principled way to integrate designer-specified heuristics. It not only addresses key shortcomings of existing approaches but also consistently leads to high-performing solutions, even when given misaligned or poorly-specified auxiliary reward functions."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1ad70b290bee87ff65a12e89de80d52101635a8c.pdf"}, "supplementary_material": {"value": "/attachment/020a46d06da4bdfece3a0304c02491988e9053cc.zip"}, "_bibtex": {"value": "@inproceedings{\ngupta2023behavior,\ntitle={Behavior Alignment via Reward Function Optimization},\nauthor={Dhawal Gupta and Yash Chandak and Scott M. Jordan and Philip S. Thomas and Bruno Castro da Silva},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=PfpAQuyZCB}\n}"}, "paperhash": {"value": "gupta|behavior_alignment_via_reward_function_optimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9410/-/Revision", "NeurIPS.cc/2023/Conference/Submission9410/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9410/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326016932, "odate": 1698949756607, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "9dp35y5C0p", "number": 9407, "cdate": 1683768599776, "tcdate": 1683768599776, "mdate": 1698949756622, "tmdate": 1698949756622, "signatures": ["NeurIPS.cc/2023/Conference/Submission9407/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9407/Authors"], "forum": "9dp35y5C0p", "content": {"title": {"value": "Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions"}, "authors": {"value": ["Dongjie Wang", "Meng Xiao", "Min Wu", "pengfei wang", "Yuanchun Zhou", "Yanjie Fu"]}, "authorids": {"value": ["~Dongjie_Wang1", "~Meng_Xiao1", "~Min_Wu2", "~pengfei_wang6", "~Yuanchun_Zhou2", "~Yanjie_Fu2"]}, "keywords": {"value": ["Feature Transformation", "Autoregressive Generation", "Continuous Space Optimization"]}, "abstract": {"value": "Feature transformation aims to generate new pattern-discriminative feature space from original features to improve downstream machine learning  (ML) task performances. However, the discrete search space for the optimal feature explosively grows on the basis of combinations of features and operations from low-order forms to high-order forms. Existing methods, such as exhaustive search, expansion reduction, evolutionary algorithms, reinforcement learning, and iterative greedy, suffer from large search space. Overly emphasizing efficiency in algorithm design usually sacrifice stability or robustness. To fundamentally fill this gap, we reformulate discrete feature transformation as a continuous space optimization task and develop an embedding-optimization-reconstruction framework. This framework includes four steps: 1) reinforcement-enhanced data preparation, aiming to prepare high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding, intending to encapsulate the knowledge of prepared training data within a continuous space; 3) gradient-steered optimal embedding search, dedicating to uncover potentially superior embeddings within the learned space; 4) transformation operation sequence reconstruction, striving to reproduce the feature transformation solution to pinpoint the optimal feature space. Finally, extensive experiments and case studies are performed to demonstrate the effectiveness and robustness of the proposed method. The code and data are publicly accessible https://www.dropbox.com/sh/imh8ckui7va3k5u/AACulQegVx0MuywYyoCqSdVPa?dl=0."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/54bb9dbdf8c5ca551e09a2634e8b4156060dbcdd.pdf"}, "supplementary_material": {"value": "/attachment/d78e24cb8352ee9f3f5eba7a84944f7599e201c7.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023reinforcementenhanced,\ntitle={Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions},\nauthor={Dongjie Wang and Meng Xiao and Min Wu and pengfei wang and Yuanchun Zhou and Yanjie Fu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=9dp35y5C0p}\n}"}, "paperhash": {"value": "wang|reinforcementenhanced_autoregressive_feature_transformation_gradientsteered_search_in_continuous_space_for_postfix_expressions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9407/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9407/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326016816, "odate": 1698949756605, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "NpyZkaEEun", "number": 9344, "cdate": 1683767342346, "tcdate": 1683767342346, "mdate": 1698949756068, "tmdate": 1698949756068, "signatures": ["NeurIPS.cc/2023/Conference/Submission9344/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9344/Authors"], "forum": "NpyZkaEEun", "content": {"title": {"value": "Distributionally Robust Skeleton Learning of Discrete Bayesian Networks"}, "authors": {"value": ["Yeshu Li", "Brian D Ziebart"]}, "authorids": {"value": ["~Yeshu_Li1", "~Brian_D_Ziebart1"]}, "keywords": {"value": ["structure learning", "Bayesian network", "robustness"]}, "TLDR": {"value": "We propose a distributionally robust method for skeleton learning of discrete Bayesian networks with tractable algorithms and out-of-sample exact recovery guarantees."}, "abstract": {"value": "We consider the problem of learning the exact skeleton of general discrete Bayesian networks from potentially corrupted data. Building on distributionally robust optimization and a regression approach, we propose to optimize the most adverse risk over a family of distributions within bounded Wasserstein distance or KL divergence to the empirical distribution. The worst-case risk accounts for the effect of outliers. The proposed approach applies for general categorical random variables without assuming faithfulness, an ordinal relationship or a specific form of conditional distribution. We present efficient algorithms and show the proposed methods are closely related to the standard regularized regression approach. Under mild assumptions, we derive non-asymptotic guarantees for successful structure learning with logarithmic sample complexities for bounded-degree graphs. Numerical study on synthetic and real datasets validates the effectiveness of our method."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/23ca2f607858c346d1788fe7fc5bad02f2e3e534.pdf"}, "supplementary_material": {"value": "/attachment/b46bebcb59c686638292e08e360b41fd6e21494f.pdf"}, "_bibtex": {"value": "@inproceedings{\nli2023distributionally,\ntitle={Distributionally Robust Skeleton Learning of Discrete Bayesian Networks},\nauthor={Yeshu Li and Brian D Ziebart},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=NpyZkaEEun}\n}"}, "paperhash": {"value": "li|distributionally_robust_skeleton_learning_of_discrete_bayesian_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9344/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9344/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326014768, "odate": 1698949756055, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "yKCLfOOIL7", "number": 9332, "cdate": 1683767098808, "tcdate": 1683767098808, "mdate": 1698949755893, "tmdate": 1698949755893, "signatures": ["NeurIPS.cc/2023/Conference/Submission9332/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9332/Authors"], "forum": "yKCLfOOIL7", "content": {"title": {"value": "Mechanism Design for Collaborative Normal Mean Estimation"}, "authors": {"value": ["Yiding Chen", "Jerry Zhu", "Kirthevasan Kandasamy"]}, "authorids": {"value": ["~Yiding_Chen1", "~Jerry_Zhu1", "~Kirthevasan_Kandasamy1"]}, "keywords": {"value": ["Mechanism design", "statistical minimax estimation", "federated learning"]}, "abstract": {"value": "We study collaborative normal mean estimation, where $m$ strategic agents collect i.i.d samples from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ at a cost. They all wish to estimate the mean $\\mu$. By sharing data with each other, agents can obtain better estimates while keeping the cost of data collection small. To facilitate this collaboration, we wish to design mechanisms that encourage agents to collect a sufficient amount of data and share it truthfully, so that they are all better off than working alone. In naive mechanisms, such as simply pooling and sharing all the data, an individual agent might find it beneficial to under-collect and/or fabricate data, which can lead to poor social outcomes. We design a novel mechanism that overcomes these challenges via two key techniques: first, when sharing the others' data with an agent, the mechanism corrupts this dataset proportional to how much the data reported by the agent differs from the others; second, we design minimax optimal estimators for the corrupted dataset. Our mechanism, which is Nash incentive compatible and individually rational, achieves a social penalty (sum of all agents' estimation errors and data collection costs) that is at most a factor 2 of the global minimum. When applied to high dimensional (non-Gaussian) distributions with bounded variance, this mechanism retains these three properties, but with slightly weaker results. Finally, in two special cases where we restrict the strategy space of the agents, we design mechanisms that essentially achieve the global minimum."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We study collaborative normal mean estimation with strategic agents."}, "pdf": {"value": "/pdf/e5e0b28ca3e80168d122756b7717735f0d4a55c9.pdf"}, "supplementary_material": {"value": "/attachment/815b1e65164aa244d8bd23699e40b991f828d0b6.zip"}, "_bibtex": {"value": "@inproceedings{\nchen2023mechanism,\ntitle={Mechanism Design for Collaborative Normal Mean Estimation},\nauthor={Yiding Chen and Jerry Zhu and Kirthevasan Kandasamy},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=yKCLfOOIL7}\n}"}, "paperhash": {"value": "chen|mechanism_design_for_collaborative_normal_mean_estimation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9332/-/Revision", "NeurIPS.cc/2023/Conference/Submission9332/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9332/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326014290, "odate": 1698949755879, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "bKqrWLCMrX", "number": 9328, "cdate": 1683767025553, "tcdate": 1683767025553, "mdate": 1698949755834, "tmdate": 1698949755834, "signatures": ["NeurIPS.cc/2023/Conference/Submission9328/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9328/Authors"], "forum": "bKqrWLCMrX", "content": {"title": {"value": "Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts"}, "authors": {"value": ["Pritam Sarkar", "Ahmad Beirami", "Ali Etemad"]}, "authorids": {"value": ["~Pritam_Sarkar1", "~Ahmad_Beirami1", "~Ali_Etemad1"]}, "keywords": {"value": ["computer vision", "self-supervised learning", "video self-supervised learning", "natural distribution shift", "video learning", "out-of-distribution generalization"]}, "abstract": {"value": "Video self-supervised learning (VSSL) has made significant progress in recent years. However, the exact behavior and dynamics of these models under different forms of distribution shift are not yet known. In this paper, we comprehensively study the behavior of six popular self-supervised methods (v-SimCLR, v-MoCo, v-BYOL, v-SimSiam, v-DINO, v-MAE) in response to various forms of natural distribution shift, i.e., (i) context shift, (ii) viewpoint shift, (iii) actor shift, (iv) source shift, (v) generalizability to unknown classes (zero-shot), and (vi) open-set recognition. To perform this extensive study, we carefully craft a test bed consisting of 17 in-distribution and out-of-distribution benchmark pairs using available public datasets and a series of evaluation protocols to stress-test the different methods under the intended shifts. Our study uncovers a series of intriguing findings and interesting behaviors of VSSL methods. For instance, we observe that while video models generally struggle with context shifts, v-MAE and supervised learning exhibit more robustness. Moreover, our study shows that v-MAE is a strong temporal learner, whereas contrastive methods, v-SimCLR and v-MoCo, exhibit strong performances against viewpoint shifts. When studying the notion of open-set recognition, we notice a trade-off between closed-set and open-set recognition performance if the pretrained VSSL encoders are used without finetuning. We hope that our work will contribute to the development of robust video representation learning frameworks for various real-world scenarios. The project page and code are available at: https://pritamqu.github.io/OOD-VSSL."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/cd7a456791e2949355710774df5b4855f141566a.pdf"}, "TLDR": {"value": "We comprehensively study the behaviour of several popular video self-supervised methods in response to various forms of natural distribution shift, uncovering a series of intriguing findings and interesting behaviors."}, "_bibtex": {"value": "@inproceedings{\nsarkar2023uncovering,\ntitle={Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts},\nauthor={Pritam Sarkar and Ahmad Beirami and Ali Etemad},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=bKqrWLCMrX}\n}"}, "paperhash": {"value": "sarkar|uncovering_the_hidden_dynamics_of_video_selfsupervised_learning_under_distribution_shifts"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9328/-/Revision", "NeurIPS.cc/2023/Conference/Submission9328/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9328/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326014154, "odate": 1698949755821, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "J66ptjMkAG", "number": 9321, "cdate": 1683766761834, "tcdate": 1683766761834, "mdate": 1698949755771, "tmdate": 1698949755771, "signatures": ["NeurIPS.cc/2023/Conference/Submission9321/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9321/Authors"], "forum": "J66ptjMkAG", "content": {"title": {"value": "Kernel Quadrature with Randomly Pivoted Cholesky"}, "authors": {"value": ["Ethan Nicholas Epperly", "Elvira Moreno Ferreira"]}, "authorids": {"value": ["~Ethan_Nicholas_Epperly1", "~Elvira_Moreno_Ferreira1"]}, "keywords": {"value": ["kernel quadrature", "Nystr\u00f6m approximation", "reproducing kernel Hilbert space", "randomly pivoted Cholesky"]}, "TLDR": {"value": "We develop new kernel quadrature schemes based on the randomly pivoted Cholesky sampling algorithm"}, "abstract": {"value": "This paper presents new quadrature rules for functions in a reproducing kernel Hilbert space using nodes drawn by a sampling algorithm known as randomly pivoted Cholesky. The resulting computational procedure compares favorably to previous kernel quadrature methods, which either achieve low accuracy or require solving a computationally challenging sampling problem. Theoretical and numerical results show that randomly pivoted Cholesky is fast and achieves comparable quadrature error rates to more computationally expensive quadrature schemes based on continuous volume sampling, thinning, and recombination. Randomly pivoted Cholesky is easily adapted to complicated geometries with arbitrary kernels, unlocking new potential for kernel quadrature."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/eb0540c72d2836388fcbb238bcfd67acb8a6d5c8.pdf"}, "supplementary_material": {"value": "/attachment/2805a46cc40c89664f8406042347a1f36c8b4404.zip"}, "_bibtex": {"value": "@inproceedings{\nepperly2023kernel,\ntitle={Kernel Quadrature with Randomly Pivoted Cholesky},\nauthor={Ethan Nicholas Epperly and Elvira Moreno Ferreira},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=J66ptjMkAG}\n}"}, "paperhash": {"value": "epperly|kernel_quadrature_with_randomly_pivoted_cholesky"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9321/-/Revision", "NeurIPS.cc/2023/Conference/Submission9321/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9321/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326013985, "odate": 1698949755755, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "TiFMYdQiqp", "number": 9316, "cdate": 1683766673450, "tcdate": 1683766673450, "mdate": 1698949755765, "tmdate": 1698949755765, "signatures": ["NeurIPS.cc/2023/Conference/Submission9316/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9316/Authors"], "forum": "TiFMYdQiqp", "content": {"title": {"value": "Bayesian target optimisation for high-precision holographic optogenetics"}, "authors": {"value": ["Marcus Triplett", "Marta Agnieszka Gajowa", "Hillel Adesnik", "Liam Paninski"]}, "authorids": {"value": ["~Marcus_Triplett1", "~Marta_Agnieszka_Gajowa1", "hadesnik@berkeley.edu", "~Liam_Paninski1"]}, "keywords": {"value": ["Neuroscience", "neural stimulation", "optogenetics", "calcium imaging"]}, "abstract": {"value": "Two-photon optogenetics has transformed our ability to probe the structure and function of neural circuits. However, achieving precise optogenetic control of neural ensemble activity has remained fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we propose a novel computational approach to this problem called Bayesian target optimisation. Our approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation, and then optimises the laser powers and optical target locations needed to achieve a desired activity pattern with minimal OTS. We validate our approach in simulations and using data from in vitro experiments, showing that Bayesian target optimisation considerably reduces OTS across all conditions we test. Together, these results establish our ability to overcome OTS, enabling optogenetic stimulation with substantially improved precision."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "A Bayesian approach to optimising stimulation parameters significantly improves the precision of two-photon optogenetics"}, "pdf": {"value": "/pdf/8714a192fbd762df8299361365d23b6398194ab3.pdf"}, "supplementary_material": {"value": "/attachment/73bec3c1190c5d2135670106e9b276815cba1305.pdf"}, "_bibtex": {"value": "@inproceedings{\ntriplett2023bayesian,\ntitle={Bayesian target optimisation for high-precision holographic optogenetics},\nauthor={Marcus Triplett and Marta Agnieszka Gajowa and Hillel Adesnik and Liam Paninski},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=TiFMYdQiqp}\n}"}, "paperhash": {"value": "triplett|bayesian_target_optimisation_for_highprecision_holographic_optogenetics"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9316/-/Revision", "NeurIPS.cc/2023/Conference/Submission9316/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9316/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326013720, "odate": 1698949755751, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "kKFDMtpeDW", "number": 9312, "cdate": 1683766626793, "tcdate": 1683766626793, "mdate": 1698949755735, "tmdate": 1698949755735, "signatures": ["NeurIPS.cc/2023/Conference/Submission9312/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9312/Authors"], "forum": "kKFDMtpeDW", "content": {"title": {"value": "On Learning Necessary and Sufficient Causal Graphs"}, "authors": {"value": ["Hengrui Cai", "Yixin Wang", "Michael Jordan", "Rui Song"]}, "authorids": {"value": ["~Hengrui_Cai1", "~Yixin_Wang1", "~Michael_Jordan1", "~Rui_Song2"]}, "keywords": {"value": ["Causal structural learning", "Necessity and sufficiency", "Natural causal effects", "Probabilities of causation", "Variable selection"]}, "TLDR": {"value": "We aim to learn necessary and sufficient causal graphs containing only causally relevant variables for specific outcomes by linking probabilities of causation and natural causal effects."}, "abstract": {"value": "The causal revolution has stimulated interest in understanding complex relationships in various fields. Most of the existing methods aim to discover causal relationships among all variables within a complex large-scale graph. However, in practice, only a small subset of variables in the graph are relevant to the outcomes of interest. Consequently, causal estimation with the full causal graph---particularly given limited data---could lead to numerous *falsely discovered, spurious* variables that exhibit high correlation with, but exert no causal impact on, the target outcome. In this paper, we propose learning a class of *necessary and sufficient causal graphs (NSCG)* that exclusively comprises causally relevant variables for an outcome of interest, which we term *causal features*. The key idea is to employ *probabilities of causation* to systematically evaluate the importance of features in the causal graph, allowing us to identify a subgraph relevant to the outcome of interest. To learn NSCG from data, we develop a *necessary and sufficient causal structural learning (NSCSL)* algorithm, by establishing theoretical properties and relationships between probabilities of causation and natural causal effects of features. Across empirical studies of simulated and real data, we demonstrate that NSCSL outperforms existing algorithms and can reveal crucial yeast genes for target heritable traits of interest."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ed58d84e7a67a22cee81c2801db83f9a3bfa14b2.pdf"}, "supplementary_material": {"value": "/attachment/3862d1632492abcc872010e8103cc5fd7c1ac2f3.zip"}, "_bibtex": {"value": "@inproceedings{\ncai2023on,\ntitle={On Learning Necessary and Sufficient Causal Graphs},\nauthor={Hengrui Cai and Yixin Wang and Michael Jordan and Rui Song},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=kKFDMtpeDW}\n}"}, "paperhash": {"value": "cai|on_learning_necessary_and_sufficient_causal_graphs"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9312/-/Revision", "NeurIPS.cc/2023/Conference/Submission9312/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9312/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326013667, "odate": 1698949755723, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "IoizwO1NLf", "number": 9274, "cdate": 1683765767236, "tcdate": 1683765767236, "mdate": 1698949755531, "tmdate": 1698949755531, "signatures": ["NeurIPS.cc/2023/Conference/Submission9274/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9274/Authors"], "forum": "IoizwO1NLf", "content": {"title": {"value": "Skill-it! A data-driven skills framework for understanding and training language models"}, "authors": {"value": ["Mayee F Chen", "Nicholas Roberts", "Kush Bhatia", "Jue WANG", "Ce Zhang", "Frederic Sala", "Christopher Re"]}, "authorids": {"value": ["~Mayee_F_Chen1", "~Nicholas_Roberts2", "~Kush_Bhatia3", "~Jue_WANG1", "~Ce_Zhang1", "~Frederic_Sala1", "~Christopher_Re1"]}, "keywords": {"value": ["language models", "data selection"]}, "abstract": {"value": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 37.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. \nWe apply our skills framework on the RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We examine if there exist ordered collections of skills in training data that are associated with how language models learn, and if this can be used to inform more a effective training data selection procedure."}, "pdf": {"value": "/pdf/2747b41210c890e7d3d00094c8fe2cc353658f22.pdf"}, "_bibtex": {"value": "@inproceedings{\nchen2023skillit,\ntitle={Skill-it! A data-driven skills framework for understanding and training language models},\nauthor={Mayee F Chen and Nicholas Roberts and Kush Bhatia and Jue WANG and Ce Zhang and Frederic Sala and Christopher Re},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=IoizwO1NLf}\n}"}, "paperhash": {"value": "chen|skillit_a_datadriven_skills_framework_for_understanding_and_training_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9274/-/Revision", "NeurIPS.cc/2023/Conference/Submission9274/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9274/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326012769, "odate": 1698949755513, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "IKQOS8rqwr", "number": 9242, "cdate": 1683765087608, "tcdate": 1683765087608, "mdate": 1698949755135, "tmdate": 1698949755135, "signatures": ["NeurIPS.cc/2023/Conference/Submission9242/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9242/Authors"], "forum": "IKQOS8rqwr", "content": {"title": {"value": "QuACK: Accelerating Gradient-Based Quantum Optimization with Koopman Operator Learning"}, "authors": {"value": ["Di Luo", "Jiayu Shen", "Rumen Dangovski", "Marin Soljacic"]}, "authorids": {"value": ["~Di_Luo1", "~Jiayu_Shen1", "~Rumen_Dangovski1", "~Marin_Soljacic1"]}, "keywords": {"value": ["Koopman operator", "quantum optimization", "machine learning"]}, "TLDR": {"value": "We develop Quantum-circuit Alternating Controlled Koopman learning for accelerating quantum optimization"}, "abstract": {"value": "Quantum optimization, a key application of quantum computing, has traditionally been stymied by the linearly increasing complexity of gradient calculations with an increasing number of parameters. This work bridges the gap between Koopman operator theory, which has found utility in applications because it allows for a linear representation of nonlinear dynamical systems, and natural gradient methods in quantum optimization, leading to a significant acceleration of gradient-based quantum optimization. We present Quantum-circuit Alternating Controlled Koopman learning (QuACK), a novel framework that leverages an alternating algorithm for efficient prediction of gradient dynamics on quantum computers. We demonstrate QuACK's remarkable ability to accelerate gradient-based optimization across a range of applications in quantum optimization and machine learning. In fact, our empirical studies, spanning quantum chemistry, quantum condensed\nmatter, quantum machine learning, and noisy environments, have shown accelerations of more than 200x speedup in the overparameterized regime, 10x speedup in the smooth regime, and 3x speedup in the non-smooth regime. With QuACK, we offer a robust advancement that\nharnesses the advantage of gradient-based quantum optimization for practical benefits."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d2486ee02f14c4f0b59e0a5d8c66b91952f6012f.pdf"}, "_bibtex": {"value": "@inproceedings{\nluo2023quack,\ntitle={Qu{ACK}: Accelerating Gradient-Based Quantum Optimization with Koopman Operator Learning},\nauthor={Di Luo and Jiayu Shen and Rumen Dangovski and Marin Soljacic},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=IKQOS8rqwr}\n}"}, "paperhash": {"value": "luo|quack_accelerating_gradientbased_quantum_optimization_with_koopman_operator_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9242/-/Revision", "NeurIPS.cc/2023/Conference/Submission9242/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9242/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326011454, "odate": 1698949755119, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "4ImZxqmT1K", "number": 9235, "cdate": 1683764919961, "tcdate": 1683764919961, "mdate": 1698949755110, "tmdate": 1698949755110, "signatures": ["NeurIPS.cc/2023/Conference/Submission9235/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9235/Authors"], "forum": "4ImZxqmT1K", "content": {"title": {"value": "Learning to Receive Help: Intervention-Aware Concept Embedding Models"}, "authors": {"value": ["Mateo Espinosa Zarlenga", "Katherine M. Collins", "Krishnamurthy Dj Dvijotham", "Adrian Weller", "Zohreh Shams", "Mateja Jamnik"]}, "authorids": {"value": ["~Mateo_Espinosa_Zarlenga1", "~Katherine_M._Collins1", "~Krishnamurthy_Dj_Dvijotham1", "~Adrian_Weller1", "~Zohreh_Shams1", "~Mateja_Jamnik1"]}, "keywords": {"value": ["Explainable Artificial Intelligence", "Concept Bottleneck Models", "Concept-based Explainability", "Interpretability", "XAI", "Concept Interventions"]}, "TLDR": {"value": "We propose a novel training loss that significantly boosts the performance of a Concept Embedding Model when expert concept interventions are available."}, "abstract": {"value": "Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories at train-time. This conditions IntCEMs to effectively select and receive concept interventions when deployed at test-time. Our experiments show that IntCEMs significantly outperform state-of-the-art concept-interpretable models when provided with test-time concept interventions, demonstrating the effectiveness of our approach."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/202c0752b8374fd102a44bb8e4af98b58fa63106.pdf"}, "supplementary_material": {"value": "/attachment/82a103495f8d1097640679212edd9bcff42ba3f7.zip"}, "_bibtex": {"value": "@inproceedings{\nzarlenga2023learning,\ntitle={Learning to Receive Help: Intervention-Aware Concept Embedding Models},\nauthor={Mateo Espinosa Zarlenga and Katherine M. Collins and Krishnamurthy Dj Dvijotham and Adrian Weller and Zohreh Shams and Mateja Jamnik},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=4ImZxqmT1K}\n}"}, "paperhash": {"value": "zarlenga|learning_to_receive_help_interventionaware_concept_embedding_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9235/-/Revision", "NeurIPS.cc/2023/Conference/Submission9235/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9235/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326011174, "odate": 1698949755096, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "BHxsP5fSHv", "number": 9204, "cdate": 1683764220317, "tcdate": 1683764220317, "mdate": 1698949754834, "tmdate": 1698949754834, "signatures": ["NeurIPS.cc/2023/Conference/Submission9204/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9204/Authors"], "forum": "BHxsP5fSHv", "content": {"title": {"value": "OKRidge: Scalable Optimal k-Sparse Ridge Regression"}, "authors": {"value": ["Jiachang Liu", "Sam Rosen", "Chudi Zhong", "Cynthia Rudin"]}, "authorids": {"value": ["~Jiachang_Liu1", "~Sam_Rosen1", "~Chudi_Zhong1", "~Cynthia_Rudin1"]}, "keywords": {"value": ["Sparse Ridge Regression", "Dynamical Systems"]}, "abstract": {"value": "We consider an important problem in scientific discovery, namely identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/cbbea3e2719073047a2497eff8c1a303822e4c28.pdf"}, "supplementary_material": {"value": "/attachment/2e29e619f59533d796d4d359785b27b115c8eee0.zip"}, "_bibtex": {"value": "@inproceedings{\nliu2023okridge,\ntitle={{OKR}idge: Scalable Optimal k-Sparse Ridge Regression},\nauthor={Jiachang Liu and Sam Rosen and Chudi Zhong and Cynthia Rudin},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BHxsP5fSHv}\n}"}, "paperhash": {"value": "liu|okridge_scalable_optimal_ksparse_ridge_regression"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9204/-/Revision", "NeurIPS.cc/2023/Conference/Submission9204/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9204/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326010083, "odate": 1698949754725, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "hJzEoQHfCe", "number": 9195, "cdate": 1683764085153, "tcdate": 1683764085153, "mdate": 1698949754609, "tmdate": 1698949754609, "signatures": ["NeurIPS.cc/2023/Conference/Submission9195/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9195/Authors"], "forum": "hJzEoQHfCe", "content": {"title": {"value": "Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems"}, "authors": {"value": ["Benjamin Coleman", "Wang-Cheng Kang", "Matthew Fahrbach", "Ruoxi Wang", "Lichan Hong", "Ed H. Chi", "Derek Zhiyuan Cheng"]}, "authorids": {"value": ["~Benjamin_Coleman1", "~Wang-Cheng_Kang3", "~Matthew_Fahrbach1", "~Ruoxi_Wang1", "~Lichan_Hong1", "~Ed_H._Chi1", "~Derek_Zhiyuan_Cheng1"]}, "keywords": {"value": ["embedding learning; recommendation systems; representation learning"]}, "abstract": {"value": "Learning high-quality feature embeddings efficiently and effectively is critical for the performance of web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on the order of millions to billions of tokens. The standard approach is to represent each feature value as a $d$-dimensional embedding, which introduces hundreds of billions of parameters for extremely high-cardinality features. This bottleneck has led to substantial progress in alternative embedding algorithms. Many of these methods, however, make the assumption that each feature uses an independent embedding table. This work introduces a simple yet highly effective framework, Feature Multiplexing, where one single representation space is used for many different categorical features. Our theoretical and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each constituent feature, allowing models to distinguish between features. We show that multiplexed representations give Pareto-optimal space-accuracy tradeoffs for three public benchmark datasets. Further, we propose a highly practical approach called Unified Embedding with three major benefits: simplified feature configuration, strong adaptation to dynamic data distributions, and compatibility with modern hardware. Unified embedding gives significant improvements in offline and online metrics compared to highly competitive baselines across five web-scale search, ads, and recommender systems, where it serves billions of users across the world in industry-leading products."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/00a6c17fcf7c7d52f4cf6efde3cb49a01d6e6a7d.pdf"}, "_bibtex": {"value": "@inproceedings{\ncoleman2023unified,\ntitle={Unified Embedding: Battle-Tested Feature Representations for Web-Scale {ML} Systems},\nauthor={Benjamin Coleman and Wang-Cheng Kang and Matthew Fahrbach and Ruoxi Wang and Lichan Hong and Ed H. Chi and Derek Zhiyuan Cheng},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=hJzEoQHfCe}\n}"}, "TLDR": {"value": "We show that a single embedding space can support multiple vocabularies (multiplexing), driving significant practical improvements to recommendation systems."}, "paperhash": {"value": "coleman|unified_embedding_battletested_feature_representations_for_webscale_ml_systems"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9195/-/Revision", "NeurIPS.cc/2023/Conference/Submission9195/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9195/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326009888, "odate": 1698949754593, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "JTKd7zYROf", "number": 9189, "cdate": 1683763937387, "tcdate": 1683763937387, "mdate": 1699303115858, "tmdate": 1699303115858, "signatures": ["NeurIPS.cc/2023/Conference/Submission9189/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9189/Authors"], "forum": "JTKd7zYROf", "content": {"title": {"value": "Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks"}, "authors": {"value": ["Jules Berman", "Benjamin Peherstorfer"]}, "authorids": {"value": ["~Jules_Berman1", "~Benjamin_Peherstorfer2"]}, "keywords": {"value": ["numerical methods", "deep networks", "evolution equations", "scientific computing", "partial differential equations", "model reduction"]}, "abstract": {"value": "Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse updates is up to two orders of magnitude more accurate at a fixed computational budget and up to two orders of magnitude faster at a fixed accuracy than schemes with dense updates."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose Neural Galerkin schemes that update randomized sparse subsets of parameters. We are up to two orders of magnitude more accurate at a fixed budget and up to two orders of magnitude faster at a fixed accuracy than dense updates."}, "pdf": {"value": "/pdf/a3229a49cf4500884369d9674a4651c55155de81.pdf"}, "supplementary_material": {"value": "/attachment/700b00b835ce5c2dcd5b58ddf002f78a649d111f.zip"}, "_bibtex": {"value": "@inproceedings{\nberman2023randomized,\ntitle={Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks},\nauthor={Jules Berman and Benjamin Peherstorfer},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=JTKd7zYROf}\n}"}, "paperhash": {"value": "berman|randomized_sparse_neural_galerkin_schemes_for_solving_evolution_equations_with_deep_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9189/-/Revision", "NeurIPS.cc/2023/Conference/Submission9189/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9189/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326009687, "odate": 1698949754519, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "tgQRMrsxht", "number": 9168, "cdate": 1683763203065, "tcdate": 1683763203065, "mdate": 1698949754247, "tmdate": 1698949754247, "signatures": ["NeurIPS.cc/2023/Conference/Submission9168/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9168/Authors"], "forum": "tgQRMrsxht", "content": {"title": {"value": "Bypassing spike sorting: Density-based decoding using spike localization from dense multielectrode probes"}, "authors": {"value": ["Yizi Zhang", "Tianxiao He", "Julien Boussard", "Charlie Windolf", "Olivier Winter", "Eric M. Trautmann", "Noam Roth", "Hailey Barrel", "Mark M Churchland", "Nick Steinmetz", "Erdem Varol", "Cole Lincoln Hurwitz", "Liam Paninski"]}, "authorids": {"value": ["~Yizi_Zhang1", "~Tianxiao_He1", "~Julien_Boussard1", "~Charlie_Windolf1", "~Olivier_Winter1", "emt2177@columbia.edu", "noamroth@uw.edu", "haileyb2@uw.edu", "~Mark_M_Churchland1", "nick.steinmetz@gmail.com", "~Erdem_Varol2", "~Cole_Lincoln_Hurwitz1", "~Liam_Paninski1"]}, "keywords": {"value": ["neural decoding", "brain-computer interfaces", "spike sorting", "variational inference", "generative models"]}, "abstract": {"value": "Neural decoding and its applications to brain computer interfaces (BCI) are essential for understanding the association between neural activity and behavior. A prerequisite for many decoding approaches is spike sorting, the assignment of action potentials (spikes) to individual neurons. Current spike sorting algorithms, however, can be inaccurate and do not properly model uncertainty of spike assignments, therefore discarding information that could potentially improve decoding performance. Recent advances in high-density probes (e.g., Neuropixels) and computational methods now allow for extracting a rich set of spike features from unsorted data; these features can in turn be used to directly decode behavioral correlates. To this end, we propose a spike sorting-free decoding method that directly models the distribution of extracted spike features using a mixture of Gaussians (MoG) encoding the uncertainty of spike assignments, without aiming to solve the spike clustering problem explicitly. We allow the mixing proportion of the MoG to change over time in response to the behavior and develop variational inference methods to fit the resulting model and to perform decoding. We benchmark our method with an extensive suite of recordings from different animals and probe geometries, demonstrating that our proposed decoder can consistently outperform current methods based on thresholding (i.e. multi-unit activity) and spike sorting. Open source code is available at https://github.com/yzhang511/density_decoding."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/80315c2643547d45b47e27d5c937f45218215fe0.pdf"}, "supplementary_material": {"value": "/attachment/d14da2c0d9f54ead9caf83eaff073b9127d8dcff.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023bypassing,\ntitle={Bypassing spike sorting: Density-based decoding using spike localization from dense multielectrode probes},\nauthor={Yizi Zhang and Tianxiao He and Julien Boussard and Charlie Windolf and Olivier Winter and Eric M. Trautmann and Noam Roth and Hailey Barrel and Mark M Churchland and Nick Steinmetz and Erdem Varol and Cole Lincoln Hurwitz and Liam Paninski},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=tgQRMrsxht}\n}"}, "paperhash": {"value": "zhang|bypassing_spike_sorting_densitybased_decoding_using_spike_localization_from_dense_multielectrode_probes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission9168/-/Revision", "NeurIPS.cc/2023/Conference/Submission9168/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9168/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326008990, "odate": 1698949754234, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "bbbbbov4Xu", "number": 9158, "cdate": 1683762998395, "tcdate": 1683762998395, "mdate": 1698949754023, "tmdate": 1698949754023, "signatures": ["NeurIPS.cc/2023/Conference/Submission9158/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9158/Authors"], "forum": "bbbbbov4Xu", "content": {"title": {"value": "Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion"}, "authors": {"value": ["Yash Sanjay Bhalgat", "Iro Laina", "Joao F. Henriques", "Andrea Vedaldi", "Andrew Zisserman"]}, "authorids": {"value": ["~Yash_Sanjay_Bhalgat1", "~Iro_Laina1", "~Joao_F._Henriques1", "~Andrea_Vedaldi1", "~Andrew_Zisserman1"]}, "keywords": {"value": ["Neural Radiance Fields", "Instance Segmentation", "Metric Learning", "Clustering", "3D Computer Vision"]}, "TLDR": {"value": "Our paper presents a novel \"slow-fast\" contrastive fusion method to lift 2D predictions to 3D for scalable instance segmentation, achieving significant improvements without requiring an upper bound on the number of objects in the scene."}, "abstract": {"value": "Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as on our newly created Messy Rooms dataset, demonstrating the effectiveness and scalability of our slow-fast clustering method."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/221f31e1b2a461ece44316d6074f9d99ef106d8c.pdf"}, "supplementary_material": {"value": "/attachment/cc1bb207f16090ab81288711480dc825efe65881.zip"}, "_bibtex": {"value": "@inproceedings{\nbhalgat2023contrastive,\ntitle={Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion},\nauthor={Yash Sanjay Bhalgat and Iro Laina and Joao F. Henriques and Andrea Vedaldi and Andrew Zisserman},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=bbbbbov4Xu}\n}"}, "paperhash": {"value": "bhalgat|contrastive_lift_3d_object_instance_segmentation_by_slowfast_contrastive_fusion"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9158/-/Revision", "NeurIPS.cc/2023/Conference/Submission9158/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9158/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326008581, "odate": 1698949753934, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "CXPUg86A1D", "number": 9155, "cdate": 1683762993066, "tcdate": 1683762993066, "mdate": 1698949753898, "tmdate": 1698949753898, "signatures": ["NeurIPS.cc/2023/Conference/Submission9155/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9155/Authors"], "forum": "CXPUg86A1D", "content": {"title": {"value": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs"}, "authors": {"value": ["Lijun Yu", "Yong Cheng", "Zhiruo Wang", "Vivek Kumar", "Wolfgang Macherey", "Yanping Huang", "David A Ross", "Irfan Essa", "Yonatan Bisk", "Ming-Hsuan Yang", "Kevin Patrick Murphy", "Alexander G Hauptmann", "Lu Jiang"]}, "authorids": {"value": ["~Lijun_Yu1", "~Yong_Cheng3", "~Zhiruo_Wang1", "~Vivek_Kumar3", "~Wolfgang_Macherey1", "~Yanping_Huang1", "~David_A_Ross1", "~Irfan_Essa1", "~Yonatan_Bisk1", "~Ming-Hsuan_Yang1", "~Kevin_Patrick_Murphy1", "~Alexander_G_Hauptmann1", "~Lu_Jiang1"]}, "keywords": {"value": ["multimodal", "generation", "large language model"]}, "TLDR": {"value": "A language-grounded quantization method that enables in-context few-shot multimodal generation with frozen LLMs."}, "abstract": {"value": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the rich semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks.\nOur method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b23bf5b6212851b812628a11b45979ce688a69ab.pdf"}, "supplementary_material": {"value": "/attachment/16f6950427cc3e294ec30028cd79211af8f332af.pdf"}, "_bibtex": {"value": "@inproceedings{\nyu2023spae,\ntitle={{SPAE}: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen {LLM}s},\nauthor={Lijun Yu and Yong Cheng and Zhiruo Wang and Vivek Kumar and Wolfgang Macherey and Yanping Huang and David A Ross and Irfan Essa and Yonatan Bisk and Ming-Hsuan Yang and Kevin Patrick Murphy and Alexander G Hauptmann and Lu Jiang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=CXPUg86A1D}\n}"}, "paperhash": {"value": "yu|spae_semantic_pyramid_autoencoder_for_multimodal_generation_with_frozen_llms"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9155/-/Revision", "NeurIPS.cc/2023/Conference/Submission9155/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9155/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326008482, "odate": 1698949753882, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "O06z2G18me", "number": 9096, "cdate": 1683761623515, "tcdate": 1683761623515, "mdate": 1698954284600, "tmdate": 1698954284600, "signatures": ["NeurIPS.cc/2023/Conference/Submission9096/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9096/Authors"], "forum": "O06z2G18me", "content": {"title": {"value": "Evaluating the Moral Beliefs Encoded in LLMs"}, "authors": {"value": ["Nino Scherrer", "Claudia Shi", "Amir Feder", "David Blei"]}, "authorids": {"value": ["~Nino_Scherrer1", "~Claudia_Shi1", "~Amir_Feder1", "~David_Blei2"]}, "keywords": {"value": ["Language Models", "Moral Decision Making", "Social Aspects of Machine Learning", "Ethics"]}, "abstract": {"value": "This paper presents a case study on the design, administration, post-processing,  and evaluation of surveys on large language models (LLMs). It comprises two components:\n(1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM \"making a choice\", the associated uncertainty, and the consistency of that choice.\n(2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious.\nWe design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., \"Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g., \"Should I stop for a pedestrian on the road?\"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., \"do not kill\"). We administer the survey to 28 open- and closed-source LLMs.\nWe find that (a) in unambiguous scenarios, most models ``choose\" actions that align with commonsense. In ambiguous cases, most models express uncertainty.\n(b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording.\n(c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We design a survey, a set of evaluation metrics, and a statistical workflow on how to elicit the moral beliefs encoded in an LLM"}, "pdf": {"value": "/pdf/930a476949f6fe7551e41090f0b42aeddb38f344.pdf"}, "supplementary_material": {"value": "/attachment/c1ac89982f3f4a1a997d590dbd49b98b65f12c64.pdf"}, "_bibtex": {"value": "@inproceedings{\nscherrer2023evaluating,\ntitle={Evaluating the Moral Beliefs Encoded in {LLM}s},\nauthor={Nino Scherrer and Claudia Shi and Amir Feder and David Blei},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=O06z2G18me}\n}"}, "paperhash": {"value": "scherrer|evaluating_the_moral_beliefs_encoded_in_llms"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9096/-/Revision", "NeurIPS.cc/2023/Conference/Submission9096/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/Submission9096/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326006544, "odate": 1698949753411, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "ZvDmna23r3", "number": 9085, "cdate": 1683761472385, "tcdate": 1683761472385, "mdate": 1698949753323, "tmdate": 1698949753323, "signatures": ["NeurIPS.cc/2023/Conference/Submission9085/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9085/Authors"], "forum": "ZvDmna23r3", "content": {"title": {"value": "Thought Cloning: Learning to Think while Acting by Imitating Human Thinking"}, "authors": {"value": ["Shengran Hu", "Jeff Clune"]}, "authorids": {"value": ["~Shengran_Hu2", "~Jeff_Clune3"]}, "keywords": {"value": ["Reinforcement learning", "Imitation Learning", "AI Safety", "Interpretability"]}, "TLDR": {"value": "We introduce Thought Cloning, a novel imitation learning framework that enhances agent capability, AI Safety, and Interpretability by training agents to think like humans."}, "abstract": {"value": "Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to $\\textit{think like humans do}$. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, $\\textit{but also the thoughts humans have as they perform these behaviors}$. While we expect Thought Cloning to truly shine at scale on internet-sized datasets (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning and its performance advantage grows the further out of distribution test tasks are, highlighting its ability to better handle novel situations. Thought Cloning also provides important benefits for AI Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the agent\u2019s thoughts, we can (1) more easily diagnose why things are going wrong, making it easier to fix the problem, (2) steer the agent by correcting its thinking, or (3) prevent it from doing unsafe things it plans to do. Overall, by training agents $\\textit{how to think}$ as well as behave, Thought Cloning creates safer, more powerful agents."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d7927d869338423e933d1102d997e569741a0255.pdf"}, "supplementary_material": {"value": "/attachment/3e227a675a08ce569a3138840ef39007e0862406.pdf"}, "_bibtex": {"value": "@inproceedings{\nhu2023thought,\ntitle={Thought Cloning: Learning to Think while Acting by Imitating Human Thinking},\nauthor={Shengran Hu and Jeff Clune},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ZvDmna23r3}\n}"}, "paperhash": {"value": "hu|thought_cloning_learning_to_think_while_acting_by_imitating_human_thinking"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9085/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9085/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/Submission9085/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326006184, "odate": 1698949753311, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "TegmlsD8oQ", "number": 9025, "cdate": 1683759985304, "tcdate": 1683759985304, "mdate": 1698949752902, "tmdate": 1698949752902, "signatures": ["NeurIPS.cc/2023/Conference/Submission9025/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission9025/Authors"], "forum": "TegmlsD8oQ", "content": {"title": {"value": "4M: Massively Multimodal Masked Modeling"}, "authors": {"value": ["David Mizrahi", "Roman Bachmann", "Oguzhan Fatih Kar", "Teresa Yeo", "Mingfei Gao", "Afshin Dehghan", "Amir Zamir"]}, "authorids": {"value": ["~David_Mizrahi1", "~Roman_Bachmann1", "~Oguzhan_Fatih_Kar1", "~Teresa_Yeo1", "~Mingfei_Gao1", "~Afshin_Dehghan5", "~Amir_Zamir1"]}, "keywords": {"value": ["multimodal learning", "multitask learning", "representation learning", "transfer learning", "foundation models", "generative models", "computer vision"]}, "abstract": {"value": "Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision.\nIn this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities  \u2013 including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens.\n\n4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility.\n\nThrough experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e2bd46e31607539ca9fd7c7ced2e9d4059c4daee.pdf"}, "_bibtex": {"value": "@inproceedings{\nmizrahi2023m,\ntitle={4M: Massively Multimodal Masked Modeling},\nauthor={David Mizrahi and Roman Bachmann and Oguzhan Fatih Kar and Teresa Yeo and Mingfei Gao and Afshin Dehghan and Amir Zamir},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=TegmlsD8oQ}\n}"}, "paperhash": {"value": "mizrahi|4m_massively_multimodal_masked_modeling"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission9025/-/Revision", "NeurIPS.cc/2023/Conference/Submission9025/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission9025/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326004149, "odate": 1698949752888, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "eJZ5vJEaaa", "number": 8992, "cdate": 1683759151287, "tcdate": 1683759151287, "mdate": 1698949752663, "tmdate": 1698949752663, "signatures": ["NeurIPS.cc/2023/Conference/Submission8992/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8992/Authors"], "forum": "eJZ5vJEaaa", "content": {"title": {"value": "What Planning Problems Can A Relational Neural Network Solve?"}, "authors": {"value": ["Jiayuan Mao", "Tom\u00e1s Lozano-P\u00e9rez", "Joshua B. Tenenbaum", "Leslie Pack Kaelbling"]}, "authorids": {"value": ["~Jiayuan_Mao1", "~Tom\u00e1s_Lozano-P\u00e9rez1", "~Joshua_B._Tenenbaum1", "~Leslie_Pack_Kaelbling1"]}, "keywords": {"value": ["Planning", "Relational Neural Network", "Circuit Complexity"]}, "abstract": {"value": "Goal-conditioned policies are generally understood to be \"feed-forward\" circuits, in the form of neural networks that map from the current state and the goal specification to the next action to take. However, under what circumstances such a policy can be learned and how efficient the policy will be are not well understood. In this paper, we present a circuit complexity analysis for relational neural networks (such as graph neural networks and transformers) representing policies for planning problems, by drawing connections with serialized goal regression search (S-GRS). We show that there are three general classes of planning problems, in terms of the growth of circuit width and depth as a function of the number of objects and planning horizon, providing constructive proofs. We also illustrate the utility of this analysis for designing neural networks for policy learning."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/cc669290ae74e1e3e09b44d6c048054018305a75.pdf"}, "_bibtex": {"value": "@inproceedings{\nmao2023what,\ntitle={What Planning Problems Can A Relational Neural Network Solve?},\nauthor={Jiayuan Mao and Tom{\\'a}s Lozano-P{\\'e}rez and Joshua B. Tenenbaum and Leslie Pack Kaelbling},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=eJZ5vJEaaa}\n}"}, "paperhash": {"value": "mao|what_planning_problems_can_a_relational_neural_network_solve"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8992/-/Revision", "NeurIPS.cc/2023/Conference/Submission8992/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8992/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326003146, "odate": 1698949752646, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "bo8q5MRcwy", "number": 8979, "cdate": 1683758824253, "tcdate": 1683758824253, "mdate": 1698949752528, "tmdate": 1698949752528, "signatures": ["NeurIPS.cc/2023/Conference/Submission8979/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8979/Authors"], "forum": "bo8q5MRcwy", "content": {"title": {"value": "Learning Universal Policies via Text-Guided Video Generation"}, "authors": {"value": ["Yilun Du", "Sherry Yang", "Bo Dai", "Hanjun Dai", "Ofir Nachum", "Joshua B. Tenenbaum", "Dale Schuurmans", "Pieter Abbeel"]}, "authorids": {"value": ["~Yilun_Du1", "~Sherry_Yang1", "~Bo_Dai1", "~Hanjun_Dai1", "~Ofir_Nachum1", "~Joshua_B._Tenenbaum1", "~Dale_Schuurmans1", "~Pieter_Abbeel2"]}, "keywords": {"value": ["sequential decision making", "general-purpose agent", "video diffusion"]}, "TLDR": {"value": "We cast sequential decision making as a text-conditioned video generation problem and propose UniPi which can generalize to combinatorial and multi-task environments and be able to utilize broad internet-scale text-video datasets."}, "abstract": {"value": "A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/33b7c3fe90b1829c51b1a54681e02d57004f37c6.pdf"}, "supplementary_material": {"value": "/attachment/0461f7825a836efb5d24f1e7e79cdc544673c109.zip"}, "_bibtex": {"value": "@inproceedings{\ndu2023learning,\ntitle={Learning Universal Policies via Text-Guided Video Generation},\nauthor={Yilun Du and Sherry Yang and Bo Dai and Hanjun Dai and Ofir Nachum and Joshua B. Tenenbaum and Dale Schuurmans and Pieter Abbeel},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=bo8q5MRcwy}\n}"}, "paperhash": {"value": "du|learning_universal_policies_via_textguided_video_generation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8979/-/Revision", "NeurIPS.cc/2023/Conference/Submission8979/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8979/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326002747, "odate": 1698949752500, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "pJQu0zpKCS", "number": 8906, "cdate": 1683756924333, "tcdate": 1683756924333, "mdate": 1698949751847, "tmdate": 1698949751847, "signatures": ["NeurIPS.cc/2023/Conference/Submission8906/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8906/Authors"], "forum": "pJQu0zpKCS", "content": {"title": {"value": "Optimal Exploration for Model-Based RL in Nonlinear Systems"}, "authors": {"value": ["Andrew Wagenmaker", "Guanya Shi", "Kevin Jamieson"]}, "authorids": {"value": ["~Andrew_Wagenmaker1", "~Guanya_Shi1", "~Kevin_Jamieson1"]}, "keywords": {"value": ["reinforcement learning", "control theory", "system identification", "experiment design", "active learning"]}, "TLDR": {"value": "We develop an approach which optimally explores unknown nonlinear dynamical systems with the goal of learning a good controller."}, "abstract": {"value": "Learning to control unknown nonlinear dynamical systems is a fundamental problem in reinforcement learning and control theory. A commonly applied approach is to first explore the environment (exploration), learn an accurate model of it (system identification), and then compute an optimal controller with the minimum cost on this estimated system (policy optimization). While existing work has shown that it is possible to learn a uniformly good model of the system (Mania et al., 2020), in practice, if we aim to learn a good controller with a low cost on the actual system, certain system parameters may be significantly more critical than others, and we therefore ought to focus our exploration on learning such parameters.\n\nIn this work, we consider the setting of nonlinear dynamical systems and seek to formally quantify, in such settings, (a) which parameters are most relevant to learning a good controller, and (b) how we can best explore so as to minimize uncertainty in such parameters. Inspired by recent work in linear systems (Wagenmaker et al., 2021), we show that minimizing the controller loss in nonlinear systems translates to estimating the system parameters in a particular, task-dependent metric. Motivated by this, we develop an algorithm able to efficiently explore the system to reduce uncertainty in this metric, and prove a lower bound showing that our approach learns a controller at a near-instance-optimal rate. Our algorithm relies on a general reduction from policy optimization to optimal experiment design in arbitrary systems, and may be of independent interest. We conclude with experiments demonstrating the effectiveness of our method in realistic nonlinear robotic systems."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a450180468afbf4ec1f7a3b952e175adc1d41a8e.pdf"}, "_bibtex": {"value": "@inproceedings{\nwagenmaker2023optimal,\ntitle={Optimal Exploration for Model-Based {RL} in Nonlinear Systems},\nauthor={Andrew Wagenmaker and Guanya Shi and Kevin Jamieson},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=pJQu0zpKCS}\n}"}, "paperhash": {"value": "wagenmaker|optimal_exploration_for_modelbased_rl_in_nonlinear_systems"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8906/-/Revision", "NeurIPS.cc/2023/Conference/Submission8906/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8906/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695326000336, "odate": 1698949751827, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "nzkWhoXUpv", "number": 8892, "cdate": 1683756617413, "tcdate": 1683756617413, "mdate": 1699824712001, "tmdate": 1699824712001, "signatures": ["NeurIPS.cc/2023/Conference/Submission8892/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8892/Authors"], "forum": "nzkWhoXUpv", "content": {"title": {"value": "Individual Arbitrariness and Group Fairness"}, "authors": {"value": ["Carol Xuan Long", "Hsiang Hsu", "Wael Alghamdi", "Flavio Calmon"]}, "authorids": {"value": ["~Carol_Xuan_Long1", "~Hsiang_Hsu1", "~Wael_Alghamdi1", "~Flavio_Calmon1"]}, "keywords": {"value": ["predictive multiplicity", "fairness in machine learning", "Rashomon effect"]}, "abstract": {"value": "Machine learning tasks may admit multiple competing models that achieve similar performance yet produce conflicting outputs for individual samples---a phenomenon known as predictive multiplicity. We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness interventions can mask high predictive multiplicity behind favorable group fairness and accuracy metrics. We argue that a third axis of ``arbitrariness'' should be considered  when deploying models to aid decision-making in applications of individual-level impact.\nTo address this challenge, we propose an ensemble  algorithm applicable to any fairness intervention that provably ensures  more consistent predictions."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity."}, "pdf": {"value": "/pdf/03b82dd7c8c81d108908f745a300e30702e22d9b.pdf"}, "_bibtex": {"value": "@inproceedings{\nlong2023individual,\ntitle={Individual Arbitrariness and Group Fairness},\nauthor={Carol Xuan Long and Hsiang Hsu and Wael Alghamdi and Flavio Calmon},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=nzkWhoXUpv}\n}"}, "paperhash": {"value": "long|individual_arbitrariness_and_group_fairness"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8892/-/Revision", "NeurIPS.cc/2023/Conference/Submission8892/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8892/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325999859, "odate": 1698949751683, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "CgJJvuLjec", "number": 8878, "cdate": 1683756263685, "tcdate": 1683756263685, "mdate": 1698949751510, "tmdate": 1698949751510, "signatures": ["NeurIPS.cc/2023/Conference/Submission8878/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8878/Authors"], "forum": "CgJJvuLjec", "content": {"title": {"value": "PAPR: Proximity Attention Point Rendering"}, "authors": {"value": ["Yanshu Zhang", "Shichong Peng", "Seyed Alireza Moazenipourasil", "Ke Li"]}, "authorids": {"value": ["~Yanshu_Zhang1", "~Shichong_Peng1", "~Seyed_Alireza_Moazenipourasil1", "~Ke_Li1"]}, "keywords": {"value": ["point cloud learning", "point cloud rendering"]}, "TLDR": {"value": "A point-based 3D representation and rendering method that can learn scene surface geometry using point cloud from scratch"}, "abstract": {"value": "Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains a challenge in 3D representation learning.  Existing point-based methods often suffer from the vanishing gradient problem or require a large number of points to accurately model scene geometry and texture. To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a novel method that consists of a point-based scene representation and a differentiable renderer. Our scene representation uses a point cloud where each point is characterized by its spatial position, foreground score, and view-independent feature vector. The renderer selects the relevant points for each ray and produces accurate colours using their associated features. PAPR effectively learns point cloud positions to represent the correct scene geometry, even when the initialization drastically differs from the target geometry. Notably, our method captures fine texture details while using only a parsimonious set of points. We also demonstrate four practical applications of our method: geometry editing, object manipulation, texture transfer, and exposure control. More results and code are available on our project website at https://zvict.github.io/papr/."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ed4cc37f94581eddd15a541f7592a4fd995d0500.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023papr,\ntitle={{PAPR}: Proximity Attention Point Rendering},\nauthor={Yanshu Zhang and Shichong Peng and Seyed Alireza Moazenipourasil and Ke Li},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=CgJJvuLjec}\n}"}, "paperhash": {"value": "zhang|papr_proximity_attention_point_rendering"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8878/-/Revision", "NeurIPS.cc/2023/Conference/Submission8878/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8878/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325999430, "odate": 1698949751497, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "KKxO6wwx8p", "number": 8844, "cdate": 1683755710085, "tcdate": 1683755710085, "mdate": 1698949751238, "tmdate": 1698949751238, "signatures": ["NeurIPS.cc/2023/Conference/Submission8844/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8844/Authors"], "forum": "KKxO6wwx8p", "content": {"title": {"value": "SE(3) Equivariant Augmented Coupling Flows"}, "authors": {"value": ["Laurence Illing Midgley", "Vincent Stimper", "Javier Antoran", "Emile Mathieu", "Bernhard Sch\u00f6lkopf", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"]}, "authorids": {"value": ["~Laurence_Illing_Midgley1", "~Vincent_Stimper1", "~Javier_Antoran1", "~Emile_Mathieu1", "~Bernhard_Sch\u00f6lkopf1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1"]}, "keywords": {"value": ["Boltzmann generator", "normalizing flow"]}, "TLDR": {"value": "We propose SE(3) equivariant augmented coupling flows for modeling physical systems."}, "abstract": {"value": "Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivariant continuous normalizing flows, while allowing sampling more than an order of magnitude faster. Moreover, to the best of our knowledge, we are the first to learn the full Boltzmann distribution of alanine dipeptide by only modeling the Cartesian positions of its atoms. Lastly, we demonstrate that our flow can be trained to approximately sample from the Boltzmann distribution of the DW4 and LJ13 particle systems using only their energy functions."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b39c57db7441b2742ff13e50f91defbefb299e85.pdf"}, "_bibtex": {"value": "@inproceedings{\nmidgley2023se,\ntitle={{SE}(3) Equivariant Augmented Coupling Flows},\nauthor={Laurence Illing Midgley and Vincent Stimper and Javier Antoran and Emile Mathieu and Bernhard Sch{\\\"o}lkopf and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=KKxO6wwx8p}\n}"}, "paperhash": {"value": "midgley|se3_equivariant_augmented_coupling_flows"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8844/-/Revision", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission8844/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8844/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325998175, "odate": 1698949751224, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "bbL20Oupi4", "number": 8771, "cdate": 1683754437349, "tcdate": 1683754437349, "mdate": 1698949750797, "tmdate": 1698949750797, "signatures": ["NeurIPS.cc/2023/Conference/Submission8771/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8771/Authors"], "forum": "bbL20Oupi4", "content": {"title": {"value": "Anonymous and Copy-Robust Delegations for Liquid Democracy"}, "authors": {"value": ["Markus Utke", "Ulrike Schmidt-Kraepelin"]}, "authorids": {"value": ["markus.utke@campus.tu-berlin.de", "~Ulrike_Schmidt-Kraepelin1"]}, "keywords": {"value": ["liquid democracy", "directed trees", "parameterized markov chain", "matrix tree theorem", "axiomatic method"]}, "abstract": {"value": "Liquid democracy with ranked delegations is a novel voting scheme that unites the practicability of representative democracy with the idealistic appeal of direct democracy: Every voter decides between casting their vote on a question at hand or delegating their voting weight to some other, trusted agent. Delegations are transitive, and since voters may end up in a delegation cycle, they are encouraged to indicate not only a single delegate, but a set of potential delegates and a ranking among them. Based on the delegation preferences of all voters, a delegation rule selects one representative per voter. Previous work has revealed a trade-off between two properties of delegation rules called anonymity and copy-robustness. \n\nTo overcome this issue we study two fractional delegation rules: Mixed Borda branching, which generalizes a rule satisfying copy-robustness, and the random walk rule, which satisfies anonymity. Using the Markov chain tree theorem, we show that the two rules are in fact equivalent, and simultaneously satisfy generalized versions of the two properties. Combining the same theorem with Fulkerson's algorithm, we develop  a polynomial-time algorithm for computing the outcome of the studied delegation rule. This algorithm is of independent interest, having applications in semi-supervised learning and graph theory."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c0f4dee796bb2ba0d0430d5e2fc53abb73477e21.pdf"}, "supplementary_material": {"value": "/attachment/e80d5e2a577bf933c7a40e3d0671484b1e39f762.pdf"}, "_bibtex": {"value": "@inproceedings{\nutke2023anonymous,\ntitle={Anonymous and Copy-Robust Delegations for Liquid Democracy},\nauthor={Markus Utke and Ulrike Schmidt-Kraepelin},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=bbL20Oupi4}\n}"}, "paperhash": {"value": "utke|anonymous_and_copyrobust_delegations_for_liquid_democracy"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8771/-/Revision", "NeurIPS.cc/2023/Conference/Submission8771/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8771/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325996218, "odate": 1698949750780, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ZdxGmJGKOo", "number": 8751, "cdate": 1683754144918, "tcdate": 1683754144918, "mdate": 1698949750750, "tmdate": 1698949750750, "signatures": ["NeurIPS.cc/2023/Conference/Submission8751/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8751/Authors"], "forum": "ZdxGmJGKOo", "content": {"title": {"value": "SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning"}, "authors": {"value": ["Yifan Yang", "Peiyao Xiao", "Kaiyi Ji"]}, "authorids": {"value": ["~Yifan_Yang13", "~Peiyao_Xiao1", "~Kaiyi_Ji1"]}, "keywords": {"value": ["Federated bilevel optimization", "federated hypergradient", "communication efficiency", "system-level heterogeneity", "linear speedup"]}, "abstract": {"value": "Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demonstrate the effectiveness of the proposed methods over existing FBO algorithms."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/56ed3c5fe1563f4aa1676734f7d1490ace897ae4.pdf"}, "supplementary_material": {"value": "/attachment/bd89b733f41a78b747636272558487f24bc33001.pdf"}, "_bibtex": {"value": "@inproceedings{\nyang2023simfbo,\ntitle={Sim{FBO}: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning},\nauthor={Yifan Yang and Peiyao Xiao and Kaiyi Ji},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ZdxGmJGKOo}\n}"}, "paperhash": {"value": "yang|simfbo_towards_simple_flexible_and_communicationefficient_federated_bilevel_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8751/-/Revision", "NeurIPS.cc/2023/Conference/Submission8751/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8751/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325995580, "odate": 1698949750668, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "0BfQT652sC", "number": 8737, "cdate": 1683753806138, "tcdate": 1683753806138, "mdate": 1698949750496, "tmdate": 1698949750496, "signatures": ["NeurIPS.cc/2023/Conference/Submission8737/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8737/Authors"], "forum": "0BfQT652sC", "content": {"title": {"value": "Stochastic Multi-armed Bandits: Optimal Trade-off among Optimality, Consistency, and Tail Risk"}, "authors": {"value": ["David Simchi-Levi", "Zeyu Zheng", "Feng Zhu"]}, "authorids": {"value": ["~David_Simchi-Levi2", "~Zeyu_Zheng2", "~Feng_Zhu7"]}, "keywords": {"value": ["multi-armed bandit", "worst-case optimality", "instance-dependent consistency", "light-tailed risk"]}, "abstract": {"value": "We consider the stochastic multi-armed bandit problem and fully characterize the interplays among three desired properties for policy design: worst-case optimality, instance-dependent consistency, and light-tailed risk. We show how the order of expected regret exactly affects the decaying rate of the regret tail probability for both the worst-case and instance-dependent scenario. A novel policy is proposed to achieve the optimal regret tail risk for any regret threshold. Concretely, for any given $\\alpha\\in[1/2, 1)$ and $\\beta\\in[0, 1)$, our policy achieves a worst-case expected regret of $\\tilde O(T^\\alpha)$ and instance-dependent expected regret of $\\tilde O(T^\\beta)$, while enjoys a probability of incurring an $\\Omega(T^\\delta)$ regret that decays exponentially with a polynomial $T$ term. Such decaying rate is proved to be best achievable. We also generalize our analysis to the stochastic multi-armed bandit problem with non-stationary baseline rewards, where in each time period $t$, the decision maker pulls one of $K$ arms and collects a reward which is the sum of three terms: the mean of the pulled arm, an independent noise, and a non-stationary baseline reward as a function of $t$. Our results reveal insights on the trade-off between expected regret and tail risk for both worst-case and instance-dependent scenario, indicating that more sub-optimality and inconsistency leaves space for more light-tailed risk of incurring a large regret."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/cfd73f4fbb7ff68a3e02e962b7486695bcf85696.pdf"}, "supplementary_material": {"value": "/attachment/6d14a597e31e19e0fff1fc1480b738c39346b31c.pdf"}, "_bibtex": {"value": "@inproceedings{\nsimchi-levi2023stochastic,\ntitle={Stochastic Multi-armed Bandits: Optimal Trade-off among Optimality, Consistency, and Tail Risk},\nauthor={David Simchi-Levi and Zeyu Zheng and Feng Zhu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=0BfQT652sC}\n}"}, "paperhash": {"value": "simchilevi|stochastic_multiarmed_bandits_optimal_tradeoff_among_optimality_consistency_and_tail_risk"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8737/-/Revision", "NeurIPS.cc/2023/Conference/Submission8737/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8737/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325995265, "odate": 1698949750482, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "i28zCSsQIc", "number": 8734, "cdate": 1683753736081, "tcdate": 1683753736081, "mdate": 1698949750404, "tmdate": 1698949750404, "signatures": ["NeurIPS.cc/2023/Conference/Submission8734/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8734/Authors"], "forum": "i28zCSsQIc", "content": {"title": {"value": "GloptiNets: Scalable Non-Convex Optimization with Certificates"}, "authors": {"value": ["Gaspard Beugnot", "Julien Mairal", "Alessandro Rudi"]}, "authorids": {"value": ["~Gaspard_Beugnot1", "~Julien_Mairal1", "~Alessandro_Rudi1"]}, "keywords": {"value": ["non-convex optimization", "polynomial optimization", "kernel sum-of-squares"]}, "TLDR": {"value": "Fast and scalable polynomial optimization on the torus with kernel sum-of-squares and overparametrized models"}, "abstract": {"value": "We present a novel approach to non-convex optimization with certificates, which handles smooth functions on the hypercube or on the torus. Unlike traditional methods that rely on algebraic properties, our algorithm exploits the regularity of the target function intrinsic in the decay of its Fourier spectrum. By defining a tractable family of models, we allow {\\em at the same time} to obtain precise certificates and to leverage the advanced and powerful computational techniques developed to optimize neural networks. In this way the scalability of our approach is naturally enhanced by parallel computing with GPUs. Our approach, when applied to the case of polynomials of moderate dimensions but with thousands of coefficients, outperforms the state-of-the-art optimization methods with certificates, as the ones based on Lasserre's hierarchy, addressing problems intractable for the competitors."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/00364b2072f2db5ca566f89fe2ed1216aacb0a94.pdf"}, "supplementary_material": {"value": "/attachment/064782d6d25948e2464ae13e04c9657614e22a34.zip"}, "_bibtex": {"value": "@inproceedings{\nbeugnot2023gloptinets,\ntitle={GloptiNets: Scalable Non-Convex Optimization with Certificates},\nauthor={Gaspard Beugnot and Julien Mairal and Alessandro Rudi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=i28zCSsQIc}\n}"}, "paperhash": {"value": "beugnot|gloptinets_scalable_nonconvex_optimization_with_certificates"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8734/-/Revision", "NeurIPS.cc/2023/Conference/Submission8734/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8734/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325995101, "odate": 1698949750392, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "9KtX12YmA7", "number": 8715, "cdate": 1683753411672, "tcdate": 1683753411672, "mdate": 1698949750315, "tmdate": 1698949750315, "signatures": ["NeurIPS.cc/2023/Conference/Submission8715/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8715/Authors"], "forum": "9KtX12YmA7", "content": {"title": {"value": "The Behavior and Convergence of Local Bayesian Optimization"}, "authors": {"value": ["Kaiwen Wu", "Kyurae Kim", "Roman Garnett", "Jacob R. Gardner"]}, "authorids": {"value": ["~Kaiwen_Wu2", "~Kyurae_Kim1", "~Roman_Garnett1", "~Jacob_R._Gardner1"]}, "keywords": {"value": ["Bayesian optimization", "convergence rates"]}, "TLDR": {"value": "We provide the first convergence rate for a local Bayesian optimization algorithm in both the noisy and noiseless settings."}, "abstract": {"value": "A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The \"folk wisdom\" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by M\u00fcller et al. (2021), and derive convergence rates in both the noisy and noiseless settings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d71748618d7d100c50042a75ed7e78e570c79546.pdf"}, "supplementary_material": {"value": "/attachment/ee30735d33e5125329b24d239095191779b33597.pdf"}, "_bibtex": {"value": "@inproceedings{\nwu2023the,\ntitle={The Behavior and Convergence of Local Bayesian Optimization},\nauthor={Kaiwen Wu and Kyurae Kim and Roman Garnett and Jacob R. Gardner},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=9KtX12YmA7}\n}"}, "paperhash": {"value": "wu|the_behavior_and_convergence_of_local_bayesian_optimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8715/-/Revision", "NeurIPS.cc/2023/Conference/Submission8715/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8715/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325994607, "odate": 1698949750300, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "YMMlHBSQdC", "number": 8694, "cdate": 1683752929025, "tcdate": 1683752929025, "mdate": 1698949750178, "tmdate": 1698949750178, "signatures": ["NeurIPS.cc/2023/Conference/Submission8694/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8694/Authors"], "forum": "YMMlHBSQdC", "content": {"title": {"value": "Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness"}, "authors": {"value": ["Suraj Srinivas", "Sebastian Bordt", "Himabindu Lakkaraju"]}, "authorids": {"value": ["~Suraj_Srinivas1", "~Sebastian_Bordt1", "~Himabindu_Lakkaraju1"]}, "keywords": {"value": ["robustness", "generative models", "perceptually aligned gradients", "bayes optimality", "manifold hypothesis"]}, "abstract": {"value": "One of the remarkable properties of robust computer vision models is that their input-gradients are often aligned with human perception, referred to in the literature as perceptually-aligned gradients (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary generative capabilities, including image generation, denoising, and in-painting. However, the underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first explanation of PAGs via \\emph{off-manifold robustness}, which states that models must be more robust off- the data manifold than they are on-manifold. We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness, and confirm the same empirically for robust models trained via gradient norm regularization, randomized smoothing, and adversarial training with projected gradient descent. Quantifying the perceptual alignment of model gradients via their similarity with the gradients of generative models, we show that off-manifold robustness correlates well with perceptual alignment. Finally, based on the levels of on- and off-manifold robustness, we identify three different regimes of robustness that affect both perceptual alignment and model accuracy: weak robustness, bayes-aligned robustness, and excessive robustness. Code is available at https://github.com/tml-tuebingen/pags."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f8c1f231295e251657c394635638147a0dc05675.pdf"}, "supplementary_material": {"value": "/attachment/0a0268e84535add108e834964bf9dd80d045bf52.zip"}, "_bibtex": {"value": "@inproceedings{\nsrinivas2023which,\ntitle={Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness},\nauthor={Suraj Srinivas and Sebastian Bordt and Himabindu Lakkaraju},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=YMMlHBSQdC}\n}"}, "paperhash": {"value": "srinivas|which_models_have_perceptuallyaligned_gradients_an_explanation_via_offmanifold_robustness"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8694/-/Revision", "NeurIPS.cc/2023/Conference/Submission8694/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8694/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325994082, "odate": 1698949750150, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Y18r0xWkSh", "number": 8676, "cdate": 1683752480461, "tcdate": 1683752480461, "mdate": 1698949749946, "tmdate": 1698949749946, "signatures": ["NeurIPS.cc/2023/Conference/Submission8676/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8676/Authors"], "forum": "Y18r0xWkSh", "content": {"title": {"value": "Posterior Contraction Rates for Mat\u00e9rn Gaussian Processes on Riemannian Manifolds"}, "authors": {"value": ["Paul Rosa", "Viacheslav Borovitskiy", "Alexander Terenin", "Judith Rousseau"]}, "authorids": {"value": ["~Paul_Rosa1", "~Viacheslav_Borovitskiy1", "~Alexander_Terenin1", "~Judith_Rousseau1"]}, "keywords": {"value": ["Gaussian processes", "posterior contraction", "manifolds", "kernels"]}, "abstract": {"value": "Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Mat\u00e9rn Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of examples, which, mirroring prior work, show that intrinsic processes can achieve better performance in practice. Therefore, our work shows that finer-grained analyses are needed to distinguish between different levels of data-efficiency of geometric Gaussian processes, particularly in settings which involve small data set sizes and non-asymptotic behavior."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/664b6e055be4131548f23f041d6a71c05044127b.pdf"}, "supplementary_material": {"value": "/attachment/628bdc2832e3a95a45afa07417804b0c22fdd034.pdf"}, "_bibtex": {"value": "@inproceedings{\nrosa2023posterior,\ntitle={Posterior Contraction Rates for Mat\\'ern Gaussian Processes on Riemannian Manifolds},\nauthor={Paul Rosa and Viacheslav Borovitskiy and Alexander Terenin and Judith Rousseau},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Y18r0xWkSh}\n}"}, "paperhash": {"value": "rosa|posterior_contraction_rates_for_mat\u00e9rn_gaussian_processes_on_riemannian_manifolds"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8676/-/Revision", "NeurIPS.cc/2023/Conference/Submission8676/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8676/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325993436, "odate": 1698949749931, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "1xPsn2gCOe", "number": 8651, "cdate": 1683752089543, "tcdate": 1683752089543, "mdate": 1698949749833, "tmdate": 1698949749833, "signatures": ["NeurIPS.cc/2023/Conference/Submission8651/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8651/Authors"], "forum": "1xPsn2gCOe", "content": {"title": {"value": "Computing a human-like reaction time metric from stable recurrent vision models"}, "authors": {"value": ["Lore Goetschalckx", "Lakshmi Narasimhan Govindarajan", "Alekh Karkada Ashok", "Aarit Ahuja", "David Sheinberg", "Thomas Serre"]}, "authorids": {"value": ["~Lore_Goetschalckx1", "~Lakshmi_Narasimhan_Govindarajan3", "~Alekh_Karkada_Ashok1", "~Aarit_Ahuja1", "~David_Sheinberg1", "~Thomas_Serre1"]}, "keywords": {"value": ["alignment", "RNNs", "reaction times", "equilibrium dynamics", "perceptual grouping", "decision making"]}, "abstract": {"value": "The meteoric rise in the adoption of deep neural networks as computational models of vision has inspired efforts to ``align\u201d these models with humans. One dimension of interest for alignment includes behavioral choices, but moving beyond characterizing choice patterns to capturing temporal aspects of visual decision-making has been challenging. Here, we sketch a general-purpose methodology to construct computational accounts of reaction times from a stimulus-computable, task-optimized model. Specifically, we introduce a novel metric leveraging insights from subjective logic theory summarizing evidence accumulation in recurrent vision models. We demonstrate that our metric aligns with patterns of human reaction times for stimulus manipulations across four disparate visual decision-making tasks spanning perceptual grouping, mental simulation, and scene categorization. This work paves the way for exploring the temporal alignment of model and human visual strategies in the context of various other cognitive tasks toward generating testable hypotheses for neuroscience. Links to the code and data can be found on the project page: https://serre-lab.github.io/rnn_rts_site/."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3da8375405acf5c66efd426ea49330c2a008771c.pdf"}, "supplementary_material": {"value": "/attachment/3301fdd8ff2004e158a84d988dacacbdeb311f37.zip"}, "_bibtex": {"value": "@inproceedings{\ngoetschalckx2023computing,\ntitle={Computing a human-like reaction time metric from stable recurrent vision models},\nauthor={Lore Goetschalckx and Lakshmi Narasimhan Govindarajan and Alekh Karkada Ashok and Aarit Ahuja and David Sheinberg and Thomas Serre},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=1xPsn2gCOe}\n}"}, "paperhash": {"value": "goetschalckx|computing_a_humanlike_reaction_time_metric_from_stable_recurrent_vision_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8651/-/Revision", "NeurIPS.cc/2023/Conference/Submission8651/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission8651/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325992646, "odate": 1698949749818, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Zt9RzHjSEy", "number": 8608, "cdate": 1683751093774, "tcdate": 1683751093774, "mdate": 1698949749600, "tmdate": 1698949749600, "signatures": ["NeurIPS.cc/2023/Conference/Submission8608/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8608/Authors"], "forum": "Zt9RzHjSEy", "content": {"title": {"value": "Differentially Private Approximate Near Neighbor Counting in High Dimensions"}, "authors": {"value": ["Alexandr Andoni", "Piotr Indyk", "Sepideh Mahabadi", "Shyam Narayanan"]}, "authorids": {"value": ["~Alexandr_Andoni1", "~Piotr_Indyk1", "~Sepideh_Mahabadi1", "~Shyam_Narayanan1"]}, "keywords": {"value": ["Differential Privacy", "Near Neighbor Search", "Locality Sensitive Hashing", "Data Structures", "Range Query"]}, "TLDR": {"value": "We provide the first differentially private approximate near neighbor data structure with low error even for high-dimensional datasets."}, "abstract": {"value": "Range counting (e.g., counting the number of data points falling into a given query ball) under differential privacy has been studied extensively. However, the current algorithms for this problem are subject to the following dichotomy. One class of algorithms suffers from an additive error that is a fixed polynomial in the number of points. Another class of algorithms allows for polylogarithmic additive error, but the error grows exponentially in the dimension. To achieve the latter, the problem is relaxed to allow a \u201cfuzzy\u201d definition of the range boundary, e.g., a count of the points in a ball of radius $r$ might also include points in a ball of radius $cr$ for some $c>1$. In this paper we present an efficient algorithm that offers a sweet spot between these two classes. The algorithm has an additive error that is an arbitrary small power of the data set size, depending on how fuzzy the range boundary is, as well as a small ($1+o(1)$) multiplicative error. Crucially, the amount of noise added has no dependence on the dimension. Our algorithm introduces a variant of Locality-Sensitive Hashing, utilizing it in a novel manner."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1fff8f2bc2b29e03293eade28cf44bbddfe6e73a.pdf"}, "_bibtex": {"value": "@inproceedings{\nandoni2023differentially,\ntitle={Differentially Private Approximate Near Neighbor Counting in High Dimensions},\nauthor={Alexandr Andoni and Piotr Indyk and Sepideh Mahabadi and Shyam Narayanan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Zt9RzHjSEy}\n}"}, "paperhash": {"value": "andoni|differentially_private_approximate_near_neighbor_counting_in_high_dimensions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8608/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8608/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325991392, "odate": 1698949749584, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "0jZH883i34", "number": 8589, "cdate": 1683750701251, "tcdate": 1683750701251, "mdate": 1698949749495, "tmdate": 1698949749495, "signatures": ["NeurIPS.cc/2023/Conference/Submission8589/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8589/Authors"], "forum": "0jZH883i34", "content": {"title": {"value": "Model Sparsity Can Simplify Machine Unlearning"}, "authors": {"value": ["Jinghan Jia", "Jiancheng Liu", "Parikshit Ram", "Yuguang Yao", "Gaowen Liu", "Yang Liu", "Pranay Sharma", "Sijia Liu"]}, "authorids": {"value": ["~Jinghan_Jia1", "~Jiancheng_Liu2", "~Parikshit_Ram1", "~Yuguang_Yao1", "~Gaowen_Liu4", "~Yang_Liu3", "~Pranay_Sharma2", "~Sijia_Liu1"]}, "keywords": {"value": ["Machine unlearning", "model pruning"]}, "abstract": {"value": "In response to recent data regulation requirements, machine unlearning (MU) has emerged as a critical process to remove the influence of specific examples from a given model. Although exact unlearning can be achieved through complete model retraining using the remaining dataset, the associated computational costs have driven the development of efficient, approximate unlearning techniques. Moving beyond data-centric MU approaches, our study introduces a novel model-based perspective: model sparsification via weight pruning, which is capable of reducing the gap between exact unlearning and approximate unlearning. We show in both theory and practice that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. This leads to a new MU paradigm,    termed prune first, then unlearn, which infuses a sparse prior to the unlearning process. Building on this insight, we also develop a sparsity-aware unlearning method that utilizes sparsity regularization to enhance the training process of approximate unlearning. Extensive experiments show that our proposals consistently benefit MU in various unlearning scenarios. A notable highlight is the 77% unlearning efficacy gain of fine-tuning (one of the simplest approximate unlearning methods) when using our proposed sparsity-aware unlearning method. Furthermore, we showcase the practical impact of our proposed MU methods through two specific use cases: defending against backdoor attacks, and enhancing transfer learning through source class removal. These applications demonstrate the versatility and effectiveness of our approaches in addressing a variety of machine learning challenges beyond unlearning for data privacy. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/14bd61d41df4f5594bd66bdb4b172b294fa49d0d.pdf"}, "supplementary_material": {"value": "/attachment/3d18b32b8da080311c768e9cbbfc0628f75507a7.zip"}, "_bibtex": {"value": "@inproceedings{\njia2023model,\ntitle={Model Sparsity Can Simplify Machine Unlearning},\nauthor={Jinghan Jia and Jiancheng Liu and Parikshit Ram and Yuguang Yao and Gaowen Liu and Yang Liu and Pranay Sharma and Sijia Liu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=0jZH883i34}\n}"}, "paperhash": {"value": "jia|model_sparsity_can_simplify_machine_unlearning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8589/-/Revision", "NeurIPS.cc/2023/Conference/Submission8589/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8589/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325990729, "odate": 1698949749480, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ez6Cb0ZGzG", "number": 8531, "cdate": 1683749511823, "tcdate": 1683749511823, "mdate": 1698949749005, "tmdate": 1698949749005, "signatures": ["NeurIPS.cc/2023/Conference/Submission8531/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8531/Authors"], "forum": "ez6Cb0ZGzG", "content": {"title": {"value": "Continual Learning for Instruction Following from Realtime Feedback"}, "authors": {"value": ["Alane Suhr", "Yoav Artzi"]}, "authorids": {"value": ["~Alane_Suhr1", "~Yoav_Artzi1"]}, "keywords": {"value": ["continual learning", "interaction", "instruction following", "user feedback", "natural language processing", "language grounding", "situated interaction", "collaboration"]}, "TLDR": {"value": "We design and deploy a learning approach for improving instruction-following agents from feedback provided in real time during collaborative interactions with human users."}, "abstract": {"value": "We propose and deploy an approach to  continually train an instruction-following agent from feedback provided by users during collaborative interactions. During interaction, human users instruct an agent using natural language, and provide realtime binary feedback as they observe the agent following their instructions. We design a contextual bandit learning approach, converting  user feedback to immediate reward. We evaluate through thousands of human-agent interactions, demonstrating 15.4% absolute improvement in instruction execution accuracy over time. We also show our approach is robust to several design variations, and that the feedback signal is roughly equivalent to the learning signal of supervised demonstration data."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6b7cef3be236dfe1bcb89f9ff2040f822232e192.pdf"}, "supplementary_material": {"value": "/attachment/060c12b55eb821626756dcc23c205a0c73a42f8d.zip"}, "_bibtex": {"value": "@inproceedings{\nsuhr2023continual,\ntitle={Continual Learning for Instruction Following from Realtime Feedback},\nauthor={Alane Suhr and Yoav Artzi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ez6Cb0ZGzG}\n}"}, "paperhash": {"value": "suhr|continual_learning_for_instruction_following_from_realtime_feedback"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8531/-/Revision", "NeurIPS.cc/2023/Conference/Submission8531/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8531/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325988810, "odate": 1698949748992, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "rwbzMiuFQl", "forum": "rwbzMiuFQl", "number": 8441, "cdate": 1683747613390, "tcdate": 1683747613390, "mdate": 1698949748281, "tmdate": 1698949748281, "signatures": ["NeurIPS.cc/2023/Conference/Submission8441/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8441/Authors"], "content": {"title": {"value": "Break It Down:  Evidence for Structural Compositionality in Neural Networks"}, "authors": {"value": ["Michael A. Lepori", "Thomas Serre", "Ellie Pavlick"]}, "authorids": {"value": ["~Michael_A._Lepori1", "~Thomas_Serre1", "~Ellie_Pavlick1"]}, "keywords": {"value": ["Deep Learning", "Compositionality", "Cognitive Science"]}, "abstract": {"value": "Though modern neural networks have achieved impressive performance in both vision and language tasks, we know little about the functions that they implement. One possibility is that neural networks implicitly break down complex tasks into subroutines, implement modular solutions to these subroutines, and compose them into an overall solution to a task --- a property we term structural compositionality.  Another possibility is that they may simply learn to match new inputs to learned templates, eliding task decomposition entirely. Here, we leverage model pruning techniques to investigate this question in both vision and language across a variety of architectures, tasks, and pretraining regimens. Our results demonstrate that models oftentimes implement solutions to subroutines via modular subnetworks, which can be ablated while maintaining the functionality of other subnetworks. This suggests that neural networks may be able to learn compositionality, obviating the need for specialized symbolic mechanisms."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/72b9072094b550b9054aab68678f0f48bac96c62.pdf"}, "_bibtex": {"value": "@inproceedings{\nlepori2023break,\ntitle={Break It Down:  Evidence for Structural Compositionality in Neural Networks},\nauthor={Michael A. Lepori and Thomas Serre and Ellie Pavlick},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=rwbzMiuFQl}\n}"}, "paperhash": {"value": "lepori|break_it_down_evidence_for_structural_compositionality_in_neural_networks"}}, "pdate": 1695325986274, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8441/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8441/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "odate": 1698949748268, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "kvXcHfBghm", "number": 8378, "cdate": 1683746146869, "tcdate": 1683746146869, "mdate": 1698949747680, "tmdate": 1698949747680, "signatures": ["NeurIPS.cc/2023/Conference/Submission8378/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8378/Authors"], "forum": "kvXcHfBghm", "content": {"title": {"value": "Minimum-Risk Recalibration of Classifiers"}, "authors": {"value": ["Zeyu Sun", "Dogyoon Song", "Alfred Hero"]}, "authorids": {"value": ["~Zeyu_Sun1", "~Dogyoon_Song1", "~Alfred_Hero1"]}, "keywords": {"value": ["probability calibration", "optimal number of bins", "label shift adaptation"]}, "TLDR": {"value": "This paper proposes a framework for evaluating and analyzing the risks for histogram binning recalibration."}, "abstract": {"value": "Recalibrating probabilistic classifiers is vital for enhancing the reliability and accuracy of predictive models. Despite the development of numerous recalibration algorithms, there is still a lack of a comprehensive theory that integrates calibration and sharpness (which is essential for maintaining predictive power). In this paper, we introduce the concept of minimum-risk recalibration within the framework of mean-squared-error (MSE) decomposition, offering a principled approach for evaluating and recalibrating probabilistic classifiers. Using this framework, we analyze the uniform-mass binning (UMB) recalibration method and establish a finite-sample risk upper bound of order $\\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of bins and $n$ is the sample size. By balancing calibration and sharpness, we further determine that the optimal number of bins for UMB scales with $n^{1/3}$, resulting in a risk bound of approximately $O(n^{-2/3})$. Additionally, we tackle the challenge of label shift by proposing a two-stage approach that adjusts the recalibration function using limited labeled data from the target domain. Our results show that transferring a calibrated classifier requires significantly fewer target samples compared to recalibrating from scratch. We validate our theoretical findings through numerical simulations, which confirm the tightness of the proposed bounds, the optimal number of bins, and the effectiveness of label shift adaptation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/10b942664c6d30a0df37f3754b604cc462a31163.pdf"}, "supplementary_material": {"value": "/attachment/d9ca762fce274fad3dcf0fe9b7c215507b394fe0.pdf"}, "_bibtex": {"value": "@inproceedings{\nsun2023minimumrisk,\ntitle={Minimum-Risk Recalibration of Classifiers},\nauthor={Zeyu Sun and Dogyoon Song and Alfred Hero},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=kvXcHfBghm}\n}"}, "paperhash": {"value": "sun|minimumrisk_recalibration_of_classifiers"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8378/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8378/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325984068, "odate": 1698949747667, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "jSuhnO9QJv", "number": 8346, "cdate": 1683745396556, "tcdate": 1683745396556, "mdate": 1698949747497, "tmdate": 1698949747497, "signatures": ["NeurIPS.cc/2023/Conference/Submission8346/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8346/Authors"], "forum": "jSuhnO9QJv", "content": {"title": {"value": "Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases"}, "authors": {"value": ["Mazda Moayeri", "Wenxiao Wang", "Sahil Singla", "Soheil Feizi"]}, "authorids": {"value": ["~Mazda_Moayeri1", "~Wenxiao_Wang1", "~Sahil_Singla1", "~Soheil_Feizi2"]}, "keywords": {"value": ["spurious correlations", "interpretability", "bias", "distributional robustness"]}, "TLDR": {"value": "Sorting data using activations of an interpretable model enables efficient discovery, measurement, and mitigation of bias caused by spurious correlations."}, "abstract": {"value": "We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft segmentations for these features along the way. Having computed spuriosity rankings via the identified spurious neural features, we assess biases for $89$ diverse models and find that class-wise biases are highly correlated across models. Our results suggest that model bias due to spurious feature reliance is influenced far more by what the model is trained on than how it is trained."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/aaae35d60f83620d1a2ea6c4c849ea107242df0d.pdf"}, "supplementary_material": {"value": "/attachment/fa7eb8210b6ab8b3f0a465d182826b2cd43660ad.zip"}, "_bibtex": {"value": "@inproceedings{\nmoayeri2023spuriosity,\ntitle={Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases},\nauthor={Mazda Moayeri and Wenxiao Wang and Sahil Singla and Soheil Feizi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=jSuhnO9QJv}\n}"}, "paperhash": {"value": "moayeri|spuriosity_rankings_sorting_data_to_measure_and_mitigate_biases"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8346/-/Revision", "NeurIPS.cc/2023/Conference/Submission8346/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8346/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325983278, "odate": 1698949747484, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Rzk3GP1HN7", "number": 8345, "cdate": 1683745394884, "tcdate": 1683745394884, "mdate": 1698949747495, "tmdate": 1698949747495, "signatures": ["NeurIPS.cc/2023/Conference/Submission8345/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8345/Authors"], "forum": "Rzk3GP1HN7", "content": {"title": {"value": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks"}, "authors": {"value": ["Bill Yuchen Lin", "Yicheng Fu", "Karina Yang", "Faeze Brahman", "Shiyu Huang", "Chandra Bhagavatula", "Prithviraj Ammanabrolu", "Yejin Choi", "Xiang Ren"]}, "authorids": {"value": ["~Bill_Yuchen_Lin1", "~Yicheng_Fu1", "~Karina_Yang1", "~Faeze_Brahman1", "~Shiyu_Huang2", "~Chandra_Bhagavatula1", "~Prithviraj_Ammanabrolu1", "~Yejin_Choi1", "~Xiang_Ren1"]}, "keywords": {"value": ["interactive reasoning", "text game", "agents", "action planning", "large language models"]}, "TLDR": {"value": "SwiftSage: An Agent Framework for Complex Interactive Reasoning Tasks, integrating the strengths of behavior cloning of smaller LMs and prompting LLMs to improve task completion performance and efficiency."}, "abstract": {"value": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/57c0b05936c8c81266e8177b580736c46f02226a.pdf"}, "_bibtex": {"value": "@inproceedings{\nlin2023swiftsage,\ntitle={SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks},\nauthor={Bill Yuchen Lin and Yicheng Fu and Karina Yang and Faeze Brahman and Shiyu Huang and Chandra Bhagavatula and Prithviraj Ammanabrolu and Yejin Choi and Xiang Ren},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Rzk3GP1HN7}\n}"}, "paperhash": {"value": "lin|swiftsage_a_generative_agent_with_fast_and_slow_thinking_for_complex_interactive_tasks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8345/-/Revision", "NeurIPS.cc/2023/Conference/Submission8345/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8345/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325983190, "odate": 1698949747481, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "mQPNcBWjGc", "number": 8344, "cdate": 1683745391811, "tcdate": 1683745391811, "mdate": 1698949747464, "tmdate": 1698949747464, "signatures": ["NeurIPS.cc/2023/Conference/Submission8344/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8344/Authors"], "forum": "mQPNcBWjGc", "content": {"title": {"value": "Scaling Open-Vocabulary Object Detection"}, "authors": {"value": ["Matthias Minderer", "Alexey A. Gritsenko", "Neil Houlsby"]}, "authorids": {"value": ["~Matthias_Minderer1", "~Alexey_A._Gritsenko1", "~Neil_Houlsby1"]}, "keywords": {"value": ["object detection", "open-vocabulary object detection", "vision transformers", "vision-language models", "scaling", "self-training"]}, "TLDR": {"value": "We scale self-training for open-vocabulary object detection to billions of examples, leading to massive improvements in detection performance."}, "abstract": {"value": "Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is still limited by the amount of available detection training data. While detection training data can be expanded by using Web image-text pairs as weak supervision, this has not been done at scales comparable to image-level pretraining. Here, we scale up detection data with self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges in scaling self-training are the choice of label space, pseudo-annotation filtering, and training efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these challenges. OWLv2 surpasses the performance of previous state-of-the-art open-vocabulary detectors already at comparable training scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples, yielding further large improvement: With an L/14 architecture, OWL-ST improves AP on LVIS rare classes, for which the model has seen no human box annotations, from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale training for open-world localization, similar to what has been seen for image classification and language modelling. Code and checkpoints are available on GitHub."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b55dc71bcd74a8dc956c0f77acfe7fc9d6f38922.pdf"}, "supplementary_material": {"value": "/attachment/53bdfa634616b3ce18b1a6fead17bb0a4675c405.pdf"}, "_bibtex": {"value": "@inproceedings{\nminderer2023scaling,\ntitle={Scaling Open-Vocabulary Object Detection},\nauthor={Matthias Minderer and Alexey A. Gritsenko and Neil Houlsby},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=mQPNcBWjGc}\n}"}, "paperhash": {"value": "minderer|scaling_openvocabulary_object_detection"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8344/-/Revision", "NeurIPS.cc/2023/Conference/Submission8344/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8344/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325983172, "odate": 1698949747450, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "VEpU9rFaQr", "number": 8311, "cdate": 1683744836141, "tcdate": 1683744836141, "mdate": 1698949747172, "tmdate": 1698949747172, "signatures": ["NeurIPS.cc/2023/Conference/Submission8311/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8311/Authors"], "forum": "VEpU9rFaQr", "content": {"title": {"value": "Auditing for Human Expertise"}, "authors": {"value": ["Rohan Alur", "Loren Laine", "Darrick K Li", "Manish Raghavan", "Devavrat Shah", "Dennis Shung"]}, "authorids": {"value": ["~Rohan_Alur1", "loren.laine@yale.edu", "darrick.li@yale.edu", "~Manish_Raghavan1", "~Devavrat_Shah1", "~Dennis_Shung1"]}, "keywords": {"value": ["hypothesis testing", "human-AI complementarity", "machine learning for healthcare"]}, "TLDR": {"value": "We develop a statistical framework to assess whether human experts add value which could not be captured by any learning algorithm for a given prediction task."}, "abstract": {"value": "High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor.\nWe develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (\u2018features\u2019). A rejection of our test thus suggests that human experts may add value to any algorithm trained on the available data, and has direct implications for whether human-AI \u2018complementarity\u2019 is achievable in a given prediction task.\nWe highlight the utility of our procedure using admissions data collected from the emergency department of a large academic hospital system, where we show that physicians\u2019 admit/discharge decisions for patients with acute gastrointestinal bleeding (AGIB) appear to be incorporating information that is not available to a standard algorithmic screening tool. This is despite the fact that the screening tool is arguably more accurate than physicians\u2019 discretionary decisions, highlighting that \u2013 even absent normative concerns about accountability or interpretability \u2013 accuracy is insufficient to justify algorithmic automation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f60c1029fd1717aa5bc41b715dccbfe4cfd99b68.pdf"}, "supplementary_material": {"value": "/attachment/57a822a418126df41b47776cb88503756dabf774.zip"}, "_bibtex": {"value": "@inproceedings{\nalur2023auditing,\ntitle={Auditing for Human Expertise},\nauthor={Rohan Alur and Loren Laine and Darrick K Li and Manish Raghavan and Devavrat Shah and Dennis Shung},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=VEpU9rFaQr}\n}"}, "paperhash": {"value": "alur|auditing_for_human_expertise"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8311/-/Revision", "NeurIPS.cc/2023/Conference/Submission8311/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8311/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325982168, "odate": 1698949747157, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "aINqoP32cb", "number": 8293, "cdate": 1683744460395, "tcdate": 1683744460395, "mdate": 1698949747007, "tmdate": 1698949747007, "signatures": ["NeurIPS.cc/2023/Conference/Submission8293/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8293/Authors"], "forum": "aINqoP32cb", "content": {"title": {"value": "CS4ML: A general framework for active learning with arbitrary data based on Christoffel functions"}, "authors": {"value": ["Juan M Cardenas", "Ben Adcock", "Nick Dexter"]}, "authorids": {"value": ["~Juan_M_Cardenas1", "ben_adcock@sfu.ca", "nick.dexter@fsu.edu"]}, "keywords": {"value": ["active learning", "regression", "arbitrary data", "leverage scores", "Christoffel functions", "generative models", "Magnetic Resonance Imaging (MRI)", "Physics-Informed Neural Networks (PINNs)"]}, "TLDR": {"value": "We introduce a general framework for active learning in regression problems that allows for general types of data and optimal sampling via so-called generalized Christoffel functions."}, "abstract": {"value": "We introduce a general framework for active learning in regression problems. Our framework extends the standard setup by allowing for general types of data, rather than merely pointwise samples of the target function. This generalization covers many cases of practical interest, such as data acquired in transform domains (e.g., Fourier data), vector-valued data (e.g., gradient-augmented data), data acquired along continuous curves, and, multimodal data (i.e., combinations of different types of measurements).  Our framework considers random sampling according to a finite number of sampling measures and arbitrary nonlinear approximation spaces (model classes). We introduce the concept of \\textit{generalized Christoffel functions} and show how these can be used to optimize the sampling measures. We prove that this leads to near-optimal sample complexity in various important cases. This paper focuses on applications in scientific computing, where active learning is often desirable, since it is usually expensive to generate data. We demonstrate the efficacy of our framework for gradient-augmented learning with polynomials, Magnetic Resonance Imaging (MRI) using generative models and adaptive sampling for solving PDEs using Physics-Informed Neural Networks (PINNs)."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7fe58529a058b920b95386c5d8835f68fa42b225.pdf"}, "supplementary_material": {"value": "/attachment/d958cae3f28bf641f27b69856a034adb8a7f0817.zip"}, "_bibtex": {"value": "@inproceedings{\ncardenas2023csml,\ntitle={{CS}4{ML}: A general framework for active learning with arbitrary data based on Christoffel functions},\nauthor={Juan M Cardenas and Ben Adcock and Nick Dexter},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=aINqoP32cb}\n}"}, "paperhash": {"value": "cardenas|cs4ml_a_general_framework_for_active_learning_with_arbitrary_data_based_on_christoffel_functions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8293/-/Revision", "NeurIPS.cc/2023/Conference/Submission8293/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8293/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325981612, "odate": 1698949746991, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "YQA28p7qNz", "number": 8287, "cdate": 1683744353700, "tcdate": 1683744353700, "mdate": 1698949746910, "tmdate": 1698949746910, "signatures": ["NeurIPS.cc/2023/Conference/Submission8287/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8287/Authors"], "forum": "YQA28p7qNz", "content": {"title": {"value": "3D-LLM: Injecting the 3D World into Large Language Models"}, "authors": {"value": ["Yining Hong", "Haoyu Zhen", "Peihao Chen", "Shuhong Zheng", "Yilun Du", "Zhenfang Chen", "Chuang Gan"]}, "authorids": {"value": ["~Yining_Hong1", "~Haoyu_Zhen1", "~Peihao_Chen1", "~Shuhong_Zheng1", "~Yilun_Du1", "~Zhenfang_Chen1", "~Chuang_Gan1"]}, "keywords": {"value": ["3D Visual Reasoning", "3D Large Language Model"]}, "TLDR": {"value": "We introduce 3D-LLMs, new foudation models for 3D reasoning."}, "abstract": {"value": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proved to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D\ngrounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs could better capture 3D spatial information.  Experiments on ScanQA  show that our model outperforms state-of-the-art baselines by a large margin (\\textit{e.g.}, the BLEU-1 score surpasses state-of-the-art score by 9\\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Our model and data will be publicly available."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5a9a8b7a906c0a6fd3f23a371c0da50c288e44e3.pdf"}, "supplementary_material": {"value": "/attachment/2fec46c28d35ae7877f3154d064392200a6d62e1.pdf"}, "_bibtex": {"value": "@inproceedings{\nhong2023dllm,\ntitle={3D-{LLM}: Injecting the 3D World into Large Language Models},\nauthor={Yining Hong and Haoyu Zhen and Peihao Chen and Shuhong Zheng and Yilun Du and Zhenfang Chen and Chuang Gan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=YQA28p7qNz}\n}"}, "paperhash": {"value": "hong|3dllm_injecting_the_3d_world_into_large_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8287/-/Revision", "NeurIPS.cc/2023/Conference/Submission8287/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8287/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325981417, "odate": 1698949746895, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "hE5RWzQyvf", "number": 8239, "cdate": 1683743466957, "tcdate": 1683743466957, "mdate": 1698949746569, "tmdate": 1698949746569, "signatures": ["NeurIPS.cc/2023/Conference/Submission8239/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8239/Authors"], "forum": "hE5RWzQyvf", "content": {"title": {"value": "Distributionally Robust Linear Quadratic Control"}, "authors": {"value": ["Bahar Taskesen", "Dan Andrei Iancu", "\u00c7a\u011f\u0131l Ko\u00e7yi\u011fit", "Daniel Kuhn"]}, "authorids": {"value": ["~Bahar_Taskesen1", "daniancu@stanford.edu", "cagil.kocyigit@uni.lu", "~Daniel_Kuhn2"]}, "keywords": {"value": ["linear quadratic control", "distributionally robust optimization", "optimal transport", "Wasserstein distance"]}, "abstract": {"value": "Linear-Quadratic-Gaussian (LQG) control is a fundamental control paradigm that is studied in various fields such as engineering, computer science, economics, and neuroscience. It involves controlling a system with linear dynamics and imperfect observations, subject to additive noise, with the goal of minimizing a quadratic cost function for the state and control variables. In this work, we consider a generalization of the discrete-time, finite-horizon LQG problem, where the noise distributions are unknown and belong to Wasserstein ambiguity sets centered at nominal (Gaussian) distributions. The objective is to minimize a worst-case cost across all distributions in the ambiguity set, including non-Gaussian distributions. Despite the added complexity, we prove that a control policy that is linear in the observations is optimal for this problem, as in the classic LQG problem. We propose a numerical solution method that efficiently characterizes this optimal control policy. Our method uses the Frank-Wolfe algorithm to identify the least-favorable distributions within the Wasserstein ambiguity sets and computes the controller's optimal policy using Kalman filter estimation under these distributions."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/db8785199f7b4678a134bb7ebadf34bf6a5ebb3d.pdf"}, "supplementary_material": {"value": "/attachment/36948c94420b6a098b8ee7bfb4be18983155ce9a.pdf"}, "_bibtex": {"value": "@inproceedings{\ntaskesen2023distributionally,\ntitle={Distributionally Robust Linear Quadratic Control},\nauthor={Bahar Taskesen and Dan Andrei Iancu and {\\c{C}}a{\\u{g}}{\\i}l Ko{\\c{c}}yi{\\u{g}}it and Daniel Kuhn},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=hE5RWzQyvf}\n}"}, "paperhash": {"value": "taskesen|distributionally_robust_linear_quadratic_control"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8239/-/Revision", "NeurIPS.cc/2023/Conference/Submission8239/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8239/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325979976, "odate": 1698949746555, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "adq0oXb9KM", "number": 8227, "cdate": 1683743304308, "tcdate": 1683743304308, "mdate": 1698949746480, "tmdate": 1698949746480, "signatures": ["NeurIPS.cc/2023/Conference/Submission8227/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8227/Authors"], "forum": "adq0oXb9KM", "content": {"title": {"value": "Tree Variational Autoencoders"}, "authors": {"value": ["Laura Manduchi", "Moritz Vandenhirtz", "Alain Ryser", "Julia E Vogt"]}, "authorids": {"value": ["~Laura_Manduchi2", "~Moritz_Vandenhirtz1", "~Alain_Ryser1", "~Julia_E_Vogt1"]}, "keywords": {"value": ["hierarchical clustering", "hierarchical VAE", "representation learning", "VAE", "deep clustering"]}, "abstract": {"value": "We propose Tree Variational Autoencoder (TreeVAE), a new generative hierarchical clustering model\n  that learns a flexible tree-based posterior distribution over latent variables. TreeVAE hierarchically divides samples according to their intrinsic characteristics, shedding light on hidden structures in the data. It adapts its architecture to discover the optimal tree for encoding dependencies between latent variables. The proposed tree-based generative architecture enables lightweight conditional inference and improves generative performance by utilizing specialized leaf decoders. \n  We show that TreeVAE uncovers underlying clusters in the data and finds meaningful hierarchical relations between the different groups on a variety of datasets, including real-world imaging data. \n  We present empirically that TreeVAE provides a more competitive log-likelihood lower bound than the sequential counterparts. \n  Finally, due to its generative nature, TreeVAE is able to generate new samples from the discovered clusters via conditional sampling."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a new generative hierarchical clustering model that learns a flexible tree-based posterior distribution over latent variables."}, "pdf": {"value": "/pdf/b7bf54b44eb8005716af215103461fe6da7fc4bb.pdf"}, "_bibtex": {"value": "@inproceedings{\nmanduchi2023tree,\ntitle={Tree Variational Autoencoders},\nauthor={Laura Manduchi and Moritz Vandenhirtz and Alain Ryser and Julia E Vogt},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=adq0oXb9KM}\n}"}, "paperhash": {"value": "manduchi|tree_variational_autoencoders"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8227/-/Revision", "NeurIPS.cc/2023/Conference/Submission8227/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8227/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325979501, "odate": 1698949746466, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "EmxpDiPgRu", "number": 8181, "cdate": 1683742323451, "tcdate": 1683742323451, "mdate": 1698949746045, "tmdate": 1698949746045, "signatures": ["NeurIPS.cc/2023/Conference/Submission8181/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8181/Authors"], "forum": "EmxpDiPgRu", "content": {"title": {"value": "Honesty Is the Best Policy: Defining and Mitigating AI Deception"}, "authors": {"value": ["Francis Rhys Ward", "Francesca Toni", "Francesco Belardinelli", "Tom Everitt"]}, "authorids": {"value": ["~Francis_Rhys_Ward1", "~Francesca_Toni1", "~Francesco_Belardinelli1", "~Tom_Everitt1"]}, "keywords": {"value": ["Deception", "Causality", "Game Theory"]}, "TLDR": {"value": "We formally define deception in the causal game framework and mitigate deception in reinforcement learning agents and language models."}, "abstract": {"value": "Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals (for instance, in our experiments with language models, the goal of being evaluated as truthful).\nThere are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. \nWe introduce a formal\ndefinition of deception in structural causal games, grounded in the philosophy\nliterature, and applicable to real-world machine learning systems.\nSeveral examples and results illustrate that our formal definition aligns with the philosophical and commonsense meaning of deception.\nOur main technical result is to provide graphical criteria for deception. \nWe show, experimentally, that these results can be used to mitigate deception in reinforcement learning agents and language models."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/90891ff818b9603a70a29bbd2c903e1166fbaa03.pdf"}, "_bibtex": {"value": "@inproceedings{\nward2023honesty,\ntitle={Honesty Is the Best Policy: Defining and Mitigating {AI} Deception},\nauthor={Francis Rhys Ward and Francesca Toni and Francesco Belardinelli and Tom Everitt},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=EmxpDiPgRu}\n}"}, "paperhash": {"value": "ward|honesty_is_the_best_policy_defining_and_mitigating_ai_deception"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8181/-/Revision", "NeurIPS.cc/2023/Conference/Submission8181/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8181/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325978201, "odate": 1698949746029, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "EEVpt3dJQj", "number": 8177, "cdate": 1683742189668, "tcdate": 1683742189668, "mdate": 1698949746018, "tmdate": 1698949746018, "signatures": ["NeurIPS.cc/2023/Conference/Submission8177/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8177/Authors"], "forum": "EEVpt3dJQj", "content": {"title": {"value": "Auditing Fairness by Betting"}, "authors": {"value": ["Ben Chugg", "Santiago Cortes-Gomez", "Bryan Wilder", "Aaditya Ramdas"]}, "authorids": {"value": ["~Ben_Chugg1", "~Santiago_Cortes-Gomez1", "~Bryan_Wilder2", "~Aaditya_Ramdas2"]}, "keywords": {"value": ["fairness", "auditing", "sequential analysis", "martingales", "testing by betting"]}, "TLDR": {"value": "We provide efficient methods to sequentially audit the fairness of models which handle continuous monitoring of data, randomized data collection policies, and distribution shift."}, "abstract": {"value": "We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a  probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics---the ``testing by betting'' framework in particular. These connections ensure that our methods are interpretable, fast, and easy to implement. We demonstrate the efficacy of our approach on three benchmark fairness datasets."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/391003696065f19bd7548be7d7b40a8249166159.pdf"}, "supplementary_material": {"value": "/attachment/110581a2f9272e3d92ad9a27400b0855ba47de73.zip"}, "_bibtex": {"value": "@inproceedings{\nchugg2023auditing,\ntitle={Auditing Fairness by Betting},\nauthor={Ben Chugg and Santiago Cortes-Gomez and Bryan Wilder and Aaditya Ramdas},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=EEVpt3dJQj}\n}"}, "paperhash": {"value": "chugg|auditing_fairness_by_betting"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8177/-/Revision", "NeurIPS.cc/2023/Conference/Submission8177/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8177/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325978028, "odate": 1698949746002, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "q8SukwaEBy", "number": 8176, "cdate": 1683742141200, "tcdate": 1683742141200, "mdate": 1698949745968, "tmdate": 1698949745968, "signatures": ["NeurIPS.cc/2023/Conference/Submission8176/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8176/Authors"], "forum": "q8SukwaEBy", "content": {"title": {"value": "Learning from Active Human Involvement through Proxy Value Propagation"}, "authors": {"value": ["Zhenghao Peng", "Wenjie Mo", "Chenda Duan", "Quanyi Li", "Bolei Zhou"]}, "authorids": {"value": ["~Zhenghao_Peng1", "~Wenjie_Mo1", "~Chenda_Duan1", "~Quanyi_Li1", "~Bolei_Zhou5"]}, "keywords": {"value": ["Machine Learning", "Human-in-the-loop Reinforcement Learning", "Safety", "Sample Efficiency", "Reward-free"]}, "abstract": {"value": "Learning from active human involvement enables the human subject to actively intervene and demonstrate to the AI agent during training. The interaction and corrective feedback from human brings safety and AI alignment to the learning process. In this work, we propose a new reward-free active human involvement method called Proxy Value Propagation for policy optimization. Our key insight is that a proxy value function can be designed to express human intents, wherein state- action pairs in the human demonstration are labeled with high values, while those agents\u2019 actions that are intervened receive low values. Through the TD-learning framework, labeled values of demonstrated state-action pairs are further propagated to other unlabeled data generated from agents\u2019 exploration. The proxy value function thus induces a policy that faithfully emulates human behaviors. Human- in-the-loop experiments show the generality and efficiency of our method. With minimal modification to existing reinforcement learning algorithms, our method can learn to solve continuous and discrete control tasks with various human control devices, including the challenging task of driving in Grand Theft Auto V. Demo video and code are available at: https://metadriverse.github.io/pvp."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a human-in-the-loop policy learning method called Proxy Value Propagation that aligns human preferences through active human involvement and achieves highly efficiency and safety, without offline pretraining and reward engineering."}, "pdf": {"value": "/pdf/c36c597dc82fe9143dc09c79f4f28914f0ad00b2.pdf"}, "_bibtex": {"value": "@inproceedings{\npeng2023learning,\ntitle={Learning from Active Human Involvement through Proxy Value Propagation},\nauthor={Zhenghao Peng and Wenjie Mo and Chenda Duan and Quanyi Li and Bolei Zhou},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=q8SukwaEBy}\n}"}, "paperhash": {"value": "peng|learning_from_active_human_involvement_through_proxy_value_propagation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8176/-/Revision", "NeurIPS.cc/2023/Conference/Submission8176/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8176/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325977953, "odate": 1698949745954, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "p40XRfBX96", "number": 8161, "cdate": 1683741899778, "tcdate": 1683741899778, "mdate": 1698949745741, "tmdate": 1698949745741, "signatures": ["NeurIPS.cc/2023/Conference/Submission8161/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8161/Authors"], "forum": "p40XRfBX96", "content": {"title": {"value": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"}, "authors": {"value": ["Zhiqing Sun", "Yikang Shen", "Qinhong Zhou", "Hongxin Zhang", "Zhenfang Chen", "David Daniel Cox", "Yiming Yang", "Chuang Gan"]}, "authorids": {"value": ["~Zhiqing_Sun1", "~Yikang_Shen1", "~Qinhong_Zhou1", "~Hongxin_Zhang1", "~Zhenfang_Chen1", "~David_Daniel_Cox1", "~Yiming_Yang1", "~Chuang_Gan1"]}, "keywords": {"value": ["AI Alignment", "Large Language Models", "In Context Learning", "Neural Symbolics"]}, "abstract": {"value": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1442772fe6da271e24e7b731a570952b813b5f64.pdf"}, "_bibtex": {"value": "@inproceedings{\nsun2023principledriven,\ntitle={Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision},\nauthor={Zhiqing Sun and Yikang Shen and Qinhong Zhou and Hongxin Zhang and Zhenfang Chen and David Daniel Cox and Yiming Yang and Chuang Gan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=p40XRfBX96}\n}"}, "TLDR": {"value": "SELF-ALIGN minimizes human supervision for training AI-assistants by using synthetic prompts and human principles to guide learning, improving response quality."}, "paperhash": {"value": "sun|principledriven_selfalignment_of_language_models_from_scratch_with_minimal_human_supervision"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8161/-/Revision", "NeurIPS.cc/2023/Conference/Submission8161/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission8161/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325977461, "odate": 1698949745726, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "gq4xkwQZ1l", "number": 8153, "cdate": 1683741694233, "tcdate": 1683741694233, "mdate": 1698949745661, "tmdate": 1698949745661, "signatures": ["NeurIPS.cc/2023/Conference/Submission8153/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8153/Authors"], "forum": "gq4xkwQZ1l", "content": {"title": {"value": "Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision"}, "authors": {"value": ["Ayush Tewari", "Tianwei Yin", "George Cazenavette", "Semon Rezchikov", "Joshua B. Tenenbaum", "Fredo Durand", "William T. Freeman", "Vincent Sitzmann"]}, "authorids": {"value": ["~Ayush_Tewari2", "~Tianwei_Yin1", "~George_Cazenavette1", "~Semon_Rezchikov1", "~Joshua_B._Tenenbaum1", "~Fredo_Durand1", "~William_T._Freeman1", "~Vincent_Sitzmann1"]}, "keywords": {"value": ["3D generative models", "neural rendering", "neural scene representations", "NeRF", "diffusion models", "differentiable rendering", "inverse graphics", "inverse problems"]}, "abstract": {"value": "Denoising diffusion models are a powerful type of generative models used to capture complex distributions of real-world signals. However, their applicability is limited to scenarios where training samples are readily available, which is not always the case in real-world applications. For example, in inverse graphics, the goal is to generate samples from a distribution of 3D scenes that align with a given image, but ground-truth 3D scenes are unavailable and only 2D images are accessible. To address this limitation, we propose a novel class of denoising diffusion probabilistic models that learn to sample from distributions of signals that are never directly observed. Instead, these signals are measured indirectly through a known differentiable forward model, which produces partial observations of the unknown signal. Our approach involves integrating the forward model directly into the denoising process. A key contribution of our work is the integration of a differentiable forward model into the denoising process. This integration effectively connects the generative modeling of observations with the generative modeling of the underlying signals, allowing for end-to-end training of a conditional generative model over signals. During inference, our approach enables sampling from the distribution of underlying signals that are consistent with a given partial observation. We demonstrate the effectiveness of our method on three challenging computer vision tasks. For instance, in the context of inverse graphics, our model enables direct sampling from the distribution of 3D scenes that align with a single 2D input image."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a novel class of diffusion models that learn to sample from distributions of signals that are never directly observed, e.g., our model learns to sample 3D scenes by only training on 2D image observations."}, "pdf": {"value": "/pdf/0bfb6f6494a3a223cfe7add56f7765d53ce5971e.pdf"}, "supplementary_material": {"value": "/attachment/bf55b6a53a6a50e538b7a9670be112ad05161a83.pdf"}, "_bibtex": {"value": "@inproceedings{\ntewari2023diffusion,\ntitle={Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision},\nauthor={Ayush Tewari and Tianwei Yin and George Cazenavette and Semon Rezchikov and Joshua B. Tenenbaum and Fredo Durand and William T. Freeman and Vincent Sitzmann},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=gq4xkwQZ1l}\n}"}, "paperhash": {"value": "tewari|diffusion_with_forward_models_solving_stochastic_inverse_problems_without_direct_supervision"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8153/-/Revision", "NeurIPS.cc/2023/Conference/Submission8153/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8153/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325977258, "odate": 1698949745648, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "JV8Ff0lgVV", "number": 8149, "cdate": 1683741658586, "tcdate": 1683741658586, "mdate": 1698949745593, "tmdate": 1698949745593, "signatures": ["NeurIPS.cc/2023/Conference/Submission8149/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8149/Authors"], "forum": "JV8Ff0lgVV", "content": {"title": {"value": "DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization"}, "authors": {"value": ["Zhiqing Sun", "Yiming Yang"]}, "authorids": {"value": ["~Zhiqing_Sun1", "~Yiming_Yang1"]}, "keywords": {"value": ["neural-symbolic reasoning", "combinatorial optimization", "diffusion models"]}, "abstract": {"value": "Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. It formulates NPC problems into a discrete {0, 1}-vector space and uses graph-based denoising diffusion models to generate high-quality solutions. Specifically, we explore diffusion models with Gaussian and Bernoulli noise, respectively, and also introduce an effective inference schedule to improve the generation quality. We evaluate our methods on two well-studied combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark. Our code is available at [this url](https://github.com/Edward-Sun/DIFUSCO)."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/db0786fdefc993498954ab1a489d781503b4d8d1.pdf"}, "_bibtex": {"value": "@inproceedings{\nsun2023difusco,\ntitle={{DIFUSCO}: Graph-based Diffusion Solvers for Combinatorial Optimization},\nauthor={Zhiqing Sun and Yiming Yang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=JV8Ff0lgVV}\n}"}, "TLDR": {"value": "We developed the first diffusion model-based solver on graphs for combinatorial optimization problems."}, "paperhash": {"value": "sun|difusco_graphbased_diffusion_solvers_for_combinatorial_optimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8149/-/Revision", "NeurIPS.cc/2023/Conference/Submission8149/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8149/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325977151, "odate": 1698949745579, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "1p6teT6F73", "number": 8101, "cdate": 1683740859607, "tcdate": 1683740859607, "mdate": 1698949745210, "tmdate": 1698949745210, "signatures": ["NeurIPS.cc/2023/Conference/Submission8101/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8101/Authors"], "forum": "1p6teT6F73", "content": {"title": {"value": "Alternating Updates for Efficient Transformers"}, "authors": {"value": ["Cenk Baykal", "Dylan J Cutler", "Nishanth Dikkala", "Nikhil Ghosh", "Rina Panigrahy", "Xin Wang"]}, "authorids": {"value": ["~Cenk_Baykal1", "~Dylan_J_Cutler1", "~Nishanth_Dikkala1", "~Nikhil_Ghosh1", "~Rina_Panigrahy1", "~Xin_Wang30"]}, "keywords": {"value": ["efficiency", "efficient transformers"]}, "TLDR": {"value": "We introduce a method to increase the capacity of modern transformers without significantly increasing the computational burden."}, "abstract": {"value": "It has been well established that increasing scale in deep transformer networks leads to improved quality and performance. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. We introduce Alternating Updates (AltUp), a simple-to-implement method to increase a model's capacity without the computational burden. AltUp enables the widening of the learned representation, i.e., the token embedding, while only incurring a negligible increase in latency. AltUp achieves this by working on a subblock of the widened representation at each layer and using a predict-and-correct mechanism to update the inactivated blocks. We present extensions of AltUp, such as its applicability to the sequence dimension, and demonstrate how AltUp can be synergistically combined with existing approaches, such as Sparse Mixture-of-Experts models, to obtain efficient models with even \nhigher capacity. Our experiments on benchmark transformer models and language tasks demonstrate the consistent effectiveness of AltUp on a diverse set of scenarios. Notably, on SuperGLUE and SQuAD benchmarks, AltUp enables up to $87\\%$ speedup relative to the dense baselines at the same accuracy."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/405c0659e0560c49557bff655394eebd66a15ab6.pdf"}, "supplementary_material": {"value": "/attachment/482743877b8911b0a43a988be5dd5626d7cbad26.pdf"}, "_bibtex": {"value": "@inproceedings{\nbaykal2023alternating,\ntitle={Alternating Updates for Efficient Transformers},\nauthor={Cenk Baykal and Dylan J Cutler and Nishanth Dikkala and Nikhil Ghosh and Rina Panigrahy and Xin Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=1p6teT6F73}\n}"}, "paperhash": {"value": "baykal|alternating_updates_for_efficient_transformers"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8101/-/Revision", "NeurIPS.cc/2023/Conference/Submission8101/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8101/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325975389, "odate": 1698949745195, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "bOQNd7tWAp", "number": 8056, "cdate": 1683739967476, "tcdate": 1683739967476, "mdate": 1698949744907, "tmdate": 1698949744907, "signatures": ["NeurIPS.cc/2023/Conference/Submission8056/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8056/Authors"], "forum": "bOQNd7tWAp", "content": {"title": {"value": "Online Control for Meta-optimization"}, "authors": {"value": ["Xinyi Chen", "Elad Hazan"]}, "authorids": {"value": ["~Xinyi_Chen1", "~Elad_Hazan1"]}, "keywords": {"value": ["online learning", "control", "hyperparameter optimization"]}, "abstract": {"value": "Choosing the optimal hyperparameters, including learning rate and momentum, for specific optimization instances is a significant yet non-convex challenge. This makes conventional iterative techniques such as hypergradient descent \\cite{baydin2017online} insufficient in obtaining global optimality guarantees.\n\nWe consider the more general task of meta-optimization -- online learning of the best optimization algorithm given problem instances, and introduce a novel approach based on control theory. We show how meta-optimization can be formulated as an optimal control problem, departing from existing literature that use stability-based methods to study optimization. Our approach leverages convex relaxation techniques in the recently-proposed nonstochastic control framework to overcome the challenge of nonconvexity, and obtains regret guarantees vs. the best offline solution. This guarantees that in meta-optimization, we can learn a method that attains convergence comparable to that of the best optimization method in hindsight from a class of methods."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c920e5ca150bcbec93696f78cd629a6e0cafb61f.pdf"}, "supplementary_material": {"value": "/attachment/f87b26706b7da5520b247383a9fa3e161a8687e8.pdf"}, "_bibtex": {"value": "@inproceedings{\nchen2023online,\ntitle={Online Control for Meta-optimization},\nauthor={Xinyi Chen and Elad Hazan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=bOQNd7tWAp}\n}"}, "paperhash": {"value": "chen|online_control_for_metaoptimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8056/-/Revision", "NeurIPS.cc/2023/Conference/Submission8056/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8056/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325973969, "odate": 1698949744893, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "voG6nEW9BV", "number": 8043, "cdate": 1683739688351, "tcdate": 1683739688351, "mdate": 1698949744791, "tmdate": 1698949744791, "signatures": ["NeurIPS.cc/2023/Conference/Submission8043/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8043/Authors"], "forum": "voG6nEW9BV", "content": {"title": {"value": "Conditional score-based diffusion models for Bayesian inference in infinite dimensions"}, "authors": {"value": ["Lorenzo Baldassari", "Ali Siahkoohi", "Josselin Garnier", "Knut Solna", "Maarten V. de Hoop"]}, "authorids": {"value": ["~Lorenzo_Baldassari1", "~Ali_Siahkoohi1", "~Josselin_Garnier1", "~Knut_Solna1", "~Maarten_V._de_Hoop2"]}, "keywords": {"value": ["score-based generative models", "diffusion models", "inverse problems", "bayesian inference", "infinite dimensions"]}, "TLDR": {"value": "We propose a method to learn the posterior distribution in infinite-dimensional Bayesian linear inverse problems using amortized conditional score-based generative models."}, "abstract": {"value": "Since their initial introduction, score-based diffusion models (SDMs) have been successfully applied to solve a variety of linear inverse problems in finite-dimensional vector spaces due to their ability to efficiently approximate the posterior distribution. However, using SDMs for inverse problems in infinite-dimensional function spaces has only been addressed recently, primarily through methods that learn the unconditional score. While this approach is advantageous for some inverse problems, it is mostly heuristic and involves numerous computationally costly forward operator evaluations during posterior sampling. To address these limitations, we propose a theoretically grounded method for sampling from the posterior of infinite-dimensional Bayesian linear inverse problems based on amortized conditional SDMs. In particular, we prove that one of the most successful approaches for estimating the conditional score in finite dimensions\u2014the conditional denoising estimator\u2014can also be applied in infinite dimensions. A significant part of our analysis is dedicated to demonstrating that extending infinite-dimensional SDMs to the conditional setting requires careful consideration, as the conditional score typically blows up for small times, contrarily to the unconditional score. We conclude by presenting stylized and large-scale numerical examples that validate our approach, offer additional insights, and demonstrate that our method enables large-scale, discretization-invariant Bayesian inference."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/34d587170ea66bdfe775207e928ff494ce5cc814.pdf"}, "_bibtex": {"value": "@inproceedings{\nbaldassari2023conditional,\ntitle={Conditional score-based diffusion models for Bayesian inference in infinite dimensions},\nauthor={Lorenzo Baldassari and Ali Siahkoohi and Josselin Garnier and Knut Solna and Maarten V. de Hoop},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=voG6nEW9BV}\n}"}, "paperhash": {"value": "baldassari|conditional_scorebased_diffusion_models_for_bayesian_inference_in_infinite_dimensions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8043/-/Revision", "NeurIPS.cc/2023/Conference/Submission8043/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8043/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325973578, "odate": 1698949744771, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "SthlUe5xDP", "number": 8041, "cdate": 1683739653640, "tcdate": 1683739653640, "mdate": 1698949744781, "tmdate": 1698949744781, "signatures": ["NeurIPS.cc/2023/Conference/Submission8041/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8041/Authors"], "forum": "SthlUe5xDP", "content": {"title": {"value": "Topological Parallax: A Geometric Specification for Deep Perception Models"}, "authors": {"value": ["Abraham David Smith", "Michael J. Catanzaro", "Gabrielle Angeloro", "Nirav Patel", "Paul Bendich"]}, "authorids": {"value": ["~Abraham_David_Smith1", "michael.catanzaro@geomdata.com", "gabrielle.angeloro@geomdata.com", "nirav.patel@geomdata.com", "~Paul_Bendich1"]}, "keywords": {"value": ["topological data analysis", "persistent homology", "convexity", "AI safety", "interpolation"]}, "TLDR": {"value": "Topological Parallax compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure, which we argue is a desirable property.."}, "abstract": {"value": "For safety and robustness of AI systems, we introduce _topological parallax_ as a \ntheoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. \n\nOur proofs and examples show that this geometric similarity between dataset and model is essential \nto trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between \"overfitting\"' and \"generalization'' in applications of deep-learning. \n\nIn typical deep-learning applications, an explicit geometric description of the model is\nimpossible, but parallax can estimate topological features (components, cycles, voids, etc.)\nin the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset.\nThus, parallax indicates whether the model shares similar multiscale geometric features with the dataset.\n\nParallax presents theoretically via topological data analysis [TDA] as a bi-filtered persistence module,\nand the key properties of this module are stable under perturbation of the reference dataset."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b262ab9403cd94254c82ed997a4c169b827d313f.pdf"}, "supplementary_material": {"value": "/attachment/e2e46ab73b0116e50b9c0f329b3aa20f6c0580df.zip"}, "_bibtex": {"value": "@inproceedings{\nsmith2023topological,\ntitle={Topological Parallax: A Geometric Specification for Deep Perception Models},\nauthor={Abraham David Smith and Michael J. Catanzaro and Gabrielle Angeloro and Nirav Patel and Paul Bendich},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=SthlUe5xDP}\n}"}, "paperhash": {"value": "smith|topological_parallax_a_geometric_specification_for_deep_perception_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8041/-/Revision", "NeurIPS.cc/2023/Conference/Submission8041/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8041/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325973519, "odate": 1698949744765, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "94rKFkcm56", "number": 8038, "cdate": 1683739547456, "tcdate": 1683739547456, "mdate": 1698949744695, "tmdate": 1698949744695, "signatures": ["NeurIPS.cc/2023/Conference/Submission8038/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8038/Authors"], "forum": "94rKFkcm56", "content": {"title": {"value": "Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power"}, "authors": {"value": ["Junru Zhou", "Jiarui Feng", "Xiyuan Wang", "Muhan Zhang"]}, "authorids": {"value": ["~Junru_Zhou1", "~Jiarui_Feng1", "~Xiyuan_Wang1", "~Muhan_Zhang1"]}, "keywords": {"value": ["Cycle counting", "graph neural networks"]}, "abstract": {"value": "The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs---$d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs, based on the well-known FWL(2) algorithm. As a heuristic method for graph isomorphism testing, FWL(2) colors all node pairs in a graph and performs message passing among those node pairs. In order to balance the expressive power and complexity, $d$-DRFWL(2) GNNs simplify FWL(2) by restricting the range of message passing to node pairs whose mutual distances are at most $d$. This way, $d$-DRFWL(2) GNNs exploit graph sparsity while avoiding the expensive subgraph extraction operations in subgraph GNNs, making both the time and space complexity lower. We theoretically investigate both the discriminative power and the cycle counting power of $d$-DRFWL(2) GNNs. Our most important finding is that $d$-DRFWL(2) GNNs have provably strong cycle counting power even with $d=2$: they can count all 3, 4, 5, 6-cycles. Since 6-cycles (e.g., benzene rings) are ubiquitous in organic molecules, being able to detect and count them is crucial for achieving robust and generalizable performance on molecular tasks. Experiments on both synthetic datasets and molecular datasets verify our theory. To the best of our knowledge, 2-DRFWL(2) GNN is the most efficient GNN model to date (both theoretically and empirically) that can count up to 6-cycles."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/93e9d040b85d6cdbfd0ca3be9efabfdb13050a43.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhou2023distancerestricted,\ntitle={Distance-Restricted Folklore Weisfeiler-Leman {GNN}s with Provable Cycle Counting Power},\nauthor={Junru Zhou and Jiarui Feng and Xiyuan Wang and Muhan Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=94rKFkcm56}\n}"}, "paperhash": {"value": "zhou|distancerestricted_folklore_weisfeilerleman_gnns_with_provable_cycle_counting_power"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8038/-/Revision", "NeurIPS.cc/2023/Conference/Submission8038/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8038/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325973476, "odate": 1698949744683, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "40L3viVWQN", "number": 8009, "cdate": 1683738909277, "tcdate": 1683738909277, "mdate": 1698949744604, "tmdate": 1698949744604, "signatures": ["NeurIPS.cc/2023/Conference/Submission8009/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8009/Authors"], "forum": "40L3viVWQN", "content": {"title": {"value": "The Pick-to-Learn Algorithm: Empowering Compression for Tight Generalization Bounds and Improved Post-training Performance"}, "authors": {"value": ["Dario Paccagnan", "Marco Campi", "Simone Garatti"]}, "authorids": {"value": ["~Dario_Paccagnan1", "~Marco_Campi1", "~Simone_Garatti1"]}, "keywords": {"value": ["Statistical learning theory", "Compression theory", "Generalization bounds"]}, "abstract": {"value": "Generalization bounds are valuable both for theory and applications. On the one hand, they shed light on the mechanisms that underpin the learning processes; on the other, they certify how well a learned model performs against unseen inputs.  In this work we build upon a recent breakthrough in compression theory to develop a new framework yielding tight generalization bounds of wide practical applicability.  The core idea is to embed any given learning algorithm into a suitably-constructed meta-algorithm (here called Pick-to-Learn, P2L) in order to instill desirable compression properties. When applied to the MNIST classification dataset and to a synthetic regression problem, P2L not only attains generalization bounds that compare favorably with the state of the art (test-set and PAC-Bayes bounds), but it also learns models with better post-training performance."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/45909699aecf02e9b0c9e33f4f2ef2a37ebcf869.pdf"}, "supplementary_material": {"value": "/attachment/5c9f4f6e2183d4780cef1e5fc5a9a997c8c1399f.zip"}, "_bibtex": {"value": "@inproceedings{\npaccagnan2023the,\ntitle={The Pick-to-Learn Algorithm: Empowering Compression for Tight Generalization Bounds and Improved Post-training Performance},\nauthor={Dario Paccagnan and Marco Campi and Simone Garatti},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=40L3viVWQN}\n}"}, "paperhash": {"value": "paccagnan|the_picktolearn_algorithm_empowering_compression_for_tight_generalization_bounds_and_improved_posttraining_performance"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8009/-/Revision", "NeurIPS.cc/2023/Conference/Submission8009/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8009/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325972630, "odate": 1698949744585, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5B1ZK60jWn", "number": 8005, "cdate": 1683738854381, "tcdate": 1683738854381, "mdate": 1698949744529, "tmdate": 1698949744529, "signatures": ["NeurIPS.cc/2023/Conference/Submission8005/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8005/Authors"], "forum": "5B1ZK60jWn", "content": {"title": {"value": "A Spectral Theory of Neural Prediction and Alignment"}, "authors": {"value": ["Abdulkadir Canatar", "Jenelle Feather", "Albert Wakhloo", "SueYeon Chung"]}, "authorids": {"value": ["~Abdulkadir_Canatar1", "~Jenelle_Feather1", "awakhloo@flatironinstitute.org", "~SueYeon_Chung1"]}, "keywords": {"value": ["computational neuroscience", "neural manifolds", "neuro-AI", "statistical physics", "representational geometry"]}, "abstract": {"value": "The representations of neural networks are often compared to those of biological systems by performing regression between the neural network responses and those measured from biological systems. Many different state-of-the-art deep neural networks yield similar neural predictions, but it remains unclear how to differentiate among models that perform equally well at predicting neural responses. To gain insight into this, we use a recent theoretical framework that relates the generalization error from regression to the spectral properties of the model and the target. We apply this theory to the case of regression between model activations and neural responses and decompose the neural prediction error in terms of the model eigenspectra, alignment of model eigenvectors and neural responses, and the training set size. Using this decomposition, we introduce geometrical measures to interpret the neural prediction error. We test a large number of deep neural networks that predict visual cortical activity and show that there are multiple types of geometries that result in low neural prediction error as measured via regression. The work demonstrates that carefully decomposing representational metrics can provide interpretability of how models are capturing neural activity and points the way towards improved models of neural activity."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6327437e33f95808d277892cc918d6bada4fe7ba.pdf"}, "_bibtex": {"value": "@inproceedings{\ncanatar2023a,\ntitle={A Spectral Theory of Neural Prediction and Alignment},\nauthor={Abdulkadir Canatar and Jenelle Feather and Albert Wakhloo and SueYeon Chung},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5B1ZK60jWn}\n}"}, "TLDR": {"value": "We offer a theoretical analysis investigating the factors contributing to the neural prediction error in terms of the spectral properties of deep neural network models and brain responses."}, "paperhash": {"value": "canatar|a_spectral_theory_of_neural_prediction_and_alignment"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8005/-/Revision", "NeurIPS.cc/2023/Conference/Submission8005/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8005/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325972479, "odate": 1698949744513, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "fHsBNNDroC", "number": 8000, "cdate": 1683738664319, "tcdate": 1683738664319, "mdate": 1698949744500, "tmdate": 1698949744500, "signatures": ["NeurIPS.cc/2023/Conference/Submission8000/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission8000/Authors"], "forum": "fHsBNNDroC", "content": {"title": {"value": "Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents"}, "authors": {"value": ["Nika Haghtalab", "Chara Podimata", "Kunhe Yang"]}, "authorids": {"value": ["~Nika_Haghtalab2", "~Chara_Podimata1", "~Kunhe_Yang1"]}, "keywords": {"value": ["calibration", "Stackelberg games", "learning in repeated games", "strategic agents", "best response", "strategic classification", "Stackelberg Security Games"]}, "abstract": {"value": "In this paper, we introduce a generalization of the standard Stackelberg Games (SGs) framework: _Calibrated Stackelberg Games_. In CSGs, a principal repeatedly interacts with an agent who (contrary to standard SGs) does not have direct access to the principal's action but instead best responds to _calibrated forecasts_ about it. CSG is a powerful modeling tool that goes beyond assuming that agents use ad hoc and highly specified algorithms for interacting in strategic settings to infer the principal's actions  and thus more robustly addresses real-life applications that SGs were originally intended to capture. Along with CSGs, we also introduce a stronger notion of calibration, termed _adaptive calibration_, that provides fine-grained any-time calibration guarantees against adversarial sequences. We give a general approach for obtaining adaptive calibration algorithms and specialize them for finite CSGs. In our main technical result, we show that in CSGs, the principal can achieve utility that converges to the optimum Stackelberg value of the game both in _finite_ and _continuous_ settings and that no higher utility is achievable. Two prominent and immediate applications of our results are the settings of learning in Stackelberg Security Games and strategic classification, both against _calibrated_ agents."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We introduce and study learning in a new class of games called \"Calibrated Stackelberg Games\" (CSGs) where the agents can only form calibrated forecasts about the principal's strategies (as opposed to having full knowledge of them)."}, "pdf": {"value": "/pdf/e5021f6ed919727e311c7a67ebad2dd531f7cdcf.pdf"}, "supplementary_material": {"value": "/attachment/1f83560d20cc37050c2e12804d5a5316184ca0d3.pdf"}, "_bibtex": {"value": "@inproceedings{\nhaghtalab2023calibrated,\ntitle={Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents},\nauthor={Nika Haghtalab and Chara Podimata and Kunhe Yang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=fHsBNNDroC}\n}"}, "paperhash": {"value": "haghtalab|calibrated_stackelberg_games_learning_optimal_commitments_against_calibrated_agents"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission8000/-/Revision", "NeurIPS.cc/2023/Conference/Submission8000/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission8000/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325972325, "odate": 1698949744484, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "haHIji0yFt", "number": 7996, "cdate": 1683738561764, "tcdate": 1683738561764, "mdate": 1698949744473, "tmdate": 1698949744473, "signatures": ["NeurIPS.cc/2023/Conference/Submission7996/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7996/Authors"], "forum": "haHIji0yFt", "content": {"title": {"value": "$SE(3)$  Equivariant Convolution and Transformer in Ray Space"}, "authors": {"value": ["Yinshuang Xu", "Jiahui Lei", "Kostas Daniilidis"]}, "authorids": {"value": ["~Yinshuang_Xu1", "~Jiahui_Lei1", "~Kostas_Daniilidis1"]}, "keywords": {"value": ["equivariance", "light field", "equivariant convolution over homogeneous space"]}, "abstract": {"value": "3D reconstruction and novel view rendering can greatly benefit from geometric priors when the input views are not sufficient in terms of coverage and inter-view baselines. Deep learning of geometric priors from 2D images requires each image to be represented in a $2D$ canonical frame and the prior to be learned in a given or learned $3D$ canonical frame. In this paper, given only the relative poses of the cameras, we show how to learn priors from multiple views equivariant to coordinate frame transformations by proposing an $SE(3)$-equivariant convolution and transformer in the space of rays in 3D. We model the ray space as a homogeneous space of $SE(3)$ and introduce the $SE(3)$-equivariant convolution in ray space. Depending on the output domain of the convolution, we present convolution-based $SE(3)$-equivariant maps from ray space to ray space and to $\\mathbb{R}^3$. Our mathematical framework allows us to go beyond convolution to $SE(3)$-equivariant attention in the ray space. We showcase how to tailor and adapt the equivariant convolution and transformer in the tasks of equivariant $3D$ reconstruction and equivariant neural rendering from multiple views. We demonstrate $SE(3)$-equivariance by obtaining robust results in roto-translated datasets without performing transformation augmentation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f129349b3786d3b65a078ed7cff225c181ff471b.pdf"}, "supplementary_material": {"value": "/attachment/963121f67a5a20a19e3dc991ad571c9e8ca82897.pdf"}, "_bibtex": {"value": "@inproceedings{\nxu2023se,\ntitle={\\${SE}(3)\\$  Equivariant Convolution and Transformer in Ray Space},\nauthor={Yinshuang Xu and Jiahui Lei and Kostas Daniilidis},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=haHIji0yFt}\n}"}, "TLDR": {"value": "We propose the $SE(3)$ equivariant convolution and transformer in $3D$ ray space."}, "paperhash": {"value": "xu|se3_equivariant_convolution_and_transformer_in_ray_space"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7996/-/Revision", "NeurIPS.cc/2023/Conference/Submission7996/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7996/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325972280, "odate": 1698949744456, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "wiidCRA3at", "number": 7925, "cdate": 1683737299162, "tcdate": 1683737299162, "mdate": 1698949744267, "tmdate": 1698949744267, "signatures": ["NeurIPS.cc/2023/Conference/Submission7925/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7925/Authors"], "forum": "wiidCRA3at", "content": {"title": {"value": "Stein $\\Pi$-Importance Sampling"}, "authors": {"value": ["Congye Wang", "Wilson Ye Chen", "Heishiro Kanagawa", "Chris J. Oates"]}, "authorids": {"value": ["~Congye_Wang1", "~Wilson_Ye_Chen2", "~Heishiro_Kanagawa1", "~Chris_J._Oates1"]}, "keywords": {"value": ["Bayesian", "discrepancy", "kernel", "sampling", "Stein's method"]}, "abstract": {"value": "Stein discrepancies have emerged as a powerful tool for retrospective improvement of Markov chain Monte Carlo output.  However, the question of how to design Markov chains that are well-suited to such post-processing has yet to be addressed.  This paper studies Stein importance sampling, in which weights are assigned to the states visited by a $\\Pi$-invariant Markov chain to obtain a consistent approximation of $P$, the intended target.  Surprisingly, the optimal choice of $\\Pi$ is not identical to the target $P$; we therefore propose an explicit construction for $\\Pi$ based on a novel variational argument.  Explicit conditions for convergence of Stein $\\Pi$-Importance Sampling are established.  For $\\approx 70$% of tasks in the PosteriorDB benchmark, a significant improvement over the analogous post-processing of $P$-invariant Markov chains is reported."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d50ac476117d4636cbd878767de563da06a760d2.pdf"}, "supplementary_material": {"value": "/attachment/a5ee66ed641d80b1eac938a95a33d72c7f4ea669.zip"}, "_bibtex": {"value": "@inproceedings{\nwang2023stein,\ntitle={Stein \\${\\textbackslash}Pi\\$-Importance Sampling},\nauthor={Congye Wang and Wilson Ye Chen and Heishiro Kanagawa and Chris J. Oates},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=wiidCRA3at}\n}"}, "paperhash": {"value": "wang|stein_\\piimportance_sampling"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7925/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7925/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325970167, "odate": 1698949744252, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "1vyAG6j9PE", "number": 7919, "cdate": 1683737191036, "tcdate": 1683737191036, "mdate": 1698949744168, "tmdate": 1698949744168, "signatures": ["NeurIPS.cc/2023/Conference/Submission7919/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7919/Authors"], "forum": "1vyAG6j9PE", "content": {"title": {"value": "Unexpected Improvements to Expected Improvement for Bayesian Optimization"}, "authors": {"value": ["Sebastian Ament", "Sam Daulton", "David Eriksson", "Maximilian Balandat", "Eytan Bakshy"]}, "authorids": {"value": ["~Sebastian_Ament1", "~Sam_Daulton1", "~David_Eriksson2", "~Maximilian_Balandat1", "~Eytan_Bakshy1"]}, "keywords": {"value": ["Bayesian Optimization", "Gaussian Process", "Multi-Objective Optimization"]}, "TLDR": {"value": "We analyze and fix pathologies of Expected Improvement and its variants for single and multiple-objective problems, leading to significant improvements in empirical optimization performance."}, "abstract": {"value": "Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in \u201cclassic\u201d analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8af24f960fda365f79cdc874de57a42690c34ded.pdf"}, "supplementary_material": {"value": "/attachment/405d93626d8e98cfe955208d89d5d300ba70895c.zip"}, "_bibtex": {"value": "@inproceedings{\nament2023unexpected,\ntitle={Unexpected Improvements to Expected Improvement for Bayesian Optimization},\nauthor={Sebastian Ament and Sam Daulton and David Eriksson and Maximilian Balandat and Eytan Bakshy},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=1vyAG6j9PE}\n}"}, "paperhash": {"value": "ament|unexpected_improvements_to_expected_improvement_for_bayesian_optimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7919/-/Revision", "NeurIPS.cc/2023/Conference/Submission7919/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7919/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325970114, "odate": 1698949744152, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "QKSejqE8Vp", "number": 7868, "cdate": 1683736289774, "tcdate": 1683736289774, "mdate": 1698949743758, "tmdate": 1698949743758, "signatures": ["NeurIPS.cc/2023/Conference/Submission7868/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7868/Authors"], "forum": "QKSejqE8Vp", "content": {"title": {"value": "An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions"}, "authors": {"value": ["Yingtai Xiao", "Guanlin He", "Danfeng Zhang", "Daniel Kifer"]}, "authorids": {"value": ["~Yingtai_Xiao1", "~Guanlin_He1", "~Danfeng_Zhang1", "~Daniel_Kifer1"]}, "keywords": {"value": ["differential privacy", "marginals", "matrix mechanism", "scalability"]}, "TLDR": {"value": "An optimal and extremely scalable algorithm for differentially private marginals with unbiased noise, exact variance/covariance guarantees and customizable loss function"}, "abstract": {"value": "Noisy marginals are a common form of confidentiality-protecting data release and are useful for many downstream tasks such as contingency table analysis, construction of Bayesian networks, and even synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries (such as marginals) are known as matrix mechanisms.\n\nWe propose ResidualPlanner, a matrix mechanism for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize for many loss functions that can be written as a convex function of marginal variances (prior work was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods quickly run out of memory, even for relatively small datasets)."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/eef5a7997e01dbcbf94ebd6cbf290643143bc593.pdf"}, "supplementary_material": {"value": "/attachment/34aa3dc085b0473f1876f0d79ace957d1d47bee4.zip"}, "_bibtex": {"value": "@inproceedings{\nxiao2023an,\ntitle={An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions},\nauthor={Yingtai Xiao and Guanlin He and Danfeng Zhang and Daniel Kifer},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=QKSejqE8Vp}\n}"}, "paperhash": {"value": "xiao|an_optimal_and_scalable_matrix_mechanism_for_noisy_marginals_under_convex_loss_functions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7868/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7868/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325968524, "odate": 1698949743743, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Qv6468llWS", "number": 7786, "cdate": 1683734788156, "tcdate": 1683734788156, "mdate": 1698949743344, "tmdate": 1698949743344, "signatures": ["NeurIPS.cc/2023/Conference/Submission7786/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7786/Authors"], "forum": "Qv6468llWS", "content": {"title": {"value": "PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers"}, "authors": {"value": ["Phillip Lippe", "Bastiaan S. Veeling", "Paris Perdikaris", "Richard E Turner", "Johannes Brandstetter"]}, "authorids": {"value": ["~Phillip_Lippe1", "~Bastiaan_S._Veeling1", "~Paris_Perdikaris1", "~Richard_E_Turner1", "~Johannes_Brandstetter1"]}, "keywords": {"value": ["Neural PDE Solvers", "Neural Operators", "Temporal Stability", "Long-Horizon Modeling", "Autoregressive Forecasting"]}, "TLDR": {"value": "PDE-Refiner enables neural operator training for accurate and stable predictions over long time horizons."}, "abstract": {"value": "Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance.  Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/86093185c7d5d659763e209b2bc01674bc53ba93.pdf"}, "supplementary_material": {"value": "/attachment/89b35c8beee094f26f62a73f6c0ad4082b021757.pdf"}, "_bibtex": {"value": "@inproceedings{\nlippe2023pderefiner,\ntitle={{PDE}-Refiner: Achieving Accurate Long Rollouts with Neural {PDE} Solvers},\nauthor={Phillip Lippe and Bastiaan S. Veeling and Paris Perdikaris and Richard E Turner and Johannes Brandstetter},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Qv6468llWS}\n}"}, "paperhash": {"value": "lippe|pderefiner_achieving_accurate_long_rollouts_with_neural_pde_solvers"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7786/-/Revision", "NeurIPS.cc/2023/Conference/Submission7786/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7786/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325965970, "odate": 1698949743330, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "UWd4ysACo4", "number": 7752, "cdate": 1683734205647, "tcdate": 1683734205647, "mdate": 1698949743147, "tmdate": 1698949743147, "signatures": ["NeurIPS.cc/2023/Conference/Submission7752/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7752/Authors"], "forum": "UWd4ysACo4", "content": {"title": {"value": "Expressive Sign Equivariant Networks for Spectral Geometric Learning"}, "authors": {"value": ["Derek Lim", "Joshua Robinson", "Stefanie Jegelka", "Haggai Maron"]}, "authorids": {"value": ["~Derek_Lim1", "~Joshua_Robinson4", "~Stefanie_Jegelka3", "~Haggai_Maron1"]}, "keywords": {"value": ["Eigenvectors", "spectral", "geometry", "universal approximation", "graph", "equivariance", "invariance"]}, "TLDR": {"value": "We prove that a novel type of equivariance to eigenvector symmetries is useful in several applications, and develop provably expressive networks with this equivariance."}, "abstract": {"value": "Recent work has shown the utility of developing machine learning models that respect the structure and symmetries of eigenvectors. These works promote sign invariance, since for any eigenvector v the negation -v is also an eigenvector. However, we show that sign invariance is theoretically limited for tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. In this work, we demonstrate the benefits of sign equivariance for these tasks. To obtain these benefits, we develop novel sign equivariant neural network architectures. Our models are based on a new analytic characterization of sign equivariant polynomials and thus inherit provable expressiveness properties. Controlled synthetic experiments show that our networks can achieve the theoretically predicted benefits of sign equivariant models."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3fa1f5e01c5cdea046b4fc7b41e55134c431c116.pdf"}, "supplementary_material": {"value": "/attachment/956798f286021e4f599f302ba26aef1e688497b6.zip"}, "_bibtex": {"value": "@inproceedings{\nlim2023expressive,\ntitle={Expressive Sign Equivariant Networks for Spectral Geometric Learning},\nauthor={Derek Lim and Joshua Robinson and Stefanie Jegelka and Haggai Maron},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=UWd4ysACo4}\n}"}, "paperhash": {"value": "lim|expressive_sign_equivariant_networks_for_spectral_geometric_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7752/-/Revision", "NeurIPS.cc/2023/Conference/Submission7752/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7752/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325964832, "odate": 1698949743134, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "3X2EbBLNsk", "number": 7656, "cdate": 1683732915008, "tcdate": 1683732915008, "mdate": 1698978395328, "tmdate": 1698978395328, "signatures": ["NeurIPS.cc/2023/Conference/Submission7656/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7656/Authors"], "forum": "3X2EbBLNsk", "content": {"title": {"value": "Birth of a Transformer: A Memory Viewpoint"}, "authors": {"value": ["Alberto Bietti", "Vivien Cabannes", "Diane Bouchacourt", "Herve Jegou", "Leon Bottou"]}, "authorids": {"value": ["~Alberto_Bietti1", "~Vivien_Cabannes1", "~Diane_Bouchacourt3", "~Herve_Jegou1", "~Leon_Bottou1"]}, "keywords": {"value": ["transformers", "language models", "deep learning theory", "interpretability"]}, "abstract": {"value": "Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an \"induction head\" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/18f0a07370deb3c1db84a94f1c73ee9d3a2bd72c.pdf"}, "TLDR": {"value": "understanding transformers and their learning dynamics through associative memories"}, "_bibtex": {"value": "@inproceedings{\nbietti2023birth,\ntitle={Birth of a Transformer: A Memory Viewpoint},\nauthor={Alberto Bietti and Vivien Cabannes and Diane Bouchacourt and Herve Jegou and Leon Bottou},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=3X2EbBLNsk}\n}"}, "paperhash": {"value": "bietti|birth_of_a_transformer_a_memory_viewpoint"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7656/-/Revision", "NeurIPS.cc/2023/Conference/Submission7656/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7656/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325961907, "odate": 1698949742625, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "AygwZzdCM0", "number": 7623, "cdate": 1683732383307, "tcdate": 1683732383307, "mdate": 1698949742476, "tmdate": 1698949742476, "signatures": ["NeurIPS.cc/2023/Conference/Submission7623/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7623/Authors"], "forum": "AygwZzdCM0", "content": {"title": {"value": "Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model"}, "authors": {"value": ["Valentyn Melnychuk", "Dennis Frauen", "Stefan Feuerriegel"]}, "authorids": {"value": ["~Valentyn_Melnychuk1", "~Dennis_Frauen1", "~Stefan_Feuerriegel1"]}, "keywords": {"value": ["causal inference", "counterfactual inference", "partial identification", "sensitivity model", "normalizing flows", "causal machine learning"]}, "abstract": {"value": "Counterfactual inference aims to answer retrospective \"what if\" questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual identification methods are special cases of our Curvature Sensitivity Model when the bound of the curvature is set to zero. We then propose an implementation of our Curvature Sensitivity Model in the form of a novel deep generative model, which we call Augmented Pseudo-Invertible Decoder. Our implementation employs (i) residual normalizing flows with (ii) variational augmentations. We empirically demonstrate the effectiveness of our Augmented Pseudo-Invertible Decoder. To the best of our knowledge, ours is the first partial identification model for Markovian structural causal models with continuous outcomes."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/01d0a87b5768e5ba4bfa18ef96cfbe2d7566f43b.pdf"}, "supplementary_material": {"value": "/attachment/1e0c2e0bdcbd2990b93ed1aa5b2828c14936f19b.zip"}, "_bibtex": {"value": "@inproceedings{\nmelnychuk2023partial,\ntitle={Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model},\nauthor={Valentyn Melnychuk and Dennis Frauen and Stefan Feuerriegel},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=AygwZzdCM0}\n}"}, "paperhash": {"value": "melnychuk|partial_counterfactual_identification_of_continuous_outcomes_with_a_curvature_sensitivity_model"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7623/-/Revision", "NeurIPS.cc/2023/Conference/Submission7623/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7623/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325960945, "odate": 1698949742463, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "hCdqDkA25J", "number": 7592, "cdate": 1683731913411, "tcdate": 1683731913411, "mdate": 1698949742287, "tmdate": 1698949742287, "signatures": ["NeurIPS.cc/2023/Conference/Submission7592/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7592/Authors"], "forum": "hCdqDkA25J", "content": {"title": {"value": "Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization"}, "authors": {"value": ["Liang Zhang", "Junchi YANG", "Amin Karbasi", "Niao He"]}, "authorids": {"value": ["~Liang_Zhang6", "~Junchi_YANG1", "~Amin_Karbasi3", "~Niao_He3"]}, "keywords": {"value": ["Reproducibility", "Convex Optimization", "Minimax Optimization", "Saddle-Point Problem"]}, "abstract": {"value": "Algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon minor changes in the training process. Previous work suggests that first-order methods would need to trade-off convergence rate (gradient complexity) for better reproducibility. In this work, we challenge this perception and demonstrate that both optimal reproducibility and near-optimal convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave minimax problems under various error-prone oracle settings. Particularly, given the inexact initialization oracle, our regularization-based algorithms achieve the best of both worlds -- optimal reproducibility and near-optimal gradient complexity -- for minimization and minimax optimization. With the inexact gradient oracle, the near-optimal guarantees also hold for minimax optimization. Additionally, with the stochastic gradient oracle, we show that stochastic gradient descent ascent is optimal in terms of both reproducibility and gradient complexity. We believe our results contribute to an enhanced understanding of the reproducibility-convergence trade-off in the context of convex optimization."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f90279769fb5f5d0f6792a64b7d79a34d938b11e.pdf"}, "supplementary_material": {"value": "/attachment/75aa85ab3e5b7296c323a31577a1c420c24ec78b.zip"}, "_bibtex": {"value": "@inproceedings{\nzhang2023optimal,\ntitle={Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization},\nauthor={Liang Zhang and Junchi YANG and Amin Karbasi and Niao He},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=hCdqDkA25J}\n}"}, "paperhash": {"value": "zhang|optimal_guarantees_for_algorithmic_reproducibility_and_gradient_complexity_in_convex_optimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7592/-/Revision", "NeurIPS.cc/2023/Conference/Submission7592/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7592/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325960195, "odate": 1698949742272, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "xtADRDRsM2", "number": 7559, "cdate": 1683731421802, "tcdate": 1683731421802, "mdate": 1698949742100, "tmdate": 1698949742100, "signatures": ["NeurIPS.cc/2023/Conference/Submission7559/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7559/Authors"], "forum": "xtADRDRsM2", "content": {"title": {"value": "Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach"}, "authors": {"value": ["Kai Zhao", "Qiyu Kang", "Yang Song", "Rui She", "Sijie Wang", "Wee Peng Tay"]}, "authorids": {"value": ["~Kai_Zhao7", "~Qiyu_Kang2", "~Yang_Song7", "~Rui_She1", "~Sijie_Wang1", "~Wee_Peng_Tay1"]}, "keywords": {"value": ["adversarial robustness", "graph neural networks"]}, "abstract": {"value": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments  is available at \\url{https://github.com/zknus/NeurIPS-2023-HANG-Robustness}."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/42d17920b5b8ca3074fcda4194ed84a934f404de.pdf"}, "supplementary_material": {"value": "/attachment/ac853cb7c389364364f49d080ef61de51593a175.zip"}, "_bibtex": {"value": "@inproceedings{\nzhao2023adversarial,\ntitle={Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach},\nauthor={Kai Zhao and Qiyu Kang and Yang Song and Rui She and Sijie Wang and Wee Peng Tay},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=xtADRDRsM2}\n}"}, "paperhash": {"value": "zhao|adversarial_robustness_in_graph_neural_networks_a_hamiltonian_approach"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7559/-/Revision", "NeurIPS.cc/2023/Conference/Submission7559/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7559/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325959332, "odate": 1698949742073, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "vO04AzsB49", "number": 7533, "cdate": 1683731159645, "tcdate": 1683731159645, "mdate": 1698949742003, "tmdate": 1698949742003, "signatures": ["NeurIPS.cc/2023/Conference/Submission7533/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7533/Authors"], "forum": "vO04AzsB49", "content": {"title": {"value": "Imitation Learning from Imperfection: Theoretical Justifications and Algorithms"}, "authors": {"value": ["Ziniu Li", "Tian Xu", "Zeyu Qin", "Yang Yu", "Zhi-Quan Luo"]}, "authorids": {"value": ["~Ziniu_Li1", "~Tian_Xu2", "~Zeyu_Qin1", "~Yang_Yu5", "~Zhi-Quan_Luo1"]}, "keywords": {"value": ["imitation learning", "distribution shift", "policy optimization", "data selection"]}, "TLDR": {"value": "We show the potential of improving imitation learning by leveraging diverse data sources through effective data selection."}, "abstract": {"value": "Imitation learning (IL) algorithms excel in acquiring high-quality policies from expert data for sequential decision-making tasks. But, their effectiveness is hampered when faced with limited expert data. To tackle this challenge, a novel framework called (offline) IL with supplementary data has emerged, which enhances learning by incorporating an additional yet imperfect dataset obtained inexpensively from sub-optimal policies. Nonetheless, learning becomes challenging due to the potential inclusion of out-of-expert-distribution samples. In this work, we pioneer the mathematical formalization of this framework, uncovering its limitations. Our theoretical analysis reveals that a naive approach\u2014applying the behavioral cloning (BC) algorithm concept to the combined set of expert and supplementary data\u2014may fall short of vanilla BC, which solely relies on expert data. This deficiency arises due to the distribution shift between the two data sources. To address this issue, we propose a new importance-sampling-based technique for selecting data within the expert distribution. We prove that the proposed method theoretically eliminates the gap of the naive approach, highlighting its efficacy when handling imperfect data. Empirical studies demonstrate that our method outperforms previous state-of-the-art methods in tasks including robotics locomotion control, Atari video games, and image classification. Overall, our work underscores the potential of improving IL by leveraging diverse data sources through effective data selection."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f326f41ea595baad46e7513f0d9be62e75bc8d91.pdf"}, "supplementary_material": {"value": "/attachment/e21515ff4231e431ef5098ddf7d8299ea75515f8.pdf"}, "_bibtex": {"value": "@inproceedings{\nli2023imitation,\ntitle={Imitation Learning from Imperfection: Theoretical Justifications and Algorithms},\nauthor={Ziniu Li and Tian Xu and Zeyu Qin and Yang Yu and Zhi-Quan Luo},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vO04AzsB49}\n}"}, "paperhash": {"value": "li|imitation_learning_from_imperfection_theoretical_justifications_and_algorithms"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7533/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7533/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325958826, "odate": 1698949741984, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5gz7npbQ6Z", "number": 7527, "cdate": 1683731034960, "tcdate": 1683731034960, "mdate": 1698949741925, "tmdate": 1698949741925, "signatures": ["NeurIPS.cc/2023/Conference/Submission7527/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7527/Authors"], "forum": "5gz7npbQ6Z", "content": {"title": {"value": "A Cross-Moment Approach for Causal Effect Estimation"}, "authors": {"value": ["Yaroslav Kivva", "Saber Salehkaleybar", "Negar Kiyavash"]}, "authorids": {"value": ["~Yaroslav_Kivva1", "~Saber_Salehkaleybar1", "~Negar_Kiyavash1"]}, "keywords": {"value": ["Causal inference", "Difference-in-Difference", "Structural causal models", "Potential outcome", "Proxy learning"]}, "abstract": {"value": "We consider the problem of estimating the causal effect of a treatment on an outcome in  linear structural causal models (SCM) with latent confounders when we have access to a single proxy variable.\nSeveral methods (such as difference-in-difference (DiD) estimator or negative outcome control) have been proposed in this setting in the literature. However, these approaches require either restrictive assumptions on the data generating model or having access to at least two proxy variables.\nWe propose a method to estimate the causal effect using cross moments between the treatment, the outcome, and the proxy variable. In particular, we show that the causal effect can be identified with simple arithmetic operations on the cross moments if the latent confounder in linear SCM is non-Gaussian.\nIn this setting, DiD estimator provides an unbiased estimate only in the special case where the latent confounder has exactly the same direct causal effects on the outcomes in the pre-treatment and post-treatment phases. This translates to the common trend assumption in DiD, which we effectively relax.\nAdditionally, we provide an impossibility result that shows the causal effect cannot be identified if the observational distribution over the treatment, the outcome, and the proxy is jointly Gaussian.\n Our experiments on both synthetic and real-world datasets showcase the effectiveness\nof the proposed approach in estimating the causal effect."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/40c4ecdac5f2da3acc004e16000e2f70c4ba5507.pdf"}, "supplementary_material": {"value": "/attachment/4b69b1bb9b655e03a659874a2ed58f7d430104cc.pdf"}, "_bibtex": {"value": "@inproceedings{\nkivva2023a,\ntitle={A Cross-Moment Approach for Causal Effect Estimation},\nauthor={Yaroslav Kivva and Saber Salehkaleybar and Negar Kiyavash},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5gz7npbQ6Z}\n}"}, "paperhash": {"value": "kivva|a_crossmoment_approach_for_causal_effect_estimation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7527/-/Revision", "NeurIPS.cc/2023/Conference/Submission7527/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7527/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325958619, "odate": 1698949741912, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "v9yC7sSXf3", "number": 7518, "cdate": 1683730904206, "tcdate": 1683730904206, "mdate": 1698949741889, "tmdate": 1698949741889, "signatures": ["NeurIPS.cc/2023/Conference/Submission7518/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7518/Authors"], "forum": "v9yC7sSXf3", "content": {"title": {"value": "Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model"}, "authors": {"value": ["Peter S\u00faken\u00edk", "Marco Mondelli", "Christoph H Lampert"]}, "authorids": {"value": ["~Peter_S\u00faken\u00edk1", "~Marco_Mondelli1", "~Christoph_H_Lampert1"]}, "keywords": {"value": ["neural collapse", "unconstrained features model", "deep learning"]}, "TLDR": {"value": "We prove the optimality of deep neural collapse for the deep unconstrained features model - an extension of the unconstrained features model to arbitrarily many layers - for binary classification, and we provide experiments to support our theory."}, "abstract": {"value": "Neural collapse (NC) refers to the surprising structure of the last layer of deep neural networks in the terminal phase of gradient descent training. Recently, an increasing amount of experimental evidence has pointed to the propagation of NC to earlier layers of neural networks. However, while the NC in the last layer is well studied theoretically, much less is known about its multi-layered counterpart - deep neural collapse (DNC). In particular, existing work focuses either on linear layers or only on the last two layers at the price of an extra assumption. Our work fills this gap by generalizing the established analytical framework for NC - the unconstrained features model - to multiple non-linear layers. Our key technical contribution is to show that, in a deep unconstrained features model, the unique global optimum for binary classification exhibits all the properties typical of DNC. This explains the existing experimental evidence of DNC. We also empirically show that (i) by optimizing deep unconstrained features models via gradient descent, the resulting solution agrees well with our theory, and (ii) trained networks recover the unconstrained features suitable for the occurrence of DNC, thus supporting the validity of this modeling principle."}, "pdf": {"value": "/pdf/1b3b55a1db25a7c4bbf451ac2a838f30d2485ca5.pdf"}, "supplementary_material": {"value": "/attachment/200989b9e784bd90017c545d20db0a3d4714fb4f.zip"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\ns{\\'u}ken{\\'\\i}k2023deep,\ntitle={Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model},\nauthor={Peter S{\\'u}ken{\\'\\i}k and Marco Mondelli and Christoph H Lampert},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=v9yC7sSXf3}\n}"}, "paperhash": {"value": "s\u00faken\u00edk|deep_neural_collapse_is_provably_optimal_for_the_deep_unconstrained_features_model"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7518/-/Revision", "NeurIPS.cc/2023/Conference/Submission7518/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7518/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325958413, "odate": 1698949741876, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ca2QmdOlIh", "number": 7515, "cdate": 1683730805538, "tcdate": 1683730805538, "mdate": 1698949741845, "tmdate": 1698949741845, "signatures": ["NeurIPS.cc/2023/Conference/Submission7515/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7515/Authors"], "forum": "ca2QmdOlIh", "content": {"title": {"value": "Bayesian Extensive-Rank Matrix Factorization with Rotational Invariant Priors"}, "authors": {"value": ["Farzad Pourkamali", "Nicolas Macris"]}, "authorids": {"value": ["~Farzad_Pourkamali1", "~Nicolas_Macris1"]}, "keywords": {"value": ["Matrix factorization", "Bayesian inference", "rotation invariant estimators", "random matrix theory", "spherical integrals", "replica method"]}, "abstract": {"value": "We consider a statistical model for matrix factorization in a regime where the rank of the two hidden matrix factors grows linearly with their dimension and their product is corrupted by additive noise. Despite various approaches, statistical and algorithmic limits of such problems have remained elusive. We study a Bayesian setting with the assumptions that (a) one of the matrix factors is symmetric, (b) both factors as well as the additive noise have rotational invariant priors, (c) the priors are known to the statistician. We derive analytical formulas for Rotation Invariant Estimators to reconstruct the two matrix factors, and conjecture that these are optimal in the large-dimension limit, in the sense that they minimize the average mean-square-error. We provide numerical checks which confirm the optimality conjecture when confronted to Oracle Estimators which are optimal by definition, but involve the ground-truth. Our derivation relies on a combination of tools, namely random matrix theory transforms, spherical integral formulas, and the replica method from statistical mechanics."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/41e3388786b85edca71eb297c4a5fb189efe760d.pdf"}, "supplementary_material": {"value": "/attachment/90d31c0f1ed234c61e696bf4560d790201cde575.zip"}, "_bibtex": {"value": "@inproceedings{\npourkamali2023bayesian,\ntitle={Bayesian Extensive-Rank Matrix Factorization with Rotational Invariant Priors},\nauthor={Farzad Pourkamali and Nicolas Macris},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ca2QmdOlIh}\n}"}, "paperhash": {"value": "pourkamali|bayesian_extensiverank_matrix_factorization_with_rotational_invariant_priors"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7515/-/Revision", "NeurIPS.cc/2023/Conference/Submission7515/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7515/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325958247, "odate": 1698949741832, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "YsYKv95jy9", "number": 7388, "cdate": 1683728983374, "tcdate": 1683728983374, "mdate": 1698949740825, "tmdate": 1698949740825, "signatures": ["NeurIPS.cc/2023/Conference/Submission7388/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7388/Authors"], "forum": "YsYKv95jy9", "content": {"title": {"value": "Deep Fractional Fourier Transform"}, "authors": {"value": ["Hu Yu", "Jie Huang", "Lingzhi Li", "Man Zhou", "Feng Zhao"]}, "authorids": {"value": ["~Hu_Yu2", "~Jie_Huang4", "~Lingzhi_Li2", "~Man_Zhou4", "~Feng_Zhao6"]}, "keywords": {"value": ["Fractional Fourier Transform", "image restoration"]}, "abstract": {"value": "Existing deep learning-based computer vision methods usually operate in the spatial and frequency domains, which are two orthogonal \\textbf{individual} perspectives for image processing.\nIn this paper, we introduce a new spatial-frequency analysis tool, Fractional Fourier Transform (FRFT), to provide comprehensive \\textbf{unified} spatial-frequency perspectives.\nThe FRFT is a unified continuous spatial-frequency transform that simultaneously reflects an image's spatial and frequency representations, making it optimal for processing non-stationary image signals.\nWe explore the properties of the FRFT for image processing and present a fast implementation of the 2D FRFT, which facilitates its widespread use.\nBased on these explorations, we introduce a simple yet effective operator, Multi-order FRactional Fourier Convolution (MFRFC), which exhibits the remarkable merits of processing images from more perspectives in the spatial-frequency plane. Our proposed MFRFC is a general and basic operator that can be easily integrated into various tasks for performance improvement.\nWe experimentally evaluate the MFRFC on various computer vision tasks, including object detection, image classification, guided super-resolution, denoising, dehazing, deraining, and low-light enhancement. Our proposed MFRFC consistently outperforms baseline methods by significant margins across all tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9c71f4fcb76aafb3077b4699f436199e8c46ac2a.pdf"}, "supplementary_material": {"value": "/attachment/a7bab4649dfe90167f6d0822953286a27ffd4331.pdf"}, "_bibtex": {"value": "@inproceedings{\nyu2023deep,\ntitle={Deep Fractional Fourier Transform},\nauthor={Hu Yu and Jie Huang and Lingzhi Li and Man Zhou and Feng Zhao},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=YsYKv95jy9}\n}"}, "paperhash": {"value": "yu|deep_fractional_fourier_transform"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7388/-/Revision", "NeurIPS.cc/2023/Conference/Submission7388/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7388/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325954324, "odate": 1698949740813, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "vTug54Uunq", "number": 7346, "cdate": 1683728380205, "tcdate": 1683728380205, "mdate": 1698949740465, "tmdate": 1698949740465, "signatures": ["NeurIPS.cc/2023/Conference/Submission7346/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7346/Authors"], "forum": "vTug54Uunq", "content": {"title": {"value": "Faster Margin Maximization Rates for Generic Optimization Methods"}, "authors": {"value": ["Guanghui Wang", "Zihao Hu", "Vidya Muthukumar", "Jacob Abernethy"]}, "authorids": {"value": ["~Guanghui_Wang3", "~Zihao_Hu1", "~Vidya_Muthukumar3", "~Jacob_Abernethy1"]}, "keywords": {"value": ["Implicit bias", "margin maximization", "zero-sum game", "online learning"]}, "TLDR": {"value": "We provide a series of state-of-the-art implicit bias rates for generic optimization algorithms."}, "abstract": {"value": "First-order optimization methods tend to inherently favor certain solutions over others when minimizing a given training objective with multiple local optima. This phenomenon, known as \\emph{implicit bias}, plays a critical role in understanding the generalization capabilities of optimization algorithms. Recent research has revealed that gradient-descent-based methods exhibit an implicit bias for the $\\ell_2$-maximal margin classifier in the context of separable binary classification. In contrast, generic optimization methods, such as mirror descent and steepest descent, have been shown to converge to maximal margin classifiers defined by alternative geometries. However, while gradient-descent-based algorithms demonstrate fast implicit bias rates, the implicit bias rates of generic optimization methods have been relatively slow. To address this limitation, in this paper, we present a series of state-of-the-art implicit bias rates for mirror descent and steepest descent algorithms. Our primary technique involves transforming a generic optimization algorithm into an online learning dynamic that solves a regularized bilinear game, providing a unified framework for analyzing the implicit bias of various optimization methods. The accelerated rates are derived leveraging the regret bounds of online learning algorithms within this game framework."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/70489a904a6f0ffa40666e63f7ac5b66dbf4340a.pdf"}, "supplementary_material": {"value": "/attachment/3921fd3fbae3c75f8d796d88a83fc3da1c9d8578.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023faster,\ntitle={Faster Margin Maximization Rates for Generic Optimization Methods},\nauthor={Guanghui Wang and Zihao Hu and Vidya Muthukumar and Jacob Abernethy},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vTug54Uunq}\n}"}, "paperhash": {"value": "wang|faster_margin_maximization_rates_for_generic_optimization_methods"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7346/-/Revision", "NeurIPS.cc/2023/Conference/Submission7346/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7346/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325953020, "odate": 1698949740453, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "BKAFLUcpBS", "number": 7296, "cdate": 1683727716138, "tcdate": 1683727716138, "mdate": 1698949740261, "tmdate": 1698949740261, "signatures": ["NeurIPS.cc/2023/Conference/Submission7296/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7296/Authors"], "forum": "BKAFLUcpBS", "content": {"title": {"value": "Outlier-Robust Gromov-Wasserstein for Graph Data"}, "authors": {"value": ["Lemin Kong", "Jiajin Li", "Jianheng Tang", "Anthony Man-Cho So"]}, "authorids": {"value": ["~Lemin_Kong1", "~Jiajin_Li2", "~Jianheng_Tang1", "~Anthony_Man-Cho_So1"]}, "keywords": {"value": ["Gromov Wasserstein", "Robust Optimization", "Nonconvex Optimization"]}, "abstract": {"value": "Gromov-Wasserstein (GW) distance is a powerful tool for comparing and aligning probability distributions supported on different metric spaces. Recently, GW has become the main modeling technique for aligning heterogeneous data for a wide range of graph learning tasks. However, the GW distance is known to be highly sensitive to outliers, which can result in large inaccuracies if the outliers are given the same weight as other samples in the objective function. To mitigate this issue, we introduce a new and robust version of the GW distance called RGW. RGW features optimistically perturbed marginal constraints within a Kullback-Leibler divergence-based ambiguity set. To make the benefits of RGW more accessible in practice, we develop a computationally efficient and theoretically provable procedure using Bregman proximal alternating linearized minimization algorithm. Through extensive experimentation, we validate our theoretical results and demonstrate the effectiveness of RGW on real-world graph learning tasks, such as subgraph matching and partial shape correspondence."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f5da127a07692da110a8565f57bf2049a58ea888.pdf"}, "supplementary_material": {"value": "/attachment/9e3cdb0421aa78549356c5792c84052e9ff34894.zip"}, "_bibtex": {"value": "@inproceedings{\nkong2023outlierrobust,\ntitle={Outlier-Robust Gromov-Wasserstein for Graph Data},\nauthor={Lemin Kong and Jiajin Li and Jianheng Tang and Anthony Man-Cho So},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BKAFLUcpBS}\n}"}, "paperhash": {"value": "kong|outlierrobust_gromovwasserstein_for_graph_data"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7296/-/Revision", "NeurIPS.cc/2023/Conference/Submission7296/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7296/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325951272, "odate": 1698949740243, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "KFm2lZiI7n", "number": 7200, "cdate": 1683725984612, "tcdate": 1683725984612, "mdate": 1698949739790, "tmdate": 1698949739790, "signatures": ["NeurIPS.cc/2023/Conference/Submission7200/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7200/Authors"], "forum": "KFm2lZiI7n", "content": {"title": {"value": "MeCo: Zero-Shot NAS with One Data and Single Forward Pass via Minimum Eigenvalue of Correlation"}, "authors": {"value": ["Tangyu Jiang", "Haodi Wang", "Rongfang Bie"]}, "authorids": {"value": ["~Tangyu_Jiang1", "whd@mail.bnu.edu.cn", "rfbie@bnu.edu.cn"]}, "keywords": {"value": ["Neural Architecture Search", "Zero-Cost Proxy", "Evaluation Strategy", "Feature Map"]}, "TLDR": {"value": "A new zero-cost proxy for NAS that requires a single forward pass using one data and achieves SOTA results."}, "abstract": {"value": "Neural Architecture Search (NAS) is a promising paradigm in automatic architecture engineering. Zero-shot NAS can evaluate the network without training via some specific metrics called zero-cost proxies. Though effective, the existing zero-cost proxies either invoke at least one backpropagation or depend highly on the data and labels. To alleviate the above issues, in this paper, we first reveal how the Pearson correlation matrix of the feature maps impacts the convergence rate and the generalization capacity of an over-parameterized neural network. Enlightened by the theoretical analysis, we propose a novel zero-cost proxy called $\\mathsf{MeCo}$, which requires only one random data for a single forward pass. We further propose an optimization approach $\\mathsf{MeCo_{opt}}$ to improve the performance of our method. We design comprehensive experiments and extensively evaluate $\\mathsf{MeCo}$ on multiple popular benchmarks. $\\mathsf{MeCo}$ achieves the highest correlation with the ground truth (e.g., 0.89 on NATS-Bench-TSS with CIFAR-10) among all the state-of-the-art proxies, which is also fully independent of the data and labels. Moreover, we integrate $\\mathsf{MeCo}$ with the existing generation method to comprise a complete NAS. The experimental results illustrate that $\\mathsf{MeCo}$-based NAS can select the architecture with the highest accuracy and a low search cost. For instance, the best network searched by $\\mathsf{MeCo}$-based NAS achieves 97.31% on CIFAR-10, which is 0.04% higher than the baselines under the same settings. Our code is available at https://github.com/HamsterMimi/MeCo"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f76a85bdab71b6aaf1ab32462ca765db9e46bddc.pdf"}, "supplementary_material": {"value": "/attachment/b4b4d5fa909f7fa62e5e8eb9b2753f3029992e14.pdf"}, "_bibtex": {"value": "@inproceedings{\njiang2023meco,\ntitle={MeCo: Zero-Shot {NAS} with One Data and Single Forward Pass via Minimum Eigenvalue of Correlation},\nauthor={Tangyu Jiang and Haodi Wang and Rongfang Bie},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=KFm2lZiI7n}\n}"}, "paperhash": {"value": "jiang|meco_zeroshot_nas_with_one_data_and_single_forward_pass_via_minimum_eigenvalue_of_correlation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7200/-/Revision", "NeurIPS.cc/2023/Conference/Submission7200/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7200/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325947922, "odate": 1698949739764, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "IHR83ufYPy", "number": 7162, "cdate": 1683725333006, "tcdate": 1683725333006, "mdate": 1698949739454, "tmdate": 1698949739454, "signatures": ["NeurIPS.cc/2023/Conference/Submission7162/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7162/Authors"], "forum": "IHR83ufYPy", "content": {"title": {"value": "Leveraging sparse and shared feature activations for disentangled representation learning"}, "authors": {"value": ["Marco Fumero", "Florian Wenzel", "Luca Zancato", "Alessandro Achille", "Emanuele Rodol\u00e0", "Stefano Soatto", "Bernhard Sch\u00f6lkopf", "Francesco Locatello"]}, "authorids": {"value": ["~Marco_Fumero1", "~Florian_Wenzel1", "~Luca_Zancato1", "~Alessandro_Achille1", "~Emanuele_Rodol\u00e01", "~Stefano_Soatto3", "~Bernhard_Sch\u00f6lkopf1", "~Francesco_Locatello1"]}, "keywords": {"value": ["disentanglement", "OOD generalization", "multitask learning"]}, "abstract": {"value": "Recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions.\nWe validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), demonstrating how disentangled representations can be transferred to real settings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a4d438d017564bfbabefcfc12facd422953e9222.pdf"}, "supplementary_material": {"value": "/attachment/c262ccc26bb622bd2daa91207159a6dc5f5d99c1.pdf"}, "_bibtex": {"value": "@inproceedings{\nfumero2023leveraging,\ntitle={Leveraging sparse and shared feature activations for disentangled representation learning},\nauthor={Marco Fumero and Florian Wenzel and Luca Zancato and Alessandro Achille and Emanuele Rodol{\\`a} and Stefano Soatto and Bernhard Sch{\\\"o}lkopf and Francesco Locatello},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=IHR83ufYPy}\n}"}, "paperhash": {"value": "fumero|leveraging_sparse_and_shared_feature_activations_for_disentangled_representation_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7162/-/Revision", "NeurIPS.cc/2023/Conference/Submission7162/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7162/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325946719, "odate": 1698949739438, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "tDAu3FPJn9", "number": 7153, "cdate": 1683725129352, "tcdate": 1683725129352, "mdate": 1698949739324, "tmdate": 1698949739324, "signatures": ["NeurIPS.cc/2023/Conference/Submission7153/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7153/Authors"], "forum": "tDAu3FPJn9", "content": {"title": {"value": "A Robust and Opponent-Aware League Training Method for StarCraft II"}, "authors": {"value": ["Ruozi Huang", "Xipeng Wu", "Hongsheng Yu", "Zhong Fan", "Haobo Fu", "QIANG FU", "Yang Wei"]}, "authorids": {"value": ["~Ruozi_Huang1", "~Xipeng_Wu1", "~Hongsheng_Yu1", "~Zhong_Fan1", "~Haobo_Fu2", "~QIANG_FU8", "~Yang_Wei2"]}, "keywords": {"value": ["StarCraft II", "league training", "AlphaStar", "opponent-modeling", "reinforcement learning"]}, "abstract": {"value": "It is extremely difficult to train a superhuman Artificial Intelligence (AI) for games of similar size to StarCraft II. AlphaStar is the first AI that beat human professionals in the full game of StarCraft II, using a league training framework that is inspired by a game-theoretic approach. In this paper, we improve AlphaStar's league training in two significant aspects.  We train goal-conditioned exploiters, whose abilities of spotting weaknesses in the main agent and the entire league are greatly improved compared to the unconditioned exploiters in AlphaStar. In addition, we endow the agents in the league with the new ability of opponent modeling, which makes the agent more responsive to the opponent's real-time strategy. Based on these improvements, we train a better and superhuman AI with orders of magnitude less resources than AlphaStar (see Table 1 for a full comparison). Considering the iconic role of StarCraft II in game AI research, we believe our method and results on StarCraft II provide valuable design principles on how one would utilize the general league training framework for obtaining a least-exploitable strategy in various, large-scale, real-world games."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "A league training method with goal-conditioned exploiters and opponent modeling that aims to improve the robustness of StarCarft II AI."}, "pdf": {"value": "/pdf/deea4bc8e2d1ac5abe9d96aa00de655f6af94ff9.pdf"}, "supplementary_material": {"value": "/attachment/580d21403f369f103a3689c61332f730ee128a14.zip"}, "_bibtex": {"value": "@inproceedings{\nhuang2023a,\ntitle={A Robust and Opponent-Aware League Training Method for StarCraft {II}},\nauthor={Ruozi Huang and Xipeng Wu and Hongsheng Yu and Zhong Fan and Haobo Fu and QIANG FU and Yang Wei},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=tDAu3FPJn9}\n}"}, "paperhash": {"value": "huang|a_robust_and_opponentaware_league_training_method_for_starcraft_ii"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7153/-/Revision", "NeurIPS.cc/2023/Conference/Submission7153/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7153/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325946405, "odate": 1698949739311, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "0Wp3VHX0Gm", "number": 7143, "cdate": 1683724905007, "tcdate": 1683724905007, "mdate": 1698949739283, "tmdate": 1698949739283, "signatures": ["NeurIPS.cc/2023/Conference/Submission7143/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7143/Authors"], "forum": "0Wp3VHX0Gm", "content": {"title": {"value": "Score-based Generative Models with L\u00e9vy Processes"}, "authors": {"value": ["Eunbi Yoon", "Keehun Park", "Sungwoong Kim", "Sungbin Lim"]}, "authorids": {"value": ["~Eunbi_Yoon1", "~Keehun_Park1", "~Sungwoong_Kim2", "~Sungbin_Lim1"]}, "keywords": {"value": ["Generative Model", "Score-based Method", "L\u00e9vy processes"]}, "abstract": {"value": "Investigating the optimal stochastic process beyond Gaussian for noise injection in a score-based generative model remains an open question. Brownian motion is a light-tailed process with continuous paths, which leads to a slow convergence rate for the Number of Function Evaluation (NFE). Recent studies have shown that diffusion models suffer from mode-collapse issues on imbalanced data.\nIn order to overcome the limitations of Brownian motion, we introduce a novel score-based generative model referred to as L\u00e9vy-It\u014d Model (LIM). This model utilizes isotropic $\\alpha$-stable L\u00e9vy processes. We first derive an exact reverse-time stochastic differential equation driven by the L\u00e9vy process and develop the corresponding fractional denoising score matching. The proposed generative model takes advantage of the heavy-tailed properties of the L\u00e9vy process. Our experimental results show LIM allows for faster and more diverse sampling while maintaining high fidelity compared to existing diffusion models across various image datasets such as CIFAR10, CelebA, and imbalanced dataset CIFAR10LT. Comparing our results to those of DDPM with 3.21 Fr\u00e9chet Inception Distance (FID) and 0.6437 Recall on the CelebA dataset, we achieve 1.58 FID and 0.7006 Recall using the same architecture. LIM shows the best performance in NFE 500 with $2\\times$ faster total wall-clock time than the baseline."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8121327da2659e2dad766e6e24c460dee8da9d1b.pdf"}, "supplementary_material": {"value": "/attachment/088c89bb39b4ce4d398454357e462d1993615c6e.zip"}, "_bibtex": {"value": "@inproceedings{\nyoon2023scorebased,\ntitle={Score-based Generative Models with L\\'evy Processes},\nauthor={Eunbi Yoon and Keehun Park and Sungwoong Kim and Sungbin Lim},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=0Wp3VHX0Gm}\n}"}, "paperhash": {"value": "yoon|scorebased_generative_models_with_l\u00e9vy_processes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7143/-/Revision", "NeurIPS.cc/2023/Conference/Submission7143/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7143/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325946180, "odate": 1698949739271, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "kCCD8d2aEu", "number": 7098, "cdate": 1683724038742, "tcdate": 1683724038742, "mdate": 1698949738938, "tmdate": 1698949738938, "signatures": ["NeurIPS.cc/2023/Conference/Submission7098/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7098/Authors"], "forum": "kCCD8d2aEu", "content": {"title": {"value": "Coherent Soft Imitation Learning"}, "authors": {"value": ["Joe Watson", "Sandy Huang", "Nicolas Heess"]}, "authorids": {"value": ["~Joe_Watson1", "~Sandy_Huang1", "~Nicolas_Heess1"]}, "keywords": {"value": ["imitation learning", "inverse reinforcement learning", "behavioral cloning", "learning from demonstration"]}, "TLDR": {"value": "Perform imitation learning by using a behavioural cloning policy to construct an estimate of a shaped reward function."}, "abstract": {"value": "Imitation learning methods seek to learn from an expert either through behavioral cloning (BC) for the policy or inverse reinforcement learning (IRL) for the reward.\nSuch methods enable agents to learn complex tasks from humans that are difficult to capture with hand-designed reward functions.\nChoosing between BC or IRL for imitation depends on the quality and state-action coverage of the demonstrations, as well as additional access to the Markov decision process. \nHybrid strategies that combine BC and IRL are rare, as initial policy optimization against inaccurate rewards diminishes the benefit of pretraining the policy with BC.\nOur work derives an imitation method that captures the strengths of both BC and IRL.\nIn the entropy-regularized (`soft') reinforcement learning setting, we show that the behavioral-cloned policy can be used as both a shaped reward and a critic hypothesis space by inverting the regularized policy update. \nThis coherency facilitates fine-tuning cloned policies using the reward estimate and additional interactions with the environment.\nThis approach conveniently achieves imitation learning through initial behavioral cloning and subsequent refinement via RL with online or offline data sources.\nThe simplicity of the approach enables graceful scaling to high-dimensional and vision-based tasks, with stable learning and minimal hyperparameter tuning, in contrast to adversarial approaches.\nFor the open-source implementation and simulation results, see https://joemwatson.github.io/csil/."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7c5304f63af575da83530da46eeb635e80517330.pdf"}, "supplementary_material": {"value": "/attachment/d25f87dd545064dd40dddd90b5c7a63db3b9d0fd.zip"}, "_bibtex": {"value": "@inproceedings{\nwatson2023coherent,\ntitle={Coherent Soft Imitation Learning},\nauthor={Joe Watson and Sandy Huang and Nicolas Heess},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=kCCD8d2aEu}\n}"}, "paperhash": {"value": "watson|coherent_soft_imitation_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7098/-/Revision", "NeurIPS.cc/2023/Conference/Submission7098/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7098/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325944863, "odate": 1698949738925, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "nDIrJmKPd5", "number": 7033, "cdate": 1683722893310, "tcdate": 1683722893310, "mdate": 1698949738606, "tmdate": 1698949738606, "signatures": ["NeurIPS.cc/2023/Conference/Submission7033/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission7033/Authors"], "forum": "nDIrJmKPd5", "content": {"title": {"value": "Private Distribution Learning with Public Data: The View from Sample Compression"}, "authors": {"value": ["Shai Ben-David", "Alex Bie", "Clement Louis Canonne", "Gautam Kamath", "Vikrant Singhal"]}, "authorids": {"value": ["~Shai_Ben-David2", "~Alex_Bie1", "~Clement_Louis_Canonne1", "~Gautam_Kamath1", "~Vikrant_Singhal2"]}, "keywords": {"value": ["differential privacy", "distribution learning", "gaussians", "mixture of gaussians", "compression schemes", "robust compression schemes", "privacy"]}, "abstract": {"value": "We study the problem of private distribution learning with access to public data. In this setup, which we refer to as *public-private learning*, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples. \n    \nWe show that the public-private learnability of a class $\\mathcal Q$ is connected to the existence of a sample compression scheme for $\\mathcal Q$, as well as to an intermediate notion we refer to as \\emph{list learning}. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability under taking mixtures and products of distributions. Finally, via the connection to list learning, we show that for Gaussians in $\\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We connect public-private distribution learning to sample compression and list learning, which yields flexible ways to prove new upper and lower bounds on sample complexity."}, "pdf": {"value": "/pdf/4ec14074eb70e5bc964729f91c99626853aa706b.pdf"}, "_bibtex": {"value": "@inproceedings{\nben-david2023private,\ntitle={Private Distribution Learning with Public Data: The View from Sample Compression},\nauthor={Shai Ben-David and Alex Bie and Clement Louis Canonne and Gautam Kamath and Vikrant Singhal},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=nDIrJmKPd5}\n}"}, "paperhash": {"value": "bendavid|private_distribution_learning_with_public_data_the_view_from_sample_compression"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission7033/-/Revision", "NeurIPS.cc/2023/Conference/Submission7033/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission7033/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325942872, "odate": 1698949738591, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "3GpIeVYw8X", "number": 6997, "cdate": 1683722384612, "tcdate": 1683722384612, "mdate": 1698949738218, "tmdate": 1698949738218, "signatures": ["NeurIPS.cc/2023/Conference/Submission6997/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6997/Authors"], "forum": "3GpIeVYw8X", "content": {"title": {"value": "The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning"}, "authors": {"value": ["Artyom Gadetsky", "Maria Brbic"]}, "authorids": {"value": ["~Artyom_Gadetsky1", "~Maria_Brbic1"]}, "keywords": {"value": ["unsupervised learning", "deep learning", "generalization", "self-supervised learning", "clustering"]}, "TLDR": {"value": "We propose an unsupervised learning framework to find human labeling by searching for consistent labelings between different representation spaces."}, "abstract": {"value": "We present HUME, a simple model-agnostic framework for inferring human labeling of a given dataset without any external supervision. The key insight behind our approach is that classes defined by many human labelings are linearly separable regardless of the representation space used to represent a dataset. HUME utilizes this insight to guide the search over all possible labelings of a dataset to discover an underlying human labeling. We show that the proposed optimization objective is strikingly well-correlated with the ground truth labeling of the dataset. In effect, we only train linear classifiers on top of pretrained representations that remain fixed during training, making our framework compatible with any large pretrained and self-supervised model. Despite its simplicity, HUME outperforms a supervised linear classifier on top of self-supervised representations on the STL-10 dataset by a large margin and achieves comparable performance on the CIFAR-10 dataset. Compared to the existing unsupervised baselines, HUME achieves state-of-the-art performance on four benchmark image classification datasets including the large-scale ImageNet-1000 dataset. Altogether, our work provides a fundamentally new view to tackle unsupervised learning by searching for consistent labelings between different representation spaces."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/81514a91c918e901f8715c96cd1096ce943a1dd4.pdf"}, "supplementary_material": {"value": "/attachment/d0958ecf984edd16144945adaa29df191f5ae4bb.pdf"}, "_bibtex": {"value": "@inproceedings{\ngadetsky2023the,\ntitle={The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning},\nauthor={Artyom Gadetsky and Maria Brbic},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=3GpIeVYw8X}\n}"}, "paperhash": {"value": "gadetsky|the_pursuit_of_human_labeling_a_new_perspective_on_unsupervised_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6997/-/Revision", "NeurIPS.cc/2023/Conference/Submission6997/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6997/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325941634, "odate": 1698949738205, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "HF6bnhfSqH", "number": 6864, "cdate": 1683719838892, "tcdate": 1683719838892, "mdate": 1698949737730, "tmdate": 1698949737730, "signatures": ["NeurIPS.cc/2023/Conference/Submission6864/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6864/Authors"], "forum": "HF6bnhfSqH", "content": {"title": {"value": "On quantum backpropagation, information reuse, and cheating measurement collapse"}, "authors": {"value": ["Amira Abbas", "Robbie King", "Hsin-Yuan Huang", "William J. Huggins", "Ramis Movassagh", "Dar Gilboa", "Jarrod Ryan McClean"]}, "authorids": {"value": ["~Amira_Abbas1", "~Robbie_King1", "~Hsin-Yuan_Huang1", "whuggins@google.com", "movassagh@google.com", "darg@google.com", "~Jarrod_Ryan_McClean1"]}, "keywords": {"value": ["Backpropagation", "quantum computing", "shadow tomography", "gentle measurement"]}, "TLDR": {"value": "We prove that resources to compute gradients of parameterized quantum models will not scale as efficiently as neural networks equipped with backpropagation, by reduction to a well-known problem in quantum information theory."}, "abstract": {"value": "The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9c16bb894bcc8b894539610589f2a8597471972f.pdf"}, "supplementary_material": {"value": "/attachment/af59182f9361b6de148b85153773b5f2b8c86dba.pdf"}, "_bibtex": {"value": "@inproceedings{\nabbas2023on,\ntitle={On quantum backpropagation, information reuse, and cheating measurement collapse},\nauthor={Amira Abbas and Robbie King and Hsin-Yuan Huang and William J. Huggins and Ramis Movassagh and Dar Gilboa and Jarrod Ryan McClean},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=HF6bnhfSqH}\n}"}, "paperhash": {"value": "abbas|on_quantum_backpropagation_information_reuse_and_cheating_measurement_collapse"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6864/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6864/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325938093, "odate": 1698949737711, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "00EKYYu3fD", "number": 6838, "cdate": 1683719044332, "tcdate": 1683719044332, "mdate": 1698949737512, "tmdate": 1698949737512, "signatures": ["NeurIPS.cc/2023/Conference/Submission6838/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6838/Authors"], "forum": "00EKYYu3fD", "content": {"title": {"value": "Complexity Matters: Rethinking the Latent Space for Generative Modeling"}, "authors": {"value": ["Tianyang Hu", "Fei Chen", "Haonan Wang", "Jiawei Li", "Wenjia Wang", "Jiacheng Sun", "Zhenguo Li"]}, "authorids": {"value": ["~Tianyang_Hu1", "~Fei_Chen8", "~Haonan_Wang1", "~Jiawei_Li5", "~Wenjia_Wang2", "~Jiacheng_Sun1", "~Zhenguo_Li1"]}, "keywords": {"value": ["generative model", "latent space", "distance between distributions", "generative adversarial network", "vqgan"]}, "TLDR": {"value": "This work characterizes the optimal latent distribution for generative models from the perspective of minimizing model complexity and proposes a two-stage training scheme that achieves practical improvements on GAN, VQGAN and DiT."}, "abstract": {"value": "In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel \"distance\" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/4dc450a5143410cc6509f38a844cbbcc8f643541.pdf"}, "_bibtex": {"value": "@inproceedings{\nhu2023complexity,\ntitle={Complexity Matters: Rethinking the Latent Space for Generative Modeling},\nauthor={Tianyang Hu and Fei Chen and Haonan Wang and Jiawei Li and Wenjia Wang and Jiacheng Sun and Zhenguo Li},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=00EKYYu3fD}\n}"}, "paperhash": {"value": "hu|complexity_matters_rethinking_the_latent_space_for_generative_modeling"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6838/-/Revision", "NeurIPS.cc/2023/Conference/Submission6838/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6838/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325937374, "odate": 1698949737495, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Lt3jqxsbVO", "number": 6738, "cdate": 1683716726382, "tcdate": 1683716726382, "mdate": 1698949736977, "tmdate": 1698949736977, "signatures": ["NeurIPS.cc/2023/Conference/Submission6738/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6738/Authors"], "forum": "Lt3jqxsbVO", "content": {"title": {"value": "Sharp Spectral Rates for Koopman Operator Learning"}, "authors": {"value": ["Vladimir R Kostic", "Karim Lounici", "Pietro Novelli", "massimiliano pontil"]}, "authorids": {"value": ["~Vladimir_R_Kostic1", "~Karim_Lounici1", "~Pietro_Novelli1", "~massimiliano_pontil1"]}, "keywords": {"value": ["Statistical Learning Theory", "Dynamical Systems"]}, "abstract": {"value": "Non-linear dynamical systems can be handily described by the associated Koopman operator, whose action evolves every observable of the system forward in time. Learning the Koopman operator and its spectral decomposition from data is enabled by a number of algorithms. In this work we present for the first time non-asymptotic learning bounds for the Koopman eigenvalues and eigenfunctions. \nWe focus on time-reversal-invariant stochastic dynamical systems, including the important example of Langevin dynamics. \nWe analyze two popular estimators: Extended Dynamic Mode Decomposition (EDMD) and Reduced Rank Regression (RRR). Our results critically hinge on novel {minimax} estimation bounds for the operator norm error, that may be of independent interest. Our spectral learning bounds are driven by the simultaneous control of the operator norm error and a novel metric distortion functional of the estimated eigenfunctions. The bounds indicates that both EDMD and RRR have similar variance, but EDMD suffers from a larger bias which might be detrimental to its learning rate. Our results shed new light on the emergence of spurious eigenvalues, an issue which is well known empirically. Numerical experiments illustrate the implications of the bounds in practice."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/4974e15b70d14f695e057205040cb00dd9797453.pdf"}, "supplementary_material": {"value": "/attachment/f3b34e6f994fb66dfbdca4dc48fe4ef1c805e5d1.zip"}, "_bibtex": {"value": "@inproceedings{\nkostic2023sharp,\ntitle={Sharp Spectral Rates for Koopman Operator Learning},\nauthor={Vladimir R Kostic and Karim Lounici and Pietro Novelli and massimiliano pontil},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Lt3jqxsbVO}\n}"}, "TLDR": {"value": "We derive minimax optimal statistical rates for learning Koopman operator and combine them with spectral perturbation theory to provide strong guarantees for learning Koopman eigenvalues and eigenfunctions."}, "paperhash": {"value": "kostic|sharp_spectral_rates_for_koopman_operator_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6738/-/Revision", "NeurIPS.cc/2023/Conference/Submission6738/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6738/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325934334, "odate": 1698949736963, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "XfYpIaKDb6", "number": 6689, "cdate": 1683714852059, "tcdate": 1683714852059, "mdate": 1698949736597, "tmdate": 1698949736597, "signatures": ["NeurIPS.cc/2023/Conference/Submission6689/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6689/Authors"], "forum": "XfYpIaKDb6", "content": {"title": {"value": "On the Minimax Regret for Online Learning with Feedback Graphs"}, "authors": {"value": ["Khaled Eldowa", "Emmanuel Esposito", "Tommaso Cesari", "Nicol\u00f2 Cesa-Bianchi"]}, "authorids": {"value": ["~Khaled_Eldowa1", "~Emmanuel_Esposito1", "~Tommaso_Cesari1", "~Nicol\u00f2_Cesa-Bianchi1"]}, "keywords": {"value": ["Online learning", "Feedback graphs", "Multiarmed bandits"]}, "TLDR": {"value": "We improve on the upper and lower bounds for the regret of online learning with undirected feedback graphs"}, "abstract": {"value": "In this work, we improve on the upper and lower bounds for the regret of online learning with strongly observable undirected feedback graphs. The best known upper bound for this problem is $\\mathcal{O}\\bigl(\\sqrt{\\alpha T\\ln K}\\bigr)$, where $K$ is the number of actions, $\\alpha$ is the independence number of the graph, and $T$ is the time horizon. The $\\sqrt{\\ln K}$ factor is known to be necessary when $\\alpha = 1$ (the experts case). On the other hand, when $\\alpha = K$ (the bandits case), the minimax rate is known to be $\\Theta\\bigl(\\sqrt{KT}\\bigr)$, and a lower bound $\\Omega\\bigl(\\sqrt{\\alpha T}\\bigr)$ is known to hold for any $\\alpha$. Our improved upper bound $\\mathcal{O}\\bigl(\\sqrt{\\alpha T(1+\\ln(K/\\alpha))}\\bigr)$ holds for any $\\alpha$ and matches the lower bounds for bandits and experts, while interpolating intermediate cases. To prove this result, we use FTRL with $q$-Tsallis entropy for a carefully chosen value of $q \\in [1/2, 1)$ that varies with $\\alpha$. The analysis of this algorithm requires a new bound on the variance term in the regret. We also show how to extend our techniques to time-varying graphs, without requiring prior knowledge of their independence numbers. Our upper bound is complemented by an improved $\\Omega\\bigl(\\sqrt{\\alpha T(\\ln K)/(\\ln\\alpha)}\\bigr)$ lower bound for all $\\alpha > 1$, whose analysis relies on a novel reduction to multitask learning. This shows that a logarithmic factor is necessary as soon as $\\alpha < K$."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f68a5f5f194f3ecde7130a64e9e97d6e8b24d3b3.pdf"}, "supplementary_material": {"value": "/attachment/9d1736323705271f6557b9dfbd32f534f2bbf0fe.pdf"}, "_bibtex": {"value": "@inproceedings{\neldowa2023on,\ntitle={On the Minimax Regret for Online Learning with Feedback Graphs},\nauthor={Khaled Eldowa and Emmanuel Esposito and Tommaso Cesari and Nicol{\\`o} Cesa-Bianchi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=XfYpIaKDb6}\n}"}, "paperhash": {"value": "eldowa|on_the_minimax_regret_for_online_learning_with_feedback_graphs"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6689/-/Revision", "NeurIPS.cc/2023/Conference/Submission6689/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6689/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325933021, "odate": 1698949736584, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "BDno5qWEFh", "number": 6577, "cdate": 1683711748547, "tcdate": 1683711748547, "mdate": 1698949735884, "tmdate": 1698949735884, "signatures": ["NeurIPS.cc/2023/Conference/Submission6577/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6577/Authors"], "forum": "BDno5qWEFh", "content": {"title": {"value": "Multi-Object Representation Learning via Feature Connectivity and Object-Centric Regularization"}, "authors": {"value": ["Alex Foo", "Wynne Hsu", "Mong-Li Lee"]}, "authorids": {"value": ["~Alex_Foo1", "~Wynne_Hsu1", "~Mong-Li_Lee1"]}, "keywords": {"value": ["Object-Centric Learning", "Multi-Object Representation Learning"]}, "TLDR": {"value": "We leverage on feature connectivity to perform fine-grained object discovery and design two object-centric regularization terms as an alternative to the pixel-based reconstruction loss used in state-of-the-art methods."}, "abstract": {"value": "Discovering object-centric representations from images has the potential to greatly improve the robustness, sample efficiency and interpretability of machine learning algorithms. Current works on multi-object images typically follow a generative approach that optimizes for input reconstruction and fail to scale to real-world datasets despite significant increases in model capacity. We address this limitation by proposing a novel method that leverages feature connectivity to cluster neighboring pixels likely to belong to the same object. We further design two object-centric regularization terms to refine object representations in the latent space, enabling our approach to scale to complex real-world images. Experimental results on simulated, real-world, complex texture and common object images demonstrate a substantial improvement in the quality of discovered objects compared to state-of-the-art methods, as well as the sample efficiency and generalizability of our approach. We also show that the discovered object-centric representations can accurately predict key object properties in downstream tasks, highlighting the potential of our method to advance the field of multi-object representation learning."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6a5d80dd95d1c4ec6b1c34de777e70fc5ba44875.pdf"}, "supplementary_material": {"value": "/attachment/e4faf9887eb40d963e28a736682aa3f66506889e.zip"}, "_bibtex": {"value": "@inproceedings{\nfoo2023multiobject,\ntitle={Multi-Object Representation Learning via Feature Connectivity and Object-Centric Regularization},\nauthor={Alex Foo and Wynne Hsu and Mong-Li Lee},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BDno5qWEFh}\n}"}, "paperhash": {"value": "foo|multiobject_representation_learning_via_feature_connectivity_and_objectcentric_regularization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6577/-/Revision", "NeurIPS.cc/2023/Conference/Submission6577/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6577/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325929966, "odate": 1698949735868, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "aRBa0lSxEB", "number": 6504, "cdate": 1683710016066, "tcdate": 1683710016066, "mdate": 1698949735559, "tmdate": 1698949735559, "signatures": ["NeurIPS.cc/2023/Conference/Submission6504/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6504/Authors"], "forum": "aRBa0lSxEB", "content": {"title": {"value": "A Dynamical System View of Langevin-Based Non-Convex Sampling"}, "authors": {"value": ["Mohammad Reza Karimi Jaghargh", "Ya-Ping Hsieh", "Andreas Krause"]}, "authorids": {"value": ["~Mohammad_Reza_Karimi_Jaghargh1", "~Ya-Ping_Hsieh1", "~Andreas_Krause1"]}, "keywords": {"value": ["Non-Convex Sampling", "Langevin Dynamics", "Dynamical Systems"]}, "TLDR": {"value": "We develop a novel framework that guarantees last-iterate convergence in Wasserstein distance for many advanced sampling algorithms at once."}, "abstract": {"value": "Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain some important challenges: Existing guarantees suffer from the drawback of lacking guarantees for the last-iterates, and little is known beyond the elementary schemes of stochastic gradient Langevin dynamics. To address these issues, we develop a novel framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yields the last-iterate Wasserstein convergence of many advanced sampling schemes such as mirror Langevin, proximal, randomized mid-point, and Runge-Kutta methods."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6782a47e92873670ddd20d488388398f68f24bee.pdf"}, "supplementary_material": {"value": "/attachment/82ed29d5f04d741ed9d1ed3c2b7f8563c61d1fd8.pdf"}, "_bibtex": {"value": "@inproceedings{\njaghargh2023a,\ntitle={A Dynamical System View of Langevin-Based Non-Convex Sampling},\nauthor={Mohammad Reza Karimi Jaghargh and Ya-Ping Hsieh and Andreas Krause},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=aRBa0lSxEB}\n}"}, "paperhash": {"value": "jaghargh|a_dynamical_system_view_of_langevinbased_nonconvex_sampling"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6504/-/Revision", "NeurIPS.cc/2023/Conference/Submission6504/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6504/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325927608, "odate": 1698949735546, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "t7ozN4AXd0", "number": 6483, "cdate": 1683709632534, "tcdate": 1683709632534, "mdate": 1698949735461, "tmdate": 1698949735461, "signatures": ["NeurIPS.cc/2023/Conference/Submission6483/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6483/Authors"], "forum": "t7ozN4AXd0", "content": {"title": {"value": "Rewiring Neurons in Non-Stationary Environments"}, "authors": {"value": ["Zhicheng Sun", "Yadong MU"]}, "authorids": {"value": ["~Zhicheng_Sun1", "~Yadong_MU1"]}, "keywords": {"value": ["continual learning", "reinforcement learning", "brain-inspired learning"]}, "TLDR": {"value": "We propose a new rewiring approach for continual reinforcement learning."}, "abstract": {"value": "The human brain rewires itself for neuroplasticity in the presence of new tasks. We are inspired to harness this key process in continual reinforcement learning, prioritizing adaptation to non-stationary environments. In distinction to existing rewiring approaches that rely on pruning or dynamic routing, which may limit network capacity and plasticity, this work presents a novel rewiring scheme by permuting hidden neurons. Specifically, the neuron permutation is parameterized to be end-to-end learnable and can rearrange all available synapses to explore a large span of weight space, thereby promoting adaptivity. In addition, we introduce two main designs to steer the rewiring process in continual reinforcement learning: first, a multi-mode rewiring strategy is proposed which diversifies the policy and encourages exploration when encountering new environments. Secondly, to ensure stability on history tasks, the network is devised to cache each learned wiring while subtly updating its weights, allowing for retrospective recovery of any previous state appropriate for the task. Meanwhile, an alignment mechanism is curated to achieve better plasticity-stability tradeoff by jointly optimizing cached wirings and weights. Our proposed method is comprehensively evaluated on 18 continual reinforcement learning scenarios ranging from locomotion to manipulation, demonstrating its advantages over state-of-the-art competitors in performance-efficiency tradeoffs. Code is available at https://github.com/feifeiobama/RewireNeuron."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/85f193c9b9af583af8f8dfbbef12c329e8884955.pdf"}, "supplementary_material": {"value": "/attachment/f46287ba07e648c1a3d91ec1b2c393adc5bcadd0.pdf"}, "_bibtex": {"value": "@inproceedings{\nsun2023rewiring,\ntitle={Rewiring Neurons in Non-Stationary Environments},\nauthor={Zhicheng Sun and Yadong MU},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=t7ozN4AXd0}\n}"}, "paperhash": {"value": "sun|rewiring_neurons_in_nonstationary_environments"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6483/-/Revision", "NeurIPS.cc/2023/Conference/Submission6483/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6483/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325927122, "odate": 1698949735448, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "H2SuXHbFIn", "number": 6361, "cdate": 1683706611538, "tcdate": 1683706611538, "mdate": 1698949734868, "tmdate": 1698949734868, "signatures": ["NeurIPS.cc/2023/Conference/Submission6361/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6361/Authors"], "forum": "H2SuXHbFIn", "content": {"title": {"value": "Tree-Based Diffusion Schr\u00f6dinger Bridge with Applications to Wasserstein Barycenters"}, "authors": {"value": ["Maxence Noble", "Valentin De Bortoli", "Arnaud Doucet", "Alain Durmus"]}, "authorids": {"value": ["~Maxence_Noble1", "~Valentin_De_Bortoli1", "~Arnaud_Doucet2", "~Alain_Durmus1"]}, "keywords": {"value": ["Schr\u00f6dinger bridge", "optimal transport", "diffusion model", "Wasserstein barycenter"]}, "abstract": {"value": "Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at minimizing the integral of a cost function with respect to a distribution with some prescribed marginals. In this paper, we consider an entropic version of mOT\n  with a tree-structured quadratic cost, i.e., a function that can be written as\n  a sum of pairwise cost functions between the nodes of a tree. To address this\n  problem, we develop Tree-based Diffusion Schr\\\"odinger Bridge (TreeDSB), an\n  extension of the Diffusion Schr\\\"odinger Bridge (DSB) algorithm. TreeDSB\n  corresponds to a dynamic and continuous state-space counterpart of the\n  multimarginal Sinkhorn algorithm. A notable use case of our methodology is to\n  compute Wasserstein barycenters which can be recast as the solution of a mOT\n  problem on a star-shaped tree. We demonstrate that our methodology can be applied in high-dimensional settings such as image interpolation and\n  Bayesian fusion."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5d975ca8415394660e39189ee3b48b04fb15cb4c.pdf"}, "supplementary_material": {"value": "/attachment/b978ff4245ec616f5708ee95e9df6a74d4f0a316.pdf"}, "_bibtex": {"value": "@inproceedings{\nnoble2023treebased,\ntitle={Tree-Based Diffusion Schr\\\"odinger Bridge with Applications to Wasserstein Barycenters},\nauthor={Maxence Noble and Valentin De Bortoli and Arnaud Doucet and Alain Durmus},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=H2SuXHbFIn}\n}"}, "paperhash": {"value": "noble|treebased_diffusion_schr\u00f6dinger_bridge_with_applications_to_wasserstein_barycenters"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6361/-/Revision", "NeurIPS.cc/2023/Conference/Submission6361/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6361/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325923337, "odate": 1698949734857, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "LVHEcVgEGm", "number": 6170, "cdate": 1683702161526, "tcdate": 1683702161526, "mdate": 1698949733908, "tmdate": 1698949733908, "signatures": ["NeurIPS.cc/2023/Conference/Submission6170/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6170/Authors"], "forum": "LVHEcVgEGm", "content": {"title": {"value": "Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels"}, "authors": {"value": ["Zebin You", "Yong Zhong", "Fan Bao", "Jiacheng Sun", "Chongxuan Li", "Jun Zhu"]}, "authorids": {"value": ["~Zebin_You1", "~Yong_Zhong2", "~Fan_Bao1", "~Jiacheng_Sun1", "~Chongxuan_Li1", "~Jun_Zhu2"]}, "keywords": {"value": ["diffusion models", "semi-supervised generation", "semi-supervised diffusion models", "semi-supervised classification", "image generation."]}, "TLDR": {"value": "Diffusion models and semi-supervised learners can benefit mutually to achieve SOTA semi-supervised classification and generation results on ImageNet-1K and CIFAR-10 with few labels."}, "abstract": {"value": "In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called *dual pseudo training* (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images.  Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\u00e9chet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet $256\\times256$. Besides, DPT outperforms competitive semi-supervised baselines substantially on  ImageNet classification tasks, *achieving top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0)* with one, two, or five labels per class, respectively. Notably, our results demonstrate that diffusion can generate realistic images with only a few labels (e.g., $<0.1$%) and generative augmentation remains viable for semi-supervised classification. Our code is available at *https://github.com/ML-GSAI/DPT*."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/68aa90eb5bb4df0cb04699b05c7f98dda1a9704a.pdf"}, "supplementary_material": {"value": "/attachment/d58442eb7dd2a35be91e7a42b8eba267436b1540.zip"}, "_bibtex": {"value": "@inproceedings{\nyou2023diffusion,\ntitle={Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels},\nauthor={Zebin You and Yong Zhong and Fan Bao and Jiacheng Sun and Chongxuan Li and Jun Zhu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=LVHEcVgEGm}\n}"}, "paperhash": {"value": "you|diffusion_models_and_semisupervised_learners_benefit_mutually_with_few_labels"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6170/-/Revision", "NeurIPS.cc/2023/Conference/Submission6170/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6170/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325917771, "odate": 1698949733897, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "NBMIsOS6B7", "number": 6143, "cdate": 1683701615970, "tcdate": 1683701615970, "mdate": 1698949733862, "tmdate": 1698949733862, "signatures": ["NeurIPS.cc/2023/Conference/Submission6143/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission6143/Authors"], "forum": "NBMIsOS6B7", "content": {"title": {"value": "Alternation makes the adversary weaker in two-player games"}, "authors": {"value": ["Volkan Cevher", "Ashok Cutkosky", "Ali Kavis", "Georgios Piliouras", "Stratis Skoulakis", "Luca Viano"]}, "authorids": {"value": ["~Volkan_Cevher1", "~Ashok_Cutkosky1", "~Ali_Kavis1", "~Georgios_Piliouras1", "~Stratis_Skoulakis2", "~Luca_Viano1"]}, "keywords": {"value": ["Online Learning", "Regret Minimization", "Game Theory"]}, "abstract": {"value": "Motivated by alternating game-play in two-player games, we study an altenating variant of the \\textit{Online Linear Optimization} (OLO). In alternating OLO,  a \\textit{learner} at each round $t \\in [n]$ selects a vector $x^t$ and then an \\textit{adversary} selects a cost-vector $c^t \\in [-1,1]^n$. The learner then experiences cost $(c^t + c^{t-1})^\\top x^t$ instead of $(c^t)^\\top x^t$ as in standard OLO. We establish that under this small twist, the $\\Omega(\\sqrt{T})$ lower bound on the regret is no longer valid. More precisely, we present two online learning algorithms for alternating OLO that respectively admit $\\mathcal{O}((\\log n)^{4/3} T^{1/3})$ regret for the $n$-dimensional simplex and $\\mathcal{O}(\\rho \\log T)$ regret for the ball of radius $\\rho>0$. Our results imply that in alternating game-play, an agent can always guarantee $\\mathcal{\\tilde{O}}((\\log n)^{4/3} T^{1/3})$ regardless the strategies of the other agent while the regret bound improves to $\\mathcal{O}(\\log T)$ in case the agent admits only two actions."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/89ca0f45581631c8511218aed59f80229109e6ae.pdf"}, "supplementary_material": {"value": "/attachment/e495930ca8604010e40e12b7287dda23a0fc3fbc.pdf"}, "_bibtex": {"value": "@inproceedings{\ncevher2023alternation,\ntitle={Alternation makes the adversary weaker in two-player games},\nauthor={Volkan Cevher and Ashok Cutkosky and Ali Kavis and Georgios Piliouras and Stratis Skoulakis and Luca Viano},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=NBMIsOS6B7}\n}"}, "paperhash": {"value": "cevher|alternation_makes_the_adversary_weaker_in_twoplayer_games"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission6143/-/Revision", "NeurIPS.cc/2023/Conference/Submission6143/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission6143/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325916989, "odate": 1698949733848, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "6iouUxI45W", "number": 5994, "cdate": 1683697491326, "tcdate": 1683697491326, "mdate": 1698949733083, "tmdate": 1698949733083, "signatures": ["NeurIPS.cc/2023/Conference/Submission5994/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5994/Authors"], "forum": "6iouUxI45W", "content": {"title": {"value": "The Exact Sample Complexity Gain from Invariances for Kernel Regression"}, "authors": {"value": ["Behrooz Tahmasebi", "Stefanie Jegelka"]}, "authorids": {"value": ["~Behrooz_Tahmasebi1", "~Stefanie_Jegelka3"]}, "keywords": {"value": ["invariances", "manifolds", "sample complexity"]}, "TLDR": {"value": "We prove minimax optimal rates for kernel ridge regression with group invariances on manifolds."}, "abstract": {"value": "In practice, encoding invariances into models improves sample complexity. In this work, we study this phenomenon from a theoretical perspective. In particular, we provide minimax optimal rates for kernel ridge regression on compact manifolds, with a target function that is invariant to a group action on the manifold. Our results hold for any smooth compact Lie group action, even groups of positive dimension. For a finite group, the gain effectively multiplies the number of samples by the group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes the viewpoint of differential geometry, in contrast to the more common strategy of using invariant polynomials. This new geometric viewpoint on learning with invariances may be of independent interest."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5077fdac57f6416c42c827728abd52287c51d900.pdf"}, "_bibtex": {"value": "@inproceedings{\ntahmasebi2023the,\ntitle={The Exact Sample Complexity Gain from Invariances for Kernel Regression},\nauthor={Behrooz Tahmasebi and Stefanie Jegelka},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=6iouUxI45W}\n}"}, "paperhash": {"value": "tahmasebi|the_exact_sample_complexity_gain_from_invariances_for_kernel_regression"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5994/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5994/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325912987, "odate": 1698949733070, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "vIGNYQ4Alv", "number": 5911, "cdate": 1683694421993, "tcdate": 1683694421993, "mdate": 1698949732550, "tmdate": 1698949732550, "signatures": ["NeurIPS.cc/2023/Conference/Submission5911/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5911/Authors"], "forum": "vIGNYQ4Alv", "content": {"title": {"value": "Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization"}, "authors": {"value": ["Ruichen Jiang", "Aryan Mokhtari"]}, "authorids": {"value": ["~Ruichen_Jiang1", "~Aryan_Mokhtari3"]}, "keywords": {"value": ["convex optimization", "quasi-Newton methods", "Monteiro-Svaiter acceleration", "Nesterov's accelerated gradient", "online learning"]}, "TLDR": {"value": "We propose a quasi-Newton-type method that converges faster than Nesterov's accelerated gradient in the \"small dimension\" regime, while matching its optimal rate in the \"large dimension\" regime."}, "abstract": {"value": "In this paper, we propose an accelerated quasi-Newton proximal extragradient method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective, we prove that our method can achieve a convergence rate of $\\mathcal{O}\\bigl(\\min\\\\{\\frac{1}{k^2}, \\frac{\\sqrt{d\\log k}}{k^{2.5}}\\\\}\\bigr)$, where $d$ is the problem dimension and $k$ is the number of iterations. In particular, in the regime where $k = \\mathcal{O}(d)$, our method matches the _optimal rate_ of $\\mathcal{O}(\\frac{1}{k^2})$ by Nesterov's accelerated gradient (NAG). Moreover, in the the regime where $k = \\Omega(d \\log d)$, it outperforms NAG and converges at a _faster rate_ of $\\mathcal{O}\\bigl(\\frac{\\sqrt{d\\log k}}{k^{2.5}}\\bigr)$. To the best of our knowledge, this result is the first to demonstrate a provable gain for a quasi-Newton-type method over NAG in the convex setting.  To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an online learning perspective to update the Hessian approximation matrices, in which we relate the convergence rate of our method to the dynamic regret of a specific online convex optimization problem in the space of matrices."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/52958aaf54ccde1d58f81829d0a71f1f027d1f66.pdf"}, "supplementary_material": {"value": "/attachment/132c301eef324dd54f79a55d677d5f5944752f7f.zip"}, "_bibtex": {"value": "@inproceedings{\njiang2023accelerated,\ntitle={Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization},\nauthor={Ruichen Jiang and Aryan Mokhtari},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vIGNYQ4Alv}\n}"}, "paperhash": {"value": "jiang|accelerated_quasinewton_proximal_extragradient_faster_rate_for_smooth_convex_optimization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5911/-/Revision", "NeurIPS.cc/2023/Conference/Submission5911/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5911/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325910842, "odate": 1698949732538, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "SoLebIqHgZ", "number": 5910, "cdate": 1683694391733, "tcdate": 1683694391733, "mdate": 1698949732529, "tmdate": 1698949732529, "signatures": ["NeurIPS.cc/2023/Conference/Submission5910/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5910/Authors"], "forum": "SoLebIqHgZ", "content": {"title": {"value": "ARTree: A Deep Autoregressive Model for Phylogenetic Inference"}, "authors": {"value": ["Tianyu Xie", "Cheng Zhang"]}, "authorids": {"value": ["~Tianyu_Xie1", "~Cheng_Zhang3"]}, "keywords": {"value": ["phylogenetic inference", "autoregressive model", "graph neural network", "density estimation", "variational inference"]}, "abstract": {"value": "Designing flexible probabilistic models over tree topologies is important for developing efficient phylogenetic inference methods. To do that, previous works often leverage the similarity of tree topologies via hand-engineered heuristic features which would require domain expertise and may suffer from limited approximation capability. In this paper, we propose a deep autoregressive model for phylogenetic inference based on graph neural networks (GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf node addition operations and modeling the involved conditional distributions based on learnable topological features via GNNs, ARTree can provide a rich family of distributions over tree topologies that have simple sampling algorithms, without using heuristic features. We demonstrate the effectiveness and efficiency of our method on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "A novel deep autoregressive model for phylogenetic inference"}, "pdf": {"value": "/pdf/45f1b2d6eed555832cea2cbf81c5d5ab2812efd1.pdf"}, "supplementary_material": {"value": "/attachment/f0327090008b982d83ba3788edd4ca5723b8d047.zip"}, "_bibtex": {"value": "@inproceedings{\nxie2023artree,\ntitle={{ART}ree: A Deep Autoregressive Model for Phylogenetic Inference},\nauthor={Tianyu Xie and Cheng Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=SoLebIqHgZ}\n}"}, "paperhash": {"value": "xie|artree_a_deep_autoregressive_model_for_phylogenetic_inference"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5910/-/Revision", "NeurIPS.cc/2023/Conference/Submission5910/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5910/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325910795, "odate": 1698949732516, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "d47iuwOt3j", "number": 5837, "cdate": 1683691710385, "tcdate": 1683691710385, "mdate": 1698949731877, "tmdate": 1698949731877, "signatures": ["NeurIPS.cc/2023/Conference/Submission5837/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5837/Authors"], "forum": "d47iuwOt3j", "content": {"title": {"value": "On the Gini-impurity Preservation For Privacy Random Forests"}, "authors": {"value": ["XinRan Xie", "Man-Jie Yuan", "Xuetong Bai", "Wei Gao", "Zhi-Hua Zhou"]}, "authorids": {"value": ["~XinRan_Xie1", "~Man-Jie_Yuan1", "~Xuetong_Bai1", "~Wei_Gao7", "~Zhi-Hua_Zhou2"]}, "keywords": {"value": ["classification", "random forests", "privacy-preserving machine learng", "data encrytion"]}, "abstract": {"value": "Random forests have been one successful ensemble algorithms in machine learning. Various techniques have been utilized to preserve the privacy of random forests from anonymization, differential privacy, homomorphic encryption, etc., whereas it rarely takes into account some crucial ingredients of learning algorithm. This work presents a new encryption to preserve data's Gini impurity, which plays a crucial role during the construction of random forests. Our basic idea is to modify the structure of binary search tree to store several examples in each node,  and encrypt data features by incorporating label and order information. Theoretically, we prove that our scheme preserves the minimum Gini impurity in ciphertexts without decrypting, and present the security guarantee for encryption. For random forests, we encrypt data features based on our Gini-impurity-preserving scheme, and take the homomorphic encryption scheme CKKS to encrypt data labels  due to their importance and privacy. We  conduct extensive experiments to show the effectiveness, efficiency and security of our proposed method."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/dd65025c5d1e578319f7bf0db1615113ad348f16.pdf"}, "supplementary_material": {"value": "/attachment/993e6d9da65469fbbe74e0277c511718921b5ab3.zip"}, "_bibtex": {"value": "@inproceedings{\nxie2023on,\ntitle={On the Gini-impurity Preservation For Privacy Random Forests},\nauthor={XinRan Xie and Man-Jie Yuan and Xuetong Bai and Wei Gao and Zhi-Hua Zhou},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=d47iuwOt3j}\n}"}, "paperhash": {"value": "xie|on_the_giniimpurity_preservation_for_privacy_random_forests"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5837/-/Revision", "NeurIPS.cc/2023/Conference/Submission5837/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5837/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325908706, "odate": 1698949731849, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "konBXvt2iS", "number": 5827, "cdate": 1683691322391, "tcdate": 1683691322391, "mdate": 1698954278585, "tmdate": 1698954278585, "signatures": ["NeurIPS.cc/2023/Conference/Submission5827/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5827/Authors"], "forum": "konBXvt2iS", "content": {"title": {"value": "Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks"}, "authors": {"value": ["Mingze Wang", "Chao Ma"]}, "authorids": {"value": ["~Mingze_Wang2", "~Chao_Ma8"]}, "keywords": {"value": ["non-convex optimization", "training dynamics", "neural network"]}, "TLDR": {"value": "We conduct a complete theoretical characterization of the whole training process and nonlinear behaviors of ReLU networks trained by Gradient Flow on a linearly separable data."}, "abstract": {"value": "The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. \nThe nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). \nIn this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. \nDespite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend.\nSpecific nonlinear behaviors can also be precisely identified and captured theoretically, such as\ninitial condensation, saddle-to-plateau dynamics, plateau escape, changes of activation patterns, \nlearning with increasing complexity, etc."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e3ca579635d4b3bc247e96b2cf90ef921c50a4f9.pdf"}, "supplementary_material": {"value": "/attachment/3f8fb63cb7aec5cc87bc9814be613378e6d4bed4.zip"}, "_bibtex": {"value": "@inproceedings{\nwang2023understanding,\ntitle={Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of Re{LU} Networks},\nauthor={Mingze Wang and Chao Ma},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=konBXvt2iS}\n}"}, "paperhash": {"value": "wang|understanding_multiphase_optimization_dynamics_and_rich_nonlinear_behaviors_of_relu_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5827/-/Revision", "NeurIPS.cc/2023/Conference/Submission5827/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/Submission5827/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325908402, "odate": 1698949731786, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "oaCDiKoJ2w", "number": 5804, "cdate": 1683690892798, "tcdate": 1683690892798, "mdate": 1698949731561, "tmdate": 1698949731561, "signatures": ["NeurIPS.cc/2023/Conference/Submission5804/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5804/Authors"], "forum": "oaCDiKoJ2w", "content": {"title": {"value": "Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts"}, "authors": {"value": ["Chaoqi Wang", "Ziyu Ye", "Zhe Feng", "Ashwinkumar Badanidiyuru", "Haifeng Xu"]}, "authorids": {"value": ["~Chaoqi_Wang1", "~Ziyu_Ye1", "~Zhe_Feng3", "~Ashwinkumar_Badanidiyuru1", "~Haifeng_Xu1"]}, "keywords": {"value": ["linear stochastic bandits", "online learning", "partial information", "contextual bandits"]}, "TLDR": {"value": "This study presents a new algorithm that improves the contextual bandit problem by using post-serving context, demonstrating superior performance on various data sets."}, "abstract": {"value": "Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which additional valuable contexts can be observed after arm selection. For example, content recommendation platforms like Youtube, Instagram, Tiktok receive much additional features about a user's reward after the user clicks a content (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications,  we  study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB,  that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate  noise in data. Such robustification is necessary for tackling our problem, though we believe it could also be of general interest.\nExtensive empirical tests on both synthetic and real-world datasets  demonstrate the significant benefit of utilitzing post-serving contexts as well as the superior performance of  our   algorithm over the state-of-the-art approaches."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/27b692ccfb4049b4dc226f20715337b97df31df0.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023followups,\ntitle={Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts},\nauthor={Chaoqi Wang and Ziyu Ye and Zhe Feng and Ashwinkumar Badanidiyuru and Haifeng Xu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=oaCDiKoJ2w}\n}"}, "paperhash": {"value": "wang|followups_also_matter_improving_contextual_bandits_via_postserving_contexts"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5804/-/Revision", "NeurIPS.cc/2023/Conference/Submission5804/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5804/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325907724, "odate": 1698949731549, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "4hturzLcKX", "number": 5730, "cdate": 1683688731373, "tcdate": 1683688731373, "mdate": 1698949731007, "tmdate": 1698949731007, "signatures": ["NeurIPS.cc/2023/Conference/Submission5730/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5730/Authors"], "forum": "4hturzLcKX", "content": {"title": {"value": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"}, "authors": {"value": ["Yann Dubois", "Xuechen Li", "Rohan Taori", "Tianyi Zhang", "Ishaan Gulrajani", "Jimmy Ba", "Carlos Guestrin", "Percy Liang", "Tatsunori Hashimoto"]}, "authorids": {"value": ["~Yann_Dubois1", "~Xuechen_Li1", "~Rohan_Taori1", "~Tianyi_Zhang2", "~Ishaan_Gulrajani1", "~Jimmy_Ba1", "~Carlos_Guestrin1", "~Percy_Liang1", "~Tatsunori_Hashimoto1"]}, "keywords": {"value": ["Instruction-Following", "Reinforcement Learning from Human Feedback", "Artificial General Intelligence", "Large Language Models"]}, "abstract": {"value": "Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well.\nDeveloping these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these bottlenecks with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM based simulator for human feedback that is 45x cheaper than crowdworkers and displays high agreement with humans. Second, we identify an evaluation dataset representative of real-world instructions and propose an automatic evaluation procedure. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, among others) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% win-rate improvement against Davinci003."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose and validate a simulator that enables research on learning from human feedback at a low cost."}, "pdf": {"value": "/pdf/7d610f4f6ca0eb994463a59a2d6d9d87aff743db.pdf"}, "supplementary_material": {"value": "/attachment/66367f91c2c42cc09b79823a7cb80f309229e32f.pdf"}, "_bibtex": {"value": "@inproceedings{\ndubois2023alpacafarm,\ntitle={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback},\nauthor={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori Hashimoto},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=4hturzLcKX}\n}"}, "paperhash": {"value": "dubois|alpacafarm_a_simulation_framework_for_methods_that_learn_from_human_feedback"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5730/-/Revision", "NeurIPS.cc/2023/Conference/Submission5730/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5730/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325904683, "odate": 1698949730992, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "WHedsAeatp", "number": 5694, "cdate": 1683687795528, "tcdate": 1683687795528, "mdate": 1698954278195, "tmdate": 1698954278195, "signatures": ["NeurIPS.cc/2023/Conference/Submission5694/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5694/Authors"], "forum": "WHedsAeatp", "content": {"title": {"value": "Rank-N-Contrast: Learning Continuous Representations for Regression"}, "authors": {"value": ["Kaiwen Zha", "Peng Cao", "Jeany Son", "Yuzhe Yang", "Dina Katabi"]}, "authorids": {"value": ["~Kaiwen_Zha3", "~Peng_Cao1", "~Jeany_Son1", "~Yuzhe_Yang1", "~Dina_Katabi1"]}, "keywords": {"value": ["regression", "representation learning", "continuity"]}, "abstract": {"value": "Deep regression models typically learn in an end-to-end fashion without explicitly emphasizing a regression-aware representation. Consequently, the learned representations exhibit fragmentation and fail to capture the continuous nature of sample orders, inducing suboptimal results across a wide range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a framework that learns continuous representations for regression by contrasting samples against each other based on their rankings in the target space. We demonstrate, theoretically and empirically, that RNC guarantees the desired order of learned representations in accordance with the target orders, enjoying not only better performance but also significantly improved robustness, efficiency, and generalization. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify that RNC achieves state-of-the-art performance, highlighting its intriguing properties including better data efficiency, robustness to spurious targets and data corruptions, and generalization to distribution shifts."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a generic framework that learns continuous representations for regression, which not only boosts performance but also improves robustness, data efficiency, and generalization."}, "pdf": {"value": "/pdf/73dd35012df7d15b5a9f3a74eaacf052aeb71b20.pdf"}, "supplementary_material": {"value": "/attachment/129484ce22bb089b8bcde526aa6ee15d8a612030.zip"}, "_bibtex": {"value": "@inproceedings{\nzha2023rankncontrast,\ntitle={Rank-N-Contrast: Learning Continuous Representations for Regression},\nauthor={Kaiwen Zha and Peng Cao and Jeany Son and Yuzhe Yang and Dina Katabi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=WHedsAeatp}\n}"}, "paperhash": {"value": "zha|rankncontrast_learning_continuous_representations_for_regression"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5694/-/Revision", "NeurIPS.cc/2023/Conference/Submission5694/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5694/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325903141, "odate": 1698949730649, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "EjiA3uWpnc", "number": 5672, "cdate": 1683687031308, "tcdate": 1683687031308, "mdate": 1698949730487, "tmdate": 1698949730487, "signatures": ["NeurIPS.cc/2023/Conference/Submission5672/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5672/Authors"], "forum": "EjiA3uWpnc", "content": {"title": {"value": "Equivariant Neural Operator Learning with Graphon Convolution"}, "authors": {"value": ["Chaoran Cheng", "Jian Peng"]}, "authorids": {"value": ["~Chaoran_Cheng2", "~Jian_Peng1"]}, "keywords": {"value": ["Neural Operator Learning", "Spectral Graph Theory", "Graphon"]}, "TLDR": {"value": "We propose InfGCN, an equivariant neural operator learning architecture which can be interpreted as graphon convolution. InfGCN achieved SOTA performance across several electron density datasets."}, "abstract": {"value": "We propose a general architecture that combines the coefficient learning scheme with a residual operator layer for learning mappings between continuous functions in the 3D Euclidean space. Our proposed model is guaranteed to achieve SE(3)-equivariance by design. From the graph spectrum view, our method can be interpreted as convolution on graphons (dense graphs with infinitely many nodes), which we term InfGCN. By leveraging both the continuous graphon structure and the discrete graph structure of the input data, our model can effectively capture the geometric information while preserving equivariance. Through extensive experiments on large-scale electron density datasets, we observed that our model significantly outperformed the current state-of-the-art architectures. Multiple ablation studies were also carried out to demonstrate the effectiveness of the proposed architecture."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ecc421366c2e6dec32d90bf4ec0d4eaacee45f31.pdf"}, "supplementary_material": {"value": "/attachment/f2ef8b7f6b5be9420c2c1ca4be792db7c06b5ddd.pdf"}, "_bibtex": {"value": "@inproceedings{\ncheng2023equivariant,\ntitle={Equivariant Neural Operator Learning with Graphon Convolution},\nauthor={Chaoran Cheng and Jian Peng},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=EjiA3uWpnc}\n}"}, "paperhash": {"value": "cheng|equivariant_neural_operator_learning_with_graphon_convolution"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5672/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5672/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325901481, "odate": 1698949730476, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "RUCFAKNDb2", "number": 5656, "cdate": 1683686365204, "tcdate": 1683686365204, "mdate": 1698949730383, "tmdate": 1698949730383, "signatures": ["NeurIPS.cc/2023/Conference/Submission5656/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5656/Authors"], "forum": "RUCFAKNDb2", "content": {"title": {"value": "Promises and Pitfalls of Threshold-based Auto-labeling"}, "authors": {"value": ["Harit Vishwakarma", "Heguang Lin", "Frederic Sala", "Ramya Korlakai Vinayak"]}, "authorids": {"value": ["~Harit_Vishwakarma1", "~Heguang_Lin1", "~Frederic_Sala1", "~Ramya_Korlakai_Vinayak1"]}, "keywords": {"value": ["Auto Labeling", "Active Learning", "Selective Classification"]}, "abstract": {"value": "Creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine learning workflows. Threshold-based auto-labeling (TBAL), where validation data obtained from humans is used to find a confidence threshold above which the data is machine-labeled, reduces reliance on manual annotation. TBAL is emerging as a widely-used solution in practice. Given the long shelf-life and diverse usage of the resulting datasets, understanding when the data obtained by such auto-labeling systems can be relied on is crucial. This is the first work to analyze TBAL systems and derive sample complexity bounds on the amount of human-labeled validation data required for guaranteeing the quality of machine-labeled data. Our results provide two crucial insights. First, reasonable chunks of unlabeled data can be automatically and accurately labeled by seemingly bad models. Second, a hidden downside of TBAL systems is potentially prohibitive validation data usage. Together, these insights describe the promise and pitfalls of using such systems. \nWe validate our theoretical guarantees with extensive experiments on synthetic and real datasets."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "In this work we analyze threshold-based auto-labeling (TBAL) that is getting widely used in practice for data labeling. Our work provides insights on when machine-labeled data obtained through TBAL can be relied on."}, "pdf": {"value": "/pdf/71e898ba8f9ff1703eefe808963dad69f323fd93.pdf"}, "supplementary_material": {"value": "/attachment/2eb55f91bd340a19457e1ecfb3ad74991aeb2524.pdf"}, "_bibtex": {"value": "@inproceedings{\nvishwakarma2023promises,\ntitle={Promises and Pitfalls of Threshold-based Auto-labeling},\nauthor={Harit Vishwakarma and Heguang Lin and Frederic Sala and Ramya Korlakai Vinayak},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=RUCFAKNDb2}\n}"}, "paperhash": {"value": "vishwakarma|promises_and_pitfalls_of_thresholdbased_autolabeling"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5656/-/Revision", "NeurIPS.cc/2023/Conference/Submission5656/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5656/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325899832, "odate": 1698949730369, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "6BZS2EAkns", "number": 5631, "cdate": 1683685565733, "tcdate": 1683685565733, "mdate": 1698949730170, "tmdate": 1698949730170, "signatures": ["NeurIPS.cc/2023/Conference/Submission5631/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5631/Authors"], "forum": "6BZS2EAkns", "content": {"title": {"value": "In-Context Learning Unlocked for Diffusion Models"}, "authors": {"value": ["Zhendong Wang", "Yifan Jiang", "Yadong Lu", "yelong shen", "Pengcheng He", "Weizhu Chen", "Zhangyang Wang", "Mingyuan Zhou"]}, "authorids": {"value": ["~Zhendong_Wang1", "~Yifan_Jiang2", "~Yadong_Lu1", "~yelong_shen1", "~Pengcheng_He2", "~Weizhu_Chen1", "~Zhangyang_Wang1", "~Mingyuan_Zhou1"]}, "keywords": {"value": ["diffusion models", "in-context learning"]}, "TLDR": {"value": "Unlock the in-context learning ability of diffusion models with vision-language prompt."}, "abstract": {"value": "We present Prompt Diffusion, a framework for enabling in-context learning in diffusion-based generative models. Given a pair of task-specific example images, such as depth from/to image and scribble from/to image, and a text guidance, our model automatically understands the underlying task and performs the same task on a new query image following the text guidance. To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input. The diffusion model is trained jointly on six different tasks using these prompts. The resulting Prompt Diffusion model becomes the first diffusion-based vision-language foundation model capable of in-context learning. It demonstrates high-quality in-context generation for the trained tasks and effectively generalizes to new, unseen vision tasks using their respective prompts. Our model also shows compelling text-guided image editing results. Our framework aims to facilitate research into in-context learning for computer vision. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Prompt-Diffusion."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/58acc4043bd94b3d36c2ffcd85f41d144e42cf30.pdf"}, "supplementary_material": {"value": "/attachment/76b23d4a40035d51fe5a19db0bd4774df3abd2f7.zip"}, "_bibtex": {"value": "@inproceedings{\nwang2023incontext,\ntitle={In-Context Learning Unlocked for Diffusion Models},\nauthor={Zhendong Wang and Yifan Jiang and Yadong Lu and yelong shen and Pengcheng He and Weizhu Chen and Zhangyang Wang and Mingyuan Zhou},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=6BZS2EAkns}\n}"}, "paperhash": {"value": "wang|incontext_learning_unlocked_for_diffusion_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5631/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5631/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325898707, "odate": 1698949730157, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "D9CMRR5Lof", "number": 5629, "cdate": 1683685524519, "tcdate": 1683685524519, "mdate": 1698949730141, "tmdate": 1698949730141, "signatures": ["NeurIPS.cc/2023/Conference/Submission5629/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5629/Authors"], "forum": "D9CMRR5Lof", "content": {"title": {"value": "MGDD: A Meta Generator for Fast Dataset Distillation"}, "authors": {"value": ["Songhua Liu", "Xinchao Wang"]}, "authorids": {"value": ["~Songhua_Liu2", "~Xinchao_Wang1"]}, "keywords": {"value": ["Dataset Distillation", "Dataset Condensation", "Efficient Learning", "Conditional Generation", "Meta Learning"]}, "TLDR": {"value": "We propose a meta learning algorithm to train a meta generator to generate dataset distillation results efficiently so that the distilled datasets can be generated in a one-stop feed-forward inference after adaptation."}, "abstract": {"value": "Existing dataset distillation (DD) techniques typically rely on iterative strategies to synthesize condensed datasets, where datasets before and after distillation are forward and backward through neural networks a massive number of times. Despite the promising results achieved, the time efficiency of prior approaches is still far from satisfactory. Moreover, when different sizes of synthetic datasets are required, they have to repeat the iterative training procedures, which is highly cumbersome and lacks flexibility. In this paper, different from the time-consuming forward-backward passes, we introduce a generative fashion for dataset distillation with significantly improved efficiency. Specifically, synthetic samples are produced by a generator network conditioned on the initialization of DD, while synthetic labels are obtained by solving a least-squares problem in a feature space. Our theoretical analysis reveals that the errors of synthetic datasets solved in the original space and then processed by any conditional generators are upper-bounded. To find a satisfactory generator efficiently, we propose a meta-learning algorithm, where a meta generator is trained on a large dataset so that only a few steps are required to adapt to a target dataset. The meta generator is termed as MGDD in our approach. Once adapted, it can handle arbitrary sizes of synthetic datasets, even for those unseen during adaptation. Experiments demonstrate that the generator adapted with only a limited number of steps performs on par with those state-of-the-art DD methods and yields $22\\times$ acceleration."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/849d39ff55c93d69c6d22cec9029c711811ac957.pdf"}, "supplementary_material": {"value": "/attachment/bd87ca6e651dd8c7c40d5037e0c562b225c7279e.zip"}, "_bibtex": {"value": "@inproceedings{\nliu2023mgdd,\ntitle={{MGDD}: A Meta Generator for Fast Dataset Distillation},\nauthor={Songhua Liu and Xinchao Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=D9CMRR5Lof}\n}"}, "paperhash": {"value": "liu|mgdd_a_meta_generator_for_fast_dataset_distillation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5629/-/Revision", "NeurIPS.cc/2023/Conference/Submission5629/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5629/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325898554, "odate": 1698949730126, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "xOJUmwwlJc", "number": 5614, "cdate": 1683685068403, "tcdate": 1683685068403, "mdate": 1698949730104, "tmdate": 1698949730104, "signatures": ["NeurIPS.cc/2023/Conference/Submission5614/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5614/Authors"], "forum": "xOJUmwwlJc", "content": {"title": {"value": "Proximity-Informed Calibration for Deep Neural Networks"}, "authors": {"value": ["Miao Xiong", "Ailin Deng", "Pang Wei Koh", "Jiaying Wu", "Shen Li", "Jianqing Xu", "Bryan Hooi"]}, "authorids": {"value": ["~Miao_Xiong2", "~Ailin_Deng1", "~Pang_Wei_Koh1", "~Jiaying_Wu2", "~Shen_Li2", "~Jianqing_Xu1", "~Bryan_Hooi1"]}, "keywords": {"value": ["Calibration", "Uncertainty Estimation", "Trustworthiness", "Fairness", "Multicalibration"]}, "TLDR": {"value": "We investigate the proximity bias issue in model calibration and propose a framework to address this issue."}, "abstract": {"value": "Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., data lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over $504$ pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are relatively more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, a plug-and-play algorithm with a theoretical guarantee to adjust sample confidence based on proximity. To further quantify the effectiveness of calibration algorithms in mitigating proximity bias, we introduce proximity-informed expected calibration error (PIECE) with theoretical analysis. We show that ProCal is effective in addressing proximity bias and improving calibration on balanced, long-tail, and distribution-shift settings under four metrics over various model architectures. We believe our findings on proximity bias will guide the development of fairer and better-calibrated} models, contributing to the broader pursuit of trustworthy AI."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/26b20fd7015cb2efe88f1a08ee43b336bea9d5ac.pdf"}, "_bibtex": {"value": "@inproceedings{\nxiong2023proximityinformed,\ntitle={Proximity-Informed Calibration for Deep Neural Networks},\nauthor={Miao Xiong and Ailin Deng and Pang Wei Koh and Jiaying Wu and Shen Li and Jianqing Xu and Bryan Hooi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=xOJUmwwlJc}\n}"}, "paperhash": {"value": "xiong|proximityinformed_calibration_for_deep_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5614/-/Revision", "NeurIPS.cc/2023/Conference/Submission5614/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5614/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325897967, "odate": 1698949730091, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "CnvZ7FIyAD", "number": 5552, "cdate": 1683683246202, "tcdate": 1683683246202, "mdate": 1698949729700, "tmdate": 1698949729700, "signatures": ["NeurIPS.cc/2023/Conference/Submission5552/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5552/Authors"], "forum": "CnvZ7FIyAD", "content": {"title": {"value": "Newton\u2013Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems"}, "authors": {"value": ["Lingbing Guo", "Weiqing Wang", "Zhuo Chen", "Ningyu Zhang", "Zequn Sun", "Yixuan Lai", "Qiang Zhang", "Huajun Chen"]}, "authorids": {"value": ["~Lingbing_Guo1", "~Weiqing_Wang2", "~Zhuo_Chen3", "~Ningyu_Zhang1", "~Zequn_Sun1", "~Yixuan_Lai1", "~Qiang_Zhang6", "~Huajun_Chen1"]}, "keywords": {"value": ["Equivariant Graph Neural Networks", "Molecular Dynamics", "N-body System", "Human Motion"]}, "abstract": {"value": "Reasoning system dynamics is one of the most important analytical approaches for many scientific studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based methods are capable of predicting the future state distant in time with high accuracy. Although these methods have diverse designs in modeling the coordinates and interacting forces of the system, we show that they actually share a common paradigm that learns the integration of the velocity over the interval between the initial and terminal coordinates. However, their integrand is constant w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration based on several velocity estimations with Newton\u2013Cotes formulas and prove its effectiveness theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and significant improvement compared with the state-of-the-art methods."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d07e5315db7d9c2f6dffa58a821af20c8754198b.pdf"}, "supplementary_material": {"value": "/attachment/a2bc94fd027019bdccf1145a28dc25f69addaf29.zip"}, "_bibtex": {"value": "@inproceedings{\nguo2023newtoncotes,\ntitle={Newton{\\textendash}Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems},\nauthor={Lingbing Guo and Weiqing Wang and Zhuo Chen and Ningyu Zhang and Zequn Sun and Yixuan Lai and Qiang Zhang and Huajun Chen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=CnvZ7FIyAD}\n}"}, "paperhash": {"value": "guo|newtoncotes_graph_neural_networks_on_the_time_evolution_of_dynamic_systems"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5552/-/Revision", "NeurIPS.cc/2023/Conference/Submission5552/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5552/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325895050, "odate": 1698949729688, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "aIUnoHuENG", "number": 5483, "cdate": 1683679879960, "tcdate": 1683679879960, "mdate": 1698949729296, "tmdate": 1698949729296, "signatures": ["NeurIPS.cc/2023/Conference/Submission5483/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5483/Authors"], "forum": "aIUnoHuENG", "content": {"title": {"value": "Feature Adaptation for Sparse Linear Regression"}, "authors": {"value": ["Jonathan Kelner", "Frederic Koehler", "Raghu Meka", "Dhruv Rohatgi"]}, "authorids": {"value": ["~Jonathan_Kelner1", "~Frederic_Koehler1", "~Raghu_Meka1", "~Dhruv_Rohatgi1"]}, "keywords": {"value": ["theory", "sparse linear regression", "feature adaptation", "lasso"]}, "abstract": {"value": "Sparse linear regression is a central problem in high-dimensional statistics. We study the correlated random design setting, where the covariates are drawn from a multivariate Gaussian $N(0,\\Sigma)$, and we seek an estimator with small excess risk. \n\nIf the true signal is $t$-sparse, information-theoretically, it is possible to achieve strong recovery guarantees with only $O(t\\log n)$ samples. However, computationally efficient algorithms have sample complexity linear in (some variant of) the *condition number* of $\\Sigma$. Classical algorithms such as the Lasso can require significantly more samples than necessary even if there is only a single sparse approximate dependency among the covariates.\n\nWe provide a polynomial-time algorithm that, given $\\Sigma$, automatically adapts the Lasso to tolerate a small number of approximate dependencies. In particular, we achieve near-optimal sample complexity for constant sparsity and if $\\Sigma$ has few ``outlier'' eigenvalues.\nOur algorithm fits into a broader framework of *feature adaptation* for sparse linear regression with ill-conditioned covariates. With this framework, we additionally provide the first polynomial-factor improvement over brute-force search for constant sparsity $t$ and arbitrary covariance $\\Sigma$."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We provably generalize Lasso to tolerate a small number of approximate dependencies, via a broad algorithmic framework of feature adaptation."}, "pdf": {"value": "/pdf/40f38e1a50d7263954ba8b73c97a5ec7e269d187.pdf"}, "supplementary_material": {"value": "/attachment/8a9583dfbb0155ca684440f3162b0e6ecdcbd423.zip"}, "_bibtex": {"value": "@inproceedings{\nkelner2023feature,\ntitle={Feature Adaptation for Sparse Linear Regression},\nauthor={Jonathan Kelner and Frederic Koehler and Raghu Meka and Dhruv Rohatgi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=aIUnoHuENG}\n}"}, "paperhash": {"value": "kelner|feature_adaptation_for_sparse_linear_regression"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5483/-/Revision", "NeurIPS.cc/2023/Conference/Submission5483/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5483/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325892677, "odate": 1698949729284, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "wRJqZRxDEX", "number": 5449, "cdate": 1683677409883, "tcdate": 1683677409883, "mdate": 1698949729084, "tmdate": 1698949729084, "signatures": ["NeurIPS.cc/2023/Conference/Submission5449/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5449/Authors"], "forum": "wRJqZRxDEX", "content": {"title": {"value": "Critical Initialization of Wide and Deep Neural Networks using Partial Jacobians: General Theory and Applications"}, "authors": {"value": ["Darshil Doshi", "Tianyu He", "Andrey Gromov"]}, "authorids": {"value": ["~Darshil_Doshi1", "~Tianyu_He2", "~Andrey_Gromov1"]}, "keywords": {"value": ["Criticality", "Gaussian Process", "Jacobian", "LayerNorm", "Residual connections", "ResNet"]}, "TLDR": {"value": "(i) We introduce a new diagnostic for critical initialization in a wide class of deep neural networks. (ii) We show that a combination of Normalization layers and residual connections leads to everywhere-critical architectures."}, "abstract": {"value": "Deep neural networks are notorious for defying theoretical treatment. However, when the number of parameters in each layer tends to infinity, the network function is a Gaussian process (GP) and quantitatively predictive description is possible. Gaussian approximation allows one to formulate criteria for selecting hyperparameters, such as variances of weights and biases, as well as the learning rate. These criteria rely on the notion of criticality defined for deep neural networks. In this work we describe a new practical way to diagnose criticality. We introduce *partial Jacobians* of a network, defined as derivatives of preactivations in layer $l$ with respect to preactivations in layer $l_0\\leq l$. We derive recurrence relations for the norms of partial Jacobians and utilize these relations to analyze criticality of deep fully connected neural networks with LayerNorm and/or residual connections. We derive and implement a simple and cheap numerical test that allows one to select optimal initialization for a broad class of deep neural networks; containing fully connected, convolutional and normalization layers. Using these tools we show quantitatively that proper stacking of the LayerNorm (applied to preactivations) and residual connections leads to an architecture that is critical for any initialization. Finally, we apply our methods to analyze ResNet and MLP-Mixer architectures; demonstrating the everywhere-critical regime."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a17012d7b8bc58aa8e32aa2e67323d12cdb9260d.pdf"}, "supplementary_material": {"value": "/attachment/29b5f17303335edb5b32e392ef062a6ad7b9b18f.zip"}, "_bibtex": {"value": "@inproceedings{\ndoshi2023critical,\ntitle={Critical Initialization of Wide and Deep Neural Networks using Partial Jacobians: General Theory and Applications},\nauthor={Darshil Doshi and Tianyu He and Andrey Gromov},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=wRJqZRxDEX}\n}"}, "paperhash": {"value": "doshi|critical_initialization_of_wide_and_deep_neural_networks_using_partial_jacobians_general_theory_and_applications"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5449/-/Revision", "NeurIPS.cc/2023/Conference/Submission5449/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5449/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325891636, "odate": 1698949729070, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "fShubymWrc", "number": 5403, "cdate": 1683673569926, "tcdate": 1683673569926, "mdate": 1698949728825, "tmdate": 1698949728825, "signatures": ["NeurIPS.cc/2023/Conference/Submission5403/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5403/Authors"], "forum": "fShubymWrc", "content": {"title": {"value": "Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks"}, "authors": {"value": ["Eshaan Nichani", "Alex Damian", "Jason D. Lee"]}, "authorids": {"value": ["~Eshaan_Nichani1", "~Alex_Damian1", "~Jason_D._Lee1"]}, "keywords": {"value": ["Deep Learning Theory", "Feature Learning", "Three-Layer Neural Network", "Depth Separation", "Gradient Descent", "Representation Learning"]}, "abstract": {"value": "One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic features -- and show that in the latter setting three-layer networks obtain a sample complexity improvement over all existing guarantees for two-layer networks. Crucially, this sample complexity improvement relies on the ability of three-layer networks to efficiently learn *nonlinear* features. We then establish a concrete optimization-based depth separation by constructing a function which is efficiently learnable via gradient descent on a three-layer network, yet cannot be learned efficiently by a two-layer network. Our work makes progress towards understanding the provable benefit of three-layer neural networks over two-layer networks in the feature learning regime."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We theoretically analyze the features learned by gradient descent on three-layer neural networks, and show that there exist classes of functions efficiently learnable by three-layer networks but not by two-layer networks."}, "pdf": {"value": "/pdf/683b0efb1f9131aac968a6ca28f574c61826e783.pdf"}, "supplementary_material": {"value": "/attachment/0aa934bb3e1307a407a7db47757fa6a337d49fa4.zip"}, "_bibtex": {"value": "@inproceedings{\nnichani2023provable,\ntitle={Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks},\nauthor={Eshaan Nichani and Alex Damian and Jason D. Lee},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=fShubymWrc}\n}"}, "paperhash": {"value": "nichani|provable_guarantees_for_nonlinear_feature_learning_in_threelayer_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5403/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5403/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325889904, "odate": 1698949728812, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "FujJO3dsNj", "number": 5392, "cdate": 1683672599367, "tcdate": 1683672599367, "mdate": 1698949728636, "tmdate": 1698949728636, "signatures": ["NeurIPS.cc/2023/Conference/Submission5392/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5392/Authors"], "forum": "FujJO3dsNj", "content": {"title": {"value": "Balancing memorization and generalization in RNNs for high performance brain-machine Interfaces"}, "authors": {"value": ["Joseph T Costello", "Hisham Temmar", "Luis H Cubillos", "Matthew J Mender", "Dylan M Wallace", "Matthew S Willsey", "Parag G Patil", "Cynthia Chestek"]}, "authorids": {"value": ["~Joseph_T_Costello1", "htemmar@umich.edu", "lhcubill@umich.edu", "mmender@umich.edu", "dywallac@umich.edu", "mwillsey@umich.edu", "pgpatil@med.umich.edu", "~Cynthia_Chestek1"]}, "keywords": {"value": ["brain computer interface", "brain machine interface", "neural decoding", "prosthetic control", "recurrent neural network", "RNN", "transformer", "real time", "closed-loop", "user interface"]}, "TLDR": {"value": "Using an intracortical brain-machine interface, we show how RNNs outperform other neural networks for closed-loop neural decoding, and can act both like a classifier and a continuous decoder for decoding movement."}, "abstract": {"value": "Brain-machine interfaces (BMIs) can restore motor function to people with paralysis but are currently limited by the accuracy of real-time decoding algorithms. Recurrent neural networks (RNNs) using modern training techniques have shown promise in accurately predicting movements from neural signals but have yet to be rigorously evaluated against other decoding algorithms in a closed-loop setting. Here we compared RNNs to other neural network architectures in real-time, continuous decoding of finger movements using intracortical signals from nonhuman primates. Across one and two finger online tasks, LSTMs (a type of RNN) outperformed convolutional and transformer-based neural networks, averaging 18% higher throughput than the convolution network. On simplified tasks with a reduced movement set, RNN decoders were allowed to memorize movement patterns and matched able-bodied control. Performance gradually dropped as the number of distinct movements increased but did not go below fully continuous decoder performance. Finally, in a two-finger task where one degree-of-freedom had poor input signals, we recovered functional control using RNNs trained to act both like a movement classifier and continuous decoder. Our results suggest that RNNs can enable functional real-time BMI control by learning and generating accurate movement patterns."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f89c25850c1478c891d8ce8412b68e384580f062.pdf"}, "supplementary_material": {"value": "/attachment/45e8c05ad1462220261e429f1ab7362715e33486.zip"}, "_bibtex": {"value": "@inproceedings{\ncostello2023balancing,\ntitle={Balancing memorization and generalization in {RNN}s for high performance brain-machine Interfaces},\nauthor={Joseph T Costello and Hisham Temmar and Luis H Cubillos and Matthew J Mender and Dylan M Wallace and Matthew S Willsey and Parag G Patil and Cynthia Chestek},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=FujJO3dsNj}\n}"}, "paperhash": {"value": "costello|balancing_memorization_and_generalization_in_rnns_for_high_performance_brainmachine_interfaces"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5392/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5392/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325889558, "odate": 1698949728623, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "8Kch0ILfQH", "number": 5390, "cdate": 1683672566437, "tcdate": 1683672566437, "mdate": 1698949728563, "tmdate": 1698949728563, "signatures": ["NeurIPS.cc/2023/Conference/Submission5390/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5390/Authors"], "forum": "8Kch0ILfQH", "content": {"title": {"value": "Bootstrapping Vision-Language Learning with Decoupled Language Pre-training"}, "authors": {"value": ["Yiren Jian", "Chongyang Gao", "Soroush Vosoughi"]}, "authorids": {"value": ["~Yiren_Jian1", "~Chongyang_Gao1", "~Soroush_Vosoughi1"]}, "keywords": {"value": ["vision-language pretraining", "multi-modal learning", "uni-modal auxiliary learning"]}, "abstract": {"value": "We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importantly, our framework is modality-agnostic and flexible in terms of architectural design, as validated by its successful application in a video learning task using varied base modules. The code will be made available at https://github.com/yiren-jian/BLIText."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5da56711048b662f4c76af902fab80aa75e1fa64.pdf"}, "supplementary_material": {"value": "/attachment/2cc14bfe1cdd90efd3723fc2c62ad9dd520190d2.pdf"}, "_bibtex": {"value": "@inproceedings{\njian2023bootstrapping,\ntitle={Bootstrapping Vision-Language Learning with Decoupled Language Pre-training},\nauthor={Yiren Jian and Chongyang Gao and Soroush Vosoughi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=8Kch0ILfQH}\n}"}, "paperhash": {"value": "jian|bootstrapping_visionlanguage_learning_with_decoupled_language_pretraining"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5390/-/Revision", "NeurIPS.cc/2023/Conference/Submission5390/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5390/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325889445, "odate": 1698949728548, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "xrk9g5vcXR", "number": 5374, "cdate": 1683671093035, "tcdate": 1683671093035, "mdate": 1698949728443, "tmdate": 1698949728443, "signatures": ["NeurIPS.cc/2023/Conference/Submission5374/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5374/Authors"], "forum": "xrk9g5vcXR", "content": {"title": {"value": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"}, "authors": {"value": ["Jerry Chee", "Yaohui Cai", "Volodymyr Kuleshov", "Christopher De Sa"]}, "authorids": {"value": ["~Jerry_Chee1", "~Yaohui_Cai1", "~Volodymyr_Kuleshov1", "~Christopher_De_Sa2"]}, "keywords": {"value": ["Quantization", "Large Language Models", "Adaptive Rounding", "Theoretical Guarantees"]}, "TLDR": {"value": "Our method QuIP yield the first viable LLM quantization at 2 bits per weight, and comes with guarantees."}, "abstract": {"value": "This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0b5a75bf31bf633848283b615a2684ff645b6f33.pdf"}, "supplementary_material": {"value": "/attachment/0238900507fd60700740ca9a2db7933aa1eb68c6.zip"}, "_bibtex": {"value": "@inproceedings{\nchee2023quip,\ntitle={Qu{IP}: 2-Bit Quantization of Large Language Models With Guarantees},\nauthor={Jerry Chee and Yaohui Cai and Volodymyr Kuleshov and Christopher De Sa},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=xrk9g5vcXR}\n}"}, "paperhash": {"value": "chee|quip_2bit_quantization_of_large_language_models_with_guarantees"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5374/-/Revision", "NeurIPS.cc/2023/Conference/Submission5374/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5374/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325889051, "odate": 1698949728430, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "bpzwUfX1UP", "number": 5370, "cdate": 1683670492828, "tcdate": 1683670492828, "mdate": 1698949728408, "tmdate": 1698949728408, "signatures": ["NeurIPS.cc/2023/Conference/Submission5370/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5370/Authors"], "forum": "bpzwUfX1UP", "content": {"title": {"value": "Parallel Sampling of Diffusion Models"}, "authors": {"value": ["Andy Shih", "Suneel Belkhale", "Stefano Ermon", "Dorsa Sadigh", "Nima Anari"]}, "authorids": {"value": ["~Andy_Shih1", "~Suneel_Belkhale1", "~Stefano_Ermon1", "~Dorsa_Sadigh1", "~Nima_Anari1"]}, "keywords": {"value": ["diffusion models", "parallel sampling"]}, "abstract": {"value": "Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we improve sampling speed by 2-4x across a range of robotics and image generation models, giving state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable degradation of task reward, FID score, or CLIP score."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We improve sampling speed of diffusion models by 2-4x using parallel computation"}, "pdf": {"value": "/pdf/41890d156ae31068d246f67f7af49c07fcd75470.pdf"}, "supplementary_material": {"value": "/attachment/83ef4ef7a2a4710daac304ded2c2ab931aa5b325.zip"}, "_bibtex": {"value": "@inproceedings{\nshih2023parallel,\ntitle={Parallel Sampling of Diffusion Models},\nauthor={Andy Shih and Suneel Belkhale and Stefano Ermon and Dorsa Sadigh and Nima Anari},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=bpzwUfX1UP}\n}"}, "paperhash": {"value": "shih|parallel_sampling_of_diffusion_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5370/-/Revision", "NeurIPS.cc/2023/Conference/Submission5370/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5370/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325888995, "odate": 1698949728394, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "XkcufOcgUc", "number": 5355, "cdate": 1683669454803, "tcdate": 1683669454803, "mdate": 1698949728243, "tmdate": 1698949728243, "signatures": ["NeurIPS.cc/2023/Conference/Submission5355/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5355/Authors"], "forum": "XkcufOcgUc", "content": {"title": {"value": "Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data"}, "authors": {"value": ["Xin Zheng", "Miao Zhang", "Chunyang Chen", "Quoc Viet Hung Nguyen", "Xingquan Zhu", "Shirui Pan"]}, "authorids": {"value": ["~Xin_Zheng4", "~Miao_Zhang4", "~Chunyang_Chen1", "~Quoc_Viet_Hung_Nguyen1", "~Xingquan_Zhu1", "~Shirui_Pan1"]}, "keywords": {"value": ["graph neural networks (GNNs)", "graph condensation", "training trajectory meta-matching", "graph neural feature score"]}, "abstract": {"value": "Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediate benefits for various graph learning tasks.\nHowever, existing graph condensation methods rely on the joint optimization of nodes and structures in the condensed graph, and overlook critical issues in effectiveness and generalization ability.\nIn this paper, we advocate a new Structure-Free Graph Condensation paradigm, named SFGC, to distill a large-scale graph into a small-scale graph node set without explicit graph structures, i.e., graph-free data.\nOur idea is to implicitly encode topology structure information into the node attributes in the synthesized graph-free data, whose topology is reduced to an identity matrix.\nSpecifically, SFGC contains two collaborative components: \n(1) a training trajectory meta-matching scheme for effectively synthesizing small-scale graph-free data;\n(2) a graph neural feature score metric for dynamically evaluating the quality of the condensed data. \nThrough training trajectory meta-matching, SFGC aligns the long-term GNN learning behaviors between the large-scale graph and the condensed small-scale graph-free data, ensuring comprehensive and compact transfer of informative knowledge to the graph-free data.\nAfterward, the underlying condensed graph-free data would be dynamically evaluated with the graph neural feature score, which is a closed-form metric for ensuring the excellent expressiveness of the condensed graph-free data.\nExtensive experiments verify the superiority of SFGC across different condensation ratios."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f6da01c95f03dc8397ee56522c3f9a3adcecebec.pdf"}, "supplementary_material": {"value": "/attachment/18ddcd7d4cf92d5fd3df8113917ca934e2f17226.pdf"}, "_bibtex": {"value": "@inproceedings{\nzheng2023structurefree,\ntitle={Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data},\nauthor={Xin Zheng and Miao Zhang and Chunyang Chen and Quoc Viet Hung Nguyen and Xingquan Zhu and Shirui Pan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=XkcufOcgUc}\n}"}, "paperhash": {"value": "zheng|structurefree_graph_condensation_from_largescale_graphs_to_condensed_graphfree_data"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5355/-/Revision", "NeurIPS.cc/2023/Conference/Submission5355/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5355/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325888333, "odate": 1698949728230, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "e0pRF9tOtm", "number": 5349, "cdate": 1683669039841, "tcdate": 1683669039841, "mdate": 1698949728218, "tmdate": 1698949728218, "signatures": ["NeurIPS.cc/2023/Conference/Submission5349/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5349/Authors"], "forum": "e0pRF9tOtm", "content": {"title": {"value": "Private (Stochastic) Non-Convex Optimization Revisited: Second-Order Stationary Points and Excess Risks"}, "authors": {"value": ["Daogao Liu", "Arun Ganesh", "Sewoong Oh", "Abhradeep Guha Thakurta"]}, "authorids": {"value": ["~Daogao_Liu1", "~Arun_Ganesh1", "~Sewoong_Oh1", "~Abhradeep_Guha_Thakurta1"]}, "keywords": {"value": ["Differential Privacy", "Non-convex optimization", "Stationary points", "Exponential Mechanism"]}, "abstract": {"value": "We reconsider the challenge of non-convex optimization under differential privacy constraint. Building upon the previous variance-reduced algorithm SpiderBoost, we propose a novel framework that employs two types of gradient oracles: one that estimates the gradient at a single point and a more cost-effective option that calculates the gradient difference between two points. Our framework can ensure continuous accuracy of gradient estimations and subsequently enhances the rates of identifying second-order stationary points.\nAdditionally, we consider a more challenging task by attempting to locate the global minima of a non-convex objective via the exponential mechanism without almost any assumptions. Our preliminary results suggest that the regularized exponential mechanism can effectively emulate previous empirical and population risk bounds, negating the need for smoothness assumptions for algorithms with polynomial running time. Furthermore, with running time factors excluded, the exponential mechanism demonstrates promising population risk bound performance, and we provide a nearly matching lower bound."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/cc0b4a4b0a973c5f6bc853ed60e5850b2b256be5.pdf"}, "supplementary_material": {"value": "/attachment/173b5f4b1ff90b7757c5e1912b86f5b3ddccffff.pdf"}, "_bibtex": {"value": "@inproceedings{\nliu2023private,\ntitle={Private (Stochastic) Non-Convex Optimization Revisited: Second-Order Stationary Points and Excess Risks},\nauthor={Daogao Liu and Arun Ganesh and Sewoong Oh and Abhradeep Guha Thakurta},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=e0pRF9tOtm}\n}"}, "paperhash": {"value": "liu|private_stochastic_nonconvex_optimization_revisited_secondorder_stationary_points_and_excess_risks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5349/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5349/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325888131, "odate": 1698949728202, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "rwrblCYb2A", "number": 5314, "cdate": 1683666642472, "tcdate": 1683666642472, "mdate": 1698949727890, "tmdate": 1698949727890, "signatures": ["NeurIPS.cc/2023/Conference/Submission5314/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5314/Authors"], "forum": "rwrblCYb2A", "content": {"title": {"value": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors"}, "authors": {"value": ["Paul Steven Scotti", "Atmadeep Banerjee", "Jimmie Goode", "Stepan Shabalin", "Alex Nguyen", "Cohen Ethan", "Aidan James Dempster", "Nathalie Verlinde", "Elad Yundler", "David Weisberg", "Kenneth Norman", "Tanishq Mathew Abraham"]}, "authorids": {"value": ["~Paul_Steven_Scotti1", "~Atmadeep_Banerjee1", "~Jimmie_Goode1", "~Stepan_Shabalin2", "~Alex_Nguyen1", "~Cohen_Ethan1", "~Aidan_James_Dempster1", "~Nathalie_Verlinde1", "~Elad_Yundler1", "~David_Weisberg1", "~Kenneth_Norman1", "~Tanishq_Mathew_Abraham1"]}, "keywords": {"value": ["fMRI", "computational neuroscience", "mind reading", "diffusion models"]}, "abstract": {"value": "We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. Our model comprises two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling image reconstruction using generative models that accept embeddings from this latent space. We comprehensively compare our approach with other existing methods, using both qualitative side-by-side comparisons and quantitative evaluations, and show that MindEye achieves state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5B. We demonstrate through ablations that MindEye's performance improvements over previous methods result from specialized submodules for retrieval and reconstruction, improved training techniques, and training models with orders of magnitude more parameters. Furthermore, we show that MindEye can better preserve low-level image features in the reconstructions by using img2img, with outputs from a separate autoencoder. All code is available on GitHub."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We present MindEye, a novel state-of-the-art fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity using contrastive learning and diffusion models."}, "pdf": {"value": "/pdf/b2d95f8895b52e427db9aa7c8aa290520398c5d1.pdf"}, "supplementary_material": {"value": "/attachment/4379a383911aa3261ab0f57df605b72934bcddb9.pdf"}, "_bibtex": {"value": "@inproceedings{\nscotti2023reconstructing,\ntitle={Reconstructing the Mind's Eye: f{MRI}-to-Image with Contrastive Learning and Diffusion Priors},\nauthor={Paul Steven Scotti and Atmadeep Banerjee and Jimmie Goode and Stepan Shabalin and Alex Nguyen and Cohen Ethan and Aidan James Dempster and Nathalie Verlinde and Elad Yundler and David Weisberg and Kenneth Norman and Tanishq Mathew Abraham},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=rwrblCYb2A}\n}"}, "paperhash": {"value": "scotti|reconstructing_the_minds_eye_fmritoimage_with_contrastive_learning_and_diffusion_priors"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5314/-/Revision", "NeurIPS.cc/2023/Conference/Submission5314/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5314/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325886794, "odate": 1698949727875, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "shePL2nbwl", "number": 5309, "cdate": 1683665898512, "tcdate": 1683665898512, "mdate": 1698949727834, "tmdate": 1698949727834, "signatures": ["NeurIPS.cc/2023/Conference/Submission5309/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5309/Authors"], "forum": "shePL2nbwl", "content": {"title": {"value": "Survival Instinct in Offline Reinforcement Learning"}, "authors": {"value": ["Anqi Li", "Dipendra Misra", "Andrey Kolobov", "Ching-An Cheng"]}, "authorids": {"value": ["~Anqi_Li1", "~Dipendra_Misra1", "~Andrey_Kolobov1", "~Ching-An_Cheng1"]}, "keywords": {"value": ["Offline RL", "safe RL"]}, "abstract": {"value": "We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with \"wrong\" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of *pessimism* in offline RL algorithms and certain implicit biases in common data collection practices. As we prove in this work, pessimism endows the agent with a *survival instinct*, i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival policies. Formally, given a reward class -- which may not even contain the true reward -- we identify conditions on the training data distribution that enable offline RL to learn a near-optimal and safe policy from any reward within the class. We argue that the survival instinct should be taken into account when interpreting results from existing offline RL benchmarks and when creating future ones. Our empirical and theoretical results suggest a new paradigm for offline RL, whereby an agent is \"nudged\" to learn a desirable behavior with imperfect reward but purposely biased data coverage. Please visit our website [https://survival-instinct.github.io](https://survival-instinct.github.io) for accompanied code and videos."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We find that offline RL can produce surprisingly good policies even when trained on wrong reward labels. We provide explanations and discuss practical implications."}, "pdf": {"value": "/pdf/3c7d0991f8b446af97d1022cb5d31974a0976f14.pdf"}, "_bibtex": {"value": "@inproceedings{\nli2023survival,\ntitle={Survival Instinct in Offline Reinforcement Learning},\nauthor={Anqi Li and Dipendra Misra and Andrey Kolobov and Ching-An Cheng},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=shePL2nbwl}\n}"}, "paperhash": {"value": "li|survival_instinct_in_offline_reinforcement_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5309/-/Revision", "NeurIPS.cc/2023/Conference/Submission5309/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5309/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325886572, "odate": 1698949727822, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "67o9UQgTD0", "number": 5273, "cdate": 1683663661291, "tcdate": 1683663661291, "mdate": 1698949727611, "tmdate": 1698949727611, "signatures": ["NeurIPS.cc/2023/Conference/Submission5273/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5273/Authors"], "forum": "67o9UQgTD0", "content": {"title": {"value": "Counterfactual Memorization in Neural Language Models"}, "authors": {"value": ["Chiyuan Zhang", "Daphne Ippolito", "Katherine Lee", "Matthew Jagielski", "Florian Tram\u00e8r", "Nicholas Carlini"]}, "authorids": {"value": ["~Chiyuan_Zhang1", "~Daphne_Ippolito1", "~Katherine_Lee1", "~Matthew_Jagielski1", "~Florian_Tram\u00e8r1", "~Nicholas_Carlini1"]}, "keywords": {"value": ["Memorization", "Language Models"]}, "TLDR": {"value": "We study a notion of counterfactual memorization on common language modeling datasets and show that memorized examples exist in all of them and are not easily captured by more standard text matching based notion of memorization."}, "abstract": {"value": "Modern neural language models that are widely used in various NLP tasks risk memorizing sensitive information from their training data.\nUnderstanding this memorization is important in real world applications and also from a learning-theoretical perspective. An open question in previous studies of language model memorization is how to filter out ``common'' memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing memorized familiar phrases, public knowledge, templated texts, or other repeated data.\nWe formulate a notion of counterfactual memorization which characterizes how a model's predictions change if a particular document is omitted during training.\nWe identify and study counterfactually-memorized training examples in standard text datasets.\nWe estimate the influence of each memorized training example on the validation set and on generated texts, showing how this can provide direct evidence of the source of memorization at test time."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/869af82690d6c3db62a60c8b58256091f7c970e2.pdf"}, "supplementary_material": {"value": "/attachment/966034dfed18d9aab4923821093171b9f7450fa3.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023counterfactual,\ntitle={Counterfactual Memorization in Neural Language Models},\nauthor={Chiyuan Zhang and Daphne Ippolito and Katherine Lee and Matthew Jagielski and Florian Tram{\\`e}r and Nicholas Carlini},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=67o9UQgTD0}\n}"}, "paperhash": {"value": "zhang|counterfactual_memorization_in_neural_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5273/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5273/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325885240, "odate": 1698949727596, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "1CpVHL10fh", "number": 5208, "cdate": 1683659516419, "tcdate": 1683659516419, "mdate": 1698949727000, "tmdate": 1698949727000, "signatures": ["NeurIPS.cc/2023/Conference/Submission5208/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5208/Authors"], "forum": "1CpVHL10fh", "content": {"title": {"value": "Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations"}, "authors": {"value": ["Hammaad Adam", "Fan Yin", "Mary Hu", "Neil Tenenholtz", "Lorin Crawford", "Lester Mackey", "Allison Koenecke"]}, "authorids": {"value": ["~Hammaad_Adam1", "~Fan_Yin2", "maryhu@microsoft.com", "~Neil_Tenenholtz1", "~Lorin_Crawford1", "~Lester_Mackey1", "~Allison_Koenecke1"]}, "keywords": {"value": ["Randomized experiments", "heterogeneous effects", "causal machine learning", "fairness", "sequential testing", "clinical trials", "A/B testing"]}, "abstract": {"value": "Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8fad17827ec7f131eb353ead7220411359f9eda7.pdf"}, "_bibtex": {"value": "@inproceedings{\nadam2023should,\ntitle={Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations},\nauthor={Hammaad Adam and Fan Yin and Mary Hu and Neil Tenenholtz and Lorin Crawford and Lester Mackey and Allison Koenecke},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=1CpVHL10fh}\n}"}, "paperhash": {"value": "adam|should_i_stop_or_should_i_go_early_stopping_with_heterogeneous_populations"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5208/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5208/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325882652, "odate": 1698949726984, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "zCFfv49MjE", "number": 5206, "cdate": 1683659388911, "tcdate": 1683659388911, "mdate": 1698949726978, "tmdate": 1698949726978, "signatures": ["NeurIPS.cc/2023/Conference/Submission5206/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5206/Authors"], "forum": "zCFfv49MjE", "content": {"title": {"value": "Quasi-Monte Carlo Graph Random Features"}, "authors": {"value": ["Isaac Reid", "Adrian Weller", "Krzysztof Marcin Choromanski"]}, "authorids": {"value": ["~Isaac_Reid3", "~Adrian_Weller1", "~Krzysztof_Marcin_Choromanski1"]}, "keywords": {"value": ["Graph", "discrete mathematics", "quasi-Monte Carlo", "kernel", "scalability", "Laplacian", "clustering", "random walks"]}, "TLDR": {"value": "A novel QMC mechanism that induces correlations between the terminations of random walkers on a graph to improve kernel estimator variance."}, "abstract": {"value": "We present a novel mechanism to improve the accuracy of the recently-introduced class of graph random features (GRFs). Our method induces negative correlations between the lengths of the algorithm's random walks by imposing antithetic termination: a procedure to sample more diverse random walks which may be of independent interest. It has a trivial drop-in implementation. We derive strong theoretical guarantees on the properties of these quasi-Monte Carlo GRFs (q-GRFs), proving that they yield lower-variance estimators of the $2$-regularised Laplacian kernel under mild conditions. Remarkably, our results hold for any graph topology. We demonstrate empirical accuracy improvements on a variety of tasks including a new practical application: time-efficient approximation of the graph diffusion process. To our knowledge, q-GRFs constitute the first rigorously studied quasi-Monte Carlo scheme for kernels defined on combinatorial objects, inviting new research on correlations between graph random walks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/864f94f5f86c524df5929f4403dc328a10b1ee64.pdf"}, "supplementary_material": {"value": "/attachment/d2ec353f598a499ecb220baa84887bfd3c5b3515.pdf"}, "_bibtex": {"value": "@inproceedings{\nreid2023quasimonte,\ntitle={Quasi-Monte Carlo Graph Random Features},\nauthor={Isaac Reid and Adrian Weller and Krzysztof Marcin Choromanski},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=zCFfv49MjE}\n}"}, "paperhash": {"value": "reid|quasimonte_carlo_graph_random_features"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5206/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5206/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325882554, "odate": 1698949726961, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cwBeRBe9hq", "number": 5205, "cdate": 1683659359056, "tcdate": 1683659359056, "mdate": 1698949726937, "tmdate": 1698949726937, "signatures": ["NeurIPS.cc/2023/Conference/Submission5205/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5205/Authors"], "forum": "cwBeRBe9hq", "content": {"title": {"value": "On the Learnability of Multilabel Ranking"}, "authors": {"value": ["Vinod Raman", "UNIQUE SUBEDI", "Ambuj Tewari"]}, "authorids": {"value": ["~Vinod_Raman1", "~UNIQUE_SUBEDI1", "~Ambuj_Tewari1"]}, "keywords": {"value": ["Multilabel Ranking", "PAC Learning", "Online Learning"]}, "abstract": {"value": "Multilabel ranking is a central task in machine learning. However, the most fundamental question of learnability in a multilabel ranking setting with relevance-score feedback remains unanswered. In this work, we characterize the learnability of multilabel ranking problems in both batch and online settings for a large family of ranking losses. Along the way, we give two equivalence classes of ranking losses based on learnability that capture most losses used in practice."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1b2954aeac992b3ebed2b372139d9b9449cb51e1.pdf"}, "supplementary_material": {"value": "/attachment/91e852a627cc0c42b229f092c49e340f07da1683.pdf"}, "TLDR": {"value": "We give a characterization of learnability for multilabel ranking problems."}, "_bibtex": {"value": "@inproceedings{\nraman2023on,\ntitle={On the Learnability of Multilabel Ranking},\nauthor={Vinod Raman and UNIQUE SUBEDI and Ambuj Tewari},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cwBeRBe9hq}\n}"}, "paperhash": {"value": "raman|on_the_learnability_of_multilabel_ranking"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5205/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5205/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325882499, "odate": 1698949726923, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "GrElRvXnEj", "number": 5200, "cdate": 1683659235804, "tcdate": 1683659235804, "mdate": 1698949726908, "tmdate": 1698949726908, "signatures": ["NeurIPS.cc/2023/Conference/Submission5200/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5200/Authors"], "forum": "GrElRvXnEj", "content": {"title": {"value": "Score-based Generative Modeling through Stochastic Evolution Equations in Hilbert Spaces"}, "authors": {"value": ["Sungbin Lim", "Eunbi Yoon", "Taehyun Byun", "Taewon Kang", "Seungwoo Kim", "Kyungjae Lee", "Sungjoon Choi"]}, "authorids": {"value": ["~Sungbin_Lim1", "~Eunbi_Yoon1", "~Taehyun_Byun1", "~Taewon_Kang2", "~Seungwoo_Kim1", "~Kyungjae_Lee1", "~Sungjoon_Choi4"]}, "keywords": {"value": ["Generative Model", "Score-based Method", "Diffusion Model"]}, "abstract": {"value": "Continuous-time score-based generative models consist of a pair of stochastic differential equations (SDEs)\u2014a forward SDE that smoothly transitions data into a noise space and a reverse SDE that incrementally eliminates noise from a Gaussian prior distribution to generate data distribution samples\u2014are intrinsically connected by the time-reversal theory on diffusion processes. In this paper, we investigate the use of stochastic evolution equations in Hilbert spaces, which expand the applicability of SDEs in two aspects: sample space and evolution operator, so they enable encompassing recent variations of diffusion models, such as generating functional data or replacing drift coefficients with image transformation. To this end, we derive a generalized time-reversal formula to build a bridge between probabilistic diffusion models and stochastic evolution equations and propose a score-based generative model called Hilbert Diffusion Model (HDM). Combining with Fourier neural operator, we verify the superiority of HDM for sampling functions from functional datasets with a power of kernel two-sample test of 4.2 on Quadratic, 0.2 on Melbourne, and 3.6 on Gridwatch, which outperforms existing diffusion models formulated in function spaces. Furthermore, the proposed method shows its strength in motion synthesis tasks by utilizing the Wiener process with values in Hilbert space. Finally, our empirical results on image datasets also validate a connection between HDM and diffusion models using heat dissipation, revealing the potential for exploring evolution operators and sample spaces."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/70df700411b9cf1189cadaa410da03331f3db173.pdf"}, "supplementary_material": {"value": "/attachment/d89004f04ac2c1e2f4279ccdeb4387bd246bc95d.pdf"}, "_bibtex": {"value": "@inproceedings{\nlim2023scorebased,\ntitle={Score-based Generative Modeling through Stochastic Evolution Equations in Hilbert Spaces},\nauthor={Sungbin Lim and Eunbi Yoon and Taehyun Byun and Taewon Kang and Seungwoo Kim and Kyungjae Lee and Sungjoon Choi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=GrElRvXnEj}\n}"}, "TLDR": {"value": "We propose a unified framework for score-based generative modeling in Hilbert spaces using stochastic evolution equations with time-dependent operators."}, "paperhash": {"value": "lim|scorebased_generative_modeling_through_stochastic_evolution_equations_in_hilbert_spaces"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5200/-/Revision", "NeurIPS.cc/2023/Conference/Submission5200/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5200/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325882285, "odate": 1698949726894, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ygjQCOyNfh", "number": 5178, "cdate": 1683658025468, "tcdate": 1683658025468, "mdate": 1698949726679, "tmdate": 1698949726679, "signatures": ["NeurIPS.cc/2023/Conference/Submission5178/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5178/Authors"], "forum": "ygjQCOyNfh", "content": {"title": {"value": "Uncertainty Quantification over Graph with Conformalized Graph Neural Networks"}, "authors": {"value": ["Kexin Huang", "Ying Jin", "Emmanuel Candes", "Jure Leskovec"]}, "authorids": {"value": ["~Kexin_Huang1", "~Ying_Jin4", "~Emmanuel_Candes1", "~Jure_Leskovec1"]}, "keywords": {"value": ["Graph Neural Networks", "Conformal Prediction", "Uncertainty Quantification"]}, "TLDR": {"value": "We study the theoretical condition for conformal validity in graph-structured data to produce guaranteed uncertainty estimates and design a topology-aware method to reduce inefficiency by up to 74%."}, "abstract": {"value": "Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the prediction and produces more efficient prediction sets/intervals. Extensive experiments show that CF-GNN achieves any pre-defined target marginal coverage while significantly reducing the prediction set/interval size by up to 74% over the baselines. It also empirically achieves satisfactory conditional coverage over various raw and network features."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1f784f6bbba8634a5f8611f0b9ca68c351477e00.pdf"}, "supplementary_material": {"value": "/attachment/f3b6a5afae552948fbda9ca4506a1f5ab2d45124.zip"}, "_bibtex": {"value": "@inproceedings{\nhuang2023uncertainty,\ntitle={Uncertainty Quantification over Graph with Conformalized Graph Neural Networks},\nauthor={Kexin Huang and Ying Jin and Emmanuel Candes and Jure Leskovec},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ygjQCOyNfh}\n}"}, "paperhash": {"value": "huang|uncertainty_quantification_over_graph_with_conformalized_graph_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5178/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5178/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325881328, "odate": 1698949726663, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "jucDLW6G9l", "number": 5168, "cdate": 1683657640892, "tcdate": 1683657640892, "mdate": 1698949726590, "tmdate": 1698949726590, "signatures": ["NeurIPS.cc/2023/Conference/Submission5168/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5168/Authors"], "forum": "jucDLW6G9l", "content": {"title": {"value": "Deep Reinforcement Learning with Plasticity Injection"}, "authors": {"value": ["Evgenii Nikishin", "Junhyuk Oh", "Georg Ostrovski", "Clare Lyle", "Razvan Pascanu", "Will Dabney", "Andre Barreto"]}, "authorids": {"value": ["~Evgenii_Nikishin1", "~Junhyuk_Oh2", "~Georg_Ostrovski1", "~Clare_Lyle1", "~Razvan_Pascanu1", "~Will_Dabney1", "~Andre_Barreto1"]}, "keywords": {"value": ["deep reinforcement learning", "continual learning", "loss of plasticity"]}, "TLDR": {"value": "We present plasticity injection, an intervention that can be used for careful analysis of the plasticity loss phenomenon in deep RL and dynamically growing a network for computational efficiency."}, "abstract": {"value": "A growing body of evidence suggests that neural networks employed in deep reinforcement learning (RL) gradually lose their plasticity, the ability to learn from new data; however, the analysis and mitigation of this phenomenon is hampered by the complex relationship between plasticity, exploration, and performance in RL. This paper introduces plasticity injection, a minimalistic intervention that increases the network plasticity without changing the number of trainable parameters or biasing the predictions. The applications of this intervention are two-fold: first, as a diagnostic tool \u2014 if injection increases the performance, we may conclude that an agent's network was losing its plasticity. This tool allows us to identify a subset of Atari environments where the lack of plasticity causes performance plateaus, motivating future studies on understanding and combating plasticity loss. Second, plasticity injection can be used to improve the computational efficiency of RL training if the agent has to re-learn from scratch due to exhausted plasticity or by growing the agent's network dynamically without compromising performance. The results on Atari show that plasticity injection attains stronger performance compared to alternative methods while being computationally efficient."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/85c2783a5be955c3dd68924c93bf36be721afd28.pdf"}, "supplementary_material": {"value": "/attachment/4c6e6562a1d839129838aeafbea2b76740e4b843.zip"}, "_bibtex": {"value": "@inproceedings{\nnikishin2023deep,\ntitle={Deep Reinforcement Learning with Plasticity Injection},\nauthor={Evgenii Nikishin and Junhyuk Oh and Georg Ostrovski and Clare Lyle and Razvan Pascanu and Will Dabney and Andre Barreto},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=jucDLW6G9l}\n}"}, "paperhash": {"value": "nikishin|deep_reinforcement_learning_with_plasticity_injection"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5168/-/Revision", "NeurIPS.cc/2023/Conference/Submission5168/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5168/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325880948, "odate": 1698949726574, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "zW1uVN6Mbv", "number": 5119, "cdate": 1683655335197, "tcdate": 1683655335197, "mdate": 1698949726255, "tmdate": 1698949726255, "signatures": ["NeurIPS.cc/2023/Conference/Submission5119/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5119/Authors"], "forum": "zW1uVN6Mbv", "content": {"title": {"value": "Unpaired Multi-Domain Causal Representation Learning"}, "authors": {"value": ["Nils Sturma", "Chandler Squires", "Mathias Drton", "Caroline Uhler"]}, "authorids": {"value": ["~Nils_Sturma1", "~Chandler_Squires1", "~Mathias_Drton2", "~Caroline_Uhler1"]}, "keywords": {"value": ["linear structural equation models", "causality", "representation learning", "independent component analysis", "structure identifiability", "multiple views", "graphical model"]}, "abstract": {"value": "The goal of causal representation learning is to find a representation of data that consists of causally related latent variables. We consider a setup where one has access to data from multiple domains that potentially share a causal representation. Crucially, observations in different domains are assumed to be unpaired, that is, we only observe the marginal distribution in each domain but not their joint distribution. In this paper, we give sufficient conditions for identifiability of the joint distribution and the shared causal graph in a linear setup. Identifiability holds if we can uniquely recover the joint distribution and the shared causal representation from the marginal distributions in each domain. We transform our results into a practical method to recover the shared latent causal graph."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8e88f0d5c37228f07723c6a426cc931807b9dbe6.pdf"}, "supplementary_material": {"value": "/attachment/7ae0996e20fba22daea144052bc2b38b1d5d7184.zip"}, "_bibtex": {"value": "@inproceedings{\nsturma2023unpaired,\ntitle={Unpaired Multi-Domain Causal Representation Learning},\nauthor={Nils Sturma and Chandler Squires and Mathias Drton and Caroline Uhler},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=zW1uVN6Mbv}\n}"}, "paperhash": {"value": "sturma|unpaired_multidomain_causal_representation_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5119/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5119/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325879526, "odate": 1698949726242, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "pLwYhNNnoR", "number": 5092, "cdate": 1683653872969, "tcdate": 1683653872969, "mdate": 1698949726088, "tmdate": 1698949726088, "signatures": ["NeurIPS.cc/2023/Conference/Submission5092/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5092/Authors"], "forum": "pLwYhNNnoR", "content": {"title": {"value": "PRODIGY: Enabling In-context Learning Over Graphs"}, "authors": {"value": ["Qian Huang", "Hongyu Ren", "Peng Chen", "Gregor Kr\u017emanc", "Daniel Zeng", "Percy Liang", "Jure Leskovec"]}, "authorids": {"value": ["~Qian_Huang2", "~Hongyu_Ren1", "~Peng_Chen7", "~Gregor_Kr\u017emanc1", "~Daniel_Zeng3", "~Percy_Liang1", "~Jure_Leskovec1"]}, "keywords": {"value": ["Graph Neural Network", "in-context learning", "pretraining"]}, "TLDR": {"value": "We enable in-context learning over graphs with a novel in-context task representation and corresponding pretraining architecture and objectives."}, "abstract": {"value": "In-context learning is the ability of a pretrained model to  adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \\textbf{Pr}etraining \\textbf{O}ver \\textbf{D}iverse \\textbf{I}n-Context  \\textbf{G}raph S\\textbf{y}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \\emph{prompt graph} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\\% on average with in-context learning."}, "pdf": {"value": "/pdf/5bc9864c69a87ef0ca6a6e9e50cf490467d8cd1e.pdf"}, "supplementary_material": {"value": "/attachment/f61fcf787622b4159393caa645d94cb6fb678703.zip"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nhuang2023prodigy,\ntitle={{PRODIGY}: Enabling In-context Learning Over Graphs},\nauthor={Qian Huang and Hongyu Ren and Peng Chen and Gregor Kr{\\v{z}}manc and Daniel Zeng and Percy Liang and Jure Leskovec},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=pLwYhNNnoR}\n}"}, "paperhash": {"value": "huang|prodigy_enabling_incontext_learning_over_graphs"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5092/-/Revision", "NeurIPS.cc/2023/Conference/Submission5092/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5092/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325878647, "odate": 1698949726073, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "EmYWJsyad4", "number": 5067, "cdate": 1683652388815, "tcdate": 1683652388815, "mdate": 1698949725845, "tmdate": 1698949725845, "signatures": ["NeurIPS.cc/2023/Conference/Submission5067/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5067/Authors"], "forum": "EmYWJsyad4", "content": {"title": {"value": "Conditional Mutual Information for Disentangled Representations in Reinforcement Learning"}, "authors": {"value": ["Mhairi Dunion", "Trevor McInroe", "Kevin Sebastian Luck", "Josiah P. Hanna", "Stefano V Albrecht"]}, "authorids": {"value": ["~Mhairi_Dunion1", "~Trevor_McInroe1", "~Kevin_Sebastian_Luck1", "~Josiah_P._Hanna1", "~Stefano_V_Albrecht1"]}, "keywords": {"value": ["Reinforcement Learning", "Representation Learning", "Disentanglement"]}, "TLDR": {"value": "We propose a conditional mutual information approach to learn disentangled representations that improve generalisation in reinforcement learning under correlation shifts"}, "abstract": {"value": "Reinforcement Learning (RL) environments can produce training data with spurious correlations between features due to the amount of training data or its limited feature coverage. This can lead to RL agents encoding these misleading correlations in their latent representation, preventing the agent from generalising if the correlation changes within the environment or when deployed in the real world. Disentangled representations can improve robustness, but existing disentanglement techniques that minimise mutual information between features require independent features, thus they cannot disentangle correlated features. We propose an auxiliary task for RL algorithms that learns a disentangled representation of high-dimensional observations with correlated features by minimising the conditional mutual information between features in the representation. We demonstrate experimentally, using continuous control tasks, that our approach improves generalisation under correlation shifts, as well as improving the training performance of RL algorithms in the presence of correlated features."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ee7658aed4668d7b31cb72967228b639b9706bf1.pdf"}, "supplementary_material": {"value": "/attachment/acaf7025e3e549afce67d4c7500874a2447eb166.zip"}, "_bibtex": {"value": "@inproceedings{\ndunion2023conditional,\ntitle={Conditional Mutual Information for Disentangled Representations in Reinforcement Learning},\nauthor={Mhairi Dunion and Trevor McInroe and Kevin Sebastian Luck and Josiah P. Hanna and Stefano V Albrecht},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=EmYWJsyad4}\n}"}, "paperhash": {"value": "dunion|conditional_mutual_information_for_disentangled_representations_in_reinforcement_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5067/-/Revision", "NeurIPS.cc/2023/Conference/Submission5067/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5067/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325877707, "odate": 1698949725833, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5otj6QKUMI", "number": 5058, "cdate": 1683651835553, "tcdate": 1683651835553, "mdate": 1698949725728, "tmdate": 1698949725728, "signatures": ["NeurIPS.cc/2023/Conference/Submission5058/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5058/Authors"], "forum": "5otj6QKUMI", "content": {"title": {"value": "Compression with Bayesian Implicit Neural Representations"}, "authors": {"value": ["Zongyu Guo", "Gergely Flamich", "Jiajun He", "Zhibo Chen", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"]}, "authorids": {"value": ["~Zongyu_Guo1", "~Gergely_Flamich1", "~Jiajun_He3", "~Zhibo_Chen1", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1"]}, "keywords": {"value": ["Neural Compression", "Implicit Neural Representation", "Relative Entropy Coding", "Bayesian Neural Network"]}, "TLDR": {"value": "We propose to compress data as variational Bayesian implicit neural representations with relative entropy coding that supports joint rate-distortion optimization."}, "abstract": {"value": "Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a progressive refinement process for the variational posterior that significantly enhances performance. Experiments show that our method achieves strong performance on image and audio compression while retaining simplicity."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d09a33bfba94ed62e3d487afe1f3874ac1ba1d85.pdf"}, "supplementary_material": {"value": "/attachment/99fd911c04b8fb388d79b97f0bb050d554daee2b.pdf"}, "_bibtex": {"value": "@inproceedings{\nguo2023compression,\ntitle={Compression with Bayesian Implicit Neural Representations},\nauthor={Zongyu Guo and Gergely Flamich and Jiajun He and Zhibo Chen and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5otj6QKUMI}\n}"}, "paperhash": {"value": "guo|compression_with_bayesian_implicit_neural_representations"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5058/-/Revision", "NeurIPS.cc/2023/Conference/Submission5058/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5058/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325877419, "odate": 1698949725716, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ffOhY40Nrh", "number": 5054, "cdate": 1683651613616, "tcdate": 1683651613616, "mdate": 1698949725702, "tmdate": 1698949725702, "signatures": ["NeurIPS.cc/2023/Conference/Submission5054/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission5054/Authors"], "forum": "ffOhY40Nrh", "content": {"title": {"value": "Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes"}, "authors": {"value": ["Aran Nayebi", "Rishi Rajalingham", "Mehrdad Jazayeri", "Guangyu Robert Yang"]}, "authorids": {"value": ["~Aran_Nayebi2", "~Rishi_Rajalingham1", "~Mehrdad_Jazayeri1", "~Guangyu_Robert_Yang1"]}, "keywords": {"value": ["neural coding", "mental simulation", "foundation models", "primate frontal cortex"]}, "TLDR": {"value": "Our findings suggest that the neural mechanisms and behaviors of primate mental simulation are thus far most consistent with being optimized to future predict on reusable visual representations on dynamic scenes."}, "abstract": {"value": "Humans and animals have a rich and flexible understanding of the physical world, which enables them to infer the underlying dynamical trajectories of objects and events, plausible future states, and use that to plan and anticipate the consequences of actions.\nHowever, the neural mechanisms underlying these computations are unclear.\nWe combine a goal-driven modeling approach with dense neurophysiological data and high-throughput human behavioral readouts that contain thousands of comparisons to directly impinge on this question.\nSpecifically, we construct and evaluate several classes of sensory-cognitive networks to predict the future state of rich, ethologically-relevant environments, ranging from self-supervised end-to-end models with pixel-wise or object-slot objectives, to models that future predict in the latent space of purely static image-pretrained or dynamic video-pretrained foundation models.\nWe find that ``scale is \\emph{not} all you need'', and that many state-of-the-art machine learning models fail to perform well on our neural and behavioral benchmarks for future prediction.\nIn fact, only one class of models matches these data well overall.\nWe find that neural responses are currently best predicted by models trained to predict the future state of their environment in the \\emph{latent} space of pretrained foundation models optimized for \\emph{dynamic} scenes in a self-supervised manner.\nThese models also approach the neurons' ability to predict the environmental state variables that are visually hidden from view, despite not being explicitly trained to do so.\nFinally, we find that not all foundation model latents are equal.\nNotably, models that future predict in the latent space of video foundation models that are optimized to support a \\emph{diverse} range of egocentric sensorimotor tasks, reasonably match \\emph{both} human behavioral error patterns and neural dynamics across all environmental scenarios that we were able to test.\nOverall, these findings suggest that the neural mechanisms and behaviors of primate mental simulation have strong inductive biases associated with them, and are thus far most consistent with being optimized to future predict on \\emph{reusable} visual representations that are useful for Embodied AI more generally."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/4a6d43b1d579b2e6b825568bfec4c6c1b2ad44d3.pdf"}, "supplementary_material": {"value": "/attachment/f3c974f38b5567bab840846cf4a41d9ad8cc09e1.pdf"}, "_bibtex": {"value": "@inproceedings{\nnayebi2023neural,\ntitle={Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes},\nauthor={Aran Nayebi and Rishi Rajalingham and Mehrdad Jazayeri and Guangyu Robert Yang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ffOhY40Nrh}\n}"}, "paperhash": {"value": "nayebi|neural_foundations_of_mental_simulation_future_prediction_of_latent_representations_on_dynamic_scenes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission5054/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission5054/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325877242, "odate": 1698949725688, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "nYgs0qZJ97", "number": 4999, "cdate": 1683649276110, "tcdate": 1683649276110, "mdate": 1698949725323, "tmdate": 1698949725323, "signatures": ["NeurIPS.cc/2023/Conference/Submission4999/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4999/Authors"], "forum": "nYgs0qZJ97", "content": {"title": {"value": "Regret Matching+: (In)Stability and Fast Convergence in Games"}, "authors": {"value": ["Gabriele Farina", "Julien Grand-Cl\u00e9ment", "Christian Kroer", "Chung-Wei Lee", "Haipeng Luo"]}, "authorids": {"value": ["~Gabriele_Farina1", "~Julien_Grand-Cl\u00e9ment1", "~Christian_Kroer1", "~Chung-Wei_Lee1", "~Haipeng_Luo1"]}, "keywords": {"value": ["Regret Matching", "Predictive algorithms", "Extensive-Form Games"]}, "abstract": {"value": "Regret Matching$^+$ (RM$^+$) and its variants are important algorithms for solving large-scale games.\nHowever, a theoretical understanding of their success in practice is still a mystery.\nMoreover, recent advances on fast convergence in games are limited to no-regret algorithms such as online mirror descent, which satisfy stability.\nIn this paper, we first give counterexamples showing that RM+ and its predictive version can be unstable, which might cause other players to suffer large regret. \nWe then provide two fixes: restarting and chopping off the positive orthant that RM$^+$ works in.\nWe show that these fixes are sufficient to get $O(T^{1/4})$ individual regret and $O(1)$ social regret in normal-form games via RM$^+$ with predictions.\nWe also apply our stabilizing techniques to clairvoyant updates in the uncoupled learning setting for RM$^+$ and prove desirable results akin to recent works for Clairvoyant online mirror descent. \nOur experiments show the advantages of our algorithms over vanilla RM$^+$-based algorithms in matrix and extensive-form games."}, "pdf": {"value": "/pdf/57bb36b9439fe55c235ebdb6d5fa85d4a5298a5b.pdf"}, "supplementary_material": {"value": "/attachment/5310f9bc1398110e7aad9f34c6c8949b0048fec9.pdf"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nfarina2023regret,\ntitle={Regret Matching+: (In)Stability and Fast Convergence in Games},\nauthor={Gabriele Farina and Julien Grand-Cl{\\'e}ment and Christian Kroer and Chung-Wei Lee and Haipeng Luo},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=nYgs0qZJ97}\n}"}, "paperhash": {"value": "farina|regret_matching_instability_and_fast_convergence_in_games"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4999/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4999/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325875232, "odate": 1698949725308, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "2CRaOpEKWh", "number": 4970, "cdate": 1683647884928, "tcdate": 1683647884928, "mdate": 1698949725109, "tmdate": 1698949725109, "signatures": ["NeurIPS.cc/2023/Conference/Submission4970/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4970/Authors"], "forum": "2CRaOpEKWh", "content": {"title": {"value": "Characterizing the Optimal $0-1$ Loss for Multi-class Classification with a Test-time Attacker"}, "authors": {"value": ["Sihui Dai", "Wenxin Ding", "Arjun Nitin Bhagoji", "Daniel Cullina", "Haitao Zheng", "Ben Y. Zhao", "Prateek Mittal"]}, "authorids": {"value": ["~Sihui_Dai1", "~Wenxin_Ding1", "~Arjun_Nitin_Bhagoji1", "~Daniel_Cullina1", "~Haitao_Zheng2", "~Ben_Y._Zhao2", "~Prateek_Mittal1"]}, "keywords": {"value": ["adversarial robustness", "graph theory", "fundamental bounds"]}, "TLDR": {"value": "Find and evaluate several methods to bound the loss of optimal multi-class classifiers against test-time attacks"}, "abstract": {"value": "Finding classifiers robust to adversarial examples is critical for their safe\ndeployment. Determining the robustness of the best possible classifier under a\ngiven threat model for a fixed data distribution and comparing it to that\nachieved by state-of-the-art training methods is thus an important diagnostic\ntool. In this paper, we find achievable information-theoretic lower bounds on\nrobust loss in the presence of a test-time attacker for *multi-class\nclassifiers on any discrete dataset*. We provide a general framework for finding\nthe optimal $0-1$ loss that revolves around the construction of a conflict\nhypergraph from the data and adversarial constraints. The prohibitive cost of\nthis formulation in practice leads us to formulate other variants of the attacker-classifier\ngame that more efficiently determine the range of the optimal loss. Our\nvaluation shows, for the first time, an analysis of the gap to optimal\nrobustness for classifiers in the multi-class setting on benchmark datasets."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c57ef3e5000d137087a60c2283ccb91cf6cb28cb.pdf"}, "supplementary_material": {"value": "/attachment/f7a8e6da095d3f02fbf52fb53f0237fb69dfd4b5.zip"}, "_bibtex": {"value": "@inproceedings{\ndai2023characterizing,\ntitle={Characterizing the Optimal \\$0-1\\$ Loss for Multi-class Classification with a Test-time Attacker},\nauthor={Sihui Dai and Wenxin Ding and Arjun Nitin Bhagoji and Daniel Cullina and Haitao Zheng and Ben Y. Zhao and Prateek Mittal},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=2CRaOpEKWh}\n}"}, "paperhash": {"value": "dai|characterizing_the_optimal_01_loss_for_multiclass_classification_with_a_testtime_attacker"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4970/-/Revision", "NeurIPS.cc/2023/Conference/Submission4970/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4970/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325874372, "odate": 1698949725096, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "TQlpqmCeMe", "number": 4931, "cdate": 1683646109743, "tcdate": 1683646109743, "mdate": 1698949724899, "tmdate": 1698949724899, "signatures": ["NeurIPS.cc/2023/Conference/Submission4931/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4931/Authors"], "forum": "TQlpqmCeMe", "content": {"title": {"value": "Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem"}, "authors": {"value": ["Tal Amir", "Steven J. Gortler", "Ilai Avni", "Ravina Ravina", "Nadav Dym"]}, "authorids": {"value": ["~Tal_Amir1", "~Steven_J._Gortler1", "~Ilai_Avni1", "ravina@campus.technion.ac.il", "~Nadav_Dym1"]}, "keywords": {"value": ["Equivariant Neural Networks", "Universal approximation", "Geometric deep learning", "multiset learning", "injective multiset functions", "learning on measures. WL test"]}, "TLDR": {"value": "We show that using analytic activations, one can construct finite-size NNs that are injective on multisets and discrete measures. As corollaries, we improve known results on approximation of multiset functions, and on equivalence of GNNs and WL tests"}, "abstract": {"value": "Injective multiset functions have a key role in the theoretical study of machine learning on multisets and graphs. Yet, there remains a gap between the provably injective multiset functions considered in theory, which typically rely on polynomial moments, and the multiset functions used in practice, which rely on $\\textit{neural moments}$ \u2014 whose injectivity on multisets has not been studied to date.\n\nIn this paper, we bridge this gap by showing that moments of neural networks do define injective multiset functions,  provided that an analytic non-polynomial activation is used. The number of moments required by our theory is optimal essentially up to a multiplicative factor of two. To prove this result, we state and prove a $\\textit{finite witness theorem}$, which is of independent interest. \n\nAs a corollary to our main theorem, we derive new approximation results for functions on multisets and measures, and new separation results for graph neural networks. We also provide two negative results: (1) moments of piecewise-linear neural networks cannot be injective multiset functions; and (2) even when moment-based multiset functions are injective, they can never be bi-Lipschitz."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8e1f6d5cbf4f532809f9250b192d0a93c7bfc75a.pdf"}, "supplementary_material": {"value": "/attachment/d0985ba45d65324a8cc79effa17615bfbb815b22.zip"}, "_bibtex": {"value": "@inproceedings{\namir2023neural,\ntitle={Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem},\nauthor={Tal Amir and Steven J. Gortler and Ilai Avni and Ravina Ravina and Nadav Dym},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=TQlpqmCeMe}\n}"}, "paperhash": {"value": "amir|neural_injective_functions_for_multisets_measures_and_graphs_via_a_finite_witness_theorem"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4931/-/Revision", "NeurIPS.cc/2023/Conference/Submission4931/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4931/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325873136, "odate": 1698949724881, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "n3XuYdvhNW", "number": 4899, "cdate": 1683644896356, "tcdate": 1683644896356, "mdate": 1698949724639, "tmdate": 1698949724639, "signatures": ["NeurIPS.cc/2023/Conference/Submission4899/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4899/Authors"], "forum": "n3XuYdvhNW", "content": {"title": {"value": "Fast Optimal Transport through Sliced Generalized Wasserstein Geodesics"}, "authors": {"value": ["Guillaume Mahey", "Laetitia Chapel", "Gilles Gasso", "Cl\u00e9ment Bonet", "Nicolas Courty"]}, "authorids": {"value": ["~Guillaume_Mahey1", "~Laetitia_Chapel1", "~Gilles_Gasso1", "~Cl\u00e9ment_Bonet1", "~Nicolas_Courty1"]}, "keywords": {"value": ["Optimal Transport", "Wasserstein distance", "Generalized Geodesics", "Sliced Wasserstein"]}, "TLDR": {"value": "A new upper bound of $W^2_2$ that is fast to compute and comes with a transport plan"}, "abstract": {"value": "Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined $\\textnormal{min-SWGG}$, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between  $\\textnormal{min-SWGG}$, and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that  $\\textnormal{min-SWGG}$, is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and topological properties. Empirical evidences support the benefits of  $\\textnormal{min-SWGG}$, in various contexts, from gradient flows, shape matching and image colorization, among others."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7693c572f558be4c7987c0693d79b4a77927f25d.pdf"}, "_bibtex": {"value": "@inproceedings{\nmahey2023fast,\ntitle={Fast Optimal Transport through Sliced Generalized Wasserstein Geodesics},\nauthor={Guillaume Mahey and Laetitia Chapel and Gilles Gasso and Cl{\\'e}ment Bonet and Nicolas Courty},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=n3XuYdvhNW}\n}"}, "paperhash": {"value": "mahey|fast_optimal_transport_through_sliced_generalized_wasserstein_geodesics"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4899/-/Revision", "NeurIPS.cc/2023/Conference/Submission4899/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4899/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325872045, "odate": 1698949724625, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "7vqlzODS28", "number": 4890, "cdate": 1683644713186, "tcdate": 1683644713186, "mdate": 1698949724566, "tmdate": 1698949724566, "signatures": ["NeurIPS.cc/2023/Conference/Submission4890/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4890/Authors"], "forum": "7vqlzODS28", "content": {"title": {"value": "HyTrel: Hypergraph-enhanced  Tabular Data Representation Learning"}, "authors": {"value": ["Pei Chen", "Soumajyoti Sarkar", "Leonard Lausen", "Balasubramaniam Srinivasan", "Sheng Zha", "Ruihong Huang", "George Karypis"]}, "authorids": {"value": ["~Pei_Chen2", "~Soumajyoti_Sarkar1", "~Leonard_Lausen1", "~Balasubramaniam_Srinivasan1", "~Sheng_Zha1", "~Ruihong_Huang1", "~George_Karypis1"]}, "keywords": {"value": ["Tabular Language Model", "Tabular Representation Learning", "Pretraining", "Tabular Data", "Table", "Hypergraph"]}, "abstract": {"value": "Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks.\nHowever, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HyTrel, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs--where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that\nHyTrel is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HyTrel\niff the two tables are identical up to permutation. Our empirical results demonstrate that HyTrel consistently outperforms other competitive baselines on four downstream tasks with minimal pretraining, illustrating the advantages of incorporating inductive biases associated with tabular data into the representations. Finally, our qualitative analyses showcase that HyTrel can assimilate the table structure to generate robust representations for the cells, rows, columns, and the entire table."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a hypergraph-based tabular language model that captures the invariance and structure of tables, with theoretical, empirical and qualitative evidences supporting the effectiveness of learning these table properties into the representations."}, "pdf": {"value": "/pdf/07295a067dd389cf5b12afd19f809019d9dde79e.pdf"}, "_bibtex": {"value": "@inproceedings{\nchen2023hytrel,\ntitle={HyTrel: Hypergraph-enhanced  Tabular Data Representation Learning},\nauthor={Pei Chen and Soumajyoti Sarkar and Leonard Lausen and Balasubramaniam Srinivasan and Sheng Zha and Ruihong Huang and George Karypis},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=7vqlzODS28}\n}"}, "paperhash": {"value": "chen|hytrel_hypergraphenhanced_tabular_data_representation_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4890/-/Revision", "NeurIPS.cc/2023/Conference/Submission4890/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4890/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325871838, "odate": 1698949724551, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "BACQLWQW8u", "number": 4882, "cdate": 1683644234503, "tcdate": 1683644234503, "mdate": 1698949724503, "tmdate": 1698949724503, "signatures": ["NeurIPS.cc/2023/Conference/Submission4882/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4882/Authors"], "forum": "BACQLWQW8u", "content": {"title": {"value": "Subspace Identification for Multi-Source Domain Adaptation"}, "authors": {"value": ["Zijian Li", "Ruichu Cai", "Guangyi Chen", "Boyang Sun", "Zhifeng Hao", "Kun Zhang"]}, "authorids": {"value": ["~Zijian_Li1", "~Ruichu_Cai1", "~Guangyi_Chen1", "~Boyang_Sun1", "~Zhifeng_Hao2", "~Kun_Zhang1"]}, "keywords": {"value": ["Domain Adaptation", "Identification"]}, "abstract": {"value": "Multi-source domain adaptation (MSDA) methods aim to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Although current methods achieve target joint distribution identifiability by enforcing minimal changes across domains, they often necessitate stringent conditions, such as an adequate number of domains, monotonic transformation of latent variables, and invariant label distributions. These requirements are challenging to satisfy in real-world applications. To mitigate the need for these strict assumptions, we propose a subspace identification theory that guarantees the disentanglement of domain-invariant and domain-specific variables under less restrictive constraints regarding domain numbers and transformation properties and thereby facilitating domain adaptation by minimizing the impact of domain shifts on invariant variables. Based on this theory, we develop a Subspace Identification Guarantee (SIG) model that leverages variational inference. Furthermore, the SIG model incorporates class-aware conditional alignment to accommodate target shifts where label distributions change with the domain. Experimental results demonstrate that our SIG model outperforms existing MSDA techniques on various benchmark datasets, highlighting its effectiveness in real-world applications."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e9c1ac90a66a37048b220c9adf1b4a2b84b4d4eb.pdf"}, "supplementary_material": {"value": "/attachment/ec60b24356b20e3048741398328b9a1b62f39708.zip"}, "_bibtex": {"value": "@inproceedings{\nli2023subspace,\ntitle={Subspace Identification for Multi-Source Domain Adaptation},\nauthor={Zijian Li and Ruichu Cai and Guangyi Chen and Boyang Sun and Zhifeng Hao and Kun Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BACQLWQW8u}\n}"}, "paperhash": {"value": "li|subspace_identification_for_multisource_domain_adaptation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4882/-/Revision", "NeurIPS.cc/2023/Conference/Submission4882/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4882/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325871686, "odate": 1698949724491, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "UkPeUXML7s", "number": 4801, "cdate": 1683641660066, "tcdate": 1683641660066, "mdate": 1698949723972, "tmdate": 1698949723972, "signatures": ["NeurIPS.cc/2023/Conference/Submission4801/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4801/Authors"], "forum": "UkPeUXML7s", "content": {"title": {"value": "Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient Descent"}, "authors": {"value": ["Krunoslav Lehman Pavasovic", "Alain Durmus", "Umut Simsekli"]}, "authorids": {"value": ["~Krunoslav_Lehman_Pavasovic1", "~Alain_Durmus1", "~Umut_Simsekli1"]}, "keywords": {"value": ["SGD", "heavy-tails", "wasserstein convergence"]}, "abstract": {"value": "A recent line of empirical studies has demonstrated that SGD might exhibit a heavy-tailed behavior in practical settings, and the heaviness of the tails might correlate with the overall performance. In this paper, we investigate the emergence of such heavy tails. Previous works on this problem only considered, up to our knowledge, online (also called single-pass) SGD, in which the emergence of heavy tails in theoretical findings is contingent upon access to an infinite amount of data. Hence, the underlying mechanism generating the reported heavy-tailed behavior in practical settings, where the amount of training data is finite, is still not well-understood. Our contribution aims to fill this gap. In particular, we show that the stationary distribution of offline (also called multi-pass) SGD exhibits \u2018approximate\u2019 power-law tails and the approximation error is controlled by how fast the empirical distribution of the training data converges to the true underlying data distribution in the Wasserstein metric. Our main takeaway is that, as the number of data points increases, offline SGD will behave increasingly \u2018power-law-like\u2019. To achieve this result, we first prove nonasymptotic Wasserstein convergence bounds for offline SGD to online SGD as the number of data points increases, which can be interesting on their own. Finally, we illustrate our theory on various experiments conducted on synthetic data and neural networks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3540a930f63eb69ed07f357f2f8c704997560b67.pdf"}, "_bibtex": {"value": "@inproceedings{\npavasovic2023approximate,\ntitle={Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient Descent},\nauthor={Krunoslav Lehman Pavasovic and Alain Durmus and Umut Simsekli},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=UkPeUXML7s}\n}"}, "paperhash": {"value": "pavasovic|approximate_heavy_tails_in_offline_multipass_stochastic_gradient_descent"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4801/-/Revision", "NeurIPS.cc/2023/Conference/Submission4801/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4801/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325869207, "odate": 1698949723958, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "tEKBU5XOTw", "number": 4791, "cdate": 1683641241776, "tcdate": 1683641241776, "mdate": 1698949723914, "tmdate": 1698949723914, "signatures": ["NeurIPS.cc/2023/Conference/Submission4791/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4791/Authors"], "forum": "tEKBU5XOTw", "content": {"title": {"value": "Safety Verification of Decision-Tree Policies in Continuous Time"}, "authors": {"value": ["Christian Schilling", "Anna Lukina", "Emir Demirovi\u0107", "Kim Guldstrand Larsen"]}, "authorids": {"value": ["~Christian_Schilling1", "a.lukina@tudelft.nl", "~Emir_Demirovi\u01071", "kgl@cs.aau.dk"]}, "keywords": {"value": ["safety verification", "decision tree", "reinforcement learning", "controller", "continuous time"]}, "TLDR": {"value": "We present the first approach to safety verification for continuous-time decision-tree policies."}, "abstract": {"value": "Decision trees have gained popularity as interpretable surrogate models for learning-based control policies. However, providing safety guarantees for systems controlled by decision trees is an open challenge. We show that the problem is undecidable even for systems with the simplest dynamics, and PSPACE-complete for finite-horizon properties. The latter can be verified for discrete-time systems via bounded model checking. However, for continuous-time systems, such an approach requires discretization, thereby weakening the guarantees for the original system. This paper presents the first algorithm to directly verify decision-tree controlled system in continuous time. The key aspect of our method is exploiting the decision-tree structure to propagate a set-based approximation through the decision nodes. We demonstrate the effectiveness of our approach by verifying safety of several decision trees distilled to imitate neural-network policies for nonlinear systems."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0f6cdcff129128b55f1bd5817ec690f7480bddee.pdf"}, "_bibtex": {"value": "@inproceedings{\nschilling2023safety,\ntitle={Safety Verification of Decision-Tree Policies in Continuous Time},\nauthor={Christian Schilling and Anna Lukina and Emir Demirovi{\\'c} and Kim Guldstrand Larsen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=tEKBU5XOTw}\n}"}, "paperhash": {"value": "schilling|safety_verification_of_decisiontree_policies_in_continuous_time"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4791/-/Revision", "NeurIPS.cc/2023/Conference/Submission4791/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4791/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325868999, "odate": 1698949723901, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "BsZNWXD3a1", "number": 4724, "cdate": 1683638722392, "tcdate": 1683638722392, "mdate": 1698949723530, "tmdate": 1698949723530, "signatures": ["NeurIPS.cc/2023/Conference/Submission4724/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4724/Authors"], "forum": "BsZNWXD3a1", "content": {"title": {"value": "Optimizing Prompts for Text-to-Image Generation"}, "authors": {"value": ["Yaru Hao", "Zewen Chi", "Li Dong", "Furu Wei"]}, "authorids": {"value": ["~Yaru_Hao1", "~Zewen_Chi1", "~Li_Dong1", "~Furu_Wei1"]}, "keywords": {"value": ["prompt adaptation", "automatic prompt engineering", "text-to-image generation"]}, "TLDR": {"value": "We propose to automatically optimize prompts for text-to-image models so that the user input and model-preferred prompts can be well aligned."}, "abstract": {"value": "Well-designed prompts can guide text-to-image models to generate amazing images. However, the performant prompts are often model-specific and misaligned with user input. Instead of laborious human engineering, we propose prompt adaptation, a general framework that automatically adapts original user input to model-preferred prompts. Specifically, we first perform supervised fine-tuning with a pretrained language model on a small collection of manually engineered prompts. Then we use reinforcement learning to explore better prompts. We define a reward function that encourages the policy to generate more aesthetically pleasing images while preserving the original user intentions. Experimental results on Stable Diffusion show that our method outperforms manual prompt engineering in terms of both automatic metrics and human preference ratings. Moreover, reinforcement learning further boosts performance, especially on out-of-domain prompts."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e842f177a19b119f7aacf34b8e2155b9b897c511.pdf"}, "_bibtex": {"value": "@inproceedings{\nhao2023optimizing,\ntitle={Optimizing Prompts for Text-to-Image Generation},\nauthor={Yaru Hao and Zewen Chi and Li Dong and Furu Wei},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BsZNWXD3a1}\n}"}, "paperhash": {"value": "hao|optimizing_prompts_for_texttoimage_generation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4724/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4724/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325866743, "odate": 1698949723515, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ooXpTZYwXa", "number": 4623, "cdate": 1683634690357, "tcdate": 1683634690357, "mdate": 1698949722805, "tmdate": 1698949722805, "signatures": ["NeurIPS.cc/2023/Conference/Submission4623/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4623/Authors"], "forum": "ooXpTZYwXa", "content": {"title": {"value": "Explore In-Context Learning for 3D Point Cloud Understanding"}, "authors": {"value": ["Zhongbin Fang", "Xiangtai Li", "Xia Li", "Joachim M. Buhmann", "Chen Change Loy", "Mengyuan Liu"]}, "authorids": {"value": ["~Zhongbin_Fang1", "~Xiangtai_Li1", "~Xia_Li3", "~Joachim_M._Buhmann1", "~Chen_Change_Loy2", "~Mengyuan_Liu2"]}, "keywords": {"value": ["In-context learning", "Point cloud", "Prompt tuning"]}, "TLDR": {"value": "We present a novel framework for in-context learning applied to 3D point cloud understanding, a learning paradigm that has demonstrated significant potential in various domains such as large language model inference and multi-task image processing."}, "abstract": {"value": "With the rise of large-scale models trained on broad data, in-context learning has become a new learning paradigm that has demonstrated significant potential in natural language processing and computer vision tasks. Meanwhile, in-context learning is still largely unexplored in the 3D point cloud domain. Although masked modeling has been successfully applied for in-context learning in 2D vision, directly extending it to 3D point clouds remains a formidable challenge. In the case of point clouds, the tokens themselves are the point cloud positions (coordinates) that are masked during inference. Moreover, position embedding in previous works may inadvertently introduce information leakage. To address these challenges, we introduce a novel framework, named Point-In-Context, designed especially for in-context learning in 3D point clouds, where both inputs and outputs are modeled as coordinates for each task. Additionally, we propose the Joint Sampling module, carefully designed to work in tandem with the general point sampling operator, effectively resolving the aforementioned technical issues. We conduct extensive experiments to validate the versatility and adaptability of our proposed methods in handling a wide range of tasks. Furthermore, with a more effective prompt selection strategy, our framework surpasses the results of individually trained models."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9fcde827da72328dc9dc149da7c789d5db4dcb59.pdf"}, "supplementary_material": {"value": "/attachment/a7933bba53708db58d3169c75a373857b4f446b9.zip"}, "_bibtex": {"value": "@inproceedings{\nfang2023explore,\ntitle={Explore In-Context Learning for 3D Point Cloud Understanding},\nauthor={Zhongbin Fang and Xiangtai Li and Xia Li and Joachim M. Buhmann and Chen Change Loy and Mengyuan Liu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ooXpTZYwXa}\n}"}, "paperhash": {"value": "fang|explore_incontext_learning_for_3d_point_cloud_understanding"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4623/-/Revision", "NeurIPS.cc/2023/Conference/Submission4623/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4623/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325863446, "odate": 1698949722792, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "qSS9izTOpo", "number": 4568, "cdate": 1683632160616, "tcdate": 1683632160616, "mdate": 1698949722452, "tmdate": 1698949722452, "signatures": ["NeurIPS.cc/2023/Conference/Submission4568/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4568/Authors"], "forum": "qSS9izTOpo", "content": {"title": {"value": "Alleviating the Semantic Gap for Generalized fMRI-to-Image Reconstruction"}, "authors": {"value": ["Tao Fang", "Qian Zheng", "Gang Pan"]}, "authorids": {"value": ["~Tao_Fang4", "~Qian_Zheng5", "~Gang_Pan1"]}, "keywords": {"value": ["fMRI", "image reconstruction", "brain decoding"]}, "abstract": {"value": "Although existing fMRI-to-image reconstruction methods could predict high-quality images, they do not explicitly consider the semantic gap between training and testing data, resulting in reconstruction with unstable and uncertain semantics. This paper addresses the problem of generalized fMRI-to-image reconstruction by explicitly alleviates the semantic gap. Specifically, we leverage the pre-trained CLIP model to map the training data to a compact feature representation, which essentially extends the sparse semantics of training data to dense ones, thus alleviating the semantic gap of the instances nearby known concepts (i.e., inside the training super-classes). Inspired by the robust low-level representation in fMRI data, which could help alleviate the semantic gap for instances that far from the known concepts (i.e., outside the training super-classes), we leverage structural information as a general cue to guide image reconstruction. Further, we quantify the semantic uncertainty based on probability density estimation and achieve Generalized fMRI-to-image reconstruction by adaptively integrating Expanded Semantics and Structural information (GESS) within a diffusion process. Experimental results demonstrate that the proposed GESS model outperforms state-of-the-art methods, and we propose a generalized scenario split strategy to evaluate the advantage of GESS in closing the semantic gap."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0663fdf3576e9dfd2ff91e2c9fb77c0a59141d8b.pdf"}, "supplementary_material": {"value": "/attachment/f443f723a61076c7fa9d1a3f15bb7af145158f41.pdf"}, "_bibtex": {"value": "@inproceedings{\nfang2023alleviating,\ntitle={Alleviating the Semantic Gap for Generalized f{MRI}-to-Image Reconstruction},\nauthor={Tao Fang and Qian Zheng and Gang Pan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=qSS9izTOpo}\n}"}, "paperhash": {"value": "fang|alleviating_the_semantic_gap_for_generalized_fmritoimage_reconstruction"}}, "pdate": 1695325861574, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4568/-/Revision", "NeurIPS.cc/2023/Conference/Submission4568/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4568/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "odate": 1698949722427, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "vz7SdRqWGM", "number": 4509, "cdate": 1683628185175, "tcdate": 1683628185175, "mdate": 1698949722120, "tmdate": 1698949722120, "signatures": ["NeurIPS.cc/2023/Conference/Submission4509/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4509/Authors"], "forum": "vz7SdRqWGM", "content": {"title": {"value": "Adaptive whitening with fast gain modulation and slow synaptic plasticity"}, "authors": {"value": ["Lyndon Duong", "Eero P Simoncelli", "Dmitri Chklovskii", "David Lipshutz"]}, "authorids": {"value": ["~Lyndon_Duong1", "~Eero_P_Simoncelli1", "~Dmitri_Chklovskii1", "~David_Lipshutz1"]}, "keywords": {"value": ["neuroscience", "adaptation", "whitening", "efficient coding", "recurrent neural network", "gain modulation", "synaptic plasticity", "local learning rules"]}, "abstract": {"value": "Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to match structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale whitening objective that factorizes the inverse whitening matrix into basis vectors, which correspond to synaptic weights, and a diagonal matrix, which corresponds to neuronal gains. We test our model on synthetic and natural datasets and find that the synapses learn optimal configurations over long timescales that enable adaptive whitening on short timescales using gain modulation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a neural circuit for adaptive whitening with synapses that learn across statistical contexts and gains that adapt within statistical contexts."}, "pdf": {"value": "/pdf/85642724098e98a3bc1c8babf1d25d5f0f183921.pdf"}, "supplementary_material": {"value": "/attachment/533afee31b7db088aeb176009341ca8a6559a93c.zip"}, "_bibtex": {"value": "@inproceedings{\nduong2023adaptive,\ntitle={Adaptive whitening with fast gain modulation and slow synaptic plasticity},\nauthor={Lyndon Duong and Eero P Simoncelli and Dmitri Chklovskii and David Lipshutz},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vz7SdRqWGM}\n}"}, "paperhash": {"value": "duong|adaptive_whitening_with_fast_gain_modulation_and_slow_synaptic_plasticity"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4509/-/Revision", "NeurIPS.cc/2023/Conference/Submission4509/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4509/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325859956, "odate": 1698949722108, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "BC1IJdsuYB", "number": 4329, "cdate": 1683619090654, "tcdate": 1683619090654, "mdate": 1698949721038, "tmdate": 1698949721038, "signatures": ["NeurIPS.cc/2023/Conference/Submission4329/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4329/Authors"], "forum": "BC1IJdsuYB", "content": {"title": {"value": "Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models"}, "authors": {"value": ["Alvin Heng", "Harold Soh"]}, "authorids": {"value": ["~Alvin_Heng1", "~Harold_Soh1"]}, "keywords": {"value": ["generative models", "forgetting"]}, "abstract": {"value": "The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting  where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We devise a method inspired by continual learning for selective forgetting of concepts in deep generative models."}, "pdf": {"value": "/pdf/be7c5f7d0b95b8679986e29c229a4d82e9d03db9.pdf"}, "supplementary_material": {"value": "/attachment/0196a5d3e1d44f9a7c9e9d76fc8ffdf5da5d14ba.zip"}, "_bibtex": {"value": "@inproceedings{\nheng2023selective,\ntitle={Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models},\nauthor={Alvin Heng and Harold Soh},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BC1IJdsuYB}\n}"}, "paperhash": {"value": "heng|selective_amnesia_a_continual_learning_approach_to_forgetting_in_deep_generative_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4329/-/Revision", "NeurIPS.cc/2023/Conference/Submission4329/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4329/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325854026, "odate": 1698949721025, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "CxjmYRP9Ji", "number": 4308, "cdate": 1683618139504, "tcdate": 1683618139504, "mdate": 1698949720841, "tmdate": 1698949720841, "signatures": ["NeurIPS.cc/2023/Conference/Submission4308/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4308/Authors"], "forum": "CxjmYRP9Ji", "content": {"title": {"value": "Non-Asymptotic Analysis of a UCB-based Top Two Algorithm"}, "authors": {"value": ["Marc Jourdan", "R\u00e9my Degenne"]}, "authorids": {"value": ["~Marc_Jourdan1", "~R\u00e9my_Degenne1"]}, "keywords": {"value": ["multi-armed bandits", "best-arm identification", "Gaussian bandits", "Top Two algorithm", "fixed confidence", "finite confidence"]}, "abstract": {"value": "A Top Two sampling rule for bandit identification is a method which selects the next arm to sample from among two candidate arms, a *leader* and a *challenger*. Due to their simplicity and good empirical performance, they have received increased attention in recent years. However, for fixed-confidence best arm identification, theoretical guarantees for Top Two methods have only been obtained in the asymptotic regime, when the error level vanishes. In this paper, we derive the first non-asymptotic upper bound on the expected sample complexity of a Top Two algorithm, which holds for any error level. Our analysis highlights sufficient properties for a regret minimization algorithm to be used as leader. These properties are satisfied by the UCB algorithm, and our proposed UCB-based Top Two algorithm simultaneously enjoys non-asymptotic guarantees and competitive empirical performance."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5bf86ad8c8848fad770b9249f328f76cebe3e780.pdf"}, "supplementary_material": {"value": "/attachment/6cc13eef45ceb04bcbc27902c253b36ef80db40a.zip"}, "TLDR": {"value": "We have studied fixed-confidence best arm identification in multi-armed bandits and provide the first non-asymptotic analysis of a Top Two algorithm."}, "_bibtex": {"value": "@inproceedings{\njourdan2023nonasymptotic,\ntitle={Non-Asymptotic Analysis of a {UCB}-based Top Two Algorithm},\nauthor={Marc Jourdan and R{\\'e}my Degenne},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=CxjmYRP9Ji}\n}"}, "paperhash": {"value": "jourdan|nonasymptotic_analysis_of_a_ucbbased_top_two_algorithm"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4308/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325853402, "odate": 1698949720827, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "financial_support"}, {"name": "reviewer_nomination"}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "dEySGIcDnI", "number": 4254, "cdate": 1683616283261, "tcdate": 1683616283261, "mdate": 1698949720510, "tmdate": 1698949720510, "signatures": ["NeurIPS.cc/2023/Conference/Submission4254/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4254/Authors"], "forum": "dEySGIcDnI", "content": {"title": {"value": "Separable Physics-Informed Neural Networks"}, "authors": {"value": ["Junwoo Cho", "Seungtae Nam", "Hyunmo Yang", "Seok-Bae Yun", "Youngjoon Hong", "Eunbyung Park"]}, "authorids": {"value": ["~Junwoo_Cho1", "~Seungtae_Nam1", "~Hyunmo_Yang1", "~Seok-Bae_Yun1", "~Youngjoon_Hong2", "~Eunbyung_Park1"]}, "keywords": {"value": ["partial differential equations", "scientific machine learning", "physics-informed neural networks", "fluid dynamics"]}, "abstract": {"value": "Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. \nHowever, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate very complex solution functions.\nThe number of training points (collocation points) required on these challenging PDEs grows substantially, and it is severely limited due to the expensive computational costs and heavy memory overhead.\nTo overcome this limit, we propose a network architecture and training algorithm for PINNs.\nThe proposed method, separable PINN (SPINN), operates on a per-axis basis to decrease the number of network propagations in multi-dimensional PDEs instead of point-wise processing in conventional PINNs.\nWe also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points ($>10^7$) on a single commodity GPU. \nThe experimental results show significantly reduced computational costs ($62\\times$ in wall-clock time, $1,394\\times$ in FLOPs given the same number of collocation points) in multi-dimensional PDEs while achieving better accuracy.\nFurthermore, we present that SPINN can solve a chaotic (2+1)-d Navier-Stokes equation much faster than the best-performing prior method (9 minutes vs. 10 hours in a single GPU), maintaining accuracy.\nFinally, we showcase that SPINN can accurately obtain the solution of a highly nonlinear and multi-dimensional PDE, a (3+1)-d Navier-Stokes equation.\nFor visualized results and code, please see https://jwcho5576.github.io/spinn.github.io/."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f5fac4f8d96b058022b3d3c9b8123a7ace9a1d9b.pdf"}, "supplementary_material": {"value": "/attachment/bda37ece307c8c9fcd66a3a6499b3a5f46440d25.zip"}, "_bibtex": {"value": "@inproceedings{\ncho2023separable,\ntitle={Separable Physics-Informed Neural Networks},\nauthor={Junwoo Cho and Seungtae Nam and Hyunmo Yang and Seok-Bae Yun and Youngjoon Hong and Eunbyung Park},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=dEySGIcDnI}\n}"}, "TLDR": {"value": "proposed a PINNs (physics-informed neural networks) architecture for fast training"}, "paperhash": {"value": "cho|separable_physicsinformed_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4254/-/Revision", "NeurIPS.cc/2023/Conference/Submission4254/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4254/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325851395, "odate": 1698949720496, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Ypbke6biDm", "number": 4251, "cdate": 1683616123613, "tcdate": 1683616123613, "mdate": 1698949720478, "tmdate": 1698949720478, "signatures": ["NeurIPS.cc/2023/Conference/Submission4251/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4251/Authors"], "forum": "Ypbke6biDm", "content": {"title": {"value": "Pareto Frontiers in Deep Feature Learning: Data, Compute, Width, and Luck"}, "authors": {"value": ["Benjamin L. Edelman", "Surbhi Goel", "Sham M. Kakade", "eran malach", "Cyril Zhang"]}, "authorids": {"value": ["~Benjamin_L._Edelman1", "~Surbhi_Goel1", "~Sham_M._Kakade1", "~eran_malach1", "~Cyril_Zhang1"]}, "keywords": {"value": ["deep learning", "feature learning", "parity", "grokking", "lottery tickets", "scaling"]}, "abstract": {"value": "In modern deep learning, algorithmic choices (such as width, depth, and learning rate) are known to modulate nuanced resource tradeoffs. This work investigates how these complexities necessarily arise for feature learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a *multi-resource tradeoff frontier*: \nsuccessful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding \"lottery ticket\" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real problems requiring axis-aligned feature learning. We demonstrate improved sample efficiency on tabular classification benchmarks by using wide, sparsely-initialized MLP models; these networks sometimes outperform tuned random forests."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e49d504e92c4fc506c25c3105bbb50e6b710b524.pdf"}, "TLDR": {"value": "To learn sparse parities from offline samples, one must be rich, knowledgeable, patient, or lucky. Deep learning naturally interpolates along this tradeoff frontier."}, "supplementary_material": {"value": "/attachment/f2dd2370e8d3ce1ceec0ff6a9359bd318533797a.pdf"}, "_bibtex": {"value": "@inproceedings{\nedelman2023pareto,\ntitle={Pareto Frontiers in Deep Feature Learning: Data, Compute, Width, and Luck},\nauthor={Benjamin L. Edelman and Surbhi Goel and Sham M. Kakade and eran malach and Cyril Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Ypbke6biDm}\n}"}, "paperhash": {"value": "edelman|pareto_frontiers_in_deep_feature_learning_data_compute_width_and_luck"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4251/-/Revision", "NeurIPS.cc/2023/Conference/Submission4251/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4251/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325851360, "odate": 1698949720465, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "VzmpXQAn6E", "number": 4247, "cdate": 1683615875609, "tcdate": 1683615875609, "mdate": 1698949720445, "tmdate": 1698949720445, "signatures": ["NeurIPS.cc/2023/Conference/Submission4247/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4247/Authors"], "forum": "VzmpXQAn6E", "content": {"title": {"value": "Exposing Attention Glitches with Flip-Flop Language Modeling"}, "authors": {"value": ["Bingbin Liu", "Jordan T. Ash", "Surbhi Goel", "Akshay Krishnamurthy", "Cyril Zhang"]}, "authorids": {"value": ["~Bingbin_Liu1", "~Jordan_T._Ash1", "~Surbhi_Goel1", "~Akshay_Krishnamurthy1", "~Cyril_Zhang1"]}, "keywords": {"value": ["Transformers", "language models", "hallucinations", "long-range dependencies", "generalization", "extrapolation", "out-of-distribution"]}, "abstract": {"value": "Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of _attention glitches_, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce _flip-flop language modeling_ (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Transformers fail to robustly keep track of a single bit of memory. The glitches are surprisingly subtle and persistent. We hypothesize that this accounts for some \"closed-domain hallucinations\"."}, "pdf": {"value": "/pdf/8ce81d94bdfa22d8b190b2b4cda6efa0b602d32a.pdf"}, "supplementary_material": {"value": "/attachment/f1e8150c55ba1f12e7b8ced818be54ac561d3746.pdf"}, "_bibtex": {"value": "@inproceedings{\nliu2023exposing,\ntitle={Exposing Attention Glitches with Flip-Flop Language Modeling},\nauthor={Bingbin Liu and Jordan T. Ash and Surbhi Goel and Akshay Krishnamurthy and Cyril Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=VzmpXQAn6E}\n}"}, "paperhash": {"value": "liu|exposing_attention_glitches_with_flipflop_language_modeling"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4247/-/Revision", "NeurIPS.cc/2023/Conference/Submission4247/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4247/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325851179, "odate": 1698949720430, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "P0Avuii9iI", "number": 4213, "cdate": 1683614222612, "tcdate": 1683614222612, "mdate": 1698949720152, "tmdate": 1698949720152, "signatures": ["NeurIPS.cc/2023/Conference/Submission4213/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4213/Authors"], "forum": "P0Avuii9iI", "content": {"title": {"value": "Differentially Private Image Classification by Learning Priors from Random Processes"}, "authors": {"value": ["Xinyu Tang", "Ashwinee Panda", "Vikash Sehwag", "Prateek Mittal"]}, "authorids": {"value": ["~Xinyu_Tang1", "~Ashwinee_Panda1", "~Vikash_Sehwag1", "~Prateek_Mittal1"]}, "keywords": {"value": ["Differential privacy", "image classification", "deep learning"]}, "TLDR": {"value": "We provide new SOTA methods for DP image classification when training from scratch by using image priors"}, "abstract": {"value": "In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient clipping and noise addition.\nA recent focus in private learning research is improving the performance of DP-SGD on private data by incorporating priors that are learned on real-world public data.\nIn this work, we explore how we can improve the privacy-utility tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these priors to private data. \nWe propose DP-RandP, a three-phase approach. \nWe attain new state-of-the-art accuracy when training from scratch on CIFAR10, CIFAR100, MedMNIST and ImageNet for a range of privacy budgets $\\\\varepsilon \\\\in [1, 8]$. In particular, we improve the previous best reported accuracy on CIFAR10 from $60.6 \\\\%$ to $72.3 \\\\%$ for $\\\\varepsilon=1$."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a040fdda51e1218b6027655a1711681ee2d495d8.pdf"}, "_bibtex": {"value": "@inproceedings{\ntang2023differentially,\ntitle={Differentially Private Image Classification by Learning Priors from Random Processes},\nauthor={Xinyu Tang and Ashwinee Panda and Vikash Sehwag and Prateek Mittal},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=P0Avuii9iI}\n}"}, "paperhash": {"value": "tang|differentially_private_image_classification_by_learning_priors_from_random_processes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4213/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4213/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325850067, "odate": 1698949720138, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "FXU4aR2uif", "number": 4189, "cdate": 1683612877302, "tcdate": 1683612877302, "mdate": 1698949719835, "tmdate": 1698949719835, "signatures": ["NeurIPS.cc/2023/Conference/Submission4189/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4189/Authors"], "forum": "FXU4aR2uif", "content": {"title": {"value": "Episodic Multi-Task Learning with Heterogeneous Neural Processes"}, "authors": {"value": ["Jiayi Shen", "Xiantong Zhen", "Cheems Wang", "Marcel Worring"]}, "authorids": {"value": ["~Jiayi_Shen3", "~Xiantong_Zhen1", "~Cheems_Wang1", "~Marcel_Worring2"]}, "keywords": {"value": ["data-insufficiency problem", "episodic training", "multi-task learning and neural processes"]}, "abstract": {"value": "This paper focuses on the data-insufficiency problem in multi-task learning within an episodic training setup. Specifically, we explore the potential of heterogeneous information across tasks and meta-knowledge among episodes to effectively tackle each task with limited data. Existing meta-learning methods often fail to take advantage of crucial heterogeneous information in a single episode, while multi-task learning models neglect reusing experience from earlier episodes. To address the problem of insufficient data, we develop Heterogeneous Neural Processes (HNPs) for the episodic multi-task setup. Within the framework of hierarchical Bayes, HNPs effectively capitalize on prior experiences as meta-knowledge and capture task-relatedness among heterogeneous tasks, mitigating data-insufficiency. Meanwhile, transformer-structured inference modules are designed to enable efficient inferences toward meta-knowledge and task-relatedness. In this way, HNPs can learn more powerful functional priors for adapting to novel heterogeneous tasks in each meta-test episode. Experimental results show the superior performance of the proposed HNPs over typical baselines, and ablation studies verify the effectiveness of the designed inference modules."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8bd41516eb1ddf4d6196aaae0869ce6f84615c34.pdf"}, "supplementary_material": {"value": "/attachment/10238bf7e0bd50823f7137e24e458a616d15d25f.pdf"}, "_bibtex": {"value": "@inproceedings{\nshen2023episodic,\ntitle={Episodic Multi-Task Learning with Heterogeneous Neural Processes},\nauthor={Jiayi Shen and Xiantong Zhen and Cheems Wang and Marcel Worring},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=FXU4aR2uif}\n}"}, "paperhash": {"value": "shen|episodic_multitask_learning_with_heterogeneous_neural_processes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4189/-/Revision", "NeurIPS.cc/2023/Conference/Submission4189/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4189/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325849385, "odate": 1698949719818, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "AuXd54odxm", "number": 4135, "cdate": 1683610147662, "tcdate": 1683610147662, "mdate": 1698949719316, "tmdate": 1698949719316, "signatures": ["NeurIPS.cc/2023/Conference/Submission4135/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4135/Authors"], "forum": "AuXd54odxm", "content": {"title": {"value": "Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model"}, "authors": {"value": ["Yule Wang", "Zijing Wu", "Chengrui Li", "Anqi Wu"]}, "authorids": {"value": ["~Yule_Wang1", "~Zijing_Wu1", "~Chengrui_Li1", "~Anqi_Wu4"]}, "keywords": {"value": ["Neural distribution alignment", "Diffusion model", "Neuroscience", "Neural decoding"]}, "TLDR": {"value": "We propose ERDiff, a method for neural distribution alignment, which preserves the trial-based spatio-temporal structure of latent dynamics with diffusion model."}, "abstract": {"value": "In the field of behavior-related brain computation, it is necessary to align raw neural signals against the drastic domain shift among them. A foundational framework within neuroscience research posits that trial-based neural population activities rely on low-dimensional latent dynamics, thus focusing on the latter greatly facilitates the alignment procedure. Despite this field's progress, existing methods ignore the intrinsic spatio-temporal structure during the alignment phase. Hence, their solutions usually lead to poor quality in latent dynamics structures and overall performance. To tackle this problem, we propose an alignment method ERDiff, which leverages the expressivity of the diffusion model to preserve the spatio-temporal structure of latent dynamics. Specifically, the latent dynamics structures of the source domain are first extracted by a diffusion model. Then, under the guidance of this diffusion model, such structures are well-recovered through a maximum likelihood alignment procedure in the target domain. We first demonstrate the effectiveness of our proposed method on a synthetic dataset. Then, when applied to neural recordings from the non-human primate motor cortex, under both cross-day and inter-subject settings, our method consistently manifests its capability of preserving the spatio-temporal structure of latent dynamics and outperforms existing approaches in alignment goodness-of-fit and neural decoding performance."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e57d17ece7747f427f8266bc8867a9e2fe804e83.pdf"}, "supplementary_material": {"value": "/attachment/323cbd7f09d126192358fb39c8b8219d787ac0c6.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023extraction,\ntitle={Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model},\nauthor={Yule Wang and Zijing Wu and Chengrui Li and Anqi Wu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=AuXd54odxm}\n}"}, "paperhash": {"value": "wang|extraction_and_recovery_of_spatiotemporal_structure_in_latent_dynamics_alignment_with_diffusion_model"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4135/-/Revision", "NeurIPS.cc/2023/Conference/Submission4135/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4135/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325847529, "odate": 1698949719304, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "KmdlUP23qh", "number": 4114, "cdate": 1683608445087, "tcdate": 1683608445087, "mdate": 1698949720176, "tmdate": 1698949720176, "signatures": ["NeurIPS.cc/2023/Conference/Submission4114/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4114/Authors"], "forum": "KmdlUP23qh", "content": {"title": {"value": "Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems"}, "authors": {"value": ["Tongtong Fang", "Nan Lu", "Gang Niu", "Masashi Sugiyama"]}, "authorids": {"value": ["~Tongtong_Fang1", "~Nan_Lu1", "~Gang_Niu1", "~Masashi_Sugiyama1"]}, "keywords": {"value": ["importance weighting", "distribution shift", "deep learning"]}, "TLDR": {"value": "We generalize importance weighting for distribution shift problems so that we no longer require the support of the test distribution must be covered by the support of the training distribution."}, "abstract": {"value": "Distribution shift (DS) may have two levels: the distribution itself changes, and the support (i.e., the set where the probability density is non-zero) also changes. When considering the support change between the training and test distributions, there can be four cases: (i) they exactly match; (ii) the training support is wider (and thus covers the test support); (iii) the test support is wider; (iv) they partially overlap. Existing methods are good at cases (i) and (ii), while cases (iii) and (iv) are more common nowadays but still under-explored. In this paper, we generalize importance weighting (IW), a golden solver for cases (i) and (ii), to a universal solver for all cases. Specifically, we first investigate why IW might fail in cases (iii) and (iv); based on the findings, we propose generalized IW (GIW) that could handle cases (iii) and (iv) and would reduce to IW in cases (i) and (ii). In GIW, the test support is split into an in-training (IT) part and an out-of-training (OOT) part, and the expected risk is decomposed into a weighted classification term over the IT part and a standard classification term over the OOT part, which guarantees the risk consistency of GIW. Then, the implementation of GIW consists of three components: (a) the split of validation data is carried out by the one-class support vector machine, (b) the first term of the empirical risk can be handled by any IW algorithm given training data and IT validation data, and (c) the second term just involves OOT validation data. Experiments demonstrate that GIW is a universal solver for DS problems, outperforming IW methods in cases (iii) and (iv)."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/4d9c6c47173b646f5512533e9f9507ce8b8a82bc.pdf"}, "_bibtex": {"value": "@inproceedings{\nfang2023generalizing,\ntitle={Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems},\nauthor={Tongtong Fang and Nan Lu and Gang Niu and Masashi Sugiyama},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=KmdlUP23qh}\n}"}, "paperhash": {"value": "fang|generalizing_importance_weighting_to_a_universal_solver_for_distribution_shift_problems"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4114/-/Revision", "NeurIPS.cc/2023/Conference/Submission4114/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4114/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325846856, "odate": 1698949719109, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "jR2FkqW6GB", "forum": "jR2FkqW6GB", "number": 4010, "cdate": 1683601661593, "tcdate": 1683601661593, "mdate": 1698949718640, "tmdate": 1698949718640, "signatures": ["NeurIPS.cc/2023/Conference/Submission4010/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission4010/Authors"], "content": {"title": {"value": "Is Learning in Games Good for the Learners?"}, "authors": {"value": ["William Brown", "Jon Schneider", "Kiran Vodrahalli"]}, "authorids": {"value": ["~William_Brown7", "~Jon_Schneider1", "~Kiran_Vodrahalli1"]}, "keywords": {"value": ["learning in games", "correlated equilibria", "Stackelberg equilibria", "swap regret", "dynamic regret"]}, "TLDR": {"value": "Tradeoffs between reward and regret for learning in 2-player games: deviating to higher regret is often better for reward"}, "abstract": {"value": "We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay between two agents. To facilitate this,  we introduce a notion of generalized equilibrium which allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and pair of regret constraints, where we show that any such equilibrium is reachable by a pair of algorithms which maintain their regret guarantees against arbitrary opponents. As a central example, we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that this captures an extension of Stackelberg equilibria with a matching optimal value, and that there exists a wide class of games where a player can significantly increase their utility by deviating from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider tradeoffs in terms of the opponent's algorithm choice. We give a tight characterization for the maximal reward obtainable against some no-regret learner, yet we also show a class of games in which this is bounded away from the value obtainable against the class of common \"mean-based\" no-regret algorithms. Finally, we consider the question of learning reward-optimal strategies via repeated play with a no-regret agent when the game is initially unknown. Again we show tradeoffs depending on the opponent's learning algorithm: the Stackelberg strategy is learnable in exponential time with any no-regret agent (and in polynomial time with any no-adaptive-regret agent) for any game where it is learnable via queries, and there are games where it is learnable in polynomial time against any no-swap-regret agent but requires exponential time against a mean-based no-regret agent."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/55531f3e520798bd4556603a30d20f7d11c3f332.pdf"}, "supplementary_material": {"value": "/attachment/d1e5a75623f806cbc8456f6545d5b2713a0619d4.pdf"}, "_bibtex": {"value": "@inproceedings{\nbrown2023is,\ntitle={Is Learning in Games Good for the Learners?},\nauthor={William Brown and Jon Schneider and Kiran Vodrahalli},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=jR2FkqW6GB}\n}"}, "paperhash": {"value": "brown|is_learning_in_games_good_for_the_learners"}}, "pdate": 1695325844043, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission4010/-/Revision", "NeurIPS.cc/2023/Conference/Submission4010/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission4010/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "odate": 1698949718625, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "FDzQQTPqEJ", "number": 3955, "cdate": 1683598405713, "tcdate": 1683598405713, "mdate": 1698949718389, "tmdate": 1698949718389, "signatures": ["NeurIPS.cc/2023/Conference/Submission3955/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3955/Authors"], "forum": "FDzQQTPqEJ", "content": {"title": {"value": "Squared Neural Families: A New Class of Tractable Density Models"}, "authors": {"value": ["Russell Tsuchida", "Cheng Soon Ong", "Dino Sejdinovic"]}, "authorids": {"value": ["~Russell_Tsuchida1", "~Cheng_Soon_Ong1", "~Dino_Sejdinovic1"]}, "keywords": {"value": ["probabilistic modelling; density estimation; exponential family;"]}, "TLDR": {"value": "We can compute normalising constants of squared neural networks to construct probability distributions with interesting and useful properties."}, "abstract": {"value": "Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation, conditional density estimation, and density estimation with missing data tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0fe4477b310ea5738965afdc5160adf95403905e.pdf"}, "_bibtex": {"value": "@inproceedings{\ntsuchida2023squared,\ntitle={Squared Neural Families: A New Class of Tractable Density Models},\nauthor={Russell Tsuchida and Cheng Soon Ong and Dino Sejdinovic},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=FDzQQTPqEJ}\n}"}, "paperhash": {"value": "tsuchida|squared_neural_families_a_new_class_of_tractable_density_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3955/-/Revision", "NeurIPS.cc/2023/Conference/Submission3955/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3955/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325842136, "odate": 1698949718374, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Jkc74vn1aZ", "number": 3953, "cdate": 1683598360821, "tcdate": 1683598360821, "mdate": 1698949718346, "tmdate": 1698949718346, "signatures": ["NeurIPS.cc/2023/Conference/Submission3953/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3953/Authors"], "forum": "Jkc74vn1aZ", "content": {"title": {"value": "Towards Symmetry-Aware Generation of Periodic Materials"}, "authors": {"value": ["Youzhi Luo", "Chengkai Liu", "Shuiwang Ji"]}, "authorids": {"value": ["~Youzhi_Luo1", "~Chengkai_Liu1", "~Shuiwang_Ji1"]}, "keywords": {"value": ["material generation", "symmetries", "variational auto-encoder", "score-based diffusion model"]}, "TLDR": {"value": "We propose SyMat, a novel deep generative model for periodic material generation."}, "abstract": {"value": "We consider the problem of generating periodic materials with deep models. While symmetry-aware molecule generation has been studied extensively, periodic materials possess different symmetries, which have not been completely captured by existing methods.\nIn this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS)."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0794e91154a212cb72eb3da32eaf70b0b7e765be.pdf"}, "_bibtex": {"value": "@inproceedings{\nluo2023towards,\ntitle={Towards Symmetry-Aware Generation of Periodic Materials},\nauthor={Youzhi Luo and Chengkai Liu and Shuiwang Ji},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Jkc74vn1aZ}\n}"}, "paperhash": {"value": "luo|towards_symmetryaware_generation_of_periodic_materials"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3953/-/Revision", "NeurIPS.cc/2023/Conference/Submission3953/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3953/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325842040, "odate": 1698949718333, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ZIyAHaLlsn", "number": 3943, "cdate": 1683597727255, "tcdate": 1683597727255, "mdate": 1698949718264, "tmdate": 1698949718264, "signatures": ["NeurIPS.cc/2023/Conference/Submission3943/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3943/Authors"], "forum": "ZIyAHaLlsn", "content": {"title": {"value": "ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting"}, "authors": {"value": ["Zongsheng Yue", "Jianyi Wang", "Chen Change Loy"]}, "authorids": {"value": ["~Zongsheng_Yue1", "~Jianyi_Wang1", "~Chen_Change_Loy2"]}, "keywords": {"value": ["Super-resolution; Diffusion model; Efficient"]}, "TLDR": {"value": "This paper specifically designs an efficient diffusion model for image super-resolution that significantly reduces the sampling steps."}, "abstract": {"value": "Diffusion-based image super-resolution (SR) methods are mainly limited by the low inference speed due to the requirements of hundreds or even thousands of sampling steps. Existing acceleration sampling techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To address this issue, we propose a novel and efficient diffusion model for SR that significantly reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during inference and its associated performance deterioration. Our method constructs a Markov chain that transfers between the high-resolution image and the low-resolution image by shifting the residual between them, substantially improving the transition efficiency. Additionally, an elaborate noise schedule is developed to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experiments demonstrate that the proposed method obtains superior or at least comparable performance to current state-of-the-art methods on both synthetic and real-world datasets, \\textit{\\textbf{even only with 20 sampling steps}}. Our code and model will be made publicly."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8f70c4f6b1bc6598c74af8d673c243159b0e81d1.pdf"}, "supplementary_material": {"value": "/attachment/031aee85a19f76e42abf0a01a7dcd7f5af275dda.pdf"}, "_bibtex": {"value": "@inproceedings{\nyue2023resshift,\ntitle={ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting},\nauthor={Zongsheng Yue and Jianyi Wang and Chen Change Loy},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ZIyAHaLlsn}\n}"}, "paperhash": {"value": "yue|resshift_efficient_diffusion_model_for_image_superresolution_by_residual_shifting"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3943/-/Revision", "NeurIPS.cc/2023/Conference/Submission3943/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3943/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325841658, "odate": 1698949718252, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "gmmXyAq8TI", "number": 3936, "cdate": 1683597135501, "tcdate": 1683597135501, "mdate": 1698949718157, "tmdate": 1698949718157, "signatures": ["NeurIPS.cc/2023/Conference/Submission3936/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3936/Authors"], "forum": "gmmXyAq8TI", "content": {"title": {"value": "Coop: Memory is not a Commodity"}, "authors": {"value": ["Jianhao Zhang", "Shihan Ma", "Peihong Liu", "Jinhui Yuan"]}, "authorids": {"value": ["~Jianhao_Zhang1", "mmasss@sjtu.edu.cn", "liupeihong@oneflow.org", "yuanjinhui@oneflow.org"]}, "keywords": {"value": ["Tensor Rematerialization; Gradient Checkpointing; Activation Recomputing; Deep Learning; Deep Learning Frameworks; Memory Allocator"]}, "abstract": {"value": "Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical. Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads to severe memory fragmentation and increases the cost of potential rematerializations.\n\nTo address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation.\n\nWe named our method \\name/ as it is a co-optimization of tensor allocation and tensor rematerialization. We evaluated \\name/ on eight representative DNNs. The experimental results demonstrate that \\name/ achieves up to $2\\times$ memory saving and hugely reduces compute overhead, search latency, and memory fragmentation compared to the state-of-the-art baselines."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b0e7533914063c9f39f81524358393cc57cc417f.pdf"}, "supplementary_material": {"value": "/attachment/99c1f92127183d6bdc4d3c7a1a77b9ab4aade21a.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023coop,\ntitle={Coop: Memory is not a Commodity},\nauthor={Jianhao Zhang and Shihan Ma and Peihong Liu and Jinhui Yuan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=gmmXyAq8TI}\n}"}, "paperhash": {"value": "zhang|coop_memory_is_not_a_commodity"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3936/-/Revision", "NeurIPS.cc/2023/Conference/Submission3936/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3936/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325841410, "odate": 1698949718144, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "K5e5tFZuur", "number": 3882, "cdate": 1683592041061, "tcdate": 1683592041061, "mdate": 1698949717648, "tmdate": 1698949717648, "signatures": ["NeurIPS.cc/2023/Conference/Submission3882/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3882/Authors"], "forum": "K5e5tFZuur", "content": {"title": {"value": "Invariant Learning via Probability of Sufficient and Necessary Causes"}, "authors": {"value": ["Mengyue Yang", "Yonggang Zhang", "Zhen Fang", "Yali Du", "Furui Liu", "Jean-Francois Ton", "Jianhong Wang", "Jun Wang"]}, "authorids": {"value": ["~Mengyue_Yang1", "~Yonggang_Zhang1", "~Zhen_Fang2", "~Yali_Du1", "~Furui_Liu1", "~Jean-Francois_Ton2", "~Jianhong_Wang1", "~Jun_Wang2"]}, "keywords": {"value": ["OOD Generalization", "Invariant Representation Learning"]}, "abstract": {"value": "Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. \nHowever, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of sufficiency and necessity conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. \nTo capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. \nTo associate PNS with OOD generalization, we propose PNS risk and formulate an algorithm to learn representation with a high PNS value. We theoretically analyze and prove the generalizability of the PNS risk. Experiments on both synthetic and real-world benchmarks demonstrate the effectiveness of the proposed method. The detailed implementation can be found at the GitHub repository: https://github.com/ymy4323460/CaSN."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6dd0a6dc08ff5b9db3373d01be124eed2fd83777.pdf"}, "supplementary_material": {"value": "/attachment/92810b496e9ac62322bdbe753729f222b8085d7b.pdf"}, "_bibtex": {"value": "@inproceedings{\nyang2023invariant,\ntitle={Invariant Learning via Probability of Sufficient and Necessary Causes},\nauthor={Mengyue Yang and Yonggang Zhang and Zhen Fang and Yali Du and Furui Liu and Jean-Francois Ton and Jianhong Wang and Jun Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=K5e5tFZuur}\n}"}, "paperhash": {"value": "yang|invariant_learning_via_probability_of_sufficient_and_necessary_causes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3882/-/Revision", "NeurIPS.cc/2023/Conference/Submission3882/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3882/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325839382, "odate": 1698949717631, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5R9bZlpZKj", "number": 3828, "cdate": 1683582092534, "tcdate": 1683582092534, "mdate": 1698949717124, "tmdate": 1698949717124, "signatures": ["NeurIPS.cc/2023/Conference/Submission3828/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3828/Authors"], "forum": "5R9bZlpZKj", "content": {"title": {"value": "Smoothed Analysis of Sequential Probability Assignment"}, "authors": {"value": ["Alankrita Bhatt", "Nika Haghtalab", "Abhishek Shetty"]}, "authorids": {"value": ["~Alankrita_Bhatt1", "~Nika_Haghtalab2", "~Abhishek_Shetty1"]}, "keywords": {"value": ["Online learning", "Log loss", "Information theory", "Smoothed Analysis", "Beyond worst case analysis", "Oracle Efficient Online Learning"]}, "abstract": {"value": "We initiate the study of smoothed analysis for the sequential probability assignment problem with contexts. We study information-theoretically optimal minmax rates as well as a framework for algorithmic reduction involving the maximum likelihood estimator oracle. Our approach establishes a general-purpose reduction from minimax rates for sequential probability assignment for smoothed adversaries to minimax rates for transductive learning. This leads to optimal (logarithmic) fast rates for parametric classes and classes with finite VC dimension. On the algorithmic front, we develop an algorithm that efficiently taps into the MLE oracle, for general classes of functions. We show that under general conditions this algorithmic approach yields sublinear regret."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7b43960449c5e36ec7dc6b44150011fb8d182aba.pdf"}, "supplementary_material": {"value": "/attachment/71ab2609d668bc92610b4745d53c708d70351db1.pdf"}, "TLDR": {"value": "We study sequential probability assignment in the smoothed adversary model, presenting both statistically and computationally efficient algorithms that achieve the rates depending on offline complexity measures."}, "_bibtex": {"value": "@inproceedings{\nbhatt2023smoothed,\ntitle={Smoothed Analysis of Sequential Probability Assignment},\nauthor={Alankrita Bhatt and Nika Haghtalab and Abhishek Shetty},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5R9bZlpZKj}\n}"}, "paperhash": {"value": "bhatt|smoothed_analysis_of_sequential_probability_assignment"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3828/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3828/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325837616, "odate": 1698949717110, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "DEiNSfh1k7", "number": 3743, "cdate": 1683573510656, "tcdate": 1683573510656, "mdate": 1698949716381, "tmdate": 1698949716381, "signatures": ["NeurIPS.cc/2023/Conference/Submission3743/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3743/Authors"], "forum": "DEiNSfh1k7", "content": {"title": {"value": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data"}, "authors": {"value": ["Stephanie Fu", "Netanel Yakir Tamir", "Shobhita Sundaram", "Lucy Chai", "Richard Zhang", "Tali Dekel", "Phillip Isola"]}, "authorids": {"value": ["~Stephanie_Fu1", "~Netanel_Yakir_Tamir1", "~Shobhita_Sundaram1", "~Lucy_Chai1", "~Richard_Zhang1", "~Tali_Dekel1", "~Phillip_Isola1"]}, "keywords": {"value": ["perceptual similarity", "foundation model", "perception", "computer vision", "image metric"]}, "TLDR": {"value": "We fit a model of human perception on a synthetic dataset, and show that it aligns better with image attributes that humans are sensitive to."}, "abstract": {"value": "Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks. Our project page: https://dreamsim-nights.github.io/"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/fb6714a8a4a6c793c072c37b2991b8f8b983fd3e.pdf"}, "supplementary_material": {"value": "/attachment/4cdf44cc4ccc6422a902592c1f514f60edba7ef3.zip"}, "_bibtex": {"value": "@inproceedings{\nfu2023dreamsim,\ntitle={DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data},\nauthor={Stephanie Fu and Netanel Yakir Tamir and Shobhita Sundaram and Lucy Chai and Richard Zhang and Tali Dekel and Phillip Isola},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=DEiNSfh1k7}\n}"}, "paperhash": {"value": "fu|dreamsim_learning_new_dimensions_of_human_visual_similarity_using_synthetic_data"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3743/-/Revision", "NeurIPS.cc/2023/Conference/Submission3743/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3743/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325835043, "odate": 1698949716369, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "qJRlz3SucN", "number": 3735, "cdate": 1683572684555, "tcdate": 1683572684555, "mdate": 1698949716308, "tmdate": 1698949716308, "signatures": ["NeurIPS.cc/2023/Conference/Submission3735/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3735/Authors"], "forum": "qJRlz3SucN", "content": {"title": {"value": "VaRT: Variational Regression Trees"}, "authors": {"value": ["Sebastian Salazar"]}, "authorids": {"value": ["~Sebastian_Salazar1"]}, "keywords": {"value": ["Probabilistic Machine Learning", "Variational Inference", "Bayesian Inference", "Bayesian Nonparametrics"]}, "TLDR": {"value": "We introduce a novel Non-parametric Bayesian model over the space of decision trees and derive a variational approximation to the posterior distribution."}, "abstract": {"value": "Decision trees are a well-established tool in machine learning for classification and regression tasks. In this paper, we introduce a novel non-parametric Bayesian model that uses variational inference to approximate a posterior distribution over the space of stochastic decision trees. We evaluate the model's performance on 18 datasets and demonstrate its competitiveness with other state-of-the-art methods in regression tasks. We also explore its application to causal inference problems. We provide a fully vectorized implementation of our algorithm in PyTorch."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f769e183cf56bf64c7799e5e0a1184bc550500dc.pdf"}, "supplementary_material": {"value": "/attachment/911cd426477928d06238a900fed1a2a64ec71dcc.zip"}, "_bibtex": {"value": "@inproceedings{\nsalazar2023vart,\ntitle={Va{RT}: Variational Regression Trees},\nauthor={Sebastian Salazar},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=qJRlz3SucN}\n}"}, "paperhash": {"value": "salazar|vart_variational_regression_trees"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3735/-/Revision", "NeurIPS.cc/2023/Conference/Submission3735/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3735/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325834783, "odate": 1698949716295, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "zQTi3pziFp", "number": 3702, "cdate": 1683568608068, "tcdate": 1683568608068, "mdate": 1698949716121, "tmdate": 1698949716121, "signatures": ["NeurIPS.cc/2023/Conference/Submission3702/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3702/Authors"], "forum": "zQTi3pziFp", "content": {"title": {"value": "Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio"}, "authors": {"value": ["Xudong XU", "Dejan Markovic", "Jacob Sandakly", "Todd Keebler", "Steven Krenn", "Alexander Richard"]}, "authorids": {"value": ["~Xudong_XU1", "~Dejan_Markovic1", "jasandakly@meta.com", "toddkeebler@meta.com", "~Steven_Krenn1", "~Alexander_Richard1"]}, "keywords": {"value": ["sound field", "spatial audio", "virtual humans", "human body", "body modeling"]}, "TLDR": {"value": "We present a model that can generate accurate 3D sound fields of human bodies from headset microphones and body pose as inputs."}, "abstract": {"value": "While 3D human body modeling has received much attention in computer vision, modeling the acoustic equivalent, i.e. modeling 3D spatial audio produced by body motion and speech, has fallen short in the community. To close this gap, we present a model that can generate accurate 3D spatial audio for full human bodies. The system consumes, as input, audio signals from headset microphones and body pose, and produces, as output, a 3D sound field surrounding the transmitter's body, from which spatial audio can be rendered at any arbitrary position in the 3D space. We collect a first-of-its-kind multimodal dataset of human bodies, recorded with multiple cameras and a spherical array of 345 microphones. In an empirical evaluation, we demonstrate that our model can produce accurate body-induced sound fields when trained with a suitable loss. Dataset and code are available online."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8c736135626d813eb9595461f7a96f9de2cf2e86.pdf"}, "supplementary_material": {"value": "/attachment/5982d20b50e57f325ed04c373c0575ec7d9543ac.zip"}, "_bibtex": {"value": "@inproceedings{\nxu2023sounding,\ntitle={Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio},\nauthor={Xudong XU and Dejan Markovic and Jacob Sandakly and Todd Keebler and Steven Krenn and Alexander Richard},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=zQTi3pziFp}\n}"}, "paperhash": {"value": "xu|sounding_bodies_modeling_3d_spatial_sound_of_humans_using_body_pose_and_audio"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3702/-/Revision", "NeurIPS.cc/2023/Conference/Submission3702/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3702/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325833797, "odate": 1698949716108, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "m21rQusNgb", "number": 3680, "cdate": 1683566733492, "tcdate": 1683566733492, "mdate": 1698949715990, "tmdate": 1698949715990, "signatures": ["NeurIPS.cc/2023/Conference/Submission3680/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3680/Authors"], "forum": "m21rQusNgb", "content": {"title": {"value": "Learning List-Level Domain-Invariant Representations for Ranking"}, "authors": {"value": ["Ruicheng Xian", "Honglei Zhuang", "Zhen Qin", "Hamed Zamani", "Jing Lu", "Ji Ma", "Kai Hui", "Han Zhao", "Xuanhui Wang", "Michael Bendersky"]}, "authorids": {"value": ["~Ruicheng_Xian1", "~Honglei_Zhuang1", "~Zhen_Qin5", "~Hamed_Zamani1", "~Jing_Lu4", "~Ji_Ma3", "~Kai_Hui1", "~Han_Zhao1", "~Xuanhui_Wang1", "~Michael_Bendersky1"]}, "keywords": {"value": ["learning to rank", "domain adaptation", "text ranking"]}, "TLDR": {"value": "We revisit invariant representation learning for ranking, propose list-level alignment, and establish a domain adaptation generalization bound for ranking."}, "abstract": {"value": "Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment\u2014learning domain-invariant representations at the higher level of lists. The benefits are twofold: it leads to the first domain adaptation generalization bound for ranking, in turn providing theoretical support for the proposed method, and it achieves better empirical transfer performance for unsupervised domain adaptation on ranking tasks, including passage reranking."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a9d24c61784cae22785050f024173a8bb1d6c9e8.pdf"}, "supplementary_material": {"value": "/attachment/1429dc0e02f2edcb4f23b6050f3a01f4f660f617.zip"}, "_bibtex": {"value": "@inproceedings{\nxian2023learning,\ntitle={Learning List-Level Domain-Invariant Representations for Ranking},\nauthor={Ruicheng Xian and Honglei Zhuang and Zhen Qin and Hamed Zamani and Jing Lu and Ji Ma and Kai Hui and Han Zhao and Xuanhui Wang and Michael Bendersky},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=m21rQusNgb}\n}"}, "paperhash": {"value": "xian|learning_listlevel_domaininvariant_representations_for_ranking"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3680/-/Revision", "NeurIPS.cc/2023/Conference/Submission3680/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3680/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325833106, "odate": 1698949715978, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Ifq8GMdqJK", "number": 3678, "cdate": 1683566623478, "tcdate": 1683566623478, "mdate": 1698949715968, "tmdate": 1698949715968, "signatures": ["NeurIPS.cc/2023/Conference/Submission3678/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3678/Authors"], "forum": "Ifq8GMdqJK", "content": {"title": {"value": "Conditional independence testing under misspecified inductive biases"}, "authors": {"value": ["Felipe Maia Polo", "Yuekai Sun", "Moulinath Banerjee"]}, "authorids": {"value": ["~Felipe_Maia_Polo1", "~Yuekai_Sun1", "~Moulinath_Banerjee1"]}, "keywords": {"value": ["conditional independence", "hypothesis testing", "misspecification"]}, "TLDR": {"value": "We derive approximations for the testing errors of three regression-based conditional independence (CI) tests depending on misspecification. We introduce the Rao-Blackwellized Predictor Test (RBPT), a CI regression-based misspecification-robust test."}, "abstract": {"value": "Conditional independence (CI) testing is a fundamental and challenging task in modern statistics and machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to learn regression functions or Bayes predictors as an intermediate step; we refer to this class of tests as regression-based tests. Although these methods are guaranteed to control Type-I error when the supervised learning methods accurately estimate the regression functions or Bayes predictors of interest, their behavior is less understood when they fail due to misspecified inductive biases; in other words, when the employed models are not flexible enough or when the training algorithm does not induce the desired predictors. Then, we study the performance of regression-based CI tests under misspecified inductive biases. Namely, we propose new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. Moreover, we introduce the Rao-Blackwellized Predictor Test (RBPT), a regression-based CI test robust against misspecified inductive biases. Finally, we conduct experiments with artificial and real data, showcasing the usefulness of our theory and methods."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/cd64b336680a893bf94dff47d0b1df663337d878.pdf"}, "_bibtex": {"value": "@inproceedings{\npolo2023conditional,\ntitle={Conditional independence testing under misspecified inductive biases},\nauthor={Felipe Maia Polo and Yuekai Sun and Moulinath Banerjee},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Ifq8GMdqJK}\n}"}, "paperhash": {"value": "polo|conditional_independence_testing_under_misspecified_inductive_biases"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3678/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3678/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325833103, "odate": 1698949715955, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "PpI7XvOXkF", "number": 3637, "cdate": 1683563340306, "tcdate": 1683563340306, "mdate": 1698949715694, "tmdate": 1698949715694, "signatures": ["NeurIPS.cc/2023/Conference/Submission3637/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3637/Authors"], "forum": "PpI7XvOXkF", "content": {"title": {"value": "A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm"}, "authors": {"value": ["Ilias Diakonikolas", "Daniel Kane", "Jasper C.H. Lee", "Ankit Pensia", "Thanasis Pittas"]}, "authorids": {"value": ["~Ilias_Diakonikolas1", "~Daniel_Kane1", "~Jasper_C.H._Lee1", "~Ankit_Pensia1", "~Thanasis_Pittas1"]}, "keywords": {"value": ["robust statistics", "covariance estimation", "list-decodable learning"]}, "TLDR": {"value": "We give a sum-of-squares-free polynomial time algorithm for list-decodable Gaussian covariance estimation in relative Frobenius norm"}, "abstract": {"value": "We study the problem of list-decodable Gaussian covariance estimation. Given a multiset $T$ of $n$ points in $\\mathbb{R}^d$ such that an unknown $\\alpha<1/2$ fraction of points in $T$ are i.i.d. samples from an unknown Gaussian $\\mathcal{N}(\\mu, \\Sigma)$, the goal is to output a list of $O(1/\\alpha)$ hypotheses at least one of which is close to $\\Sigma$ in relative Frobenius norm. Our main result is a $\\mathrm{poly}(d,1/\\alpha)$ sample and time algorithm for this task that guarantees relative Frobenius norm error of $\\mathrm{poly}(1/\\alpha)$. Importantly, our algorithm relies purely on spectral techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of Gaussian mixture models (GMMs) --- a key ingredient in the recent work of [BakDJKKV22] on robustly learning arbitrary GMMs. Combined with the other components of [BakDJKKV22], our new method yields the first Sum-of-Squares-free algorithm for robustly learning GMMs, resolving an open problem proposed by Vempala and Kothari. At the technical level, we develop a novel multi-filtering method for list-decodable covariance estimation that may be useful in other settings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/76fb4989e1efd76efac005bc4c632f421c1b58ef.pdf"}, "_bibtex": {"value": "@inproceedings{\ndiakonikolas2023a,\ntitle={A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm},\nauthor={Ilias Diakonikolas and Daniel Kane and Jasper C.H. Lee and Ankit Pensia and Thanasis Pittas},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=PpI7XvOXkF}\n}"}, "paperhash": {"value": "diakonikolas|a_spectral_algorithm_for_listdecodable_covariance_estimation_in_relative_frobenius_norm"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3637/-/Revision", "NeurIPS.cc/2023/Conference/Submission3637/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3637/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325831437, "odate": 1698949715680, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "IT9mWLYNpQ", "number": 3622, "cdate": 1683561699485, "tcdate": 1683561699485, "mdate": 1698949715507, "tmdate": 1698949715507, "signatures": ["NeurIPS.cc/2023/Conference/Submission3622/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3622/Authors"], "forum": "IT9mWLYNpQ", "content": {"title": {"value": "Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability"}, "authors": {"value": ["Jingfeng Wu", "Vladimir Braverman", "Jason D. Lee"]}, "authorids": {"value": ["~Jingfeng_Wu1", "~Vladimir_Braverman1", "~Jason_D._Lee1"]}, "keywords": {"value": ["gd; implicit bias; edge of stability"]}, "TLDR": {"value": "We show the convergence and implicit bias of GD with any constant stepsize for logistic regression on linearly separable data."}, "abstract": {"value": "Recent research has observed that in machine learning optimization, gradient descent (GD) often operates at the edge of stability (EoS) [Cohen et al., 2021], where the stepsizes are set to be large, resulting in non-monotonic losses induced by the GD iterates. This paper studies the convergence and implicit bias of constant-stepsize GD for logistic regression on linearly separable data in the EoS regime. Despite the presence of local oscillations, we prove that the logistic loss can be minimized by GD with any constant stepsize over a long time scale. Furthermore, we prove that with any constant stepsize, the GD iterates tend to infinity when projected to a max-margin direction (the hard-margin SVM direction) and converge to a fixed vector that minimizes a strongly convex potential when projected to the orthogonal complement of the max-margin direction. In contrast, we also show that in the EoS regime, GD iterates may diverge catastrophically under the exponential loss, highlighting the superiority of the logistic loss. These theoretical findings are in line with numerical simulations and complement existing theories on the convergence and implicit bias of GD for logistic regression, which are only applicable when the stepsizes are sufficiently small."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3c216637211c16a4e4abf74a36876ceb31672457.pdf"}, "supplementary_material": {"value": "/attachment/055c83481e1e11e1182e9a5be1447a06da1a8f80.zip"}, "_bibtex": {"value": "@inproceedings{\nwu2023implicit,\ntitle={Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability},\nauthor={Jingfeng Wu and Vladimir Braverman and Jason D. Lee},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=IT9mWLYNpQ}\n}"}, "paperhash": {"value": "wu|implicit_bias_of_gradient_descent_for_logistic_regression_at_the_edge_of_stability"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3622/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3622/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325830931, "odate": 1698949715495, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "HYGnmSLBCf", "number": 3618, "cdate": 1683561559365, "tcdate": 1683561559365, "mdate": 1698949715445, "tmdate": 1698949715445, "signatures": ["NeurIPS.cc/2023/Conference/Submission3618/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3618/Authors"], "forum": "HYGnmSLBCf", "content": {"title": {"value": "Alignment with human representations supports robust few-shot learning"}, "authors": {"value": ["Ilia Sucholutsky", "Thomas L. Griffiths"]}, "authorids": {"value": ["~Ilia_Sucholutsky1", "~Thomas_L._Griffiths1"]}, "keywords": {"value": ["representation learning", "supervised learning", "human alignment", "few-shot learning"]}, "TLDR": {"value": "Models that are highly aligned with human representations are better at few-shot learning and are more robust to domain shift and adversarial examples"}, "abstract": {"value": "Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both natural adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5b510f95f64c3901675678cb2b14e328a650ed18.pdf"}, "supplementary_material": {"value": "/attachment/1d45a56584da02312f8987fa3e2f55a31b4e43b9.zip"}, "_bibtex": {"value": "@inproceedings{\nsucholutsky2023alignment,\ntitle={Alignment with human representations supports robust few-shot learning},\nauthor={Ilia Sucholutsky and Thomas L. Griffiths},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=HYGnmSLBCf}\n}"}, "paperhash": {"value": "sucholutsky|alignment_with_human_representations_supports_robust_fewshot_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3618/-/Revision", "NeurIPS.cc/2023/Conference/Submission3618/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3618/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325830760, "odate": 1698949715433, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "dlDFakG6kJ", "number": 3608, "cdate": 1683560971347, "tcdate": 1683560971347, "mdate": 1698949715373, "tmdate": 1698949715373, "signatures": ["NeurIPS.cc/2023/Conference/Submission3608/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3608/Authors"], "forum": "dlDFakG6kJ", "content": {"title": {"value": "Sample Complexity of Forecast Aggregation"}, "authors": {"value": ["Tao Lin", "Yiling Chen"]}, "authorids": {"value": ["~Tao_Lin2", "~Yiling_Chen1"]}, "keywords": {"value": ["information aggregation", "sample complexity", "distribution learning", "Bayesian forecast aggregation"]}, "abstract": {"value": "We consider a Bayesian forecast aggregation model where $n$ experts, after observing private signals about an unknown binary event, report their posterior beliefs about the event to a principal, who then aggregates the reports into a single prediction for the event. The signals of the experts and the outcome of the event follow a joint distribution that is unknown to the principal, but the principal has access to i.i.d. \"samples\" from the distribution, where each sample is a tuple of the experts' reports (not signals) and the realization of the event. Using these samples, the principal aims to find an $\\varepsilon$-approximately optimal aggregator, where optimality is measured in terms of the expected squared distance between the aggregated prediction and the realization of the event. We show that the sample complexity of this problem is at least $\\tilde \\Omega(m^{n-2} / \\varepsilon)$ for arbitrary discrete distributions, where $m$ is the size of each expert's signal space.  This sample complexity grows exponentially in the number of experts $n$. But, if the experts' signals are independent conditioned on the realization of the event, then the sample complexity is significantly reduced, to $\\tilde O(1 / \\varepsilon^2)$, which does not depend on $n$. Our results can be generalized to non-binary events.  The proof of our results uses a reduction from the distribution learning problem and reveals the fact that forecast aggregation is almost as difficult as distribution learning."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We give the first sample complexity result for the classical Bayesian forecast aggregation problem."}, "pdf": {"value": "/pdf/dabd84cb934f337167ba18dc4ab64e347f10c9fe.pdf"}, "supplementary_material": {"value": "/attachment/1826cfad51e23e6ecf711e647edc4516735cfa7d.pdf"}, "_bibtex": {"value": "@inproceedings{\nlin2023sample,\ntitle={Sample Complexity of Forecast Aggregation},\nauthor={Tao Lin and Yiling Chen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=dlDFakG6kJ}\n}"}, "paperhash": {"value": "lin|sample_complexity_of_forecast_aggregation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3608/-/Revision", "NeurIPS.cc/2023/Conference/Submission3608/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3608/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325830555, "odate": 1698949715357, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "2BFZ8cPIf6", "number": 3602, "cdate": 1683560660774, "tcdate": 1683560660774, "mdate": 1698949715332, "tmdate": 1698949715332, "signatures": ["NeurIPS.cc/2023/Conference/Submission3602/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3602/Authors"], "forum": "2BFZ8cPIf6", "content": {"title": {"value": "Learning Functional Transduction"}, "authors": {"value": ["Mathieu Chalvidal", "Thomas Serre", "Rufin VanRullen"]}, "authorids": {"value": ["~Mathieu_Chalvidal1", "~Thomas_Serre1", "~Rufin_VanRullen1"]}, "keywords": {"value": ["Meta-learning", "Neural Operators", "Kernel methods", "In-context learning"]}, "TLDR": {"value": "We introduce a meta-learned regression program that can approximate any target function of general kernel Banach spaces from any finite collection of input/output examples in a feedforward way."}, "abstract": {"value": "Research in statistical learning has polarized into two general approaches to perform regression analysis: Transductive methods construct estimates directly based on exemplar data using generic relational principles which might suffer from the curse of dimensionality. Conversely, inductive methods can potentially fit highly complex functions at the cost of compute-intensive solution searches. In this work, we leverage the theory of vector-valued Reproducing Kernel Banach Spaces (RKBS) to propose a hybrid approach: We show that transductive regression systems can be meta-learned with gradient descent to form efficient _in-context_ neural approximators of function defined over both finite and infinite-dimensional spaces (operator regression). Once trained, our _Transducer_ can almost instantaneously capture new functional relationships and produce original image estimates, given a few pairs of input and output examples. We demonstrate the benefit of our meta-learned transductive approach to model physical systems influenced by varying external factors with little data at a fraction of the usual deep learning training costs for partial differential equations and climate modeling applications."}, "pdf": {"value": "/pdf/3df296ff4ceeb481fbbd7478232cbb704bea237d.pdf"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "supplementary_material": {"value": "/attachment/9af8d6800bb25b88e10dad582c2949ad3344de69.pdf"}, "_bibtex": {"value": "@inproceedings{\nchalvidal2023learning,\ntitle={Learning Functional Transduction},\nauthor={Mathieu Chalvidal and Thomas Serre and Rufin VanRullen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=2BFZ8cPIf6}\n}"}, "paperhash": {"value": "chalvidal|learning_functional_transduction"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3602/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3602/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325830386, "odate": 1698949715319, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ce59j806df", "number": 3526, "cdate": 1683556483397, "tcdate": 1683556483397, "mdate": 1698949714820, "tmdate": 1698949714820, "signatures": ["NeurIPS.cc/2023/Conference/Submission3526/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3526/Authors"], "forum": "ce59j806df", "content": {"title": {"value": "Semi-Supervised Domain Generalization with Known and Unknown Classes"}, "authors": {"value": ["Lei Zhang", "Ji-Fu Li", "Wei Wang"]}, "authorids": {"value": ["~Lei_Zhang55", "~Ji-Fu_Li1", "~Wei_Wang10"]}, "keywords": {"value": ["Domain Generalization", "Semi-Supervised Learning", "Out-of-Distribution Detection", "Deep Learning"]}, "abstract": {"value": "Semi-Supervised Domain Generalization (SSDG) aims to learn a model that is generalizable to an unseen target domain with only a few labels, and most existing SSDG methods assume that unlabeled training and testing samples are all known classes. However, a more realistic scenario is that known classes may be mixed with some unknown classes in unlabeled training and testing data. To deal with such a scenario, we propose the Class-Wise Adaptive Exploration and Exploitation (CWAEE) method. In particular, we explore unlabeled training data by using one-vs-rest classifiers and class-wise adaptive thresholds to detect known and unknown classes, and exploit them by adopting consistency regularization on augmented samples based on Fourier Transformation to improve the unseen domain generalization. The experiments conducted on real-world datasets verify the effectiveness and superiority of our method."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ad9ff5480577964853bca71e1603991c8fb3f502.pdf"}, "supplementary_material": {"value": "/attachment/ffa04b4e6fb7881fcca766baa2734de6dcd1f6c1.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023semisupervised,\ntitle={Semi-Supervised Domain Generalization with Known and Unknown Classes},\nauthor={Lei Zhang and Ji-Fu Li and Wei Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ce59j806df}\n}"}, "paperhash": {"value": "zhang|semisupervised_domain_generalization_with_known_and_unknown_classes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3526/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3526/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325828103, "odate": 1698949714805, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ppJuFSOAnM", "number": 3503, "cdate": 1683554651868, "tcdate": 1683554651868, "mdate": 1698949714570, "tmdate": 1698949714570, "signatures": ["NeurIPS.cc/2023/Conference/Submission3503/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3503/Authors"], "forum": "ppJuFSOAnM", "content": {"title": {"value": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation"}, "authors": {"value": ["Zhengyi Wang", "Cheng Lu", "Yikai Wang", "Fan Bao", "Chongxuan Li", "Hang Su", "Jun Zhu"]}, "authorids": {"value": ["~Zhengyi_Wang1", "~Cheng_Lu5", "~Yikai_Wang2", "~Fan_Bao1", "~Chongxuan_Li1", "~Hang_Su3", "~Jun_Zhu2"]}, "keywords": {"value": ["diffusion models", "text to 3D"]}, "abstract": {"value": "Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present *variational score distillation* (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., 7.5). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed *ProlificDreamer*, can generate high rendering resolution (i.e., 512$\\times$512) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9fcdd24de7aa78b124377c974f30459ac23c79f6.pdf"}, "TLDR": {"value": "We achieve photo-realistic text-to-3D results by only using 2D diffusion models, with our variational score distillation training objective."}, "supplementary_material": {"value": "/attachment/79a6d817a5ae7c4cd256cc63e8dbc119cf0bbd0d.zip"}, "_bibtex": {"value": "@inproceedings{\nwang2023prolificdreamer,\ntitle={ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation},\nauthor={Zhengyi Wang and Cheng Lu and Yikai Wang and Fan Bao and Chongxuan Li and Hang Su and Jun Zhu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ppJuFSOAnM}\n}"}, "paperhash": {"value": "wang|prolificdreamer_highfidelity_and_diverse_textto3d_generation_with_variational_score_distillation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3503/-/Revision", "NeurIPS.cc/2023/Conference/Submission3503/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3503/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325827278, "odate": 1698949714542, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "wbbTqsiKzl", "number": 3486, "cdate": 1683553716114, "tcdate": 1683553716114, "mdate": 1698949714494, "tmdate": 1698949714494, "signatures": ["NeurIPS.cc/2023/Conference/Submission3486/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3486/Authors"], "forum": "wbbTqsiKzl", "content": {"title": {"value": "High-dimensional Asymptotics of Denoising Autoencoders"}, "authors": {"value": ["Hugo Cui", "Lenka Zdeborova"]}, "authorids": {"value": ["~Hugo_Cui1", "~Lenka_Zdeborova1"]}, "keywords": {"value": ["statistical physics", "replica method", "autoencoder", "exact asymptotics"]}, "TLDR": {"value": "We provide sharp asymptotics for the test error of non-linear denoising auto-encoders in high dimensions."}, "abstract": {"value": "We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection that relates closely to principal component analysis. We further show that our results capture accurately the learning curves on a range of real datasets."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b808492dd0d7aac41f7086d5dd6cbeb961c29ebd.pdf"}, "supplementary_material": {"value": "/attachment/1fa164d922c2ba949162e3f4f62f8b03468a7da5.zip"}, "_bibtex": {"value": "@inproceedings{\ncui2023highdimensional,\ntitle={High-dimensional Asymptotics of Denoising Autoencoders},\nauthor={Hugo Cui and Lenka Zdeborova},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=wbbTqsiKzl}\n}"}, "paperhash": {"value": "cui|highdimensional_asymptotics_of_denoising_autoencoders"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3486/-/Revision", "NeurIPS.cc/2023/Conference/Submission3486/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3486/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325826843, "odate": 1698949714479, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "HZQZli6amV", "number": 3478, "cdate": 1683553069344, "tcdate": 1683553069344, "mdate": 1698954273613, "tmdate": 1698954273613, "signatures": ["NeurIPS.cc/2023/Conference/Submission3478/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3478/Authors"], "forum": "HZQZli6amV", "content": {"title": {"value": "ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets"}, "authors": {"value": ["Damien Teney", "LIN Yong", "Seong Joon Oh", "Ehsan Abbasnejad"]}, "authorids": {"value": ["~Damien_Teney1", "~LIN_Yong1", "~Seong_Joon_Oh1", "~Ehsan_Abbasnejad3"]}, "keywords": {"value": ["Generalisation; machine learning"]}, "abstract": {"value": "Several studies have compared the in-distribution (ID) and out-of-distribution (OOD) performance of models in computer vision and NLP. They report a frequent positive correlation and some surprisingly never even observe an inverse correlation indicative of a necessary trade-off. The possibility of inverse patterns is important to determine whether ID performance can serve as a proxy for OOD generalization capabilities.\n\nThis paper shows that inverse correlations between ID and OOD performance do happen with multiple real-world datasets, not only in artificial worst-case settings. We explain theoretically how these cases arise and how past studies missed them because of improper methodologies that examined a biased selection of models.\n\nOur observations lead to recommendations that contradict those found in much of the current literature.\n- High OOD performance sometimes requires trading off ID performance.\n- Focusing on ID performance alone may not lead to optimal OOD performance. It may produce diminishing (eventually negative) returns in OOD performance.\n- In these cases, studies on OOD generalization that use ID performance for model selection (a common recommended practice) will necessarily miss the best-performing models, making these studies blind to a whole range of phenomena."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/d1162eb8cc1be10c66ad5934d45d2632fef90074.pdf"}, "supplementary_material": {"value": "/attachment/c64b202c4fa03cd25d98dcb4585a142b56a1b7c4.pdf"}, "TLDR": {"value": "ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets"}, "_bibtex": {"value": "@inproceedings{\nteney2023id,\ntitle={{ID} and {OOD} Performance Are Sometimes Inversely Correlated on Real-world Datasets},\nauthor={Damien Teney and LIN Yong and Seong Joon Oh and Ehsan Abbasnejad},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=HZQZli6amV}\n}"}, "paperhash": {"value": "teney|id_and_ood_performance_are_sometimes_inversely_correlated_on_realworld_datasets"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3478/-/Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3478/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325826638, "odate": 1698949714443, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "QkLpGxUboF", "number": 3469, "cdate": 1683552645692, "tcdate": 1683552645692, "mdate": 1698954273594, "tmdate": 1698954273594, "signatures": ["NeurIPS.cc/2023/Conference/Submission3469/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3469/Authors"], "forum": "QkLpGxUboF", "content": {"title": {"value": "ProPILE: Probing Privacy Leakage in Large Language Models"}, "authors": {"value": ["Siwon Kim", "Sangdoo Yun", "Hwaran Lee", "Martin Gubri", "Sungroh Yoon", "Seong Joon Oh"]}, "authorids": {"value": ["~Siwon_Kim1", "~Sangdoo_Yun1", "~Hwaran_Lee1", "~Martin_Gubri1", "~Sungroh_Yoon1", "~Seong_Joon_Oh1"]}, "keywords": {"value": ["Personal identifiable information", "Private data leakage", "Large language model"]}, "abstract": {"value": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/76a64d6a782630dee52cd257d406310ab5ed06c7.pdf"}, "supplementary_material": {"value": "/attachment/34810b9e68f6a9c5c4b715017fd914e51382d010.pdf"}, "_bibtex": {"value": "@inproceedings{\nkim2023propile,\ntitle={Pro{PILE}: Probing Privacy Leakage in Large Language Models},\nauthor={Siwon Kim and Sangdoo Yun and Hwaran Lee and Martin Gubri and Sungroh Yoon and Seong Joon Oh},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=QkLpGxUboF}\n}"}, "paperhash": {"value": "kim|propile_probing_privacy_leakage_in_large_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3469/-/Revision", "NeurIPS.cc/2023/Conference/Submission3469/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3469/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325826524, "odate": 1698949714361, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "pLsPFxqn7J", "number": 3459, "cdate": 1683551855012, "tcdate": 1683551855012, "mdate": 1698949714305, "tmdate": 1698949714305, "signatures": ["NeurIPS.cc/2023/Conference/Submission3459/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3459/Authors"], "forum": "pLsPFxqn7J", "content": {"title": {"value": "Kernelized Cumulants: Beyond Kernel Mean Embeddings"}, "authors": {"value": ["Patric Bonnier", "Harald Oberhauser", "Zolt\u00e1n Szab\u00f3"]}, "authorids": {"value": ["~Patric_Bonnier1", "~Harald_Oberhauser1", "~Zolt\u00e1n_Szab\u00f31"]}, "keywords": {"value": ["kernel", "cumulant", "mean embedding", "Hilbert-Schmidt independence criterion", "maximum mean discrepancy"]}, "TLDR": {"value": "We define cumulants for random variables in RKHSs and demonstrate their properties and usefulness."}, "abstract": {"value": "In $\\mathbb{R}^d$, it is well-known that cumulants provide an alternative to moments that can achieve the same goals with numerous benefits such as lower variance estimators. In this paper we extend cumulants to reproducing kernel Hilbert spaces (RKHS) using tools from tensor algebras and show that they are computationally tractable by a kernel trick. These kernelized cumulants provide a new set of all-purpose statistics; the classical maximum mean discrepancy and Hilbert-Schmidt independence criterion arise as the degree one objects in our general construction. We argue both theoretically and empirically (on synthetic, environmental, and traffic data analysis) that going beyond degree one has several advantages and can be achieved with the same computational complexity and minimal overhead in our experiments."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/59b9cbc263627bbb6ad6de91c9f8384e65e0f202.pdf"}, "supplementary_material": {"value": "/attachment/a52db897c2393c0f1412faeb62a783d4a48d12e9.zip"}, "_bibtex": {"value": "@inproceedings{\nbonnier2023kernelized,\ntitle={Kernelized Cumulants: Beyond Kernel Mean Embeddings},\nauthor={Patric Bonnier and Harald Oberhauser and Zolt{\\'a}n Szab{\\'o}},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=pLsPFxqn7J}\n}"}, "paperhash": {"value": "bonnier|kernelized_cumulants_beyond_kernel_mean_embeddings"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3459/-/Revision", "NeurIPS.cc/2023/Conference/Submission3459/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3459/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325826137, "odate": 1698949714289, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "sTjW3JHs2V", "number": 3442, "cdate": 1683550785558, "tcdate": 1683550785558, "mdate": 1698949714143, "tmdate": 1698949714143, "signatures": ["NeurIPS.cc/2023/Conference/Submission3442/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3442/Authors"], "forum": "sTjW3JHs2V", "content": {"title": {"value": "Let the Flows Tell:  Solving Graph Combinatorial Problems with GFlowNets"}, "authors": {"value": ["Dinghuai Zhang", "Hanjun Dai", "Nikolay Malkin", "Aaron Courville", "Yoshua Bengio", "Ling Pan"]}, "authorids": {"value": ["~Dinghuai_Zhang1", "~Hanjun_Dai1", "~Nikolay_Malkin1", "~Aaron_Courville3", "~Yoshua_Bengio1", "~Ling_Pan1"]}, "keywords": {"value": ["graph; combinatorial optimization; sampling; gflownets"]}, "abstract": {"value": "Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space.\nOn the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates.\nIn this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. \nEfficient training techniques are also developed to benefit long-range credit assignment.\nThrough extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quality solutions.\nOur implementation is open-sourced at https://github.com/zdhNarsil/GFlowNet-CombOpt."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a GFlowNet-based method to efficiently solve graph combinatorial optimization problems."}, "pdf": {"value": "/pdf/fa53ae4cbd77f49185168e396ec4b6e1d3beaac8.pdf"}, "supplementary_material": {"value": "/attachment/7aff2f3b0fda6955ed62fda837acfa3b135adc9f.zip"}, "_bibtex": {"value": "@inproceedings{\nzhang2023let,\ntitle={Let the Flows Tell:  Solving Graph Combinatorial Problems with {GF}lowNets},\nauthor={Dinghuai Zhang and Hanjun Dai and Nikolay Malkin and Aaron Courville and Yoshua Bengio and Ling Pan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=sTjW3JHs2V}\n}"}, "paperhash": {"value": "zhang|let_the_flows_tell_solving_graph_combinatorial_problems_with_gflownets"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3442/-/Revision", "NeurIPS.cc/2023/Conference/Submission3442/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3442/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325825623, "odate": 1698949714131, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "rheCTpRrxI", "number": 3440, "cdate": 1683550368285, "tcdate": 1683550368285, "mdate": 1698949714121, "tmdate": 1698949714121, "signatures": ["NeurIPS.cc/2023/Conference/Submission3440/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3440/Authors"], "forum": "rheCTpRrxI", "content": {"title": {"value": "DreamHuman: Animatable 3D Avatars from Text"}, "authors": {"value": ["Nikos Kolotouros", "Thiemo Alldieck", "Andrei Zanfir", "Eduard Gabriel Bazavan", "Mihai Fieraru", "Cristian Sminchisescu"]}, "authorids": {"value": ["~Nikos_Kolotouros1", "~Thiemo_Alldieck1", "~Andrei_Zanfir1", "~Eduard_Gabriel_Bazavan2", "~Mihai_Fieraru1", "~Cristian_Sminchisescu1"]}, "keywords": {"value": ["text to 3d; 3d avatars"]}, "TLDR": {"value": "Generate Controllable 3D Avatars from Textual Descriptions"}, "abstract": {"value": "We present \\emph{DreamHuman}, a method to generate realistic animatable 3D human avatar models entirely from textual descriptions. Recent text-to-3D methods have made considerable strides in generation, but are still lacking in important aspects. Control and often spatial resolution remain limited, existing methods produce fixed rather than 3D human models that can be placed in different poses (i.e. re-posable or animatable), and anthropometric consistency for complex structures like people remains a challenge. \\emph{DreamHuman} connects large text-to-image synthesis models, neural radiance fields, and statistical human body models in a novel optimization framework. This makes it possible to generate dynamic 3D human avatars with high-quality textures and learnt per-instance rigid and non rigid geometric deformations. We demonstrate that our method is capable to generate a wide variety of animatable, realistic 3D human models from text. These have diverse appearance, clothing, skin tones and body shapes, and outperform both generic text-to-3D approaches and previous text-based 3D avatar generators in visual fidelity."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a12d0f13ba219468b302a0494838590719066107.pdf"}, "supplementary_material": {"value": "/attachment/2bcc964526c62d30e01d9f306b41295f0ae96050.pdf"}, "_bibtex": {"value": "@inproceedings{\nkolotouros2023dreamhuman,\ntitle={DreamHuman: Animatable 3D Avatars from Text},\nauthor={Nikos Kolotouros and Thiemo Alldieck and Andrei Zanfir and Eduard Gabriel Bazavan and Mihai Fieraru and Cristian Sminchisescu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=rheCTpRrxI}\n}"}, "paperhash": {"value": "kolotouros|dreamhuman_animatable_3d_avatars_from_text"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3440/-/Revision", "NeurIPS.cc/2023/Conference/Submission3440/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3440/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325825510, "odate": 1698949714105, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "iuqCXg1Gng", "number": 3412, "cdate": 1683547572989, "tcdate": 1683547572989, "mdate": 1698949713862, "tmdate": 1698949713862, "signatures": ["NeurIPS.cc/2023/Conference/Submission3412/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3412/Authors"], "forum": "iuqCXg1Gng", "content": {"title": {"value": "Saddle-to-Saddle Dynamics in Diagonal Linear Networks"}, "authors": {"value": ["Scott Pesme", "Nicolas Flammarion"]}, "authorids": {"value": ["~Scott_Pesme1", "~Nicolas_Flammarion1"]}, "keywords": {"value": ["gradient flow", "saddle-to-saddle", "diagonal linear network", "incremental learning"]}, "TLDR": {"value": "We prove and characterise a saddle-to-saddle dynamics of gradient flow with vanishing initialisation over 2-layer diagonal linear networks."}, "abstract": {"value": "In this paper we fully describe the trajectory of gradient flow over $2$-layer diagonal linear networks for the regression setting in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum $\\ell_1$-norm solution. We explicitly characterise the visited saddles as well as the jump times through a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path.  Starting from the zero vector, coordinates are successively activated until the minimum $\\ell_1$-norm solution is recovered, revealing an incremental learning. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of active coordinates. We provide numerical experiments to support our findings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/2076d0adc02feb62e5df2cd9acf0eeabff55a83a.pdf"}, "supplementary_material": {"value": "/attachment/5339d46d2e70903a601815bf36f393ec74f3db3b.pdf"}, "_bibtex": {"value": "@inproceedings{\npesme2023saddletosaddle,\ntitle={Saddle-to-Saddle Dynamics in Diagonal Linear Networks},\nauthor={Scott Pesme and Nicolas Flammarion},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=iuqCXg1Gng}\n}"}, "paperhash": {"value": "pesme|saddletosaddle_dynamics_in_diagonal_linear_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3412/-/Revision", "NeurIPS.cc/2023/Conference/Submission3412/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3412/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325824652, "odate": 1698949713849, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "MvCq52yt9Y", "number": 3258, "cdate": 1683534489184, "tcdate": 1683534489184, "mdate": 1698949712754, "tmdate": 1698949712754, "signatures": ["NeurIPS.cc/2023/Conference/Submission3258/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3258/Authors"], "forum": "MvCq52yt9Y", "content": {"title": {"value": "Mitigating the Popularity Bias of  Graph Collaborative Filtering: A Dimensional Collapse Perspective"}, "authors": {"value": ["Yifei Zhang", "Hao Zhu", "yankai Chen", "Zixing Song", "Piotr Koniusz", "Irwin King"]}, "authorids": {"value": ["~Yifei_Zhang6", "~Hao_Zhu2", "~yankai_Chen1", "~Zixing_Song2", "~Piotr_Koniusz1", "~Irwin_King1"]}, "keywords": {"value": ["Graph", "Collaborative Filtering", "Recommendation"]}, "abstract": {"value": "Graph-based Collaborative Filtering (GCF) is widely used in personalized recommendation systems. However, GCF suffers from a fundamental problem where features tend to occupy the embedding space inefficiently (by spanning only a low-dimensional subspace). Such an effect is characterized in GCF by the embedding space being dominated by a few of popular items with the user embeddings highly concentrated around them. This enhances the so-called Matthew effect of the popularity bias where popular items are highly recommend whereas remaining items are ignored. In this paper, we analyze the above effect in GCF and reveal that the simplified graph convolution operation (typically used in GCF) shrinks the singular space of the feature matrix. As typical approaches  (i.e., optimizing the uniformity term) fail to prevent the embedding space degradation, we propose a decorrelation-enhanced GCF objective that promotes feature diversity by leveraging the so-called principle of redundancy reduction in embeddings. However, unlike conventional methods that use the Euclidean geometry to relax hard constraints for decorrelation, we exploit non-Euclidean geometry. Such a choice helps  maintain the range space of the matrix and obtain small condition number, which prevents the embedding space degradation. Our  method  outperforms contrastive-based GCF models on several benchmark datasets and improves the performance for unpopular items."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/0fc21176cec2c74a569003e7a35fdceaa19f493a.pdf"}, "supplementary_material": {"value": "/attachment/fd6925ebec0942ee43e3c07c6d485007c9c1595f.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023mitigating,\ntitle={Mitigating the Popularity Bias of  Graph Collaborative Filtering: A Dimensional Collapse Perspective},\nauthor={Yifei Zhang and Hao Zhu and yankai Chen and Zixing Song and Piotr Koniusz and Irwin King},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=MvCq52yt9Y}\n}"}, "paperhash": {"value": "zhang|mitigating_the_popularity_bias_of_graph_collaborative_filtering_a_dimensional_collapse_perspective"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3258/-/Revision", "NeurIPS.cc/2023/Conference/Submission3258/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3258/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325820076, "odate": 1698949712735, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cNb5hkTfGC", "number": 3233, "cdate": 1683533223855, "tcdate": 1683533223855, "mdate": 1698949712673, "tmdate": 1698949712673, "signatures": ["NeurIPS.cc/2023/Conference/Submission3233/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3233/Authors"], "forum": "cNb5hkTfGC", "content": {"title": {"value": "A Scalable Neural Network for DSIC Affine Maximizer Auction Design"}, "authors": {"value": ["Zhijian Duan", "Haoran Sun", "Yurong Chen", "Xiaotie Deng"]}, "authorids": {"value": ["~Zhijian_Duan1", "~Haoran_Sun6", "~Yurong_Chen3", "~Xiaotie_Deng1"]}, "keywords": {"value": ["Automated Mechanism Design", "Auction Design", "Affine Maximizer Auctions", "Deep Learning", "Game Theory"]}, "abstract": {"value": "Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both contextual and non-contextual multi-item auctions, scales well to larger auctions, generalizes well to different settings, and identifies useful deterministic allocations. Overall, our proposed approach offers an effective solution to automated DSIC auction design, with improved scalability and strong revenue performance in various settings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose AMenuNet, a scalable neural network for DSIC affine maximizer auction design."}, "pdf": {"value": "/pdf/f1ed79ea6ead9390dbcaab5e6d392a14508cf749.pdf"}, "_bibtex": {"value": "@inproceedings{\nduan2023a,\ntitle={A Scalable Neural Network for {DSIC} Affine Maximizer Auction Design},\nauthor={Zhijian Duan and Haoran Sun and Yurong Chen and Xiaotie Deng},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cNb5hkTfGC}\n}"}, "paperhash": {"value": "duan|a_scalable_neural_network_for_dsic_affine_maximizer_auction_design"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3233/-/Revision", "NeurIPS.cc/2023/Conference/Submission3233/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3233/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325819454, "odate": 1698949712662, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "AiEipk1X0c", "number": 3209, "cdate": 1683531750114, "tcdate": 1683531750114, "mdate": 1698949712563, "tmdate": 1698949712563, "signatures": ["NeurIPS.cc/2023/Conference/Submission3209/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3209/Authors"], "forum": "AiEipk1X0c", "content": {"title": {"value": "A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability"}, "authors": {"value": ["Zijie Geng", "Xijun Li", "Jie Wang", "Xiao Li", "Yongdong Zhang", "Feng Wu"]}, "authorids": {"value": ["~Zijie_Geng1", "~Xijun_Li1", "~Jie_Wang1", "~Xiao_Li19", "~Yongdong_Zhang2", "~Feng_Wu1"]}, "keywords": {"value": ["Learning to Optimize", "Machine Learning for Combinatorial Optimization", "Mixed-Integer Linear Programming", "Graph Generation"]}, "abstract": {"value": "In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, *the first* deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instances without prior expert-designed formulations, while preserving the structures and computational hardness of real-world datasets, simultaneously. Thus the generated instances can facilitate downstream tasks for enhancing MILP solvers under limited data availability. We design a suite of benchmarks to evaluate the quality of the generated MILP instances. Experiments demonstrate that our method can produce instances that closely resemble real-world datasets in terms of both structures and computational hardness. The deliverables are released at [https://miralab-ustc.github.io/L2O-G2MILP](https://miralab-ustc.github.io/L2O-G2MILP)."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose G2MILP, the first deep generative framework for mixed-integer linear program (MILP) instances, thus enhancing MILP solvers under limited data availability."}, "pdf": {"value": "/pdf/5c519fa211c8f36cdf47b97c748f13a852f2da93.pdf"}, "supplementary_material": {"value": "/attachment/085d49bd34c6f1a30c99d5ca3e5cb090ad09b44f.pdf"}, "_bibtex": {"value": "@inproceedings{\ngeng2023a,\ntitle={A Deep Instance Generative Framework for {MILP} Solvers Under Limited Data Availability},\nauthor={Zijie Geng and Xijun Li and Jie Wang and Xiao Li and Yongdong Zhang and Feng Wu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=AiEipk1X0c}\n}"}, "paperhash": {"value": "geng|a_deep_instance_generative_framework_for_milp_solvers_under_limited_data_availability"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3209/-/Revision", "NeurIPS.cc/2023/Conference/Submission3209/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3209/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325818304, "odate": 1698949712551, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "dSRyKIYRnP", "number": 3193, "cdate": 1683530441799, "tcdate": 1683530441799, "mdate": 1698949712428, "tmdate": 1698949712428, "signatures": ["NeurIPS.cc/2023/Conference/Submission3193/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3193/Authors"], "forum": "dSRyKIYRnP", "content": {"title": {"value": "Slow and Weak Attractor Computation Embedded in Fast and Strong E-I Balanced Neural Dynamics"}, "authors": {"value": ["Xiaohan Lin", "Liyuan Li", "Boxin Shi", "Tiejun Huang", "Yuanyuan Mi", "Si Wu"]}, "authorids": {"value": ["~Xiaohan_Lin1", "liyuanli@pku.edu.cn", "~Boxin_Shi3", "~Tiejun_Huang1", "~Yuanyuan_Mi1", "~Si_Wu1"]}, "keywords": {"value": ["Continuous attractor neural network; Excitation inhibition balance; Brain-inspired algorithms; Object tracking;"]}, "TLDR": {"value": "Continuous attractor network dynamics and excitation-inhibition balanced dynamics can coexist in one circuit with synergistic computation benefits."}, "abstract": {"value": "Attractor networks require neuronal connections to be highly structured in order to maintain attractor states that represent information, while excitation and inhibition balanced networks (E-INNs) require neuronal connections to be random and sparse to generate irregular neuronal firings. Despite being regarded as canonical models of neural circuits, both types of networks are usually studied in isolation, and it remains unclear how they coexist in the brain, given their very different structural demands. In this study, we investigate the compatibility of continuous attractor neural networks (CANNs) and E-INNs. In line with recent experimental data, we find that a neural circuit can exhibit both the traits of CANNs and E-INNs if the neuronal synapses consist of two sets: one set is strong and fast for irregular firing, and the other set is weak and slow for attractor dynamics. Our results from simulations and theoretical analysis reveal that the network also exhibits enhanced performance compared to the case of using only one set of synapses, with accelerated convergence of attractor states and retained E-I balanced condition for localized input. We also apply the network model to solve a real-world tracking problem and demonstrate that it can track fast-moving objects well. We hope that this study provides insight into how structured neural computations are realized by irregular firings of neurons."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c2644c16208ec0c0ae0c105f959938b258f0a7bf.pdf"}, "supplementary_material": {"value": "/attachment/5d551c1ac1d18aa24198ab3fd74b8ea9a093197d.pdf"}, "_bibtex": {"value": "@inproceedings{\nlin2023slow,\ntitle={Slow and Weak Attractor Computation Embedded in Fast and Strong E-I Balanced Neural Dynamics},\nauthor={Xiaohan Lin and Liyuan Li and Boxin Shi and Tiejun Huang and Yuanyuan Mi and Si Wu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=dSRyKIYRnP}\n}"}, "paperhash": {"value": "lin|slow_and_weak_attractor_computation_embedded_in_fast_and_strong_ei_balanced_neural_dynamics"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3193/-/Revision", "NeurIPS.cc/2023/Conference/Submission3193/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3193/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325817480, "odate": 1698949712416, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "DBz9E5aZey", "number": 3145, "cdate": 1683525058316, "tcdate": 1683525058316, "mdate": 1698949712194, "tmdate": 1698949712194, "signatures": ["NeurIPS.cc/2023/Conference/Submission3145/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3145/Authors"], "forum": "DBz9E5aZey", "content": {"title": {"value": "Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation"}, "authors": {"value": ["Aniket Das", "Dheeraj Mysore Nagaraj"]}, "authorids": {"value": ["~Aniket_Das1", "~Dheeraj_Mysore_Nagaraj1"]}, "keywords": {"value": ["Stein Variational Gradient Descent", "Variational Inference", "Sampling"]}, "TLDR": {"value": "Novel, computationally efficient variants of SVGD that exhibit provably fast convergence in the finite-particle regime.."}, "abstract": {"value": "Stein Variational Gradient Descent (SVGD) is a popular particle-based variational inference algorithm with impressive empirical performance across various domains. Although the population (i.e, infinite-particle) limit dynamics of SVGD is well characterized, its behavior in the finite-particle regime is far less understood. To this end, our work introduces the notion of *virtual particles* to develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, that are exactly realizable using finite particles. As a result, we design two computationally efficient variants of SVGD, namely VP-SVGD and GB-SVGD, with provably fast finite-particle convergence rates. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinary SVGD. We show that the $n$ particles output by VP-SVGD and GB-SVGD, run for $T$ steps with batch-size $K$, are at-least as good as i.i.d samples from a distribution whose Kernel Stein Discrepancy to the target is at most $O(\\tfrac{d^{1/3}}{(KT)^{1/6}})$ under standard assumptions. Our results also hold under a mild growth condition on the potential function, which is much weaker than the isoperimetric (e.g. Poincare Inequality) or information-transport conditions (e.g. Talagrand's Inequality $\\mathsf{T}_1$) generally considered in prior works. As a corollary, we analyze the convergence of the empirical measure (of the particles output by VP-SVGD and GB-SVGD) to the target distribution and demonstrate a **double exponential improvement** over the best known finite-particle analysis of SVGD. Beyond this, our results present the **first known oracle complexities for this setting with polynomial dimension dependence**, thereby completely eliminating the curse of dimensionality exhibited by previously known finite-particle rates."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e7d1f3cfb56b35b48beef258a171dd9e81f5896a.pdf"}, "supplementary_material": {"value": "/attachment/ab0fc6d155184a3edda82b3eb28a0e701da5c2f3.pdf"}, "_bibtex": {"value": "@inproceedings{\ndas2023provably,\ntitle={Provably Fast Finite Particle Variants of {SVGD} via Virtual Particle Stochastic Approximation},\nauthor={Aniket Das and Dheeraj Mysore Nagaraj},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=DBz9E5aZey}\n}"}, "paperhash": {"value": "das|provably_fast_finite_particle_variants_of_svgd_via_virtual_particle_stochastic_approximation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3145/-/Revision", "NeurIPS.cc/2023/Conference/Submission3145/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3145/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325816101, "odate": 1698949712179, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "qlnlamFQEa", "number": 3139, "cdate": 1683524416601, "tcdate": 1683524416601, "mdate": 1698949712110, "tmdate": 1698949712110, "signatures": ["NeurIPS.cc/2023/Conference/Submission3139/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3139/Authors"], "forum": "qlnlamFQEa", "content": {"title": {"value": "Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback"}, "authors": {"value": ["Shenghuan Sun", "Gregory Goldgof", "Atul Butte", "Ahmed Alaa"]}, "authorids": {"value": ["shenghuan.sun@ucsf.edu", "goldgofg@mskcc.org", "atul.butte@ucsf.edu", "~Ahmed_Alaa1"]}, "keywords": {"value": ["Synthetic clinical data", "Machine learning for healthcare"]}, "abstract": {"value": "Generative models capable of precisely capturing nuanced clinical features in medical images hold great promise for facilitating clinical data sharing, enhancing rare disease datasets, and efficiently synthesizing (annotated) medical images at scale. Despite their potential, assessing the quality of synthetic medical images remains a challenge. While modern generative models can synthesize visually-realistic medical images, the clinical plausibility of these images may be called into question. Domain-agnostic scores, such as FID score, precision, and recall, cannot incorporate clinical knowledge and are, therefore, not suitable for assessing clinical sensibility. Additionally, there are numerous unpredictable ways in which generative models may fail to synthesize clinically plausible images, making it challenging to anticipate potential failures and design automated scores for their detection. To address these challenges, this paper introduces a pathologist-in-the-loop framework for generating clinically-plausible synthetic medical images. Our framework comprises three steps: (1) pretraining a conditional diffusion model to generate medical images conditioned on a clinical concept, (2) expert pathologist evaluation of the generated images to assess whether they satisfy clinical desiderata, and (3) training a reward model that predicts human feedback on new samples, which we use to incorporate expert knowledge into the finetuning objective of the diffusion model. Our results show that human feedback significantly improves the quality of synthetic images in terms of fidelity, diversity, utility in downstream applications, and plausibility as evaluated by experts. We also demonstrate that human feedback can teach the model new clinical concepts not annotated in the original training data. Our results demonstrate the value of incorporating human feedback in clinical applications where generative models may struggle to capture extensive domain knowledge from raw data alone."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e7974dfbf220744484b1fe53cde36c23670ffab5.pdf"}, "_bibtex": {"value": "@inproceedings{\nsun2023aligning,\ntitle={Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback},\nauthor={Shenghuan Sun and Gregory Goldgof and Atul Butte and Ahmed Alaa},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=qlnlamFQEa}\n}"}, "paperhash": {"value": "sun|aligning_synthetic_medical_images_with_clinical_knowledge_using_human_feedback"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3139/-/Revision", "NeurIPS.cc/2023/Conference/Submission3139/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3139/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325815866, "odate": 1698949712097, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ginTcBUnL8", "number": 3046, "cdate": 1683514425014, "tcdate": 1683514425014, "mdate": 1698949711383, "tmdate": 1698949711383, "signatures": ["NeurIPS.cc/2023/Conference/Submission3046/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3046/Authors"], "forum": "ginTcBUnL8", "content": {"title": {"value": "SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling"}, "authors": {"value": ["Jiaxiang Dong", "Haixu Wu", "Haoran Zhang", "Li Zhang", "Jianmin Wang", "Mingsheng Long"]}, "authorids": {"value": ["~Jiaxiang_Dong1", "~Haixu_Wu1", "~Haoran_Zhang9", "lizhang@tsinghua.edu.cn", "~Jianmin_Wang1", "~Mingsheng_Long5"]}, "keywords": {"value": ["Time-series analysis", "pre-training", "masked time-series modeling"]}, "TLDR": {"value": "We present a new masked modeling method for time series as SimMTM. Unlike the previous, it reconstructs the original time series with multiple series. SimMTM achieves state-of-the-art fine-tuning performance in both forecasting and classification."}, "abstract": {"value": "Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from multiple masked series. SimMTM further learns to uncover the local structure of the manifold, which is helpful for masked modeling. Experimentally, SimMTM achieves state-of-the-art fine-tuning performance compared to the most advanced time series pre-training methods in two canonical time series analysis tasks: forecasting and classification, covering both in- and cross-domain settings."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/332b6e165678e4078784fd73e5ebc74bff3d3711.pdf"}, "supplementary_material": {"value": "/attachment/729113596cb5c91f613b9f361a53bfda34cf3227.pdf"}, "_bibtex": {"value": "@inproceedings{\ndong2023simmtm,\ntitle={Sim{MTM}: A Simple Pre-Training Framework for Masked Time-Series Modeling},\nauthor={Jiaxiang Dong and Haixu Wu and Haoran Zhang and Li Zhang and Jianmin Wang and Mingsheng Long},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ginTcBUnL8}\n}"}, "paperhash": {"value": "dong|simmtm_a_simple_pretraining_framework_for_masked_timeseries_modeling"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3046/-/Revision", "NeurIPS.cc/2023/Conference/Submission3046/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3046/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325813384, "odate": 1698949711369, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Pz8xvVCLNJ", "number": 3025, "cdate": 1683512240291, "tcdate": 1683512240291, "mdate": 1698949711245, "tmdate": 1698949711245, "signatures": ["NeurIPS.cc/2023/Conference/Submission3025/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3025/Authors"], "forum": "Pz8xvVCLNJ", "content": {"title": {"value": "Vulnerabilities in Video Quality Assessment Models: The Challenge of Adversarial Attacks"}, "authors": {"value": ["Aoxiang Zhang", "Yu Ran", "Weixuan Tang", "Yuan-Gen Wang"]}, "authorids": {"value": ["~Aoxiang_Zhang1", "~Yu_Ran3", "~Weixuan_Tang1", "~Yuan-Gen_Wang1"]}, "keywords": {"value": ["video quality assessment", "adversarial attack", "black-box", "just noticeable difference"]}, "abstract": {"value": "No-Reference Video Quality Assessment (NR-VQA) plays an essential role in improving the viewing experience of end-users. Driven by deep learning, recent NR-VQA models based on Convolutional Neural Networks (CNNs) and Transformers have achieved outstanding performance. To build a reliable and practical assessment system, it is of great necessity to evaluate their robustness. However, such issue has received little attention in the academic community. In this paper, we make the first attempt to evaluate the robustness of NR-VQA models against\nadversarial attacks, and propose a patch-based random search method for black-box attack. Specifically, considering both the attack effect on quality score and the visual quality of adversarial video, the attack problem is formulated as misleading the estimated quality score under the constraint of just-noticeable difference (JND). Built upon such formulation, a novel loss function called Score-Reversed Boundary Loss is designed to push the adversarial video\u2019s estimated quality score far away from its ground-truth score towards a specific boundary, and the JND constraint is modeled as a strict $L_2$ and $L_\\infty$ norm restriction. By this means, both white-box and black-box attacks can be launched in an effective and imperceptible manner. The source code is available at https://github.com/GZHU-DVL/AttackVQA."}, "pdf": {"value": "/pdf/a2d28d7679533afb7507649e1a942167bcd3cc15.pdf"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "supplementary_material": {"value": "/attachment/8cfebfc442d9b792582e5a2bc13f9ba478395a7c.zip"}, "_bibtex": {"value": "@inproceedings{\nzhang2023vulnerabilities,\ntitle={Vulnerabilities in Video Quality Assessment Models: The Challenge of Adversarial Attacks},\nauthor={Aoxiang Zhang and Yu Ran and Weixuan Tang and Yuan-Gen Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Pz8xvVCLNJ}\n}"}, "paperhash": {"value": "zhang|vulnerabilities_in_video_quality_assessment_models_the_challenge_of_adversarial_attacks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3025/-/Revision", "NeurIPS.cc/2023/Conference/Submission3025/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3025/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325812763, "odate": 1698949711229, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "8rDbUoYc0p", "number": 3005, "cdate": 1683510167067, "tcdate": 1683510167067, "mdate": 1698949711132, "tmdate": 1698949711132, "signatures": ["NeurIPS.cc/2023/Conference/Submission3005/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission3005/Authors"], "forum": "8rDbUoYc0p", "content": {"title": {"value": "Constant Approximation for Individual Preference Stable Clustering"}, "authors": {"value": ["Anders Aamand", "Justin Y. Chen", "Allen Liu", "Sandeep Silwal", "Pattara Sukprasert", "Ali Vakilian", "Fred Zhang"]}, "authorids": {"value": ["~Anders_Aamand1", "~Justin_Y._Chen1", "~Allen_Liu1", "~Sandeep_Silwal1", "~Pattara_Sukprasert2", "~Ali_Vakilian1", "~Fred_Zhang1"]}, "keywords": {"value": ["clustering", "fairness", "approximation algorithms"]}, "TLDR": {"value": "We give a constant-factor approximation algorithm for individual preference (IP) stability clustering."}, "abstract": {"value": "Individual preference (IP) stability, introduced by Ahmadi et al. (ICML 2022), is a natural clustering objective inspired by stability and fairness constraints. A clustering is $\\alpha$-IP stable if the average distance of every data point to its own cluster is at most $\\alpha$ times the average distance to any other cluster. Unfortunately, determining if a dataset admits a $1$-IP stable clustering is NP-Hard. Moreover, before this work, it was unknown if an $o(n)$-IP stable clustering always exists, as the prior state of the art only guaranteed an $O(n)$-IP stable clustering. We close this gap in understanding and show that an $O(1)$-IP stable clustering always exists for general metrics, and we give an efficient algorithm which outputs such a clustering.  We also introduce generalizations of IP stability beyond average distance and give efficient near optimal algorithms in the cases where we consider the maximum and minimum distances within and between clusters."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a5154f82f616f4d08113676041d6d230abdda11f.pdf"}, "supplementary_material": {"value": "/attachment/179505fb8c0860a64bb47a8bf3c49e5b50186404.zip"}, "_bibtex": {"value": "@inproceedings{\naamand2023constant,\ntitle={Constant Approximation for Individual Preference Stable Clustering},\nauthor={Anders Aamand and Justin Y. Chen and Allen Liu and Sandeep Silwal and Pattara Sukprasert and Ali Vakilian and Fred Zhang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=8rDbUoYc0p}\n}"}, "paperhash": {"value": "aamand|constant_approximation_for_individual_preference_stable_clustering"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission3005/-/Revision", "NeurIPS.cc/2023/Conference/Submission3005/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission3005/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325812231, "odate": 1698949711113, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "WXc8O8ghLH", "number": 2945, "cdate": 1683498905134, "tcdate": 1683498905134, "mdate": 1698949710638, "tmdate": 1698949710638, "signatures": ["NeurIPS.cc/2023/Conference/Submission2945/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2945/Authors"], "forum": "WXc8O8ghLH", "content": {"title": {"value": "Max-Margin Token Selection in Attention Mechanism"}, "authors": {"value": ["Davoud Ataee Tarzanagh", "Yingcong Li", "Xuechen Zhang", "Samet Oymak"]}, "authorids": {"value": ["~Davoud_Ataee_Tarzanagh1", "~Yingcong_Li1", "~Xuechen_Zhang2", "~Samet_Oymak2"]}, "keywords": {"value": ["attention mechanism", "implicit bias", "margin maximization", "nonconvex optimization", "prompt tuning"]}, "TLDR": {"value": "We prove that softmax-attention weights trained with gradient descent converge to a max-margin solution that separates locally-optimal tokens from non-optimal ones."}, "abstract": {"value": "Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(X)=\\langle Xv, \\texttt{softmax}(XWp)\\rangle$, where $X$ is the token sequence and $(v,W,p)$ are trainable parameters. We prove that running gradient descent on $p$, or equivalently $W$, converges in direction to a max-margin solution that separates *locally-optimal* tokens from non-optimal ones. This clearly formalizes attention as an optimal token selection mechanism. Remarkably, our results are applicable to general data and precisely characterize *optimality* of tokens in terms of the value embeddings $Xv$ and problem geometry. We also provide a broader regularization path analysis that establishes the margin maximizing nature of attention even for nonlinear prediction heads. When optimizing $v$ and $p$ simultaneously with logistic loss, we identify conditions under which the regularization paths directionally converge to their respective hard-margin SVM solutions where $v$ separates the input features based on their labels. Interestingly, the SVM formulation of $p$ is influenced by the support vector geometry of $v$. Finally, we verify our theoretical findings via numerical experiments and provide insights."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9201a833535116656cac442a02c2776bd7955621.pdf"}, "supplementary_material": {"value": "/attachment/e4826752f146b9ba14b163f6ffc5aff5340def46.zip"}, "_bibtex": {"value": "@inproceedings{\ntarzanagh2023maxmargin,\ntitle={Max-Margin Token Selection in Attention Mechanism},\nauthor={Davoud Ataee Tarzanagh and Yingcong Li and Xuechen Zhang and Samet Oymak},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=WXc8O8ghLH}\n}"}, "paperhash": {"value": "tarzanagh|maxmargin_token_selection_in_attention_mechanism"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2945/-/Revision", "NeurIPS.cc/2023/Conference/Submission2945/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2945/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325810388, "odate": 1698949710626, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ZITOHWeAy7", "number": 2918, "cdate": 1683487700750, "tcdate": 1683487700750, "mdate": 1698949710205, "tmdate": 1698949710205, "signatures": ["NeurIPS.cc/2023/Conference/Submission2918/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2918/Authors"], "forum": "ZITOHWeAy7", "content": {"title": {"value": "A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning"}, "authors": {"value": ["Yiyou Sun", "Zhenmei Shi", "Yixuan Li"]}, "authorids": {"value": ["~Yiyou_Sun1", "~Zhenmei_Shi1", "~Yixuan_Li1"]}, "keywords": {"value": ["open-world learning", "clustering", "spectral analysis"]}, "TLDR": {"value": "N/A"}, "abstract": {"value": "Open-world semi-supervised learning aims at inferring both known and novel classes in unlabeled data, by harnessing prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for this problem. This paper bridges the gap by formalizing a graph-theoretic framework tailored for the open-world setting, where the clustering can be theoretically characterized by graph factorization. Our graph-theoretic framework illuminates practical algorithms and provides guarantees. In particular, based on our graph formulation, we apply the algorithm called Spectral Open-world Representation Learning (SORL), and show that minimizing our loss is equivalent to performing spectral decomposition on the graph. Such equivalence allows us to derive a provable error bound on the clustering performance for both known and novel classes, and analyze rigorously when labeled data helps. Empirically, SORL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7175d11f4135ef083594ca10ec92993ba03929e4.pdf"}, "_bibtex": {"value": "@inproceedings{\nsun2023a,\ntitle={A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning},\nauthor={Yiyou Sun and Zhenmei Shi and Yixuan Li},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ZITOHWeAy7}\n}"}, "paperhash": {"value": "sun|a_graphtheoretic_framework_for_understanding_openworld_semisupervised_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2918/-/Revision", "NeurIPS.cc/2023/Conference/Submission2918/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2918/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325809367, "odate": 1698949710187, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "D94QKZA7UP", "number": 2911, "cdate": 1683485068284, "tcdate": 1683485068284, "mdate": 1698949710104, "tmdate": 1698949710104, "signatures": ["NeurIPS.cc/2023/Conference/Submission2911/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2911/Authors"], "forum": "D94QKZA7UP", "content": {"title": {"value": "A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment"}, "authors": {"value": ["Yixuan Even Xu", "Steven Jecmen", "Zimeng Song", "Fei Fang"]}, "authorids": {"value": ["~Yixuan_Even_Xu1", "~Steven_Jecmen1", "~Zimeng_Song1", "~Fei_Fang1"]}, "keywords": {"value": ["peer review", "randomized paper assignment", "mitigating malicious behavior", "convex optimization"]}, "abstract": {"value": "The assignment of papers to reviewers is a crucial part of the peer review processes of large publication venues, where organizers (e.g., conference program chairs) rely on algorithms to perform automated paper assignment. As such, a major challenge for the organizers of these processes is to specify paper assignment algorithms that find appropriate assignments with respect to various desiderata. Although the main objective when choosing a good paper assignment is to maximize the expertise of each reviewer for their assigned papers, several other considerations make introducing randomization into the paper assignment desirable: robustness to malicious behavior, the ability to evaluate alternative paper assignments, reviewer diversity, and reviewer anonymity. However, it is unclear in what way one should randomize the paper assignment in order to best satisfy all of these considerations simultaneously. In this work, we present a practical, one-size-fits-all method for randomized paper assignment intended to perform well across different motivations for randomness. We show theoretically and experimentally that our method outperforms currently-deployed methods for randomized paper assignment on several intuitive randomness metrics, demonstrating that the randomized assignments produced by our method are general-purpose."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a practical, general-purpose method for randomizing paper assignments in peer review, and show its effect both theoretically and empirically."}, "pdf": {"value": "/pdf/4445bd1215e69aab536240bd2fd30887b33be477.pdf"}, "supplementary_material": {"value": "/attachment/4db7432766968ff25ada1571a7690047131fbd03.zip"}, "_bibtex": {"value": "@inproceedings{\nxu2023a,\ntitle={A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment},\nauthor={Yixuan Even Xu and Steven Jecmen and Zimeng Song and Fei Fang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=D94QKZA7UP}\n}"}, "paperhash": {"value": "xu|a_onesizefitsall_approach_to_improving_randomness_in_paper_assignment"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2911/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2911/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325809115, "odate": 1698949710089, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "DQgTewaKzt", "number": 2891, "cdate": 1683480430563, "tcdate": 1683480430563, "mdate": 1698949709872, "tmdate": 1698949709872, "signatures": ["NeurIPS.cc/2023/Conference/Submission2891/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2891/Authors"], "forum": "DQgTewaKzt", "content": {"title": {"value": "ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking"}, "authors": {"value": ["Yutong Kou", "Jin Gao", "Bing Li", "Gang Wang", "Weiming Hu", "Yizheng Wang", "Liang Li"]}, "authorids": {"value": ["~Yutong_Kou1", "~Jin_Gao1", "~Bing_Li1", "~Gang_Wang22", "~Weiming_Hu1", "~Yizheng_Wang2", "~Liang_Li9"]}, "keywords": {"value": ["Visual tracking", "non-uniform resizing", "HVS-inspired processing"]}, "abstract": {"value": "Recently, the transformer has enabled the speed-oriented trackers to approach state-of-the-art (SOTA) performance with high-speed thanks to the smaller input size or the lighter feature extraction backbone, though they still substantially lag behind their corresponding performance-oriented versions. In this paper, we demonstrate that it is possible to narrow or even close this gap while achieving high tracking speed based on the smaller input size. To this end, we non-uniformly resize the cropped image to have a smaller input size while the resolution of the area where the target is more likely to appear is higher and vice versa. This enables us to solve the dilemma of attending to a larger visual field while retaining more raw information for the target despite a smaller input size. Our formulation for the non-uniform resizing can be efficiently solved through quadratic programming (QP) and naturally integrated into most of the crop-based local trackers. Comprehensive experiments on five challenging datasets based on two kinds  of transformer trackers, \\ie, OSTrack and TransT, demonstrate consistent improvements over them. In particular, applying our method to the speed-oriented version of OSTrack even outperforms its performance-oriented counterpart by 0.6\\% AUC on TNL2K, while running 50\\% faster and saving over 55\\% MACs. Codes and models are available at https://github.com/Kou-99/ZoomTrack."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e4ddd3ad98aecb55de1b84afaf4ed35340e10c07.pdf"}, "_bibtex": {"value": "@inproceedings{\nkou2023zoomtrack,\ntitle={ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking},\nauthor={Yutong Kou and Jin Gao and Bing Li and Gang Wang and Weiming Hu and Yizheng Wang and Liang Li},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=DQgTewaKzt}\n}"}, "paperhash": {"value": "kou|zoomtrack_targetaware_nonuniform_resizing_for_efficient_visual_tracking"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2891/-/Revision", "NeurIPS.cc/2023/Conference/Submission2891/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2891/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325808434, "odate": 1698949709855, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "rHAX0LRwk8", "number": 2886, "cdate": 1683480002218, "tcdate": 1683480002218, "mdate": 1698949709829, "tmdate": 1698949709829, "signatures": ["NeurIPS.cc/2023/Conference/Submission2886/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2886/Authors"], "forum": "rHAX0LRwk8", "content": {"title": {"value": "Adversarial Counterfactual Environment Model Learning"}, "authors": {"value": ["Xiong-Hui Chen", "Yang Yu", "Zhengmao Zhu", "ZhiHua Yu", "Chen Zhenjun", "Chenghe Wang", "Yinan Wu", "Rong-Jun Qin", "Hongqiu Wu", "Ruijin Ding", "Huang Fangsheng"]}, "authorids": {"value": ["~Xiong-Hui_Chen1", "~Yang_Yu5", "~Zhengmao_Zhu1", "~ZhiHua_Yu2", "~Chen_Zhenjun1", "~Chenghe_Wang1", "~Yinan_Wu2", "~Rong-Jun_Qin1", "~Hongqiu_Wu2", "~Ruijin_Ding1", "~Huang_Fangsheng1"]}, "keywords": {"value": ["environment model learning", "offline reinforcement learning", "off-policy evaluation", "individual treatment effects estimation", "causal inference", "adversarial learning"]}, "TLDR": {"value": "We propose a new environment model learning techniques with better generalization ability on counterfactual data."}, "abstract": {"value": "An accurate environment dynamics model is crucial for various downstream tasks, such as counterfactual prediction, off-policy evaluation, and offline reinforcement learning. \nCurrently, these models were learned through empirical risk minimization (ERM) by step-wise fitting of historical transition data.\nHowever, we first show that, particularly in the sequential decision-making setting, this approach may catastrophically fail to predict counterfactual action effects due to the selection bias of behavior policies during data collection.\nTo tackle this problem, we introduce a novel model-learning objective called adversarial weighted empirical risk minimization (AWRM).  AWRM incorporates an adversarial policy that exploits the model to generate a data distribution that weakens the model's prediction accuracy, and subsequently, the model is learned under this adversarial data distribution.\nWe implement a practical algorithm, GALILEO, for AWRM and evaluate it on two synthetic tasks, three continuous-control tasks, and  \\textit{a real-world application}. The experiments demonstrate that GALILEO can accurately predict counterfactual actions and improve various downstream tasks, including offline policy evaluation and improvement, as well as online decision-making."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1f0dd9bd881e753cfed0b604e4bf751df3ac0644.pdf"}, "_bibtex": {"value": "@inproceedings{\nchen2023adversarial,\ntitle={Adversarial Counterfactual Environment Model Learning},\nauthor={Xiong-Hui Chen and Yang Yu and Zhengmao Zhu and ZhiHua Yu and Chen Zhenjun and Chenghe Wang and Yinan Wu and Rong-Jun Qin and Hongqiu Wu and Ruijin Ding and Huang Fangsheng},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=rHAX0LRwk8}\n}"}, "paperhash": {"value": "chen|adversarial_counterfactual_environment_model_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2886/-/Revision", "NeurIPS.cc/2023/Conference/Submission2886/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2886/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325808260, "odate": 1698949709813, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "c2eedxSlPJ", "number": 2830, "cdate": 1683468547695, "tcdate": 1683468547695, "mdate": 1698949709441, "tmdate": 1698949709441, "signatures": ["NeurIPS.cc/2023/Conference/Submission2830/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2830/Authors"], "forum": "c2eedxSlPJ", "content": {"title": {"value": "Tight Risk Bounds for Gradient Descent on Separable Data"}, "authors": {"value": ["Matan Schliserman", "Tomer Koren"]}, "authorids": {"value": ["~Matan_Schliserman1", "~Tomer_Koren1"]}, "keywords": {"value": ["Convex optimization", "Gradient Descent", "separable data", "generalization bounds", "Stochastic Gradient Descent."]}, "TLDR": {"value": "We establish tight upper and lower (population) risk bounds for Gradient Descent in the setting of linear classification with separable data."}, "abstract": {"value": "We study the generalization properties of unregularized gradient methods applied to separable linear classification---a setting that has received considerable attention since the pioneering work of Soudry et al. (2018).\nWe establish tight upper and lower (population) risk bounds for gradient descent in this setting, for any smooth loss function, expressed in terms of its tail decay rate.\nOur bounds take the form $\\Theta(r_{\\ell,T}^2 / \\gamma^2 T + r_{\\ell,T}^2 / \\gamma^2 n)$, \nwhere $T$ is the number of gradient steps, $n$ is size of the training set, $\\gamma$ is the data margin, and $r_{\\ell,T}$ is a complexity term that depends on the tail decay rate of the loss function (and on $T$).\nOur upper bound greatly improves the existing risk bounds due to Shamir (2021) and Schliserman and Koren (2022), that either applied to specific loss functions or imposed extraneous technical assumptions, and applies to virtually any convex and smooth loss function.\nOur risk lower bound is the first in this context and establish the tightness of our general upper bound for any given tail decay rate and in all parameter regimes.\nThe proof technique used to show these results is also markedly simpler compared to previous work, and is straightforward to extend to other gradient methods; we illustrate this by providing analogous results for Stochastic Gradient Descent."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/faa87a767992a1cae53e7d06a358287e41b7eee9.pdf"}, "supplementary_material": {"value": "/attachment/ebd57170322b95c4999e871d40792b0a545b4d02.pdf"}, "_bibtex": {"value": "@inproceedings{\nschliserman2023tight,\ntitle={Tight Risk Bounds for Gradient Descent on Separable Data},\nauthor={Matan Schliserman and Tomer Koren},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=c2eedxSlPJ}\n}"}, "paperhash": {"value": "schliserman|tight_risk_bounds_for_gradient_descent_on_separable_data"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2830/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2830/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325806720, "odate": 1698949709428, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "exg62lfHrB", "number": 2818, "cdate": 1683466334362, "tcdate": 1683466334362, "mdate": 1698949709290, "tmdate": 1698949709290, "signatures": ["NeurIPS.cc/2023/Conference/Submission2818/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2818/Authors"], "forum": "exg62lfHrB", "content": {"title": {"value": "Model Spider: Learning to Rank Pre-Trained Models Efficiently"}, "authors": {"value": ["Yi-Kai Zhang", "Ting-Ji Huang", "Yao-Xiang Ding", "De-Chuan Zhan", "Han-Jia Ye"]}, "authorids": {"value": ["~Yi-Kai_Zhang2", "~Ting-Ji_Huang1", "~Yao-Xiang_Ding2", "~De-Chuan_Zhan1", "~Han-Jia_Ye1"]}, "keywords": {"value": ["Pre-trained Model Ranking", "Transfer Learning"]}, "TLDR": {"value": "We propose Model Spider to learn how to tokenize both Pre-Trained Models (PTMs) and tasks. Model Spider accelerates selecting a helpful PTM from visual models or Large Language Models (LLMs) for a downstream task and improves the PTM ranking quality."}, "abstract": {"value": "Figuring out which Pre-Trained Model (PTM) from a model zoo fits the target task is essential to take advantage of plentiful model resources. With the availability of numerous heterogeneous PTMs from diverse fields, efficiently selecting the most suitable one is challenging due to the time-consuming costs of carrying out forward or backward passes over all PTMs. In this paper, we propose Model Spider, which tokenizes both PTMs and tasks by summarizing their characteristics into vectors to enable efficient PTM selection. By leveraging the approximated performance of PTMs on a separate set of training tasks, Model Spider learns to construct representation and measure the fitness score between a model-task pair via their representation. The ability to rank relevant PTMs higher than others generalizes to new tasks. With the top-ranked PTM candidates, we further learn to enrich task repr. with their PTM-specific semantics to re-rank the PTMs for better selection. Model Spider balances efficiency and selection ability, making PTM selection like a spider preying on a web. Model Spider exhibits promising performance across diverse model zoos, including visual models and Large Language Models (LLMs). Code is available at https://github.com/zhangyikaii/Model-Spider."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/507a5192fabf9d17b6240561b5a88d2cf81c9e54.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023model,\ntitle={Model Spider: Learning to Rank Pre-Trained Models Efficiently},\nauthor={Yi-Kai Zhang and Ting-Ji Huang and Yao-Xiang Ding and De-Chuan Zhan and Han-Jia Ye},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=exg62lfHrB}\n}"}, "paperhash": {"value": "zhang|model_spider_learning_to_rank_pretrained_models_efficiently"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2818/-/Revision", "NeurIPS.cc/2023/Conference/Submission2818/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2818/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325806277, "odate": 1698949709278, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "tIzbNQko3c", "number": 2807, "cdate": 1683465158002, "tcdate": 1683465158002, "mdate": 1698949709196, "tmdate": 1698949709196, "signatures": ["NeurIPS.cc/2023/Conference/Submission2807/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2807/Authors"], "forum": "tIzbNQko3c", "content": {"title": {"value": "Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction"}, "authors": {"value": ["Zuobai Zhang", "Minghao Xu", "Aurelie Lozano", "Vijil Chenthamarakshan", "Payel Das", "Jian Tang"]}, "authorids": {"value": ["~Zuobai_Zhang1", "~Minghao_Xu1", "~Aurelie_Lozano1", "~Vijil_Chenthamarakshan1", "~Payel_Das1", "~Jian_Tang1"]}, "keywords": {"value": ["Protein representation learning", "diffusion models", "self-supervised learning"]}, "abstract": {"value": "Self-supervised pre-training methods on proteins have recently gained attention, with most approaches focusing on either protein sequences or structures, neglecting the exploration of their joint distribution, which is crucial for a comprehensive understanding of protein functions by integrating co-evolutionary information and structural characteristics. In this work, inspired by the success of denoising diffusion models in generative tasks, we propose the DiffPreT approach to pre-train a protein encoder by sequence-structure joint diffusion modeling. DiffPreT guides the encoder to recover the native protein sequences and structures from the perturbed ones along the joint diffusion trajectory, which acquires the joint distribution of sequences and structures. Considering the essential protein conformational variations, we enhance DiffPreT by a method called Siamese Diffusion Trajectory Prediction (SiamDiff) to capture the correlation between different conformers of a protein. SiamDiff attains this goal by maximizing the mutual information between representations of diffusion trajectories of structurally-correlated conformers. We study the effectiveness of DiffPreT and SiamDiff on both atom- and residue-level structure-based protein understanding tasks. Experimental results show that the performance of DiffPreT is consistently competitive on all tasks, and SiamDiff achieves new state-of-the-art performance, considering the mean ranks on all tasks. Code will be released upon acceptance."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/54557a567d51315f120e9d9917a0d6e3323d44f3.pdf"}, "supplementary_material": {"value": "/attachment/0ca2e77c53ad804a7361b384e32c7fc5f222cfe4.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023pretraining,\ntitle={Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction},\nauthor={Zuobai Zhang and Minghao Xu and Aurelie Lozano and Vijil Chenthamarakshan and Payel Das and Jian Tang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=tIzbNQko3c}\n}"}, "paperhash": {"value": "zhang|pretraining_protein_encoder_via_siamese_sequencestructure_diffusion_trajectory_prediction"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2807/-/Revision", "NeurIPS.cc/2023/Conference/Submission2807/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2807/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325805875, "odate": 1698949709181, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "t6nA7x3GAC", "number": 2744, "cdate": 1683449894289, "tcdate": 1683449894289, "mdate": 1698949708944, "tmdate": 1698949708944, "signatures": ["NeurIPS.cc/2023/Conference/Submission2744/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2744/Authors"], "forum": "t6nA7x3GAC", "content": {"title": {"value": "Trans-Dimensional Generative Modeling via Jump Diffusion Models"}, "authors": {"value": ["Andrew Campbell", "William Harvey", "Christian Dietrich Weilbach", "Valentin De Bortoli", "Tom Rainforth", "Arnaud Doucet"]}, "authorids": {"value": ["~Andrew_Campbell4", "~William_Harvey1", "~Christian_Dietrich_Weilbach1", "~Valentin_De_Bortoli1", "~Tom_Rainforth1", "~Arnaud_Doucet2"]}, "keywords": {"value": ["diffusion", "score-based", "score", "markov chain", "jump diffusion", "poisson"]}, "abstract": {"value": "We propose a new class of generative model that naturally handles data of varying dimensionality by jointly modeling the state and dimension of each datapoint. The generative process is formulated as a jump diffusion process that makes jumps between different dimensional spaces. We first define a dimension destroying forward noising process, before deriving the dimension creating time-reversed generative process along with a novel evidence lower bound training objective for learning to approximate it.\nSimulating our learned approximation to the time-reversed generative process then provides an effective way of sampling data of varying dimensionality by jointly generating state values and dimensions. \nWe demonstrate our approach on molecular and video datasets of varying dimensionality, reporting better compatibility with test-time diffusion guidance imputation tasks and improved interpolation capabilities versus fixed dimensional models that generate state values and dimensions separately."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/dc6202bef125541e2c2e564538ab6bd859d8a13c.pdf"}, "TLDR": {"value": "We propose a new model class for generating varying dimensional data using jump diffusions."}, "supplementary_material": {"value": "/attachment/dbeb3903ffb630f740ad784eb13a5648ee5c4e5f.pdf"}, "_bibtex": {"value": "@inproceedings{\ncampbell2023transdimensional,\ntitle={Trans-Dimensional Generative Modeling via Jump Diffusion Models},\nauthor={Andrew Campbell and William Harvey and Christian Dietrich Weilbach and Valentin De Bortoli and Tom Rainforth and Arnaud Doucet},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=t6nA7x3GAC}\n}"}, "paperhash": {"value": "campbell|transdimensional_generative_modeling_via_jump_diffusion_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2744/-/Revision", "NeurIPS.cc/2023/Conference/Submission2744/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2744/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325804092, "odate": 1698949708932, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "oyV9FslE3j", "number": 2667, "cdate": 1683434475770, "tcdate": 1683434475770, "mdate": 1698949708358, "tmdate": 1698949708358, "signatures": ["NeurIPS.cc/2023/Conference/Submission2667/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2667/Authors"], "forum": "oyV9FslE3j", "content": {"title": {"value": "Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training"}, "authors": {"value": ["Yefan Zhou", "Tianyu Pang", "Keqin Liu", "charles h martin", "Michael W. Mahoney", "Yaoqing Yang"]}, "authorids": {"value": ["~Yefan_Zhou1", "~Tianyu_Pang2", "~Keqin_Liu1", "~charles_h_martin1", "~Michael_W._Mahoney1", "~Yaoqing_Yang1"]}, "keywords": {"value": ["Heavy-tail self-regularization", "learning rate schedule"]}, "abstract": {"value": "Regularization in modern machine learning is crucial, and it can take various forms in algorithmic design: training set, model family, error function, regularization terms, and optimizations. \nIn particular, the learning rate, which can be interpreted as a temperature-like parameter within the statistical mechanics of learning, plays a crucial role in neural network training. \nIndeed, many widely adopted training strategies basically just define the decay of the learning rate over time. \nThis process can be interpreted as decreasing a temperature, using either a global learning rate (for the entire model) or a learning rate that varies for each parameter. \nThis paper proposes TempBalance, a straightforward yet effective layer-wise learning rate method. TempBalance is based on Heavy-Tailed Self-Regularization (HT-SR) Theory, an approach which characterizes the implicit self-regularization of different layers in trained models. \nWe demonstrate the efficacy of using HT-SR-motivated metrics to guide the scheduling and balancing of temperature across all network layers during model training, resulting in improved performance during testing. \nWe implement TempBalance on CIFAR10, CIFAR100, SVHN, and TinyImageNet datasets using ResNets, VGGs and WideResNets with various depths and widths. \nOur results show that TempBalance significantly outperforms ordinary SGD and carefully-tuned spectral norm regularization. \nWe also show that TempBalance outperforms a number of state-of-the-art optimizers and learning rate schedulers."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c4ebe366b19b2ca0392cc14298913a8f34ea42ff.pdf"}, "supplementary_material": {"value": "/attachment/ff00e503c95378736143ea1656bbaaf77cc0d133.zip"}, "_bibtex": {"value": "@inproceedings{\nzhou2023temperature,\ntitle={Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training},\nauthor={Yefan Zhou and Tianyu Pang and Keqin Liu and charles h martin and Michael W. Mahoney and Yaoqing Yang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=oyV9FslE3j}\n}"}, "paperhash": {"value": "zhou|temperature_balancing_layerwise_weight_analysis_and_neural_network_training"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2667/-/Revision", "NeurIPS.cc/2023/Conference/Submission2667/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2667/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325801745, "odate": 1698949708344, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "lXuByUeHhd", "number": 2581, "cdate": 1683403563092, "tcdate": 1683403563092, "mdate": 1698949707685, "tmdate": 1698949707685, "signatures": ["NeurIPS.cc/2023/Conference/Submission2581/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2581/Authors"], "forum": "lXuByUeHhd", "content": {"title": {"value": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"}, "authors": {"value": ["Sang Michael Xie", "Hieu Pham", "Xuanyi Dong", "Nan Du", "Hanxiao Liu", "Yifeng Lu", "Percy Liang", "Quoc V Le", "Tengyu Ma", "Adams Wei Yu"]}, "authorids": {"value": ["~Sang_Michael_Xie1", "~Hieu_Pham1", "~Xuanyi_Dong1", "~Nan_Du1", "~Hanxiao_Liu1", "~Yifeng_Lu1", "~Percy_Liang1", "~Quoc_V_Le1", "~Tengyu_Ma1", "~Adams_Wei_Yu1"]}, "keywords": {"value": ["language models", "pretraining", "domain reweighting", "data curation"]}, "TLDR": {"value": "We present an algorithm that reweights how much of each domain/data source is in a language modeling dataset (e.g. The Pile), resulting in faster LM training and improvements in perplexity and downstream accuracy."}, "abstract": {"value": "The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/800a691c3d5810920bada64305674a9b1029e840.pdf"}, "_bibtex": {"value": "@inproceedings{\nxie2023doremi,\ntitle={DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining},\nauthor={Sang Michael Xie and Hieu Pham and Xuanyi Dong and Nan Du and Hanxiao Liu and Yifeng Lu and Percy Liang and Quoc V Le and Tengyu Ma and Adams Wei Yu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=lXuByUeHhd}\n}"}, "paperhash": {"value": "xie|doremi_optimizing_data_mixtures_speeds_up_language_model_pretraining"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2581/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2581/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325798769, "odate": 1698949707674, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "x2xQEszznV", "number": 2568, "cdate": 1683397513303, "tcdate": 1683397513303, "mdate": 1698949707481, "tmdate": 1698949707481, "signatures": ["NeurIPS.cc/2023/Conference/Submission2568/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2568/Authors"], "forum": "x2xQEszznV", "content": {"title": {"value": "Online Constrained Meta-Learning: Provable Guarantees for Generalization"}, "authors": {"value": ["Siyuan Xu", "Minghui Zhu"]}, "authorids": {"value": ["~Siyuan_Xu4", "~Minghui_Zhu1"]}, "keywords": {"value": ["meta-learning; generalization"]}, "abstract": {"value": "Meta-learning has attracted attention due to its strong ability to learn experiences from known tasks, which can speed up and enhance the learning process for new tasks. However, most existing meta-learning approaches only can learn from tasks without any constraint. This paper proposes an online constrained meta-learning framework, which continuously learns meta-knowledge from sequential learning tasks, and the learning tasks are subject to hard constraints. Beyond existing meta-learning analyses, we provide the upper bounds of optimality gaps and constraint violations produced by the proposed framework, which considers the dynamic regret of online learning, as well as the generalization ability of the task-specific models. Moreover, we provide a practical algorithm for the framework, and validate its superior effectiveness through experiments conducted on meta-imitation learning and few-shot image classification."}, "pdf": {"value": "/pdf/7c3c77082809745760e82a55985e875464479682.pdf"}, "supplementary_material": {"value": "/attachment/2a007094a051af2766872b5f413efecdd20c1501.zip"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nxu2023online,\ntitle={Online Constrained Meta-Learning: Provable Guarantees for Generalization},\nauthor={Siyuan Xu and Minghui Zhu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=x2xQEszznV}\n}"}, "paperhash": {"value": "xu|online_constrained_metalearning_provable_guarantees_for_generalization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2568/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2568/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325798272, "odate": 1698949707469, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "fxNQJVMwK2", "number": 2540, "cdate": 1683389997733, "tcdate": 1683389997733, "mdate": 1698949707298, "tmdate": 1698949707298, "signatures": ["NeurIPS.cc/2023/Conference/Submission2540/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2540/Authors"], "forum": "fxNQJVMwK2", "content": {"title": {"value": "Text-to-Image Diffusion Models are Zero Shot Classifiers"}, "authors": {"value": ["Kevin Clark", "Priyank Jaini"]}, "authorids": {"value": ["~Kevin_Clark1", "~Priyank_Jaini1"]}, "keywords": {"value": ["diffusion models", "zero-shot", "text-to-image", "generative models", "foundation models", "stable diffusion"]}, "abstract": {"value": "The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data.\nHowever, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks.\nWe investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers.\nThe key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood.\nWe apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. \nThey perform competitively with CLIP on a wide range of zero-shot image classification datasets. \nAdditionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot.\nAlthough generative pre-training is prevalent in NLP, visual foundation models often use other methods such as contrastive learning. \nBased on our findings, we argue that generative pre-training should be explored as a compelling alternative for vision and vision-language problems."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3a730820a561e57bcd208b1b994e19f718c08ec0.pdf"}, "supplementary_material": {"value": "/attachment/881b155796790b4df20236a292373c11916888e8.zip"}, "_bibtex": {"value": "@inproceedings{\nclark2023texttoimage,\ntitle={Text-to-Image Diffusion Models are Zero Shot Classifiers},\nauthor={Kevin Clark and Priyank Jaini},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=fxNQJVMwK2}\n}"}, "paperhash": {"value": "clark|texttoimage_diffusion_models_are_zero_shot_classifiers"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2540/-/Revision", "NeurIPS.cc/2023/Conference/Submission2540/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2540/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325797599, "odate": 1698949707282, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "xRfTcZdQxq", "number": 2510, "cdate": 1683380415723, "tcdate": 1683380415723, "mdate": 1698949707195, "tmdate": 1698949707195, "signatures": ["NeurIPS.cc/2023/Conference/Submission2510/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2510/Authors"], "forum": "xRfTcZdQxq", "content": {"title": {"value": "Robust Model Reasoning and Fitting via Dual Sparsity Pursuit"}, "authors": {"value": ["Xingyu Jiang", "Jiayi Ma"]}, "authorids": {"value": ["~Xingyu_Jiang1", "~Jiayi_Ma2"]}, "keywords": {"value": ["Model reasoning; Model fitting; Outliers; Sparse subspace learning; Feature matching"]}, "abstract": {"value": "In this paper, we contribute to solving a threefold problem: outlier rejection, true model reasoning and parameter estimation with a unified optimization modeling. To this end, we first pose this task as a sparse subspace recovering problem, to search a maximum of independent bases under an over-embedded data space. Then we convert the objective into a continuous optimization paradigm that estimates sparse solutions for both bases and errors. Wherein a fast and robust solver is proposed to accurately estimate the sparse subspace parameters and error entries, which is implemented by a proximal approximation method under the alternating optimization framework with the ``optimal'' sub-gradient descent. Extensive experiments regarding known and unknown model fitting on synthetic and challenging real datasets have demonstrated the superiority of our method against the state-of-the-art. We also apply our method to multi-class multi-model fitting and loop closure detection, and achieve promising results both in accuracy and efficiency. Code is released at: https://github.com/StaRainJ/DSP."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Dual Sparsity Pursuit"}, "pdf": {"value": "/pdf/639371f41f9b3fc2794cb478bb85135e29e503a6.pdf"}, "supplementary_material": {"value": "/attachment/8c7a7e9e2df706422d543a072327bbfaa45c7cc6.zip"}, "_bibtex": {"value": "@inproceedings{\njiang2023robust,\ntitle={Robust Model Reasoning and Fitting via Dual Sparsity Pursuit},\nauthor={Xingyu Jiang and Jiayi Ma},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=xRfTcZdQxq}\n}"}, "paperhash": {"value": "jiang|robust_model_reasoning_and_fitting_via_dual_sparsity_pursuit"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2510/-/Revision", "NeurIPS.cc/2023/Conference/Submission2510/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2510/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325796913, "odate": 1698949707182, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "lSLYXuLqRQ", "number": 2470, "cdate": 1683365447118, "tcdate": 1683365447118, "mdate": 1698949706955, "tmdate": 1698949706955, "signatures": ["NeurIPS.cc/2023/Conference/Submission2470/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2470/Authors"], "forum": "lSLYXuLqRQ", "content": {"title": {"value": "Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction"}, "authors": {"value": ["Feng Wang", "Zilong Chen", "Guokang Wang", "Yafei Song", "Huaping Liu"]}, "authorids": {"value": ["~Feng_Wang12", "~Zilong_Chen1", "~Guokang_Wang2", "~Yafei_Song1", "~Huaping_Liu3"]}, "keywords": {"value": ["NeRF", "Dynamic Scenes"]}, "abstract": {"value": "In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novel method for efficiently reconstructing dynamic 3D scenes from multi-view or monocular videos. Based on the observation that dynamic scenes often contain substantial static areas that result in redundancy in storage and computations, MSTH represents a dynamic scene as a weighted combination of a 3D hash encoding and a 4D hash encoding. The weights for the two components are represented by a learnable mask which is guided by an uncertainty-based objective to reflect the spatial and temporal importance of each 3D position. With this design, our method can reduce the hash collision rate by avoiding redundant queries and modifications on static areas, making it feasible to represent a large number of space-time voxels by hash tables with small size.Besides, without the requirements to fit the large numbers of temporally redundant features independently, our method is easier to optimize and converge rapidly with only twenty minutes of training for a 300-frame dynamic scene. We evaluate our method on extensive dynamic scenes. As a result, MSTH obtains consistently better results than previous state-of-the-art methods with only 20 minutes of training time and 130 MB of memory storage."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/bdbf55017fe0607ffa903940f2afdeb17062f977.pdf"}, "supplementary_material": {"value": "/attachment/db7e4738174a93a631717addb9722530b5543e71.zip"}, "_bibtex": {"value": "@inproceedings{\nwang2023masked,\ntitle={Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction},\nauthor={Feng Wang and Zilong Chen and Guokang Wang and Yafei Song and Huaping Liu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=lSLYXuLqRQ}\n}"}, "TLDR": {"value": "4D video in a fast training and rendering speed."}, "paperhash": {"value": "wang|masked_spacetime_hash_encoding_for_efficient_dynamic_scene_reconstruction"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2470/-/Revision", "NeurIPS.cc/2023/Conference/Submission2470/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2470/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325795671, "odate": 1698949706939, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cto6jIIbMZ", "number": 2400, "cdate": 1683351824193, "tcdate": 1683351824193, "mdate": 1698949706437, "tmdate": 1698949706437, "signatures": ["NeurIPS.cc/2023/Conference/Submission2400/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2400/Authors"], "forum": "cto6jIIbMZ", "content": {"title": {"value": "Demystifying Softmax Gating Function in Gaussian Mixture of Experts"}, "authors": {"value": ["Huy Nguyen", "TrungTin Nguyen", "Nhat Ho"]}, "authorids": {"value": ["~Huy_Nguyen5", "~TrungTin_Nguyen1", "~Nhat_Ho1"]}, "keywords": {"value": ["Mixture of Experts", "Maximum Likelihood Estimation", "Voronoi Loss Function", "Algebraic Geometry."]}, "abstract": {"value": "Understanding the parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating function: (i) the identifiability only up to the translation of parameters; (ii) the intrinsic interaction via partial differential equations between the softmax gating and the expert functions in the Gaussian density; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Voronoi loss functions among parameters and establishing the convergence rates of maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the true number of experts is unknown and over-specified, our findings show a connection between the convergence rate of the MLE and a solvability problem of a system of polynomial equations."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5807c342498b6f046eca3c6d8ec7b49b386c969c.pdf"}, "_bibtex": {"value": "@inproceedings{\nnguyen2023demystifying,\ntitle={Demystifying Softmax Gating Function in Gaussian Mixture of Experts},\nauthor={Huy Nguyen and TrungTin Nguyen and Nhat Ho},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cto6jIIbMZ}\n}"}, "paperhash": {"value": "nguyen|demystifying_softmax_gating_function_in_gaussian_mixture_of_experts"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2400/-/Revision", "NeurIPS.cc/2023/Conference/Submission2400/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2400/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325793704, "odate": 1698949706423, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "gMS6FVZvmF", "number": 2374, "cdate": 1683343401147, "tcdate": 1683343401147, "mdate": 1698949706330, "tmdate": 1698949706330, "signatures": ["NeurIPS.cc/2023/Conference/Submission2374/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2374/Authors"], "forum": "gMS6FVZvmF", "content": {"title": {"value": "One Fits All: Power General Time Series Analysis by Pretrained LM"}, "authors": {"value": ["Tian Zhou", "Peisong Niu", "Xue Wang", "Liang Sun", "Rong Jin"]}, "authorids": {"value": ["~Tian_Zhou2", "~Peisong_Niu1", "~Xue_Wang9", "~Liang_Sun2", "~Rong_Jin1"]}, "keywords": {"value": ["general time series analysis", "time series forecasting", "cross modality knowledge transfer; pretrained language model;"]}, "TLDR": {"value": "One Fits All: Power General Time Series Analysis by Pretrained LM, a work that synthesis practical performance and theoretical analysis"}, "abstract": {"value": "Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time series. Our results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure1. We also found both theoretically and empirically that the self-attention module behaviors similarly to principle component analysis (PCA), an observation that helps explains how transformer bridges the domain gap and a crucial step towards understanding the universality of a pre-trained transformer. \nThe code is publicly available at https://anonymous.4open.science/r/Pretrained-LM-for-TSForcasting-C561."}, "pdf": {"value": "/pdf/1d90f713505f6ea9aebcd5dc09115cab3964fcbc.pdf"}, "supplementary_material": {"value": "/attachment/383a3761223cb1f1dcd6d3b510c4b29b06184b7f.pdf"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nzhou2023one,\ntitle={One Fits All: Power General Time Series Analysis by Pretrained {LM}},\nauthor={Tian Zhou and Peisong Niu and Xue Wang and Liang Sun and Rong Jin},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=gMS6FVZvmF}\n}"}, "paperhash": {"value": "zhou|one_fits_all_power_general_time_series_analysis_by_pretrained_lm"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2374/-/Revision", "NeurIPS.cc/2023/Conference/Submission2374/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2374/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325792809, "odate": 1698949706316, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "A57UMlUJdc", "number": 2361, "cdate": 1683342568630, "tcdate": 1683342568630, "mdate": 1698949706266, "tmdate": 1698949706266, "signatures": ["NeurIPS.cc/2023/Conference/Submission2361/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2361/Authors"], "forum": "A57UMlUJdc", "content": {"title": {"value": "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration"}, "authors": {"value": ["Zhihan Liu", "Miao Lu", "Wei Xiong", "Han Zhong", "Hao Hu", "Shenao Zhang", "Sirui Zheng", "Zhuoran Yang", "Zhaoran Wang"]}, "authorids": {"value": ["~Zhihan_Liu1", "~Miao_Lu3", "~Wei_Xiong9", "~Han_Zhong1", "~Hao_Hu3", "~Shenao_Zhang1", "~Sirui_Zheng2", "~Zhuoran_Yang1", "~Zhaoran_Wang1"]}, "keywords": {"value": ["reinforcement learning; online learning; game"]}, "abstract": {"value": "In reinforcement learning (RL), balancing exploration and exploitation is crucial for achieving an optimal policy in a sample-efficient way. To this end, existing sample- efficient algorithms typically consist of three components: estimation, planning, and exploration. However, to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as data-dependent level-set constraints or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called Maximize to Explore (MEX), which only needs to optimize unconstrainedly a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that the MEX achieves a sublinear regret with general function approximators and is extendable to the zero-sum Markov game setting. Meanwhile, we adapt deep RL baselines to design practical versions of MEX in both the model-based and model-free settings, which outperform baselines in various MuJoCo environments with sparse reward by a stable margin. Compared with existing sample-efficient algorithms with general function approximators, MEX achieves similar sample efficiency while also enjoying a lower computational cost and is more compatible with modern deep RL methods."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We design a both provable sample-efficient and easy-to-implement RL framework for exploration."}, "pdf": {"value": "/pdf/7d4e8b0576c0fd00967ff26406c06c77c6a39881.pdf"}, "supplementary_material": {"value": "/attachment/0dc60c4753b90ef229a1ffc1d08bdce143a884cf.zip"}, "_bibtex": {"value": "@inproceedings{\nliu2023maximize,\ntitle={Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration},\nauthor={Zhihan Liu and Miao Lu and Wei Xiong and Han Zhong and Hao Hu and Shenao Zhang and Sirui Zheng and Zhuoran Yang and Zhaoran Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=A57UMlUJdc}\n}"}, "paperhash": {"value": "liu|maximize_to_explore_one_objective_function_fusing_estimation_planning_and_exploration"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2361/-/Revision", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission2361/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2361/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325792686, "odate": 1698949706182, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "9XieH21Tlf", "number": 2307, "cdate": 1683333216067, "tcdate": 1683333216067, "mdate": 1698949705912, "tmdate": 1698949705912, "signatures": ["NeurIPS.cc/2023/Conference/Submission2307/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2307/Authors"], "forum": "9XieH21Tlf", "content": {"title": {"value": "Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality"}, "authors": {"value": ["Liyuan Wang", "Jingyi Xie", "Xingxing Zhang", "Mingyi Huang", "Hang Su", "Jun Zhu"]}, "authorids": {"value": ["~Liyuan_Wang1", "~Jingyi_Xie3", "~Xingxing_Zhang3", "~Mingyi_Huang1", "~Hang_Su3", "~Jun_Zhu2"]}, "keywords": {"value": ["Continual Learning", "Catastrophic Forgetting", "Pre-training", "Prompt Tuning"]}, "abstract": {"value": "Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively)."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We extensively analyze state-of-the-art prompt-based continual learning from both empirical and theoretical perspectives, and propose an innovative approach to explicitly optimize its objective."}, "pdf": {"value": "/pdf/538fc64ef00cad0634f6813c92e77cc1bacb5458.pdf"}, "supplementary_material": {"value": "/attachment/80d04cf7f2746d89585cc430e98ba9a4c1152817.zip"}, "_bibtex": {"value": "@inproceedings{\nwang2023hierarchical,\ntitle={Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality},\nauthor={Liyuan Wang and Jingyi Xie and Xingxing Zhang and Mingyi Huang and Hang Su and Jun Zhu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=9XieH21Tlf}\n}"}, "paperhash": {"value": "wang|hierarchical_decomposition_of_promptbased_continual_learning_rethinking_obscured_suboptimality"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2307/-/Revision", "NeurIPS.cc/2023/Conference/Submission2307/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2307/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325791166, "odate": 1698949705898, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "QatZNssk7T", "number": 2268, "cdate": 1683320313010, "tcdate": 1683320313010, "mdate": 1698949705672, "tmdate": 1698949705672, "signatures": ["NeurIPS.cc/2023/Conference/Submission2268/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2268/Authors"], "forum": "QatZNssk7T", "content": {"title": {"value": "Adaptive Data Analysis in a Balanced Adversarial Model"}, "authors": {"value": ["Kobbi Nissim", "Uri Stemmer", "Eliad Tsfadia"]}, "authorids": {"value": ["~Kobbi_Nissim2", "~Uri_Stemmer1", "~Eliad_Tsfadia1"]}, "keywords": {"value": ["Adaptive Data Analysis", "Differential Privacy", "Statistical Queries"]}, "abstract": {"value": "In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an unknown distribution $\\cal{D}$, and\nis required to provide accurate estimations to a sequence of adaptively chosen statistical queries with respect to $\\cal{D}$.\nHardt and Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in general, it is computationally hard to answer more than $\\Theta(n^2)$ adaptive queries, assuming the existence of one-way functions. \n\nHowever, these negative results strongly rely on an adversarial model that significantly advantages the adversarial analyst over the mechanism, as the analyst, who chooses the adaptive queries, also chooses the underlying distribution $\\cal{D}$. \nThis imbalance raises questions with respect to the applicability of the obtained hardness results -- an analyst who has complete knowledge of the underlying distribution $\\cal{D}$ would have little need, if at all, to issue statistical queries to a mechanism which only holds a finite number of samples from $\\cal{D}$.\n\nWe consider more restricted adversaries, called \\emph{balanced}, where each such adversary consists of two separated algorithms: The \\emph{sampler} who is the entity that chooses the distribution and provides the samples to the mechanism, and the \\emph{analyst} who chooses the adaptive queries, but has no prior knowledge of the underlying distribution (and hence has no a priori advantage with respect to the mechanism). \nWe improve the quality of previous lower bounds by revisiting them using an efficient \\emph{balanced} adversary, under standard public-key cryptography assumptions. We show that these stronger hardness assumptions are unavoidable in the sense that any computationally bounded \\emph{balanced} adversary that has the structure of all known attacks, implies the existence of public-key cryptography."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/b4d6df9b1b9097fce91f97ce0bdf1725bef92c0f.pdf"}, "supplementary_material": {"value": "/attachment/493e9ab222eae3538f56d70be0864d89b0adc6e3.pdf"}, "_bibtex": {"value": "@inproceedings{\nnissim2023adaptive,\ntitle={Adaptive Data Analysis in a Balanced Adversarial Model},\nauthor={Kobbi Nissim and Uri Stemmer and Eliad Tsfadia},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=QatZNssk7T}\n}"}, "paperhash": {"value": "nissim|adaptive_data_analysis_in_a_balanced_adversarial_model"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2268/-/Revision", "NeurIPS.cc/2023/Conference/Submission2268/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2268/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325790008, "odate": 1698949705660, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "5Fgdk3hZpb", "number": 2254, "cdate": 1683313420251, "tcdate": 1683313420251, "mdate": 1698949705573, "tmdate": 1698949705573, "signatures": ["NeurIPS.cc/2023/Conference/Submission2254/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2254/Authors"], "forum": "5Fgdk3hZpb", "content": {"title": {"value": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective"}, "authors": {"value": ["Zeyuan Yin", "Eric Xing", "Zhiqiang Shen"]}, "authorids": {"value": ["~Zeyuan_Yin1", "~Eric_Xing1", "~Zhiqiang_Shen1"]}, "keywords": {"value": ["dataset condensation and distillation", "ImageNet Scale"]}, "abstract": {"value": "We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for efficient dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5\\% and 60.8\\% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5\\% and 32.9\\%, respectively. Our approach also surpasses MTT in terms of speed by approximately 52$\\times$ (ConvNet-4) and 16$\\times$ (ResNet-18) faster with less memory consumption of 11.6$\\times$ and 6.4$\\times$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://github.com/VILA-Lab/SRe2L."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/2874e5cf93350c5eaa8e6ed977391968756390d1.pdf"}, "supplementary_material": {"value": "/attachment/2dff5b9f7344118296971d608f637e43e828da91.zip"}, "_bibtex": {"value": "@inproceedings{\nyin2023squeeze,\ntitle={Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective},\nauthor={Zeyuan Yin and Eric Xing and Zhiqiang Shen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=5Fgdk3hZpb}\n}"}, "paperhash": {"value": "yin|squeeze_recover_and_relabel_dataset_condensation_at_imagenet_scale_from_a_new_perspective"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2254/-/Revision", "NeurIPS.cc/2023/Conference/Submission2254/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2254/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325789683, "odate": 1698949705560, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "DqfdhM64LI", "number": 2243, "cdate": 1683309716439, "tcdate": 1683309716439, "mdate": 1698949705478, "tmdate": 1698949705478, "signatures": ["NeurIPS.cc/2023/Conference/Submission2243/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2243/Authors"], "forum": "DqfdhM64LI", "content": {"title": {"value": "Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards"}, "authors": {"value": ["Mengfan Xu", "Diego Klabjan"]}, "authorids": {"value": ["~Mengfan_Xu1", "~Diego_Klabjan1"]}, "keywords": {"value": ["decentralized multi-agent MAB", "heterogeneous light-tailed and heavy-tailed rewards", "time dependent random graphs"]}, "abstract": {"value": "We study a decentralized multi-agent multi-armed bandit problem in which multiple clients are connected by time dependent random graphs provided by an environment. The reward distributions of each arm vary across clients and rewards are generated independently over time by an environment based on distributions that include both sub-exponential and sub-gaussian distributions. Each client pulls an arm and communicates with neighbors based on the graph provided by the environment. The goal is to minimize the overall regret of the entire system through collaborations. To this end, we introduce a novel algorithmic framework, which first provides robust simulation methods for generating random graphs using rapidly mixing markov chains or the random graph model, and then combines an averaging-based consensus approach with a newly proposed weighting technique and the upper confidence bound to deliver a UCB-type solution. Our algorithms account for the randomness in the graphs, removing the conventional doubly stochasticity assumption, and only require the knowledge of the number of clients at initialization. We derive optimal instance-dependent regret upper bounds of order $\\log{T}$ in both sub-gaussian and sub-exponential environments, and a nearly optimal instance-free regret upper bound of order $\\sqrt{T}\\log T$ up to a $\\log T$ factor. Importantly, our regret bounds hold with high probability and capture graph randomness, whereas prior works consider expected regret under assumptions and require more stringent reward distributions."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/81dc481ab4317c7703c04f8416998c5c21617153.pdf"}, "supplementary_material": {"value": "/attachment/a11e2b0853b3d2ad3f268014dbd803fa0bbea802.pdf"}, "_bibtex": {"value": "@inproceedings{\nxu2023decentralized,\ntitle={Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards},\nauthor={Mengfan Xu and Diego Klabjan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=DqfdhM64LI}\n}"}, "paperhash": {"value": "xu|decentralized_randomly_distributed_multiagent_multiarmed_bandit_with_heterogeneous_rewards"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2243/-/Revision", "NeurIPS.cc/2023/Conference/Submission2243/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2243/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325789315, "odate": 1698949705452, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "XRy4YQYLe0", "number": 2195, "cdate": 1683296387483, "tcdate": 1683296387483, "mdate": 1698949705091, "tmdate": 1698949705091, "signatures": ["NeurIPS.cc/2023/Conference/Submission2195/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2195/Authors"], "forum": "XRy4YQYLe0", "content": {"title": {"value": "Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions"}, "authors": {"value": ["Hao Wang", "Luxi He", "Rui Gao", "Flavio Calmon"]}, "authorids": {"value": ["~Hao_Wang22", "~Luxi_He1", "~Rui_Gao3", "~Flavio_Calmon1"]}, "keywords": {"value": ["information theory", "fair machine learning"]}, "abstract": {"value": "Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination on standard (overused) tabular datasets. However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination."}, "pdf": {"value": "/pdf/32d73712da460cd4d21ab1ce9a6cf4a1ac45d2b2.pdf"}, "supplementary_material": {"value": "/attachment/5516fc923d97bb91de1a66f263d39fc5c395b0ff.pdf"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nwang2023aleatoric,\ntitle={Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions},\nauthor={Hao Wang and Luxi He and Rui Gao and Flavio Calmon},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=XRy4YQYLe0}\n}"}, "TLDR": {"value": "https://github.com/Lumos23/aleatoric_epistemic_discrimination"}, "paperhash": {"value": "wang|aleatoric_and_epistemic_discrimination_fundamental_limits_of_fairness_interventions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2195/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2195/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325788045, "odate": 1698949705077, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "K8gLHZIgVW", "number": 2179, "cdate": 1683293749044, "tcdate": 1683293749044, "mdate": 1698949704983, "tmdate": 1698949704983, "signatures": ["NeurIPS.cc/2023/Conference/Submission2179/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2179/Authors"], "forum": "K8gLHZIgVW", "content": {"title": {"value": "Regularization properties of adversarially-trained linear regression"}, "authors": {"value": ["Antonio H. Ribeiro", "Dave Zachariah", "Francis Bach", "Thomas B. Sch\u00f6n"]}, "authorids": {"value": ["~Antonio_H._Ribeiro1", "~Dave_Zachariah1", "~Francis_Bach1", "~Thomas_B._Sch\u00f6n1"]}, "keywords": {"value": ["adversarial training; regularization; linear models"]}, "abstract": {"value": "State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\\ell_\\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We study the solution of adversarial training in linear regression, presenting connections with Lasso, square-root Lasso, ridge regression, and minimum-norm interpolators."}, "pdf": {"value": "/pdf/a4adaf3a8d7bf51da3c77b7da57ff5fb7ab4b9dc.pdf"}, "supplementary_material": {"value": "/attachment/4865be2bd7fbc66848b12bfdd1e71946955f7e0d.pdf"}, "_bibtex": {"value": "@inproceedings{\nribeiro2023regularization,\ntitle={Regularization properties of adversarially-trained linear regression},\nauthor={Antonio H. Ribeiro and Dave Zachariah and Francis Bach and Thomas B. Sch{\\\"o}n},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=K8gLHZIgVW}\n}"}, "paperhash": {"value": "ribeiro|regularization_properties_of_adversariallytrained_linear_regression"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2179/-/Revision", "NeurIPS.cc/2023/Conference/Submission2179/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2179/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325787574, "odate": 1698949704970, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "7GyYpomkEa", "number": 2116, "cdate": 1683278368723, "tcdate": 1683278368723, "mdate": 1699630135625, "tmdate": 1699630135625, "signatures": ["NeurIPS.cc/2023/Conference/Submission2116/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2116/Authors"], "forum": "7GyYpomkEa", "content": {"title": {"value": "AbDiffuser: full-atom generation of in-vitro functioning antibodies"}, "authors": {"value": ["Karolis Martinkus", "Jan Ludwiczak", "WEI-CHING LIANG", "Julien Lafrance-Vanasse", "Isidro Hotzel", "Arvind Rajpal", "Yan Wu", "Kyunghyun Cho", "Richard Bonneau", "Vladimir Gligorijevic", "Andreas Loukas"]}, "authorids": {"value": ["~Karolis_Martinkus1", "~Jan_Ludwiczak1", "~WEI-CHING_LIANG1", "~Julien_Lafrance-Vanasse1", "~Isidro_Hotzel1", "~Arvind_Rajpal1", "~Yan_Wu7", "~Kyunghyun_Cho1", "~Richard_Bonneau1", "~Vladimir_Gligorijevic2", "~Andreas_Loukas1"]}, "keywords": {"value": ["antibody generation", "diffusion", "equivariance"]}, "TLDR": {"value": "We propose a new and more efficient diffusion model specifically tailored for antibody generation. Generated samples are confirmed to bind in-Vitro."}, "abstract": {"value": "We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude, enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of the selected designs were tight binders."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/4dc4fd57be2aad52ec7e251c6f5bea7cae048f25.pdf"}, "_bibtex": {"value": "@inproceedings{\nmartinkus2023abdiffuser,\ntitle={AbDiffuser: full-atom generation of in-vitro functioning antibodies},\nauthor={Karolis Martinkus and Jan Ludwiczak and WEI-CHING LIANG and Julien Lafrance-Vanasse and Isidro Hotzel and Arvind Rajpal and Yan Wu and Kyunghyun Cho and Richard Bonneau and Vladimir Gligorijevic and Andreas Loukas},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=7GyYpomkEa}\n}"}, "paperhash": {"value": "martinkus|abdiffuser_fullatom_generation_of_invitro_functioning_antibodies"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission2116/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2116/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325785799, "odate": 1698949704509, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "LAGxc2ybuH", "number": 2112, "cdate": 1683278064925, "tcdate": 1683278064925, "mdate": 1698949704425, "tmdate": 1698949704425, "signatures": ["NeurIPS.cc/2023/Conference/Submission2112/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2112/Authors"], "forum": "LAGxc2ybuH", "content": {"title": {"value": "Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models"}, "authors": {"value": ["Siu Lun Chau", "Krikamol Muandet", "Dino Sejdinovic"]}, "authorids": {"value": ["~Siu_Lun_Chau1", "~Krikamol_Muandet1", "~Dino_Sejdinovic1"]}, "keywords": {"value": ["Gaussian Processes", "Shapley values", "Uncertainty Modelling"]}, "TLDR": {"value": "A new SHAP algorithm design for GPs that take into account the analytical covariance and result in explanations as Gaussian processes. A Shapley kernel is also proposed for predicting Shapley values of new observations."}, "abstract": {"value": "We present a novel approach for explaining Gaussian processes (GPs) that can utilize the full analytical covariance structure present in GPs. Our method is based on the popular solution concept of Shapley values extended to stochastic cooperative games, resulting in explanations that are random variables. The GP explanations generated using our approach satisfy similar favorable axioms to standard Shapley values and possess a tractable covariance function across features and data observations. This covariance allows for quantifying explanation uncertainties and studying the statistical dependencies between explanations. We further extend our framework to the problem of predictive explanation, and propose a Shapley prior over the explanation function to predict Shapley values for new data based on previously computed ones. Our extensive illustrations demonstrate the effectiveness of the proposed approach."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8c65716ba86246030b8c885b5692522f8fef2a49.pdf"}, "supplementary_material": {"value": "/attachment/e70cbce4d1920fdd780d8eb0828e40b90df837b5.zip"}, "_bibtex": {"value": "@inproceedings{\nchau2023explaining,\ntitle={Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models},\nauthor={Siu Lun Chau and Krikamol Muandet and Dino Sejdinovic},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=LAGxc2ybuH}\n}"}, "paperhash": {"value": "chau|explaining_the_uncertain_stochastic_shapley_values_for_gaussian_process_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2112/-/Revision", "NeurIPS.cc/2023/Conference/Submission2112/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2112/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325785653, "odate": 1698949704413, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "JpU5YmMKx7", "number": 2067, "cdate": 1683269083222, "tcdate": 1683269083222, "mdate": 1698949704144, "tmdate": 1698949704144, "signatures": ["NeurIPS.cc/2023/Conference/Submission2067/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission2067/Authors"], "forum": "JpU5YmMKx7", "content": {"title": {"value": "Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect"}, "authors": {"value": ["Xiaolei Ru", "Xin-Ya Zhang", "Zijia Liu", "Jack Murdoch Moore", "Gang Yan"]}, "authorids": {"value": ["~Xiaolei_Ru1", "~Xin-Ya_Zhang1", "~Zijia_Liu1", "~Jack_Murdoch_Moore1", "~Gang_Yan2"]}, "keywords": {"value": ["Directed coupled network reconstruction; Neuronal dynamics; Mutual information estimator; Attention mechanism; Transfer entropy."]}, "abstract": {"value": "We consider the problem of reconstructing coupled networks (e.g., biological neural networks) connecting large numbers of variables (e.g.,nerve cells), of which state evolution is governed by dissipative dynamics consisting of strong self-drive (dominants the evolution) and weak coupling-drive. The core difficulty is sparseness of coupling effect that emerges (the coupling force is significant) only momentarily and otherwise remains quiescent in time series (e.g., neuronal activity sequence). Here we learn the idea from attention mechanism to guide the classifier to make inference focusing on the critical regions of time series data where coupling effect may manifest. Specifically, attention coefficients are assigned autonomously by artificial neural networks trained to maximise the Attentive Transfer Entropy (ATEn), which is a novel generalization of the iconic transfer entropy metric. Our results show that, without any prior knowledge of dynamics, ATEn explicitly identifies areas where the strength of coupling-drive is distinctly greater than zero. This innovation substantially improves reconstruction performance for both synthetic and real directed coupling networks using data generated by neuronal models widely used in neuroscience."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Directed coupled network reconstruction based on time series generated by neuronal dynamics."}, "pdf": {"value": "/pdf/fbece38df1489d5fda94aaed7ccfde4530ac83d3.pdf"}, "supplementary_material": {"value": "/attachment/f5e8b45764858a46e8e8da367ec62b456790a338.zip"}, "_bibtex": {"value": "@inproceedings{\nru2023attentive,\ntitle={Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect},\nauthor={Xiaolei Ru and Xin-Ya Zhang and Zijia Liu and Jack Murdoch Moore and Gang Yan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=JpU5YmMKx7}\n}"}, "paperhash": {"value": "ru|attentive_transfer_entropy_to_exploit_transient_emergence_of_coupling_effect"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission2067/-/Revision", "NeurIPS.cc/2023/Conference/Submission2067/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission2067/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325784351, "odate": 1698949704132, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "NWEbeI2HNQ", "number": 1939, "cdate": 1683222271391, "tcdate": 1683222271391, "mdate": 1698949703456, "tmdate": 1698949703456, "signatures": ["NeurIPS.cc/2023/Conference/Submission1939/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1939/Authors"], "forum": "NWEbeI2HNQ", "content": {"title": {"value": "Prefix-Tree Decoding for Predicting Mass Spectra from Molecules"}, "authors": {"value": ["Samuel Goldman", "John Bradshaw", "Jiayi Xin", "Connor W. Coley"]}, "authorids": {"value": ["~Samuel_Goldman1", "~John_Bradshaw1", "~Jiayi_Xin1", "~Connor_W._Coley1"]}, "keywords": {"value": ["molecules", "prefix tree", "mass spectra", "mass spectrum prediction", "metabolomics", "GNNs", "chemistry", "biology"]}, "TLDR": {"value": "Predicting mass spectra from molecules by first predicting molecular formulae (factorized as an autoregressive prefix tree generation task) and secondly estimating intensities at each formula peak (set2set transformer)."}, "abstract": {"value": "Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating  (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we use a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of molecular formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of molecular subformulae, each of which specify a predicted peak in the mass spectrum, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for molecular subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, representing a general method for ordered multiset decoding. We show promising empirical results on mass spectra prediction tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/dab7acf19d8f9b7f1625f303c1be7f86456c9503.pdf"}, "supplementary_material": {"value": "/attachment/b62b754ce7c54890e4b459685a4fd0c5acd30b4e.pdf"}, "_bibtex": {"value": "@inproceedings{\ngoldman2023prefixtree,\ntitle={Prefix-Tree Decoding for Predicting Mass Spectra from Molecules},\nauthor={Samuel Goldman and John Bradshaw and Jiayi Xin and Connor W. Coley},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=NWEbeI2HNQ}\n}"}, "paperhash": {"value": "goldman|prefixtree_decoding_for_predicting_mass_spectra_from_molecules"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1939/-/Revision", "NeurIPS.cc/2023/Conference/Submission1939/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1939/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325780407, "odate": 1698949703444, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "n3fPDW87is", "number": 1929, "cdate": 1683218948291, "tcdate": 1683218948291, "mdate": 1698949703387, "tmdate": 1698949703387, "signatures": ["NeurIPS.cc/2023/Conference/Submission1929/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1929/Authors"], "forum": "n3fPDW87is", "content": {"title": {"value": "Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity"}, "authors": {"value": ["Youssef Allouah", "Rachid Guerraoui", "Nirupam Gupta", "Rafael Pinot", "Geovani Rizk"]}, "authorids": {"value": ["~Youssef_Allouah1", "~Rachid_Guerraoui1", "~Nirupam_Gupta1", "~Rafael_Pinot1", "~Geovani_Rizk1"]}, "keywords": {"value": ["Optimization", "Byzantine resilience", "Distributed machine learning", "federated learning"]}, "abstract": {"value": "The theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is homogeneous. Under data heterogeneity however, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely $(G,B)$-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction $\\frac{1}{2}$. We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysis reduces the gap between theory and practice."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/91fad19003dc5448ada057a5f2e89d425ed7b79f.pdf"}, "_bibtex": {"value": "@inproceedings{\nallouah2023robust,\ntitle={Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity},\nauthor={Youssef Allouah and Rachid Guerraoui and Nirupam Gupta and Rafael Pinot and Geovani Rizk},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=n3fPDW87is}\n}"}, "paperhash": {"value": "allouah|robust_distributed_learning_tight_error_bounds_and_breakdown_point_under_data_heterogeneity"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1929/-/Revision", "NeurIPS.cc/2023/Conference/Submission1929/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1929/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325780100, "odate": 1698949703373, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "EjMLpTgvKH", "number": 1889, "cdate": 1683209427651, "tcdate": 1683209427651, "mdate": 1698949703065, "tmdate": 1698949703065, "signatures": ["NeurIPS.cc/2023/Conference/Submission1889/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1889/Authors"], "forum": "EjMLpTgvKH", "content": {"title": {"value": "Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics"}, "authors": {"value": ["Leon Klein", "Andrew Y. K. Foong", "Tor Erlend Fjelde", "Bruno Kacper Mlodozeniec", "Marc Brockschmidt", "Sebastian Nowozin", "Frank Noe", "Ryota Tomioka"]}, "authorids": {"value": ["~Leon_Klein1", "~Andrew_Y._K._Foong1", "~Tor_Erlend_Fjelde1", "~Bruno_Kacper_Mlodozeniec2", "~Marc_Brockschmidt1", "~Sebastian_Nowozin1", "~Frank_Noe1", "~Ryota_Tomioka1"]}, "keywords": {"value": ["Molecular Dynamics", "Normalizing Flows", "MCMC"]}, "TLDR": {"value": "We learn a transferable flow model that predicts the future state of a molecule to accelerate Molecular Dynamics simulations"}, "abstract": {"value": "*Molecular dynamics* (MD) simulation is a widely used technique to simulate molecular systems, most commonly at the all-atom resolution where equations of motion are integrated with timesteps on the order of femtoseconds ($1\\textrm{fs}=10^{-15}\\textrm{s}$). \nMD is often used to compute equilibrium properties, which requires sampling from an equilibrium distribution such as the Boltzmann distribution. \nHowever, many important processes, such as binding and folding, occur over timescales of milliseconds or beyond, and cannot be efficiently sampled with conventional MD.\nFurthermore, new MD simulations need to be performed for each molecular system studied.\nWe present *Timewarp*, an enhanced sampling method which uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the Boltzmann distribution. \nThe flow is trained offline on MD trajectories and learns to make large steps in time, simulating the molecular dynamics of $10^{5} - 10^{6} \\textrm{fs}$.\nCrucially, Timewarp is *transferable* between molecular systems: once trained, we show that it generalises to unseen small peptides (2-4 amino acids) at all-atom resolution, exploring their metastable states and providing wall-clock acceleration of sampling compared to standard MD.\nOur method constitutes an important step towards general, transferable algorithms for accelerating MD."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c213887945323ececbb1ae538dcfbc0e1673d598.pdf"}, "_bibtex": {"value": "@inproceedings{\nklein2023timewarp,\ntitle={Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics},\nauthor={Leon Klein and Andrew Y. K. Foong and Tor Erlend Fjelde and Bruno Kacper Mlodozeniec and Marc Brockschmidt and Sebastian Nowozin and Frank Noe and Ryota Tomioka},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=EjMLpTgvKH}\n}"}, "paperhash": {"value": "klein|timewarp_transferable_acceleration_of_molecular_dynamics_by_learning_timecoarsened_dynamics"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1889/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325778752, "odate": 1698949703054, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "financial_support"}, {"name": "reviewer_nomination"}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "4aIpgq1nuI", "number": 1857, "cdate": 1683202996021, "tcdate": 1683202996021, "mdate": 1698949702798, "tmdate": 1698949702798, "signatures": ["NeurIPS.cc/2023/Conference/Submission1857/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1857/Authors"], "forum": "4aIpgq1nuI", "content": {"title": {"value": "What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement."}, "authors": {"value": ["Yotam Alexander", "Nimrod De La Vega", "Noam Razin", "Nadav Cohen"]}, "authorids": {"value": ["~Yotam_Alexander1", "~Nimrod_De_La_Vega2", "~Noam_Razin1", "~Nadav_Cohen1"]}, "keywords": {"value": ["Deep Learning", "Locally Connected Neural Networks", "Data Distributions", "Quantum Entanglement", "Tensor Networks"]}, "TLDR": {"value": "Importing tools from quantum physics and tensor analysis, we study what makes a data distribution suitable for locally connected neural networks, and develop, based on our theory, an algorithm for enhancing the suitability of data to such models."}, "abstract": {"value": "The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about the relation between deep learning and real-world data."}, "pdf": {"value": "/pdf/04cccfdb3f2c329f5b975f0f14eb9390ca392083.pdf"}, "supplementary_material": {"value": "/attachment/56f13b78aee1749116ae441c18f1e5427c65b955.zip"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nalexander2023what,\ntitle={What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.},\nauthor={Yotam Alexander and Nimrod De La Vega and Noam Razin and Nadav Cohen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=4aIpgq1nuI}\n}"}, "paperhash": {"value": "alexander|what_makes_data_suitable_for_a_locally_connected_neural_network_a_necessary_and_sufficient_condition_based_on_quantum_entanglement"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1857/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1857/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325777783, "odate": 1698949702785, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "81snFfE3vR", "number": 1846, "cdate": 1683201174124, "tcdate": 1683201174124, "mdate": 1698949702666, "tmdate": 1698949702666, "signatures": ["NeurIPS.cc/2023/Conference/Submission1846/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1846/Authors"], "forum": "81snFfE3vR", "content": {"title": {"value": "One-step differentiation of iterative algorithms"}, "authors": {"value": ["Jerome Bolte", "Edouard Pauwels", "Samuel Vaiter"]}, "authorids": {"value": ["~Jerome_Bolte1", "~Edouard_Pauwels1", "~Samuel_Vaiter1"]}, "keywords": {"value": ["automatic differentiation", "implicit differentiation", "super-linear algorithms", "bilevel optimization."]}, "TLDR": {"value": "One-step differentiation is as easy as autodiff, and as performant as implicit differentiation."}, "abstract": {"value": "In appropriate frameworks, automatic differentiation is transparent to the user, at the cost of being a significant computational burden when the number of operations is large. For iterative algorithms, implicit differentiation alleviates this issue but requires custom implementation of Jacobian evaluation. In this paper, we study one-step differentiation, also known as Jacobian-free backpropagation, a method as easy as automatic differentiation and as performant as implicit differentiation for fast algorithms (e.g. superlinear optimization methods). We provide a complete theoretical approximation analysis with specific examples (Newton's method, gradient descent) along with its consequences in bilevel optimization. Several numerical examples illustrate the well-foundness of the one-step estimator."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9da099203aa9a9ff005e843c58be14317293222d.pdf"}, "_bibtex": {"value": "@inproceedings{\nbolte2023onestep,\ntitle={One-step differentiation of iterative algorithms},\nauthor={Jerome Bolte and Edouard Pauwels and Samuel Vaiter},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=81snFfE3vR}\n}"}, "paperhash": {"value": "bolte|onestep_differentiation_of_iterative_algorithms"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1846/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1846/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325777339, "odate": 1698949702654, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "pb1OwZNgr2", "number": 1844, "cdate": 1683200370475, "tcdate": 1683200370475, "mdate": 1698949702588, "tmdate": 1698949702588, "signatures": ["NeurIPS.cc/2023/Conference/Submission1844/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1844/Authors"], "forum": "pb1OwZNgr2", "content": {"title": {"value": "Learning Generalizable Agents via Saliency-guided Features Decorrelation"}, "authors": {"value": ["Sili Huang", "Yanchao Sun", "Jifeng Hu", "Siyuan Guo", "Hechang Chen", "Yi Chang", "Lichao Sun", "Bo Yang"]}, "authorids": {"value": ["~Sili_Huang1", "~Yanchao_Sun1", "~Jifeng_Hu1", "~Siyuan_Guo2", "~Hechang_Chen2", "~Yi_Chang4", "~Lichao_Sun1", "~Bo_Yang6"]}, "keywords": {"value": ["reinforcement learning", "generalization"]}, "abstract": {"value": "In visual-based Reinforcement Learning (RL), agents often struggle to generalize well to environmental variations in the state space that were not observed during training. The variations can arise in both task-irrelevant features, such as background noise, and task-relevant features, such as robot configurations, that are related to the optimal decisions. To achieve generalization in both situations, agents are required to accurately understand the impact of changed features on the decisions, i.e., establishing the true associations between changed features and decisions in the policy model. However, due to the inherent correlations among features in the state space, the associations between features and decisions become entangled, making it difficult for the policy to distinguish them. To this end, we propose Saliency-Guided Features Decorrelation (SGFD) to eliminate these correlations through sample reweighting. Concretely, SGFD consists of two core techniques: Random Fourier Functions (RFF) and the saliency map. RFF is utilized to estimate the complex non-linear correlations in high-dimensional images, while the saliency map is designed to identify the changed features. Under the guidance of the saliency map, SGFD employs sample reweighting to minimize the estimated correlations related to changed features, thereby achieving decorrelation in visual RL tasks. Our experimental results demonstrate that SGFD can generalize well on a wide range of test environments and significantly outperforms state-of-the-art methods in handling both task-irrelevant variations and task-relevant variations."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1364331896dfce19e3e862444c039f383b72c5fc.pdf"}, "supplementary_material": {"value": "/attachment/1cacb6a30beede257612bf61c67cadd9b97dfa03.pdf"}, "_bibtex": {"value": "@inproceedings{\nhuang2023learning,\ntitle={Learning Generalizable Agents via Saliency-guided Features Decorrelation},\nauthor={Sili Huang and Yanchao Sun and Jifeng Hu and Siyuan Guo and Hechang Chen and Yi Chang and Lichao Sun and Bo Yang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=pb1OwZNgr2}\n}"}, "paperhash": {"value": "huang|learning_generalizable_agents_via_saliencyguided_features_decorrelation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1844/-/Revision", "NeurIPS.cc/2023/Conference/Submission1844/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1844/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325777244, "odate": 1698949702574, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ikkdTD3hQJ", "number": 1824, "cdate": 1683195540209, "tcdate": 1683195540209, "mdate": 1698949702432, "tmdate": 1698949702432, "signatures": ["NeurIPS.cc/2023/Conference/Submission1824/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1824/Authors"], "forum": "ikkdTD3hQJ", "content": {"title": {"value": "AIMS: All-Inclusive Multi-Level Segmentation for Anything"}, "authors": {"value": ["Lu Qi", "Jason Kuen", "Weidong Guo", "Jiuxiang Gu", "Zhe Lin", "Bo Du", "Yu Xu", "Ming-Hsuan Yang"]}, "authorids": {"value": ["~Lu_Qi1", "~Jason_Kuen1", "~Weidong_Guo1", "~Jiuxiang_Gu2", "~Zhe_Lin1", "~Bo_Du3", "~Yu_Xu1", "~Ming-Hsuan_Yang1"]}, "keywords": {"value": ["Image Segmentation"]}, "abstract": {"value": "Despite the progress of image segmentation for accurate visual entity segmentation, completing the diverse requirements of image editing applications for different-level region-of-interest selections remains unsolved. In this paper, we propose a new task, All-Inclusive Multi-Level Segmentation (AIMS), which segments visual regions into three levels: part, entity, and relation (two entities with some semantic relationships). We also build a unified AIMS model through multi-dataset multi-task training to address the two major challenges of annotation inconsistency and task correlation. Specifically, we propose task complementarity, association, and prompt mask encoder for three-level predictions. Extensive experiments demonstrate the effectiveness and generalization capacity of our method compared to other state-of-the-art methods on a single dataset or the concurrent work on segment anything. We will make our code and training model publicly available."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/74deb03ba4f9e864611c15fc1b796261c2ae20a1.pdf"}, "supplementary_material": {"value": "/attachment/edcbf718425fd75760154c79c08ed4d82a1dd350.pdf"}, "_bibtex": {"value": "@inproceedings{\nqi2023aims,\ntitle={{AIMS}: All-Inclusive Multi-Level Segmentation for Anything},\nauthor={Lu Qi and Jason Kuen and Weidong Guo and Jiuxiang Gu and Zhe Lin and Bo Du and Yu Xu and Ming-Hsuan Yang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ikkdTD3hQJ}\n}"}, "paperhash": {"value": "qi|aims_allinclusive_multilevel_segmentation_for_anything"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1824/-/Revision", "NeurIPS.cc/2023/Conference/Submission1824/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1824/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325776761, "odate": 1698949702419, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "TcmjewOAd1", "number": 1788, "cdate": 1683187234395, "tcdate": 1683187234395, "mdate": 1698949702190, "tmdate": 1698949702190, "signatures": ["NeurIPS.cc/2023/Conference/Submission1788/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1788/Authors"], "forum": "TcmjewOAd1", "content": {"title": {"value": "L-CAD: Language-based Colorization with Any-level Descriptions using Diffusion Priors"}, "authors": {"value": ["Zheng Chang", "Shuchen Weng", "Peixuan Zhang", "Yu Li", "Si Li", "Boxin Shi"]}, "authorids": {"value": ["~Zheng_Chang2", "~Shuchen_Weng1", "~Peixuan_Zhang1", "~Yu_Li4", "~Si_Li5", "~Boxin_Shi3"]}, "keywords": {"value": ["Colorization", "Language-based generation", "Diffusion model"]}, "abstract": {"value": "Language-based colorization produces plausible and visually pleasing colors under the guidance of user-friendly natural language descriptions. Previous methods implicitly assume that users provide comprehensive color descriptions for most of the objects in the image, which leads to suboptimal performance. In this paper, we propose a unified model to perform language-based colorization with any-level descriptions. We leverage the pretrained cross-modality generative model for its robust language understanding and rich color priors to handle the inherent ambiguity of any-level descriptions. We further design modules to align with input conditions to preserve local spatial structures and prevent the ghosting effect. With the proposed novel sampling strategy, our model achieves instance-aware colorization in diverse and complex scenarios. Extensive experimental results demonstrate our advantages of effectively handling any-level descriptions and outperforming both language-based and automatic colorization methods. The code and pretrained models\nare available at: https://github.com/changzheng123/L-CAD."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/9ade572b3c3235e52c1a33dfcb11553139108bf3.pdf"}, "supplementary_material": {"value": "/attachment/ea25d9bfc96c3d838c5aa5d229474863f0fe63b0.pdf"}, "_bibtex": {"value": "@inproceedings{\nchang2023lcad,\ntitle={L-{CAD}: Language-based Colorization with Any-level Descriptions using Diffusion Priors},\nauthor={Zheng Chang and Shuchen Weng and Peixuan Zhang and Yu Li and Si Li and Boxin Shi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=TcmjewOAd1}\n}"}, "paperhash": {"value": "chang|lcad_languagebased_colorization_with_anylevel_descriptions_using_diffusion_priors"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1788/-/Revision", "NeurIPS.cc/2023/Conference/Submission1788/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1788/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325775593, "odate": 1698949702174, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "mmmd2vp0n0", "number": 1732, "cdate": 1683170147810, "tcdate": 1683170147810, "mdate": 1698949701971, "tmdate": 1698949701971, "signatures": ["NeurIPS.cc/2023/Conference/Submission1732/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1732/Authors"], "forum": "mmmd2vp0n0", "content": {"title": {"value": "Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction"}, "authors": {"value": ["Anagh Malik", "Parsa Mirdehghan", "Sotiris Nousias", "Kyros Kutulakos", "David B. Lindell"]}, "authorids": {"value": ["~Anagh_Malik2", "~Parsa_Mirdehghan1", "~Sotiris_Nousias2", "~Kyros_Kutulakos1", "~David_B._Lindell1"]}, "keywords": {"value": ["neural radiance fields", "3D reconstruction", "single-photon lidar", "computational imaging"]}, "abstract": {"value": "Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling scene appearance and geometry from multiview imagery. Recent work has also begun to explore how to use additional supervision from lidar or depth sensor measurements in the NeRF framework. However, previous lidar-supervised NeRFs focus on rendering conventional camera imagery and use lidar-derived point cloud data as auxiliary supervision; thus, they fail to incorporate the underlying image formation model of the lidar. Here, we propose a novel method for rendering transient NeRFs that take as input the raw, time-resolved photon count histograms measured by a single-photon lidar system, and we seek to render such histograms from novel views. Different from conventional NeRFs, the approach relies on a time-resolved version of the volume rendering equation to render the lidar measurements and capture transient light transport phenomena at picosecond timescales. We evaluate our method on a first-of-its-kind dataset of simulated and captured transient multiview scans from a prototype single-photon lidar. Overall, our work brings NeRFs to a new dimension of imaging at transient timescales, newly enabling rendering of transient imagery from novel views. Additionally, we show that our approach recovers improved geometry and conventional appearance compared to point cloud-based supervision when training on few input viewpoints. Transient NeRFs may be especially useful for applications which seek to simulate raw lidar measurements for downstream tasks in autonomous driving, robotics, and remote sensing."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We develop a NeRF-based approach for view synthesis of transient, picosecond-scale measurements captured by a lidar."}, "pdf": {"value": "/pdf/a52815f4d6b3d74bba72be4a5454c4ecaa767cff.pdf"}, "supplementary_material": {"value": "/attachment/5ef701db8fa940b8c206172914a0f3cbfe723e97.pdf"}, "_bibtex": {"value": "@inproceedings{\nmalik2023transient,\ntitle={Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction},\nauthor={Anagh Malik and Parsa Mirdehghan and Sotiris Nousias and Kyros Kutulakos and David B. Lindell},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=mmmd2vp0n0}\n}"}, "paperhash": {"value": "malik|transient_neural_radiance_fields_for_lidar_view_synthesis_and_3d_reconstruction"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1732/-/Revision", "NeurIPS.cc/2023/Conference/Submission1732/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1732/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325774050, "odate": 1698949701957, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "ETk6cfS3vk", "number": 1725, "cdate": 1683168987534, "tcdate": 1683168987534, "mdate": 1698949701900, "tmdate": 1698949701900, "signatures": ["NeurIPS.cc/2023/Conference/Submission1725/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1725/Authors"], "forum": "ETk6cfS3vk", "content": {"title": {"value": "SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models"}, "authors": {"value": ["Ziyi Wu", "Jingyu Hu", "Wuyue Lu", "Igor Gilitschenski", "Animesh Garg"]}, "authorids": {"value": ["~Ziyi_Wu1", "~Jingyu_Hu1", "~Wuyue_Lu1", "~Igor_Gilitschenski1", "~Animesh_Garg1"]}, "keywords": {"value": ["Unsupervised object-centric learning", "diffusion model", "generative modeling"]}, "TLDR": {"value": "We incorporate diffusion models to the Slot Attention framework and achieve better segmentation and generation results"}, "abstract": {"value": "Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization.\nLeveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery.\nIn addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing.\nHowever, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities.\nIn this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation.\nWe introduce SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for both image and video data.\nThanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation across six datasets.\nFurthermore, our learned object features can be utilized by existing object-centric dynamics models, improving video prediction quality and downstream temporal reasoning tasks.\nFinally, we demonstrate the scalability of SlotDiffusion to unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated with self-supervised pre-trained image encoders."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f2d3a79a58f247d1daef0de12eeaf801dee8761d.pdf"}, "_bibtex": {"value": "@inproceedings{\nwu2023slotdiffusion,\ntitle={SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models},\nauthor={Ziyi Wu and Jingyu Hu and Wuyue Lu and Igor Gilitschenski and Animesh Garg},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ETk6cfS3vk}\n}"}, "paperhash": {"value": "wu|slotdiffusion_objectcentric_generative_modeling_with_diffusion_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1725/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1725/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325773748, "odate": 1698949701885, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "z06npyCwDq", "number": 1724, "cdate": 1683168287650, "tcdate": 1683168287650, "mdate": 1698949701859, "tmdate": 1698949701859, "signatures": ["NeurIPS.cc/2023/Conference/Submission1724/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1724/Authors"], "forum": "z06npyCwDq", "content": {"title": {"value": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers"}, "authors": {"value": ["Zixuan Jiang", "Jiaqi Gu", "Hanqing Zhu", "David Z. Pan"]}, "authorids": {"value": ["~Zixuan_Jiang1", "~Jiaqi_Gu3", "~Hanqing_Zhu1", "~David_Z._Pan1"]}, "keywords": {"value": ["Transformer", "Normalization", "Layer Normalization", "RMSNorm", "Efficient Machine Learning"]}, "abstract": {"value": "Transformers have achieved great success in machine learning applications.\nNormalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers.\nWhile LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value.\nDespite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers.\nThere is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models.\nIt is challenging to convert Transformers with one normalization to the other type.\nWhile there is an ongoing disagreement between the two normalization types,\nwe propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers.\nBy removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency.\nWe further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors.\nWe formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference.\nIt implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement.\nExperiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We unify LayerNorm and RMSNorm in pre-normalization Transformers and propose equivalent and efficient Transformer variants."}, "pdf": {"value": "/pdf/072e3fdb5b7970f3d914c21ca2311d6aa1b207a0.pdf"}, "supplementary_material": {"value": "/attachment/804e530f5a015ec5b467a0e0e83d195f006af499.zip"}, "_bibtex": {"value": "@inproceedings{\njiang2023prermsnorm,\ntitle={Pre-{RMSN}orm and Pre-{CRMSN}orm Transformers: Equivalent and Efficient Pre-{LN} Transformers},\nauthor={Zixuan Jiang and Jiaqi Gu and Hanqing Zhu and David Z. Pan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=z06npyCwDq}\n}"}, "paperhash": {"value": "jiang|prermsnorm_and_precrmsnorm_transformers_equivalent_and_efficient_preln_transformers"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission1724/-/Revision", "NeurIPS.cc/2023/Conference/Submission1724/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1724/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325773667, "odate": 1698949701846, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "2doqt9r0r0", "number": 1664, "cdate": 1683137946907, "tcdate": 1683137946907, "mdate": 1698949701473, "tmdate": 1698949701473, "signatures": ["NeurIPS.cc/2023/Conference/Submission1664/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1664/Authors"], "forum": "2doqt9r0r0", "content": {"title": {"value": "Efficient Online Clustering with Moving Costs"}, "authors": {"value": ["Dimitris Christou", "EFSTRATIOS PANTELEIMON SKOULAKIS", "Volkan Cevher"]}, "authorids": {"value": ["~Dimitris_Christou1", "~EFSTRATIOS_PANTELEIMON_SKOULAKIS1", "~Volkan_Cevher1"]}, "keywords": {"value": ["Online Learning", "Regret Analysis", "Clustering", "k-Median"]}, "abstract": {"value": "In this work we consider an online learning problem, called Online $k$-Clustering with Moving Costs, at which a learner maintains a set of $k$ facilities over $T$ rounds so as to minimize the connection cost of an adversarially selected sequence of clients. The learner is informed on the positions of the clients at each round $t$ only after its facility-selection and can use this information to update its decision in the next round. However, updating the facility positions comes with an additional moving cost based on the moving distance of the facilities. We present the first $\\mathcal{O}(\\log n)$-regret polynomial-time online learning algorithm guaranteeing that the overall cost (connection $+$ moving) is at most $\\mathcal{O}(\\log n)$ times the time-averaged connection cost of the best fixed solution. Our work improves on the recent result of (Fotakis et al., 2021) establishing $\\mathcal{O}(k)$-regret guarantees only on the connection cost."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/362284fb92d590b85ff56162f5b3c2711135502a.pdf"}, "supplementary_material": {"value": "/attachment/f29ff9bdfb09832fd844ece9e5387350cf26e944.zip"}, "_bibtex": {"value": "@inproceedings{\nchristou2023efficient,\ntitle={Efficient Online Clustering with Moving Costs},\nauthor={Dimitris Christou and EFSTRATIOS PANTELEIMON SKOULAKIS and Volkan Cevher},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=2doqt9r0r0}\n}"}, "paperhash": {"value": "christou|efficient_online_clustering_with_moving_costs"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1664/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1664/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325771640, "odate": 1698949701457, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "I9xE1Jsjfx", "number": 1591, "cdate": 1683110777040, "tcdate": 1683110777040, "mdate": 1698949700829, "tmdate": 1698949700829, "signatures": ["NeurIPS.cc/2023/Conference/Submission1591/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1591/Authors"], "forum": "I9xE1Jsjfx", "content": {"title": {"value": "Evaluating and Inducing Personality in Pre-trained Language Models"}, "authors": {"value": ["Guangyuan Jiang", "Manjie Xu", "Song-Chun Zhu", "Wenjuan Han", "Chi Zhang", "Yixin Zhu"]}, "authorids": {"value": ["~Guangyuan_Jiang1", "~Manjie_Xu1", "~Song-Chun_Zhu1", "~Wenjuan_Han1", "~Chi_Zhang12", "~Yixin_Zhu1"]}, "keywords": {"value": ["machine personality", "machine behavior", "personality trait theory", "psychometric", "large language models", "prompt"]}, "abstract": {"value": "Standardized and quantified evaluation of machine behaviors is a crux of understanding LLMs. In this study, we draw inspiration from psychometric studies by leveraging human personality theory as a tool for studying machine behaviors. Originating as a philosophical quest for human behaviors, the study of personality delves into how individuals differ in thinking, feeling, and behaving. Toward building and understanding human-like social machines, we are motivated to ask: Can we assess machine behaviors by leveraging human psychometric tests in a **principled** and **quantitative** manner? If so, can we induce a specific personality in LLMs? To answer these questions, we introduce the Machine Personality Inventory (MPI) tool for studying machine behaviors; MPI follows standardized\npersonality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise a Personality Prompting (P$^2$) method to induce LLMs with specific personalities in a **controllable** way, capable of producing diverse and verifiable behaviors. We hope this work sheds light on future studies by adopting personality as the essential indicator for various downstream tasks, and could further motivate research into equally intriguing human-like machine behaviors."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a3ee57ef9848f291b9fee54e97d99eceda11d8ac.pdf"}, "supplementary_material": {"value": "/attachment/d2d76d2f0518e389831da55c9003258e3b8f6be2.zip"}, "_bibtex": {"value": "@inproceedings{\njiang2023evaluating,\ntitle={Evaluating and Inducing Personality in Pre-trained Language Models},\nauthor={Guangyuan Jiang and Manjie Xu and Song-Chun Zhu and Wenjuan Han and Chi Zhang and Yixin Zhu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=I9xE1Jsjfx}\n}"}, "paperhash": {"value": "jiang|evaluating_and_inducing_personality_in_pretrained_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1591/-/Revision", "NeurIPS.cc/2023/Conference/Submission1591/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1591/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325768943, "odate": 1698949700816, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "jkPDRHff3s", "number": 1485, "cdate": 1683060339505, "tcdate": 1683060339505, "mdate": 1698949700141, "tmdate": 1698949700141, "signatures": ["NeurIPS.cc/2023/Conference/Submission1485/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1485/Authors"], "forum": "jkPDRHff3s", "content": {"title": {"value": "Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory"}, "authors": {"value": ["Sokhna Diarra Mbacke", "Florence Clerc", "Pascal Germain"]}, "authorids": {"value": ["~Sokhna_Diarra_Mbacke1", "~Florence_Clerc1", "~Pascal_Germain1"]}, "keywords": {"value": ["Variational Autoencoders", "PAC-Bayes", "Statistical Learning Theory"]}, "TLDR": {"value": "We provide reconstruction, regeneration and generation guarantees for VAEs using PAC-Bayesian Theory"}, "abstract": {"value": "Since their inception, Variational Autoencoders (VAEs) have become central in machine learning. Despite their widespread use, numerous questions regarding their theoretical properties remain open. Using PAC-Bayesian theory, this work develops statistical guarantees for VAEs. First, we derive the first PAC-Bayesian bound for posterior distributions conditioned on individual samples from the data-generating distribution. Then, we utilize this result to develop generalization guarantees for the VAE's reconstruction loss, as well as upper bounds on the distance between the input and the regenerated distributions. More importantly, we provide upper bounds on the Wasserstein distance between the input distribution and the distribution defined by the VAE's generative model."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/07cbcb742e06fed80d60955d31982d09100fed05.pdf"}, "supplementary_material": {"value": "/attachment/81f776b7fd9138bb6f26968f4bfdab929ced4517.pdf"}, "_bibtex": {"value": "@inproceedings{\nmbacke2023statistical,\ntitle={Statistical Guarantees for Variational Autoencoders using {PAC}-Bayesian Theory},\nauthor={Sokhna Diarra Mbacke and Florence Clerc and Pascal Germain},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=jkPDRHff3s}\n}"}, "paperhash": {"value": "mbacke|statistical_guarantees_for_variational_autoencoders_using_pacbayesian_theory"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1485/-/Revision", "NeurIPS.cc/2023/Conference/Submission1485/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1485/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325760470, "odate": 1698949700127, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Izt7rDD7jN", "number": 1471, "cdate": 1683052181081, "tcdate": 1683052181081, "mdate": 1698949699975, "tmdate": 1698949699975, "signatures": ["NeurIPS.cc/2023/Conference/Submission1471/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1471/Authors"], "forum": "Izt7rDD7jN", "content": {"title": {"value": "Smoothed Online Learning for Prediction in Piecewise Affine Systems"}, "authors": {"value": ["Adam Block", "Max Simchowitz", "Russ Tedrake"]}, "authorids": {"value": ["~Adam_Block1", "~Max_Simchowitz1", "~Russ_Tedrake1"]}, "keywords": {"value": ["Smoothed Online Learning", "Piecewise Affine Prediction", "Learning Dynamics"]}, "TLDR": {"value": "In this paper, we provide the first provably oracle-efficient, no-regret algorithm for prediction in piecewise affine systems"}, "abstract": {"value": "The problem of piecewise affine (PWA) regression and planning is of foundational importance to the study of online learning, control, and robotics, where it provides a theoretically and empirically tractable setting to study systems undergoing sharp changes in the dynamics.  Unfortunately, due to the discontinuities that arise when crossing into different ``pieces,'' learning in general sequential settings is impossible and practical algorithms are forced to resort to heuristic approaches.  This paper builds on the recently developed smoothed online learning framework and provides the first algorithms for prediction and simulation in PWA systems whose regret is polynomial in all relevant problem parameters under a weak smoothness assumption; moreover, our algorithms are efficient in the number of calls to an optimization oracle.  We further apply our results to the problems of one-step prediction and multi-step simulation regret in piecewise affine dynamical systems, where the learner is tasked with simulating trajectories and regret is measured in terms of the Wasserstein distance between simulated and true data.  Along the way, we develop several technical tools of more general interest."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/7221eedc0cae7f32704c3024b2acd1d8c568ef0e.pdf"}, "supplementary_material": {"value": "/attachment/68d9c9ae14c9310a95a97ff8ad82941210cbd106.pdf"}, "_bibtex": {"value": "@inproceedings{\nblock2023smoothed,\ntitle={Smoothed Online Learning for Prediction in Piecewise Affine Systems},\nauthor={Adam Block and Max Simchowitz and Russ Tedrake},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Izt7rDD7jN}\n}"}, "paperhash": {"value": "block|smoothed_online_learning_for_prediction_in_piecewise_affine_systems"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1471/-/Revision", "NeurIPS.cc/2023/Conference/Submission1471/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1471/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325759886, "odate": 1698949699963, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "bfmSc1ETT9", "number": 1466, "cdate": 1683052060384, "tcdate": 1683052060384, "mdate": 1698954268890, "tmdate": 1698954268890, "signatures": ["NeurIPS.cc/2023/Conference/Submission1466/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1466/Authors"], "forum": "bfmSc1ETT9", "content": {"title": {"value": "Kiki or Bouba? Sound Symbolism in Vision-and-Language Models"}, "authors": {"value": ["Morris Alper", "Hadar Averbuch-Elor"]}, "authorids": {"value": ["~Morris_Alper1", "~Hadar_Averbuch-Elor2"]}, "keywords": {"value": ["multimodal learning", "computer vision", "NLP", "cognitive science"]}, "abstract": {"value": "Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/6b0e89981c8c90a4eddeb5692f0fdd7f549488a3.pdf"}, "supplementary_material": {"value": "/attachment/1d6fc72be351e08df6e8501e82ac97e4b9f0fa03.pdf"}, "_bibtex": {"value": "@inproceedings{\nalper2023kiki,\ntitle={Kiki or Bouba? Sound Symbolism in Vision-and-Language Models},\nauthor={Morris Alper and Hadar Averbuch-Elor},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=bfmSc1ETT9}\n}"}, "paperhash": {"value": "alper|kiki_or_bouba_sound_symbolism_in_visionandlanguage_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1466/-/Revision", "NeurIPS.cc/2023/Conference/Submission1466/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/Submission1466/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325759718, "odate": 1698949699904, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "ARrwf7Ev2T", "number": 1378, "cdate": 1683010483666, "tcdate": 1683010483666, "mdate": 1698949699266, "tmdate": 1698949699266, "signatures": ["NeurIPS.cc/2023/Conference/Submission1378/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1378/Authors"], "forum": "ARrwf7Ev2T", "content": {"title": {"value": "Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models"}, "authors": {"value": ["Sivan Doveh", "Assaf Arbelle", "Sivan Harary", "Roei Herzig", "Donghyun Kim", "Paola Cascante-Bonilla", "Amit Alfassy", "Rameswar Panda", "Raja Giryes", "Rogerio Feris", "Shimon Ullman", "Leonid Karlinsky"]}, "authorids": {"value": ["~Sivan_Doveh1", "~Assaf_Arbelle1", "~Sivan_Harary1", "~Roei_Herzig2", "~Donghyun_Kim2", "~Paola_Cascante-Bonilla1", "~Amit_Alfassy1", "~Rameswar_Panda1", "~Raja_Giryes1", "~Rogerio_Feris1", "~Shimon_Ullman1", "~Leonid_Karlinsky3"]}, "keywords": {"value": ["computer vision", "deep learning", "vision and language models"]}, "abstract": {"value": "Vision and Language (VL) models offer an effective method for aligning representation spaces of images and text allowing for numerous applications such as cross-modal retrieval, visual and multi-hop question answering, captioning, and many more. However, the aligned image-text spaces learned by all the popular VL models are still suffering from the so-called 'object bias' - their representations behave as 'bags of nouns' mostly ignoring or downsizing the attributes, relations, and states of objects described/appearing in texts/images. Although some great attempts at fixing these `compositional reasoning' issues were proposed in the recent literature, the problem is still far from being solved. In this paper, we uncover two factors limiting the VL models' compositional reasoning performance. These two factors are properties of the paired VL dataset used for finetuning (or pre-training) the VL model: (i) the caption quality, or in other words 'image-alignment', of the texts; and (ii) the 'density' of the captions in the sense of mentioning all the details appearing on the image. We propose a fine-tuning approach for automatically treating these factors on a standard collection of paired VL data (CC3M). Applied to CLIP, we demonstrate its significant compositional reasoning performance increase of up to $\\sim27$\\% over the base model, up to $\\sim20$\\% over the strongest baseline, and by $6.7$\\% on average. Our code is provided in the Supplementary and would be released upon acceptance."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Improve Vision and Language models' compositional reasoning using improved captions quality and density."}, "pdf": {"value": "/pdf/b62610fe2339b1527bbbbe14b9d5ac709f82e577.pdf"}, "supplementary_material": {"value": "/attachment/d3966094ad9cd2b47f12dcac7c7937832a1cd64f.zip"}, "_bibtex": {"value": "@inproceedings{\ndoveh2023dense,\ntitle={Dense and Aligned Captions ({DAC}) Promote Compositional Reasoning in {VL} Models},\nauthor={Sivan Doveh and Assaf Arbelle and Sivan Harary and Roei Herzig and Donghyun Kim and Paola Cascante-Bonilla and Amit Alfassy and Rameswar Panda and Raja Giryes and Rogerio Feris and Shimon Ullman and Leonid Karlinsky},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=ARrwf7Ev2T}\n}"}, "paperhash": {"value": "doveh|dense_and_aligned_captions_dac_promote_compositional_reasoning_in_vl_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1378/-/Revision", "NeurIPS.cc/2023/Conference/Submission1378/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1378/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325757101, "odate": 1698949699251, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "vA0vj1mY77", "number": 1349, "cdate": 1682994926184, "tcdate": 1682994926184, "mdate": 1698949699053, "tmdate": 1698949699053, "signatures": ["NeurIPS.cc/2023/Conference/Submission1349/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1349/Authors"], "forum": "vA0vj1mY77", "content": {"title": {"value": "MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion"}, "authors": {"value": ["Shitao Tang", "Fuyang Zhang", "Jiacheng Chen", "Peng Wang", "Yasutaka Furukawa"]}, "authorids": {"value": ["~Shitao_Tang1", "~Fuyang_Zhang1", "~Jiacheng_Chen1", "~Peng_Wang2", "~Yasutaka_Furukawa1"]}, "keywords": {"value": ["multiview; image generation; generative model; diffusion models"]}, "abstract": {"value": "This paper introduces MVDiffusion, a simple yet effective multi-view image generation method for scenarios where pixel-to-pixel correspondences are available, such as perspective crops from panorama or multi-view images given geometry (depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion concurrently generates all images with a global awareness, encompassing high resolution and rich content, effectively addressing the error accumulation prevalent in preceding models. MVDiffusion specifically incorporates a correspondence-aware attention mechanism, enabling effective cross-view interaction. This mechanism underpins three pivotal modules: 1) a generation module that produces low-resolution images while maintaining global correspondence, 2) an interpolation module that densifies spatial coverage between images, and 3) a super-resolution module that upscales into high-resolution images. In terms of panoramic imagery, MVDiffusion generates high-resolution photorealistic images up to 1024*1024 pixels. For geometry-conditioned multi-view image generation, MVDiffusion demonstrates state-of-the-art performance on texture-map generation for a given scene mesh. We recommend referring to our Arxiv version at https://arxiv.org/pdf/2307.01097.pdf for the latest update. The project page is at https://mvdiffusion.github.io/."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "This paper introduces MVDiffusion, a simple yet effective multi-view image generation method, tailored for scenarios where pixel-to-pixel correspondences are available, such as perspective crops from panorama or multi-view images given depth/pose."}, "pdf": {"value": "/pdf/8358157158caba4f940ba919afc8c39c940fa0b4.pdf"}, "supplementary_material": {"value": "/attachment/355d4598983d392c0252a5aedbf7c5973366aec9.zip"}, "_bibtex": {"value": "@inproceedings{\ntang2023mvdiffusion,\ntitle={{MVD}iffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion},\nauthor={Shitao Tang and Fuyang Zhang and Jiacheng Chen and Peng Wang and Yasutaka Furukawa},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vA0vj1mY77}\n}"}, "paperhash": {"value": "tang|mvdiffusion_enabling_holistic_multiview_image_generation_with_correspondenceaware_diffusion"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1349/-/Revision", "NeurIPS.cc/2023/Conference/Submission1349/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1349/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325756139, "odate": 1698949699037, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "MziFFGjpkb", "number": 1286, "cdate": 1682952593692, "tcdate": 1682952593692, "mdate": 1698949698586, "tmdate": 1698949698586, "signatures": ["NeurIPS.cc/2023/Conference/Submission1286/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1286/Authors"], "forum": "MziFFGjpkb", "content": {"title": {"value": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation"}, "authors": {"value": ["Thomas FEL", "Victor Boutin", "Louis B\u00e9thune", "Remi Cadene", "Mazda Moayeri", "L\u00e9o And\u00e9ol", "Mathieu Chalvidal", "Thomas Serre"]}, "authorids": {"value": ["~Thomas_FEL1", "~Victor_Boutin2", "~Louis_B\u00e9thune1", "~Remi_Cadene1", "~Mazda_Moayeri1", "~L\u00e9o_And\u00e9ol1", "~Mathieu_Chalvidal1", "~Thomas_Serre1"]}, "keywords": {"value": ["Explainable AI", "Concept-based explainability", "Interpretability", "Concept extraction", "Concept importance", "Attribution methods"]}, "abstract": {"value": "In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual ``concepts'' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that recast the first step -- concept extraction problem -- as a special case of **dictionary learning**, and we formalize the second step -- concept importance estimation -- as a more general form of **attribution method**.\nThis framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii)  to derive theoretical guarantees regarding the optimality of such methods. \n\nWe further leverage our framework to try to tackle a crucial question in explainability: how to *efficiently* identify clusters of data points that are classified based on a similar shared strategy.\nTo illustrate these findings and to highlight the main strategies of a model, we introduce a visual representation called the strategic cluster graph. Finally, we present Lens, a dedicated website that offers a complete compilation of these visualizations for all classes of the ImageNet dataset."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3e6bb9e001d33f16a8c13f5b7ad9974e6605721c.pdf"}, "_bibtex": {"value": "@inproceedings{\nfel2023a,\ntitle={A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation},\nauthor={Thomas FEL and Victor Boutin and Louis B{\\'e}thune and Remi Cadene and Mazda Moayeri and L{\\'e}o And{\\'e}ol and Mathieu Chalvidal and Thomas Serre},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=MziFFGjpkb}\n}"}, "TLDR": {"value": "We introduce a new framework that unifies all existing concept based explainability methods"}, "supplementary_material": {"value": "/attachment/ed855997c1ffe9b478e9cbff5e8860f495c0bc6d.pdf"}, "paperhash": {"value": "fel|a_holistic_approach_to_unifying_automatic_concept_extraction_and_concept_importance_estimation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1286/-/Revision", "NeurIPS.cc/2023/Conference/Submission1286/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1286/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325754314, "odate": 1698949698571, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "i39yXaUKuF", "number": 1175, "cdate": 1682858511555, "tcdate": 1682858511555, "mdate": 1698949697741, "tmdate": 1698949697741, "signatures": ["NeurIPS.cc/2023/Conference/Submission1175/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1175/Authors"], "forum": "i39yXaUKuF", "content": {"title": {"value": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models"}, "authors": {"value": ["Youquan Liu", "Lingdong Kong", "Jun CEN", "Runnan Chen", "Wenwei Zhang", "Liang Pan", "Kai Chen", "Ziwei Liu"]}, "authorids": {"value": ["~Youquan_Liu1", "~Lingdong_Kong1", "~Jun_CEN1", "~Runnan_Chen1", "~Wenwei_Zhang1", "~Liang_Pan2", "~Kai_Chen4", "~Ziwei_Liu1"]}, "keywords": {"value": ["autonomous driving", "point cloud segmentation", "self-supervised learning", "3D scene understanding"]}, "abstract": {"value": "Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, obviating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment regularization stages, facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notably, Seal achieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing random initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU. Moreover, Seal demonstrates significant performance gains over existing methods across 20 different few-shot fine-tuning tasks on all eleven tested point cloud datasets. The code is available at this link."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "Seal is a spatial and temporal consistent framework that leverages vision foundation models for self-supervised learning on large-scale point clouds."}, "pdf": {"value": "/pdf/7d3d077fa1c2341a156744922bec0648efc4a56a.pdf"}, "supplementary_material": {"value": "/attachment/5260ba04ad3d8612196638064a2655382445178c.zip"}, "_bibtex": {"value": "@inproceedings{\nliu2023segment,\ntitle={Segment Any Point Cloud Sequences by Distilling Vision Foundation Models},\nauthor={Youquan Liu and Lingdong Kong and Jun CEN and Runnan Chen and Wenwei Zhang and Liang Pan and Kai Chen and Ziwei Liu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=i39yXaUKuF}\n}"}, "paperhash": {"value": "liu|segment_any_point_cloud_sequences_by_distilling_vision_foundation_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1175/-/Revision", "NeurIPS.cc/2023/Conference/Submission1175/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1175/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325751342, "odate": 1698949697727, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "GEWzHeHpLr", "number": 1166, "cdate": 1682845663060, "tcdate": 1682845663060, "mdate": 1698949697675, "tmdate": 1698949697675, "signatures": ["NeurIPS.cc/2023/Conference/Submission1166/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1166/Authors"], "forum": "GEWzHeHpLr", "content": {"title": {"value": "Transition-constant Normalization for Image Enhancement"}, "authors": {"value": ["Jie Huang", "Man Zhou", "JingHao Zhang", "Gang Yang", "Mingde Yao", "Chongyi Li", "Zhiwei Xiong", "Feng Zhao"]}, "authorids": {"value": ["~Jie_Huang4", "~Man_Zhou4", "~JingHao_Zhang2", "~Gang_Yang4", "~Mingde_Yao1", "~Chongyi_Li1", "~Zhiwei_Xiong1", "~Feng_Zhao6"]}, "keywords": {"value": ["Image Enhancement", "Normalization", "Image Restoration"]}, "abstract": {"value": "Normalization techniques that capture image style by statistical representation have become a popular component in deep neural networks.\nAlthough image enhancement can be considered as a form of style transformation, there has been little exploration of how normalization affect the enhancement performance. \nTo fully leverage the potential of normalization, we present a novel Transition-Constant Normalization (TCN) for various image enhancement tasks.\nSpecifically, it consists of two streams of normalization operations arranged under an invertible constraint, along with a feature sub-sampling operation that satisfies the normalization constraint.\nTCN enjoys several merits, including being parameter-free, plug-and-play, and incurring no additional computational costs.\nWe provide various formats to utilize TCN for image enhancement, including seamless  integration with enhancement networks, incorporation into encoder-decoder architectures for downsampling, and implementation of efficient architectures.\nThrough extensive experiments on multiple image enhancement tasks, like low-light enhancement, exposure correction, SDR2HDR translation, and image dehazing, our TCN consistently demonstrates performance improvements.\nBesides, it showcases extensive ability in other tasks including pan-sharpening and medical segmentation.\nThe code is available at  \\textit{\\textcolor{blue}{https://github.com/huangkevinj/TCNorm}}."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/e21d4335e7b1cf49369dcf9a860839dda015e116.pdf"}, "supplementary_material": {"value": "/attachment/8f1a4268e2e617fb6e04eef297348d437053094d.pdf"}, "_bibtex": {"value": "@inproceedings{\nhuang2023transitionconstant,\ntitle={Transition-constant Normalization for Image Enhancement},\nauthor={Jie Huang and Man Zhou and JingHao Zhang and Gang Yang and Mingde Yao and Chongyi Li and Zhiwei Xiong and Feng Zhao},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=GEWzHeHpLr}\n}"}, "paperhash": {"value": "huang|transitionconstant_normalization_for_image_enhancement"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1166/-/Revision", "NeurIPS.cc/2023/Conference/Submission1166/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1166/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325751104, "odate": 1698949697663, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "tesBViWnbx", "number": 1160, "cdate": 1682838447696, "tcdate": 1682838447696, "mdate": 1698949697602, "tmdate": 1698949697602, "signatures": ["NeurIPS.cc/2023/Conference/Submission1160/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1160/Authors"], "forum": "tesBViWnbx", "content": {"title": {"value": "Stable Diffusion is Unstable"}, "authors": {"value": ["Chengbin Du", "Yanxi Li", "Zhongwei Qiu", "Chang Xu"]}, "authorids": {"value": ["~Chengbin_Du1", "~Yanxi_Li1", "~Zhongwei_Qiu1", "~Chang_Xu4"]}, "keywords": {"value": ["Adversarial Attack", "Generative Model", "Diffusion Model", "Latent Diffusion Model", "Conditional Latent Diffusion Model"]}, "abstract": {"value": "Recently, text-to-image models have been thriving. Despite their powerful generative capacity, our research has uncovered a lack of robustness in this generation process. Specifically, the introduction of small perturbations to the text prompts can result in the blending of primary subjects with other categories or their complete disappearance in the generated images. In this paper, we propose **Auto-attack on Text-to-image Models (ATM)**, a gradient-based approach, to effectively and efficiently generate such perturbations. By learning a Gumbel Softmax distribution, we can make the discrete process of word replacement or extension continuous, thus ensuring the differentiability of the perturbation generation. Once the distribution is learned, ATM can sample multiple attack samples simultaneously. These attack samples can prevent the generative model from generating the desired subjects without tampering with the category keywords in the prompt. ATM has achieved a 91.1\\% success rate in short-text attacks and an 81.2\\% success rate in long-text attacks. Further empirical analysis revealed three attack patterns based on: 1) variability in generation speed, 2) similarity of coarse-grained characteristics, and 3) polysemy of words. The code is available at https://github.com/duchengbin8/Stable_Diffusion_is_Unstable"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/071f9bc33aa32b1ec9e3de75110097991d094560.pdf"}, "supplementary_material": {"value": "/attachment/8885243f15fbdf2a426ed9bc28f04e36d74a158b.pdf"}, "_bibtex": {"value": "@inproceedings{\ndu2023stable,\ntitle={Stable Diffusion is Unstable},\nauthor={Chengbin Du and Yanxi Li and Zhongwei Qiu and Chang Xu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=tesBViWnbx}\n}"}, "paperhash": {"value": "du|stable_diffusion_is_unstable"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1160/-/Revision", "NeurIPS.cc/2023/Conference/Submission1160/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1160/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325751066, "odate": 1698949697588, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "LGqIAn2OaZ", "number": 1153, "cdate": 1682832247115, "tcdate": 1682832247115, "mdate": 1698949697518, "tmdate": 1698949697518, "signatures": ["NeurIPS.cc/2023/Conference/Submission1153/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1153/Authors"], "forum": "LGqIAn2OaZ", "content": {"title": {"value": "Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes"}, "authors": {"value": ["Zheng Wang", "Shikai Fang", "Shibo Li", "Shandian Zhe"]}, "authorids": {"value": ["~Zheng_Wang2", "~Shikai_Fang2", "~Shibo_Li1", "~Shandian_Zhe1"]}, "keywords": {"value": ["Tensor Decomposition", "Representation Learning"]}, "abstract": {"value": "Tensor decomposition is an important tool for multiway data analysis. In practice, the data is often sparse yet associated with rich temporal information. Existing methods, however, often under-use the time information and ignore the structural knowledge within the sparsely observed tensor entries. To overcome these limitations and to better capture the underlying temporal structure, we propose Dynamic EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE). We develop a neural diffusion-reaction process to estimate dynamic embeddings for the entities in each tensor mode. Specifically, based on the observed tensor entries, we build a multi-partite graph to encode the correlation between the entities. We construct a graph diffusion process to co-evolve the embedding trajectories of the correlated entities and use a neural network to construct a reaction process for each individual entity. In this way, our model can capture both the commonalities and personalities during the evolution of the embeddings for different entities. We then use a neural network to model the entry value as a nonlinear function of the embedding trajectories. For model estimation, we combine ODE solvers to develop a stochastic mini-batch learning algorithm. We propose a stratified sampling method to balance the cost of processing each mini-batch so as to improve the overall efficiency. We show the advantage of our approach in both simulation studies and real-world applications. The code is available at https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes."}, "pdf": {"value": "/pdf/9bad765e3f2c9ced71c6657d1421c727e78b2509.pdf"}, "supplementary_material": {"value": "/attachment/b7622a2daef46baae4bd9f3e86f442b61cc054f5.pdf"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nwang2023dynamic,\ntitle={Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes},\nauthor={Zheng Wang and Shikai Fang and Shibo Li and Shandian Zhe},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=LGqIAn2OaZ}\n}"}, "paperhash": {"value": "wang|dynamic_tensor_decomposition_via_neural_diffusionreaction_processes"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1153/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325750882, "odate": 1698949697506, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "financial_support"}, {"name": "reviewer_nomination"}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "u6Ibs4hTJH", "number": 1121, "cdate": 1682788759746, "tcdate": 1682788759746, "mdate": 1698949697340, "tmdate": 1698949697340, "signatures": ["NeurIPS.cc/2023/Conference/Submission1121/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1121/Authors"], "forum": "u6Ibs4hTJH", "content": {"title": {"value": "Real-World Image Variation by Aligning Diffusion Inversion Chain"}, "authors": {"value": ["Yuechen ZHANG", "Jinbo Xing", "Eric Lo", "Jiaya Jia"]}, "authorids": {"value": ["~Yuechen_ZHANG1", "~Jinbo_Xing1", "~Eric_Lo1", "~Jiaya_Jia1"]}, "keywords": {"value": ["image variation", "diffusion model", "image generation", "text-driven image editing"]}, "abstract": {"value": "Recent diffusion model advancements have enabled high-fidelity images to be generated using text prompts. However, a domain gap exists between generated images and real-world images, which poses a challenge in generating high-quality variations of real-world images. Our investigation uncovers that this domain gap originates from a latents' distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Our pipeline enhances the generation quality of image variations by aligning the image generation process to the source image's inversion chain. \nSpecifically, we demonstrate that step-wise latent distribution alignment is essential for generating high-quality variations. \nTo attain this, we design a cross-image self-attention injection for feature interaction and a step-wise distribution normalization to align the latent features. Incorporating these alignment processes into a diffusion model allows RIVAL to generate high-quality image variations without further parameter optimization. Our experimental results demonstrate that our proposed approach outperforms existing methods concerning semantic similarity and perceptual quality. This generalized inference pipeline can be easily applied to other diffusion-based generation tasks, such as image-conditioned text-to-image generation and stylization. Project page: https://rival-diff.github.io"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "RIVAL is a novel approach that uses diffusion models to generate high-quality image variations by aligning the image generation process to the source image's inversion chain."}, "pdf": {"value": "/pdf/553cfb6472aacf2835a5e8c988fec812446f2360.pdf"}, "supplementary_material": {"value": "/attachment/a4ddd773166fadce6f6f734fd28d98d9fbfab736.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023realworld,\ntitle={Real-World Image Variation by Aligning Diffusion Inversion Chain},\nauthor={Yuechen ZHANG and Jinbo Xing and Eric Lo and Jiaya Jia},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=u6Ibs4hTJH}\n}"}, "paperhash": {"value": "zhang|realworld_image_variation_by_aligning_diffusion_inversion_chain"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1121/-/Revision", "NeurIPS.cc/2023/Conference/Submission1121/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1121/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325750171, "odate": 1698949697326, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "WPdGRRJaPb", "number": 1091, "cdate": 1682759214109, "tcdate": 1682759214109, "mdate": 1698949697301, "tmdate": 1698949697301, "signatures": ["NeurIPS.cc/2023/Conference/Submission1091/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1091/Authors"], "forum": "WPdGRRJaPb", "content": {"title": {"value": "Full-Atom Protein Pocket Design via Iterative Refinement"}, "authors": {"value": ["ZAIXI ZHANG", "Zepu Lu", "Zhongkai Hao", "Marinka Zitnik", "Qi Liu"]}, "authorids": {"value": ["~ZAIXI_ZHANG2", "~Zepu_Lu1", "~Zhongkai_Hao1", "~Marinka_Zitnik1", "~Qi_Liu3"]}, "keywords": {"value": ["Graph Representation Learning", "AI for Science"]}, "abstract": {"value": "The design of \\emph{de novo} functional proteins that bind with specific ligand molecules is crucial in various domains like therapeutics and bio-engineering. One vital yet challenging step is to design the protein pocket, the cavity region of protein where the ligand binds with. Existing methods suffer from inefficient generation, insufficient context modeling (ligand molecule), and incapability of generating sidechain atoms. To overcome the limitations, we propose a \\textbf{F}ull-\\textbf{A}tom \\textbf{I}terative \\textbf{R}efinement framework (\\textbf{FAIR}) for protein pocket sequence (i.e., residue types) and 3D structure co-design. Generally, FAIR consists of two steps that follow a coarse-to-fine pipeline (backbone atoms to full atoms including sidechain) for full-atom generation. For efficiency, all residue types and structures are updated together in each round (i.e., full-shot refinement). In the first step, the residue types and backbone coordinates are updated with a hierarchical context encoder and two structure refinement modules capturing inter-residue and pocket-ligand interactions. The second step further models the sidechain atoms of pockets and updates residue types to achieve sequence-structure consistency. The structure of the binding ligand is also updated along with the above refinement iterations accounting for its flexibility. Finally, extensive evaluations show\nthat FAIR outperforms baselines in efficiently designing high-quality pocket sequences and structures. Specifically, the average improvements on AAR and RMSD are over 10$\\%$."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/648cfaa79fc68534e9f3584841174e483a673695.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhang2023fullatom,\ntitle={Full-Atom Protein Pocket Design via Iterative Refinement},\nauthor={ZAIXI ZHANG and Zepu Lu and Zhongkai Hao and Marinka Zitnik and Qi Liu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=WPdGRRJaPb}\n}"}, "paperhash": {"value": "zhang|fullatom_protein_pocket_design_via_iterative_refinement"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1091/-/Revision", "NeurIPS.cc/2023/Conference/Submission1091/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1091/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325749417, "odate": 1698949697284, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "iT9MOAZqsb", "number": 1078, "cdate": 1682748311198, "tcdate": 1682748311198, "mdate": 1698949697262, "tmdate": 1698949697262, "signatures": ["NeurIPS.cc/2023/Conference/Submission1078/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1078/Authors"], "forum": "iT9MOAZqsb", "content": {"title": {"value": "Adversarial Training from Mean Field Perspective"}, "authors": {"value": ["Soichiro Kumano", "Hiroshi Kera", "Toshihiko Yamasaki"]}, "authorids": {"value": ["~Soichiro_Kumano1", "~Hiroshi_Kera1", "~Toshihiko_Yamasaki1"]}, "keywords": {"value": ["adversarial training; mean field theory"]}, "TLDR": {"value": "We propose a new framework based on mean field theory and analyze adversarial training theoretically."}, "abstract": {"value": "Although adversarial training is known to be effective against adversarial examples, training dynamics are not well understood. In this study, we present the first theoretical analysis of adversarial training in random deep neural networks without any assumptions on data distributions. We introduce a new theoretical framework based on mean field theory, which addresses the limitations of existing mean field-based approaches. Based on the framework, we derive the (empirically tight) upper bounds of $\\ell_q$ norm-based adversarial loss with $\\ell_p$ norm-based adversarial examples for various values of $p$ and $q$. Moreover, we prove that networks without shortcuts are generally not adversarially trainable and that adversarial training reduces network capacity. We also show that the network width alleviates these issues. Furthermore, the various impacts of input and output dimensions on the upper bounds and time evolution of weight variance are presented."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/770124aaee18ba1d0dc548eb747316a912678f20.pdf"}, "_bibtex": {"value": "@inproceedings{\nkumano2023adversarial,\ntitle={Adversarial Training from Mean Field Perspective},\nauthor={Soichiro Kumano and Hiroshi Kera and Toshihiko Yamasaki},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=iT9MOAZqsb}\n}"}, "paperhash": {"value": "kumano|adversarial_training_from_mean_field_perspective"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1078/-/Revision", "NeurIPS.cc/2023/Conference/Submission1078/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1078/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325749233, "odate": 1698949697250, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "dZqcC1qCmB", "number": 1056, "cdate": 1682712694185, "tcdate": 1682712694185, "mdate": 1698949696975, "tmdate": 1698949696975, "signatures": ["NeurIPS.cc/2023/Conference/Submission1056/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1056/Authors"], "forum": "dZqcC1qCmB", "content": {"title": {"value": "Epistemic Neural Networks"}, "authors": {"value": ["Ian Osband", "Zheng Wen", "Seyed Mohammad Asghari", "Vikranth Dwaracherla", "Morteza Ibrahimi", "Xiuyuan Lu", "Benjamin Van Roy"]}, "authorids": {"value": ["~Ian_Osband1", "~Zheng_Wen1", "~Seyed_Mohammad_Asghari1", "~Vikranth_Dwaracherla1", "~Morteza_Ibrahimi2", "~Xiuyuan_Lu1", "~Benjamin_Van_Roy3"]}, "keywords": {"value": ["Uncertainty", "Deep Learning", "Neural Networks"]}, "TLDR": {"value": "Neural networks that know what they don't know, but are not necessarily Bayesian."}, "abstract": {"value": "Intelligence relies on an agent's knowledge of what it does not know.\nThis capability can be assessed based on the quality of joint predictions of labels across multiple inputs.\nIn principle, ensemble-based approaches can produce effective joint predictions, but the computational costs of large ensembles become prohibitive.\nWe introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty.\nWith an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation.\nThe epinet does not fit the traditional framework of Bayesian neural networks.\nTo accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as a general interface for models that produce joint predictions."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/86494b2c599f77486acf1aa4e34ba2fcf5224dac.pdf"}, "_bibtex": {"value": "@inproceedings{\nosband2023epistemic,\ntitle={Epistemic Neural Networks},\nauthor={Ian Osband and Zheng Wen and Seyed Mohammad Asghari and Vikranth Dwaracherla and Morteza Ibrahimi and Xiuyuan Lu and Benjamin Van Roy},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=dZqcC1qCmB}\n}"}, "paperhash": {"value": "osband|epistemic_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1056/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325748485, "odate": 1698949696957, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "financial_support"}, {"name": "reviewer_nomination"}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "aLLuYpn83y", "number": 1051, "cdate": 1682703681956, "tcdate": 1682703681956, "mdate": 1698949696974, "tmdate": 1698949696974, "signatures": ["NeurIPS.cc/2023/Conference/Submission1051/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission1051/Authors"], "forum": "aLLuYpn83y", "content": {"title": {"value": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"}, "authors": {"value": ["Kenneth Li", "Oam Patel", "Fernanda Vi\u00e9gas", "Hanspeter Pfister", "Martin Wattenberg"]}, "authorids": {"value": ["~Kenneth_Li1", "~Oam_Patel1", "~Fernanda_Vi\u00e9gas1", "~Hanspeter_Pfister1", "~Martin_Wattenberg1"]}, "keywords": {"value": ["Large Language Model", "AI Safety"]}, "abstract": {"value": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the \"truthfulness\" of large language models (LLMs). ITI operates by shifting model activations during inference, following a learned set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from $32.5\\%$ to $65.1\\%$. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We introduce Inference-Time Intervention (ITI), a light-weight technique that enhances the truthfulness of large language models (LLMs)."}, "pdf": {"value": "/pdf/f8231bfc5b98fc20e2532901e878e7858d1c0970.pdf"}, "supplementary_material": {"value": "/attachment/083a1de25d25e7d19a4c3eff1ea4c2291a1d924a.pdf"}, "_bibtex": {"value": "@inproceedings{\nli2023inferencetime,\ntitle={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},\nauthor={Kenneth Li and Oam Patel and Fernanda Vi{\\'e}gas and Hanspeter Pfister and Martin Wattenberg},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=aLLuYpn83y}\n}"}, "paperhash": {"value": "li|inferencetime_intervention_eliciting_truthful_answers_from_a_language_model"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission1051/-/Revision", "NeurIPS.cc/2023/Conference/Submission1051/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission1051/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325748375, "odate": 1698949696895, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "vtLNwa6uX0", "number": 940, "cdate": 1682609522688, "tcdate": 1682609522688, "mdate": 1698949696237, "tmdate": 1698949696237, "signatures": ["NeurIPS.cc/2023/Conference/Submission940/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission940/Authors"], "forum": "vtLNwa6uX0", "content": {"title": {"value": "The Geometry of Neural Nets' Parameter Spaces Under Reparametrization"}, "authors": {"value": ["Agustinus Kristiadi", "Felix Dangel", "Philipp Hennig"]}, "authorids": {"value": ["~Agustinus_Kristiadi1", "~Felix_Dangel1", "~Philipp_Hennig1"]}, "keywords": {"value": ["neural network", "invariance", "equivariance", "reparametrization", "riemannian geometry", "parameter space"]}, "abstract": {"value": "Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way to improve the training of neural nets. But it can also be problematic since it can induce inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes of probability densities. This complicates downstream analyses: e.g. one cannot definitively relate flatness with generalization since arbitrary reparametrization changes their relationship. In this work, we study the invariance of neural nets under reparametrization from the perspective of Riemannian geometry. From this point of view, invariance is an inherent property of any neural net _if_ one explicitly represents the metric and uses the correct associated transformation rules. This is important since although the metric is always present, it is often implicitly assumed as identity, and thus dropped from the notation, then lost under reparametrization. We discuss implications for measuring the flatness of minima, optimization, and for probability-density maximization. Finally, we explore some interesting directions where invariance is useful."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/ccc5769019b3f8058d38890bfb071b60f19991cf.pdf"}, "supplementary_material": {"value": "/attachment/47246039bf5b108df6975a8525fbc542aa839dfb.pdf"}, "_bibtex": {"value": "@inproceedings{\nkristiadi2023the,\ntitle={The Geometry of Neural Nets' Parameter Spaces Under Reparametrization},\nauthor={Agustinus Kristiadi and Felix Dangel and Philipp Hennig},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vtLNwa6uX0}\n}"}, "paperhash": {"value": "kristiadi|the_geometry_of_neural_nets_parameter_spaces_under_reparametrization"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission940/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission940/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325745660, "odate": 1698949696223, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "XeMryhpniy", "number": 887, "cdate": 1682585057262, "tcdate": 1682585057262, "mdate": 1698949695840, "tmdate": 1698949695840, "signatures": ["NeurIPS.cc/2023/Conference/Submission887/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission887/Authors"], "forum": "XeMryhpniy", "content": {"title": {"value": "Hierarchical Integration Diffusion Model for Realistic Image Deblurring"}, "authors": {"value": ["Zheng Chen", "Yulun Zhang", "Ding Liu", "Bin Xia", "Jinjin Gu", "Linghe Kong", "Xin Yuan"]}, "authorids": {"value": ["~Zheng_Chen11", "~Yulun_Zhang1", "~Ding_Liu6", "~Bin_Xia1", "~Jinjin_Gu1", "~Linghe_Kong1", "~Xin_Yuan4"]}, "keywords": {"value": ["image deblurring", "diffusion model"]}, "abstract": {"value": "Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising performance, particularly in terms of details reconstruction. However, the diffusion model requires a large number of inference iterations to recover the clean image from pure Gaussian noise, which consumes massive computational resources. Moreover, the distribution synthesized by the diffusion model is often misaligned with the target results, leading to restrictions in distortion-based metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted latent space to generate the prior feature for the deblurring process. The deblurring process is implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and trained models are available at https://github.com/zhengchen1999/HI-Diff."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1d52451a3690f62b18528eedcb064098ba8fd3ec.pdf"}, "supplementary_material": {"value": "/attachment/49ed8aea4dbdb6d628df22d86b0a003a14350caf.pdf"}, "_bibtex": {"value": "@inproceedings{\nchen2023hierarchical,\ntitle={Hierarchical Integration Diffusion Model for Realistic Image Deblurring},\nauthor={Zheng Chen and Yulun Zhang and Ding Liu and Bin Xia and Jinjin Gu and Linghe Kong and Xin Yuan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=XeMryhpniy}\n}"}, "TLDR": {"value": "Hierarchical integrate generative model (Diffusion Model) and regression-based model (Transformer) for realistic image deblurring"}, "paperhash": {"value": "chen|hierarchical_integration_diffusion_model_for_realistic_image_deblurring"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission887/-/Revision", "NeurIPS.cc/2023/Conference/Submission887/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission887/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325744379, "odate": 1698949695820, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "1IOU2329Za", "number": 862, "cdate": 1682567450781, "tcdate": 1682567450781, "mdate": 1698949695711, "tmdate": 1698949695711, "signatures": ["NeurIPS.cc/2023/Conference/Submission862/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission862/Authors"], "forum": "1IOU2329Za", "content": {"title": {"value": "Banana: Banach Fixed-Point Network for Pointcloud Segmentation with Inter-Part Equivariance"}, "authors": {"value": ["Congyue Deng", "Jiahui Lei", "Bokui Shen", "Kostas Daniilidis", "Leonidas Guibas"]}, "authorids": {"value": ["~Congyue_Deng1", "~Jiahui_Lei1", "~Bokui_Shen1", "~Kostas_Daniilidis1", "~Leonidas_Guibas1"]}, "keywords": {"value": ["3D deep learning", "equivariant network", "pointcloud segmentation", "multi-body system"]}, "abstract": {"value": "Equivariance has gained strong interest as a desirable network property that inherently ensures robust generalization. However, when dealing with complex systems such as articulated objects or multi-object scenes, effectively capturing inter-part transformations poses a challenge, as it becomes entangled with the overall structure and local transformations. The interdependence of part assignment and per-part group action necessitates a novel equivariance formulation that allows for their co-evolution. In this paper, we present Banana, a Banach fixed-point network for equivariant segmentation with inter-part equivariance by construction. Our key insight is to iteratively solve a fixed-point problem, where point-part assignment labels and per-part SE(3)-equivariance co-evolve simultaneously. We provide theoretical derivations of both per-step equivariance and global convergence, which induces an equivariant final convergent state. Our formulation naturally provides a strict definition of inter-part equivariance that generalizes to unseen inter-part configurations. Through experiments conducted on both articulated objects and multi-object scans, we demonstrate the efficacy of our approach in achieving strong generalization under inter-part transformations, even when confronted with substantial changes in pointcloud geometry and topology."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/dcd887d591f2440bf98d371e613ffb318fe5fc84.pdf"}, "supplementary_material": {"value": "/attachment/b299aa60669ad44fccd7f456600bb5de2ea1639f.pdf"}, "_bibtex": {"value": "@inproceedings{\ndeng2023banana,\ntitle={Banana: Banach Fixed-Point Network for Pointcloud Segmentation with Inter-Part Equivariance},\nauthor={Congyue Deng and Jiahui Lei and Bokui Shen and Kostas Daniilidis and Leonidas Guibas},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=1IOU2329Za}\n}"}, "paperhash": {"value": "deng|banana_banach_fixedpoint_network_for_pointcloud_segmentation_with_interpart_equivariance"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission862/-/Revision", "NeurIPS.cc/2023/Conference/Submission862/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission862/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325743802, "odate": 1698949695695, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "WjWifKqmcG", "number": 848, "cdate": 1682553551823, "tcdate": 1682553551823, "mdate": 1698949695640, "tmdate": 1698949695640, "signatures": ["NeurIPS.cc/2023/Conference/Submission848/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission848/Authors"], "forum": "WjWifKqmcG", "content": {"title": {"value": "Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching"}, "authors": {"value": ["Junsheng Zhou", "Baorui Ma", "Wenyuan Zhang", "Yi Fang", "Yu-Shen Liu", "Zhizhong Han"]}, "authorids": {"value": ["~Junsheng_Zhou3", "~Baorui_Ma1", "~Wenyuan_Zhang1", "~Yi_Fang2", "~Yu-Shen_Liu1", "~Zhizhong_Han2"]}, "keywords": {"value": ["LiDAR Point Clouds", "2D images", "Cross-modality registration", "Matching"]}, "abstract": {"value": "Cross-modality registration between 2D images captured by cameras and 3D point clouds from LiDARs is a crucial task in computer vision and robotic. Previous methods estimate 2D-3D correspondences by matching point and pixel patterns learned by neural networks, and use Perspective-n-Points (PnP) to estimate rigid transformation during post-processing. However, these methods struggle to map points and pixels to a shared latent space robustly since points and pixels have very different characteristics with patterns learned in different manners (MLP and CNN), and they also fail to construct supervision directly on the transformation since the PnP is non-differentiable, which leads to unstable registration results. To address these problems, we propose to learn a structured cross-modality latent space to represent pixel features and 3D features via a differentiable probabilistic PnP solver. Specifically, we design a triplet network to learn VoxelPoint-to-Pixel matching, where we represent 3D elements using both voxels and points to learn the cross-modality latent space with pixels. We design both the voxel and pixel branch based on CNNs to operate convolutions on voxels/pixels represented in grids, and integrate an additional point branch to regain the information lost during voxelization. We train our framework end-to-end by imposing supervisions directly on the predicted pose distribution with a probabilistic PnP solver. To explore distinctive patterns of cross-modality features, we design a novel loss with adaptive-weighted optimization for cross-modality feature description. The experimental results on KITTI and nuScenes datasets show significant improvements over the state-of-the-art methods."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1b0cc4e7c98315e52ed7a5dd8fd0cbaabab0d0ff.pdf"}, "supplementary_material": {"value": "/attachment/3df1f16f13058ef12f31ccf94a313ed87fc2b0a0.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhou2023differentiable,\ntitle={Differentiable Registration of Images and Li{DAR} Point Clouds with VoxelPoint-to-Pixel Matching},\nauthor={Junsheng Zhou and Baorui Ma and Wenyuan Zhang and Yi Fang and Yu-Shen Liu and Zhizhong Han},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=WjWifKqmcG}\n}"}, "paperhash": {"value": "zhou|differentiable_registration_of_images_and_lidar_point_clouds_with_voxelpointtopixel_matching"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission848/-/Revision", "NeurIPS.cc/2023/Conference/Submission848/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission848/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325743557, "odate": 1698949695625, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "fpzA8uRA95", "number": 824, "cdate": 1682524442285, "tcdate": 1682524442285, "mdate": 1698949695362, "tmdate": 1698949695362, "signatures": ["NeurIPS.cc/2023/Conference/Submission824/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission824/Authors"], "forum": "fpzA8uRA95", "content": {"title": {"value": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"}, "authors": {"value": ["Xilie Xu", "Jingfeng Zhang", "Feng Liu", "Masashi Sugiyama", "Mohan Kankanhalli"]}, "authorids": {"value": ["~Xilie_Xu1", "~Jingfeng_Zhang1", "~Feng_Liu2", "~Masashi_Sugiyama1", "~Mohan_Kankanhalli1"]}, "keywords": {"value": ["robust pre-training", "adversarial contrastive learning", "coreset selection"]}, "abstract": {"value": "Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our comprehensive results corroborate that RCS can speed up ACL by a large margin without significantly hurting the robustness transferability. Notably, to the best of our knowledge, we are the first to conduct ACL efficiently on the large-scale ImageNet-1K dataset to obtain an effective robust representation via RCS. Our source code is at https://github.com/GodXuxilie/Efficient_ACL_via_RCS."}, "pdf": {"value": "/pdf/7ae96615a2080be5d50757300f286d1ee32772ba.pdf"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nxu2023efficient,\ntitle={Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection},\nauthor={Xilie Xu and Jingfeng Zhang and Feng Liu and Masashi Sugiyama and Mohan Kankanhalli},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=fpzA8uRA95}\n}"}, "paperhash": {"value": "xu|efficient_adversarial_contrastive_learning_via_robustnessaware_coreset_selection"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission824/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission824/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325743002, "odate": 1698949695349, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "QwvaqV48fB", "number": 802, "cdate": 1682507777297, "tcdate": 1682507777297, "mdate": 1699258851233, "tmdate": 1699258851233, "signatures": ["NeurIPS.cc/2023/Conference/Submission802/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission802/Authors"], "forum": "QwvaqV48fB", "content": {"title": {"value": "Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends"}, "authors": {"value": ["Wang Xinrui", "wan wenhai", "Chuanxing Geng", "Shao-Yuan Li", "Songcan Chen"]}, "authorids": {"value": ["~Wang_Xinrui1", "~wan_wenhai2", "~Chuanxing_Geng1", "~Shao-Yuan_Li1", "~Songcan_Chen1"]}, "keywords": {"value": ["positive and unlabeled learning", "machine learning", "deep learning", "temporal point process", "data imbalance"]}, "TLDR": {"value": "Learning from Positive and Unlabeled Data through Holistic Predictive Trends"}, "abstract": {"value": "Learning binary classifiers from positive and unlabeled data (PUL) is vital in many real-world applications, especially when verifying negative examples is difficult. Despite the impressive empirical performance of recent PUL methods, challenges like accumulated errors and increased estimation bias persist due to the absence of negative labels. In this paper, we unveil an intriguing yet long-overlooked observation in PUL: \\textit{resampling the positive data in each training iteration to ensure a balanced distribution between positive and unlabeled examples results in strong early-stage performance. Furthermore, predictive trends for positive and negative classes display distinctly different patterns.} Specifically, the scores (output probability) of unlabeled negative examples consistently decrease, while those of unlabeled positive examples show largely chaotic trends. Instead of focusing on classification within individual time frames, we innovatively adopt a holistic approach, interpreting the scores of each example as a temporal point process (TPP). This reformulates the core problem of PUL as recognizing trends in these scores. We then propose a novel TPP-inspired measure for trend detection and prove its asymptotic unbiasedness in predicting changes. Notably, our method accomplishes PUL without requiring additional parameter tuning or prior assumptions, offering an alternative perspective for tackling this problem. Extensive experiments verify the superiority of our method, particularly in a highly imbalanced real-world setting, where it achieves improvements of up to $11.3\\%$ in key metrics."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5d167e5d8563268cbebba6ad650708358a5407db.pdf"}, "supplementary_material": {"value": "/attachment/828c8cafd8d9db9cbf09fcfd0bedfdebd0455d7c.zip"}, "_bibtex": {"value": "@inproceedings{\nxinrui2023beyond,\ntitle={Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends},\nauthor={Wang Xinrui and wan wenhai and Chuanxing Geng and Shao-Yuan Li and Songcan Chen},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=QwvaqV48fB}\n}"}, "paperhash": {"value": "xinrui|beyond_myopia_learning_from_positive_and_unlabeled_data_through_holistic_predictive_trends"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission802/-/Revision", "NeurIPS.cc/2023/Conference/Submission802/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission802/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325742292, "odate": 1698949695163, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cnpkzQZaLU", "number": 743, "cdate": 1682440231152, "tcdate": 1682440231152, "mdate": 1698949694809, "tmdate": 1698949694809, "signatures": ["NeurIPS.cc/2023/Conference/Submission743/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission743/Authors"], "forum": "cnpkzQZaLU", "content": {"title": {"value": "Context-PIPs: Persistent Independent Particles Demands Context Features"}, "authors": {"value": ["Weikang BIAN", "Zhaoyang Huang", "Xiaoyu Shi", "Yitong Dong", "Yijin Li", "Hongsheng Li"]}, "authorids": {"value": ["~Weikang_BIAN1", "~Zhaoyang_Huang2", "~Xiaoyu_Shi1", "~Yitong_Dong1", "~Yijin_Li1", "~Hongsheng_Li3"]}, "keywords": {"value": ["Optical Flow; Video Correspondence; Computer Vision;"]}, "TLDR": {"value": "Tracking any point in videos demands spatial context features"}, "abstract": {"value": "We tackle the problem of Persistent Independent Particles (PIPs), also called Tracking Any Point (TAP), in videos, which specifically aims at estimating persistent long-term trajectories of query points in videos. Previous methods attempted to estimate these trajectories independently to incorporate longer image sequences, therefore, ignoring the potential benefits of incorporating spatial context features. \nWe argue that independent video point tracking also demands spatial context features.\nTo this end, we propose a novel framework Context-PIPs, which effectively improves point trajectory accuracy by aggregating spatial context features in videos. Context-PIPs contains two main modules: 1) a SOurse Feature Enhancement (SOFE) module, and 2) a TArget Feature Aggregation (TAFA) module. Context-PIPs significantly improves PIPs all-sided, reducing 11.4\\% Average Trajectory Error of Occluded Points (ATE-Occ) on CroHD and increasing 11.8\\% Average Percentage of Correct Keypoint (A-PCK) on TAP-Vid-Kinectics. Demos are available at \\url{https://wkbian.github.io/Projects/Context-PIPs/}."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3af8e986edabaca13827c6bcbbebf4b51abc4917.pdf"}, "_bibtex": {"value": "@inproceedings{\nbian2023contextpips,\ntitle={Context-{PIP}s: Persistent Independent Particles Demands Context Features},\nauthor={Weikang BIAN and Zhaoyang Huang and Xiaoyu Shi and Yitong Dong and Yijin Li and Hongsheng Li},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cnpkzQZaLU}\n}"}, "paperhash": {"value": "bian|contextpips_persistent_independent_particles_demands_context_features"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission743/-/Revision", "NeurIPS.cc/2023/Conference/Submission743/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission743/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325740928, "odate": 1698949694794, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Jw0KRTjsGA", "number": 706, "cdate": 1682413121301, "tcdate": 1682413121301, "mdate": 1698949694573, "tmdate": 1698949694573, "signatures": ["NeurIPS.cc/2023/Conference/Submission706/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission706/Authors"], "forum": "Jw0KRTjsGA", "content": {"title": {"value": "CODA: Generalizing to Open and Unseen Domains with Compaction and Disambiguation"}, "authors": {"value": ["Chaoqi Chen", "Luyao Tang", "Yue Huang", "Xiaoguang Han", "Yizhou Yu"]}, "authorids": {"value": ["~Chaoqi_Chen2", "~Luyao_Tang1", "~Yue_Huang1", "~Xiaoguang_Han2", "~Yizhou_Yu1"]}, "keywords": {"value": ["Domain generalization", "domain shift", "open class", "source compaction", "target disambiguation"]}, "abstract": {"value": "The generalization capability of machine learning systems degenerates notably when the test distribution drifts from the training distribution. Recently, Domain Generalization (DG) has been gaining momentum in enabling machine learning models to generalize to unseen domains. However, most DG methods assume that training and test data share an identical label space, ignoring the potential unseen categories in many real-world applications. In this paper, we delve into a more general but difficult problem termed Open Test-Time DG (OTDG), where both domain shift and open class may occur on the unseen test data. We propose Compaction and Disambiguation (CODA), a novel two-stage framework for learning compact representations and adapting to open classes in the wild. To meaningfully regularize the model's decision boundary, CODA introduces virtual unknown classes and optimizes a new training objective to insert unknowns into the latent space by compacting the embedding space of source known classes. To adapt target samples to the source model, we then disambiguate the decision boundaries between known and unknown classes with a test-time training objective, mitigating the adaptivity gap and catastrophic forgetting challenges. Experiments reveal that CODA can significantly outperform the previous best method on standard DG datasets and harmonize the classification accuracy between known and unknown classes."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "We propose a principled framework (CODA) for a new and challenging domain generalization setting where both domain shift and open class occur on test data."}, "pdf": {"value": "/pdf/0332a85638804b0a87f63416de213d9efafe278f.pdf"}, "supplementary_material": {"value": "/attachment/4fd417fc2d1bba0f91f6ad0765313e57e7703ffe.pdf"}, "_bibtex": {"value": "@inproceedings{\nchen2023coda,\ntitle={{CODA}: Generalizing to Open and Unseen Domains with Compaction and Disambiguation},\nauthor={Chaoqi Chen and Luyao Tang and Yue Huang and Xiaoguang Han and Yizhou Yu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Jw0KRTjsGA}\n}"}, "paperhash": {"value": "chen|coda_generalizing_to_open_and_unseen_domains_with_compaction_and_disambiguation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission706/-/Revision", "NeurIPS.cc/2023/Conference/Submission706/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission706/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325739994, "odate": 1698949694560, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "vtoY8qJjTR", "number": 703, "cdate": 1682412234641, "tcdate": 1682412234641, "mdate": 1698949694508, "tmdate": 1698949694508, "signatures": ["NeurIPS.cc/2023/Conference/Submission703/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission703/Authors"], "forum": "vtoY8qJjTR", "content": {"title": {"value": "Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning"}, "authors": {"value": ["Shenzhi Wang", "Qisen Yang", "Jiawei Gao", "Matthieu Gaetan Lin", "HAO CHEN", "Liwei Wu", "Ning Jia", "Shiji Song", "Gao Huang"]}, "authorids": {"value": ["~Shenzhi_Wang1", "~Qisen_Yang1", "~Jiawei_Gao1", "~Matthieu_Gaetan_Lin1", "~HAO_CHEN41", "~Liwei_Wu5", "~Ning_Jia1", "~Shiji_Song1", "~Gao_Huang1"]}, "keywords": {"value": ["reinforcement learning", "offline-to-online reinforcement learning", "offline reinforcement learning", "policy improvement", "policy constraint"]}, "abstract": {"value": "Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a balance model to select a suitable policy for each state. Theoretically, we prove that state-adaptive balances are necessary for achieving a higher policy performance upper bound. Empirically, extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark. Codes are available at https://github.com/LeapLabTHU/FamO2O."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/f8d3b15d72a1aec1b6f804a2f203a947d9d5a1cd.pdf"}, "TLDR": {"value": "FamO2O, a novel plug-in framework for offline-to-online reinforcement learning, effectively determines state-adaptive improvement-constraint balances, resulting in significant performance improvements on the D4RL benchmark."}, "_bibtex": {"value": "@inproceedings{\nwang2023train,\ntitle={Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning},\nauthor={Shenzhi Wang and Qisen Yang and Jiawei Gao and Matthieu Gaetan Lin and HAO CHEN and Liwei Wu and Ning Jia and Shiji Song and Gao Huang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vtoY8qJjTR}\n}"}, "paperhash": {"value": "wang|train_once_get_a_family_stateadaptive_balances_for_offlinetoonline_reinforcement_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission703/-/Revision", "NeurIPS.cc/2023/Conference/-/PC_Revision", "NeurIPS.cc/2023/Conference/Submission703/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission703/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325739879, "odate": 1698949694492, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "UAow2kPsYP", "number": 686, "cdate": 1682407533863, "tcdate": 1682407533863, "mdate": 1698949694370, "tmdate": 1698949694370, "signatures": ["NeurIPS.cc/2023/Conference/Submission686/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission686/Authors"], "forum": "UAow2kPsYP", "content": {"title": {"value": "A Unified Generalization Analysis of Re-Weighting and Logit-Adjustment for Imbalanced Learning"}, "authors": {"value": ["Zitai Wang", "Qianqian Xu", "Zhiyong Yang", "Yuan He", "Xiaochun Cao", "Qingming Huang"]}, "authorids": {"value": ["~Zitai_Wang1", "~Qianqian_Xu2", "~Zhiyong_Yang1", "~Yuan_He2", "~Xiaochun_Cao3", "~Qingming_Huang1"]}, "keywords": {"value": ["Imbalanced Learning", "Re-weighting", "Logit Adjustment", "Genralization Analysis"]}, "abstract": {"value": "Real-world datasets are typically imbalanced in the sense that only a few classes have numerous samples, while many classes are associated with only a few samples. As a result, a naive ERM learning process will be biased towards the majority classes, making it difficult to generalize to the minority classes. To address this issue, one simple but effective approach is to modify the loss function to emphasize the learning on minority classes, such as re-weighting the losses or adjusting the logits via class-dependent terms. However, existing generalization analysis of such losses is still coarse-grained and fragmented, failing to explain some empirical results. To bridge this gap between theory and practice, we propose a novel technique named data-dependent contraction to capture how these modified losses handle different classes. On top of this technique, a fine-grained generalization bound is established for imbalanced learning, which helps reveal the mystery of re-weighting and logit-adjustment in a unified manner. Furthermore, a principled learning algorithm is developed based on the theoretical insights. Finally, the empirical results on benchmark datasets not only validate the theoretical results but also demonstrate the effectiveness of the proposed method."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/5058158cc4bb101095eef09a1e9447749c4f25a2.pdf"}, "supplementary_material": {"value": "/attachment/d077a5ee547db516aceb662fc2cb49d57bb2079b.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023a,\ntitle={A Unified Generalization Analysis of Re-Weighting and Logit-Adjustment for Imbalanced Learning},\nauthor={Zitai Wang and Qianqian Xu and Zhiyong Yang and Yuan He and Xiaochun Cao and Qingming Huang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=UAow2kPsYP}\n}"}, "paperhash": {"value": "wang|a_unified_generalization_analysis_of_reweighting_and_logitadjustment_for_imbalanced_learning"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission686/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission686/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325739481, "odate": 1698949694357, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "8IvW2k5VeA", "number": 643, "cdate": 1682389266614, "tcdate": 1682389266614, "mdate": 1698949694205, "tmdate": 1698949694205, "signatures": ["NeurIPS.cc/2023/Conference/Submission643/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission643/Authors"], "forum": "8IvW2k5VeA", "content": {"title": {"value": "Exploring Loss Functions for Time-based Training Strategy in Spiking Neural Networks"}, "authors": {"value": ["Yaoyu Zhu", "Wei Fang", "Xiaodong Xie", "Tiejun Huang", "Zhaofei Yu"]}, "authorids": {"value": ["~Yaoyu_Zhu1", "~Wei_Fang2", "~Xiaodong_Xie1", "~Tiejun_Huang1", "~Zhaofei_Yu1"]}, "keywords": {"value": ["Spiking neural networks", "Spike encoding", "Time-based training"]}, "abstract": {"value": "Spiking Neural Networks (SNNs) are considered promising brain-inspired energy-efficient models due to their event-driven computing paradigm.\nThe spatiotemporal spike patterns used to convey information in SNNs consist of both rate coding and temporal coding, where the temporal coding is crucial to biological-plausible learning rules such as spike-timing-dependent-plasticity.\nThe time-based training strategy is proposed to better utilize the temporal information in SNNs and learn in an asynchronous fashion.\nHowever, some recent works train SNNs by the time-based scheme with rate-coding-dominated loss functions.\nIn this paper, we first map rate-based loss functions to time-based counterparts and explain why they are also applicable to the time-based training scheme.\nAfter that, we infer that loss functions providing adequate positive overall gradients help training by theoretical analysis.\nBased on this, we propose the enhanced counting loss to replace the commonly used mean square counting loss.\nIn addition, we transfer the training of scale factor in weight standardization into thresholds.\nExperiments show that our approach outperforms previous time-based training methods in most datasets. \nOur work provides insights for training SNNs with time-based schemes and offers a fresh perspective on the correlation between rate coding and temporal coding.\nOur code is available at https://github.com/zhuyaoyu/SNN-temporal-training-losses."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/a67c3ae6dd02ffd67d7c77947050c92c418144b0.pdf"}, "supplementary_material": {"value": "/attachment/ca4797b9351dbfeb4ead8e8641bbf14a654bbd01.pdf"}, "_bibtex": {"value": "@inproceedings{\nzhu2023exploring,\ntitle={Exploring Loss Functions for Time-based Training Strategy in Spiking Neural Networks},\nauthor={Yaoyu Zhu and Wei Fang and Xiaodong Xie and Tiejun Huang and Zhaofei Yu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=8IvW2k5VeA}\n}"}, "paperhash": {"value": "zhu|exploring_loss_functions_for_timebased_training_strategy_in_spiking_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission643/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission643/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325738673, "odate": 1698949694190, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "NiQTy0NW1L", "number": 638, "cdate": 1682388002033, "tcdate": 1682388002033, "mdate": 1698949694147, "tmdate": 1698949694147, "signatures": ["NeurIPS.cc/2023/Conference/Submission638/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission638/Authors"], "forum": "NiQTy0NW1L", "content": {"title": {"value": "Lexinvariant Language Models"}, "authors": {"value": ["Qian Huang", "Eric Zelikman", "Sarah Li Chen", "Yuhuai Wu", "Gregory Valiant", "Percy Liang"]}, "authorids": {"value": ["~Qian_Huang2", "~Eric_Zelikman1", "~Sarah_Li_Chen1", "~Yuhuai_Wu1", "~Gregory_Valiant1", "~Percy_Liang1"]}, "keywords": {"value": ["Large Language Model", "in-context learning", "pretraining"]}, "TLDR": {"value": "We study a language model without stable token embedding and show that it converges to standard language model given a sufficiently long context, implicitly performs in-context deciphering, and has better in-context reasoning performance."}, "abstract": {"value": "Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without \\emph{any} fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the \\textit{a priori} identity of any token. To answer this, we study \\textit{lexinvariant}language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gaussian vectors, such that each token maps to the same representation within each sequence but different representations across sequences. Empirically, we demonstrate that it can indeed attain perplexity comparable to that of a standard language model, given a sufficiently long context. We further explore two properties of the lexinvariant language models: First, given text generated from a substitution cipher of English, it implicitly implements Bayesian in-context deciphering and infers the mapping to the underlying real tokens with high accuracy. Second, it has on average 4X better accuracy over synthetic in-context reasoning tasks. Finally, we discuss regularizing standard language models towards lexinvariance and potential practical applications."}, "pdf": {"value": "/pdf/c20f23fbf078b6ada397580eeabf672e8c0eb624.pdf"}, "supplementary_material": {"value": "/attachment/4b00044e1bd6b1dc0fe524d521a338377cef8b00.zip"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nhuang2023lexinvariant,\ntitle={Lexinvariant Language Models},\nauthor={Qian Huang and Eric Zelikman and Sarah Li Chen and Yuhuai Wu and Gregory Valiant and Percy Liang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=NiQTy0NW1L}\n}"}, "paperhash": {"value": "huang|lexinvariant_language_models"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission638/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission638/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325738508, "odate": 1698949694133, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "qd9qcbVAwQ", "number": 614, "cdate": 1682374712282, "tcdate": 1682374712282, "mdate": 1698949693962, "tmdate": 1698949693962, "signatures": ["NeurIPS.cc/2023/Conference/Submission614/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission614/Authors"], "forum": "qd9qcbVAwQ", "content": {"title": {"value": "Parsel\ud83d\udc0d: Algorithmic Reasoning with Language Models by Composing Decompositions"}, "authors": {"value": ["Eric Zelikman", "Qian Huang", "Gabriel Poesia", "Noah Goodman", "Nick Haber"]}, "authorids": {"value": ["~Eric_Zelikman1", "~Qian_Huang2", "~Gabriel_Poesia1", "~Noah_Goodman1", "~Nick_Haber1"]}, "keywords": {"value": ["reasoning", "language models", "code synthesis", "decomposition"]}, "TLDR": {"value": "Language models can solve algorithmic reasoning tasks by decomposing them, solving subparts, and composing them."}, "abstract": {"value": "Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/71fdc840d8db15ba5f93d82efbe8c173c6bfefac.pdf"}, "supplementary_material": {"value": "/attachment/64363b03300c24ccfa1bc3607fd44766b3cbba71.pdf"}, "_bibtex": {"value": "@inproceedings{\nzelikman2023parsel,\ntitle={Parsel\ud83d\udc0d: Algorithmic Reasoning with Language Models by Composing Decompositions},\nauthor={Eric Zelikman and Qian Huang and Gabriel Poesia and Noah Goodman and Nick Haber},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=qd9qcbVAwQ}\n}"}, "paperhash": {"value": "zelikman|parsel_algorithmic_reasoning_with_language_models_by_composing_decompositions"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission614/-/Revision", "NeurIPS.cc/2023/Conference/Submission614/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission614/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325737913, "odate": 1698949693948, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "IL5zJqfxAa", "number": 557, "cdate": 1682328316962, "tcdate": 1682328316962, "mdate": 1698949693656, "tmdate": 1698949693656, "signatures": ["NeurIPS.cc/2023/Conference/Submission557/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission557/Authors"], "forum": "IL5zJqfxAa", "content": {"title": {"value": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"}, "authors": {"value": ["Yao Mu", "Qinglong Zhang", "Mengkang Hu", "Wenhai Wang", "Mingyu Ding", "Jun Jin", "Bin Wang", "Jifeng Dai", "Yu Qiao", "Ping Luo"]}, "authorids": {"value": ["~Yao_Mu1", "~Qinglong_Zhang1", "~Mengkang_Hu1", "~Wenhai_Wang2", "~Mingyu_Ding1", "~Jun_Jin1", "~Bin_Wang12", "~Jifeng_Dai1", "~Yu_Qiao1", "~Ping_Luo2"]}, "keywords": {"value": ["Embodied AI", "Multi-modal Foundation Model", "Embodied Control"]}, "abstract": {"value": "Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments.\nIn this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the \"Chain of Thoughts\" mode for effective embodied planning.\n(ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control.\nExtensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering.\nNotably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/c6fd810b6afce13a0fd3e68501e7443b1cf733c0.pdf"}, "TLDR": {"value": "We introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI."}, "supplementary_material": {"value": "/attachment/f2e0bf80a50e171403948a0ceb7398d88660ed64.pdf"}, "_bibtex": {"value": "@inproceedings{\nmu2023embodiedgpt,\ntitle={Embodied{GPT}: Vision-Language Pre-Training via Embodied Chain of Thought},\nauthor={Yao Mu and Qinglong Zhang and Mengkang Hu and Wenhai Wang and Mingyu Ding and Jun Jin and Bin Wang and Jifeng Dai and Yu Qiao and Ping Luo},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=IL5zJqfxAa}\n}"}, "paperhash": {"value": "mu|embodiedgpt_visionlanguage_pretraining_via_embodied_chain_of_thought"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission557/-/Revision", "NeurIPS.cc/2023/Conference/Submission557/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission557/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325736683, "odate": 1698949693640, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "cx9a4Xvb3l", "number": 439, "cdate": 1682215285167, "tcdate": 1682215285167, "mdate": 1698954266917, "tmdate": 1698954266917, "signatures": ["NeurIPS.cc/2023/Conference/Submission439/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission439/Authors"], "forum": "cx9a4Xvb3l", "content": {"title": {"value": "Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?"}, "authors": {"value": ["Xiaoxiao Sun", "Nidham Gazagnadou", "Vivek Sharma", "Lingjuan Lyu", "Hongdong Li", "Liang Zheng"]}, "authorids": {"value": ["~Xiaoxiao_Sun1", "~Nidham_Gazagnadou1", "~Vivek_Sharma1", "~Lingjuan_Lyu1", "~Hongdong_Li1", "~Liang_Zheng4"]}, "keywords": {"value": ["Privacy Assessment", "Reconstructed Images", "Evaluation Metrics", "Human Perception"]}, "abstract": {"value": "Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used to evaluate model privacy risk under reconstruction attacks. Under these metrics, reconstructed images that are determined to resemble the original one generally indicate more privacy leakage. Images determined as overall dissimilar, on the other hand, indicate higher robustness against attack. However, there is no guarantee that these metrics well reflect human opinions, which offers trustworthy judgement for model privacy leakage.  In this paper, we comprehensively study the faithfulness of these hand-crafted metrics to human perception of privacy information from the reconstructed images. On 5 datasets ranging from natural images, faces, to fine-grained classes, we use 4 existing attack methods to reconstruct images from many different classification models and, for each reconstructed image, we ask multiple human annotators to assess whether this image is recognizable. Our studies reveal that the hand-crafted metrics only have a weak correlation with the human evaluation of privacy leakage and that even these metrics themselves often contradict each other. These observations suggest risks of current metrics  in the community. To address this potential risk, we propose a learning-based measure called SemSim to evaluate the Semantic Similarity between the original and reconstructed images. SemSim is trained with a standard triplet loss, using an original image as an anchor, one of its recognizable reconstructed images as a positive sample, and an unrecognizable one as a negative. By training on human annotations, SemSim exhibits a greater reflection of privacy leakage on the semantic level. We show that SemSim has a significantly higher correlation with human judgment compared with existing metrics. Moreover, this strong correlation generalizes to unseen datasets, models and attack methods. We envision this work as a milestone for image quality evaluation closer to the human level. The project webpage can be accessed at https://sites.google.com/view/semsim."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "This paper assesses image quality metrics' faithfulness to human perception of privacy in reconstructed images and proposes SemSim, a learning-based measure with higher correlation to human judgment and generalization"}, "pdf": {"value": "/pdf/0f56ac2210c7006a4d5e8cb088d67aef16032ae8.pdf"}, "supplementary_material": {"value": "/attachment/8f8f659da2e6ec7e88e8614489dffc482dc35318.pdf"}, "_bibtex": {"value": "@inproceedings{\nsun2023privacy,\ntitle={Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?},\nauthor={Xiaoxiao Sun and Nidham Gazagnadou and Vivek Sharma and Lingjuan Lyu and Hongdong Li and Liang Zheng},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=cx9a4Xvb3l}\n}"}, "paperhash": {"value": "sun|privacy_assessment_on_reconstructed_images_are_existing_evaluation_metrics_faithful_to_human_perception"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission439/-/Revision", "NeurIPS.cc/2023/Conference/Submission439/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Ethics_Review_Flag", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission439/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325734065, "odate": 1698949693143, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "flagged_for_ethics_review"}, {"name": "ethics_comments", "input": "textarea", "markdown": true}, {"name": "_bibtex"}]}}, {"id": "3gamyee9Yh", "number": 422, "cdate": 1682172462516, "tcdate": 1682172462516, "mdate": 1698949693040, "tmdate": 1698949693040, "signatures": ["NeurIPS.cc/2023/Conference/Submission422/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission422/Authors"], "forum": "3gamyee9Yh", "content": {"title": {"value": "QuantSR: Accurate Low-bit Quantization for Efficient Image Super-Resolution"}, "authors": {"value": ["Haotong Qin", "Yulun Zhang", "Yifu Ding", "Yifan liu", "Xianglong Liu", "Martin Danelljan", "Fisher Yu"]}, "authorids": {"value": ["~Haotong_Qin1", "~Yulun_Zhang1", "~Yifu_Ding2", "~Yifan_liu3", "~Xianglong_Liu3", "~Martin_Danelljan4", "~Fisher_Yu2"]}, "keywords": {"value": ["Super Resolution", "Model Quantization", "Deep Learning"]}, "TLDR": {"value": "We propose a novel quantized image SR network, called QuantSR, which achieves accurate and efficient SR processing under low-bit quantization."}, "abstract": {"value": "Low-bit quantization in image super-resolution (SR) has attracted copious attention in recent research due to its ability to reduce parameters and operations significantly. However, many quantized SR models suffer from accuracy degradation compared to their full-precision counterparts, especially at ultra-low bit widths (2-4 bits), limiting their practical applications. To address this issue, we propose a novel quantized image SR network, called QuantSR, which achieves accurate and efficient SR processing under low-bit quantization. To overcome the representation homogeneity caused by quantization in the network, we introduce the Redistribution-driven Learnable Quantizer (RLQ). This is accomplished through an inference-agnostic efficient redistribution design, which adds additional information in both forward and backward passes to improve the representation ability of quantized networks. Furthermore, to achieve flexible inference and break the upper limit of accuracy, we propose the Depth-dynamic Quantized Architecture (DQA). Our DQA allows for the trade-off between efficiency and accuracy during inference through weight sharing. Our comprehensive experiments show that QuantSR outperforms existing state-of-the-art quantized SR networks in terms of accuracy while also providing more competitive computational efficiency. In addition, we demonstrate the scheme's satisfactory architecture generality by providing QuantSR-C and QuantSR-T for both convolution and Transformer versions, respectively. Our code and models are released at https://github.com/htqin/QuantSR ."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/1cbcbcfa8d7ff991f8774fae4c0455a2d08075bb.pdf"}, "supplementary_material": {"value": "/attachment/fb945c444662d777c4f66f5757710d1757fba64f.zip"}, "_bibtex": {"value": "@inproceedings{\nqin2023quantsr,\ntitle={Quant{SR}: Accurate Low-bit Quantization for Efficient Image Super-Resolution},\nauthor={Haotong Qin and Yulun Zhang and Yifu Ding and Yifan liu and Xianglong Liu and Martin Danelljan and Fisher Yu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=3gamyee9Yh}\n}"}, "paperhash": {"value": "qin|quantsr_accurate_lowbit_quantization_for_efficient_image_superresolution"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission422/-/Revision", "NeurIPS.cc/2023/Conference/Submission422/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission422/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325733530, "odate": 1698949693025, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "GRHZiTbDDI", "number": 379, "cdate": 1682090519347, "tcdate": 1682090519347, "mdate": 1698949692646, "tmdate": 1698949692646, "signatures": ["NeurIPS.cc/2023/Conference/Submission379/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission379/Authors"], "forum": "GRHZiTbDDI", "content": {"title": {"value": "4D Panoptic Scene Graph Generation"}, "authors": {"value": ["Jingkang Yang", "Jun CEN", "Wenxuan Peng", "Shuai Liu", "Fangzhou Hong", "Xiangtai Li", "Kaiyang Zhou", "Qifeng Chen", "Ziwei Liu"]}, "authorids": {"value": ["~Jingkang_Yang1", "~Jun_CEN1", "~Wenxuan_Peng2", "~Shuai_Liu14", "~Fangzhou_Hong1", "~Xiangtai_Li1", "~Kaiyang_Zhou1", "~Qifeng_Chen1", "~Ziwei_Liu1"]}, "keywords": {"value": ["Scene Graph Generation", "4D Understanding", "4D Perception."]}, "abstract": {"value": "We are living in a three-dimensional space while moving forward through a fourth dimension: time. To allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we introduce **4D Panoptic Scene Graph (PSG-4D)**, a new representation that bridges the raw visual data perceived in a dynamic 4D world and high-level visual understanding. Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and status information, and edges, which capture the temporal relations. To facilitate research in this new area, we build a richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of 1M frames, each of which is labeled with 4D panoptic segmentation masks as well as fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer, a Transformer-based model that can predict panoptic segmentation masks, track masks along the time axis, and generate the corresponding scene graphs via a relation component. Extensive experiments on the new dataset show that our method can serve as a strong baseline for future research on PSG-4D. In the end, we provide a real-world application example to demonstrate how we can achieve dynamic scene understanding by integrating a large language model into our PSG-4D system."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/3cb857a78be29b3492f26e2242871efa8ddaf567.pdf"}, "_bibtex": {"value": "@inproceedings{\nyang2023d,\ntitle={4D Panoptic Scene Graph Generation},\nauthor={Jingkang Yang and Jun CEN and Wenxuan Peng and Shuai Liu and Fangzhou Hong and Xiangtai Li and Kaiyang Zhou and Qifeng Chen and Ziwei Liu},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=GRHZiTbDDI}\n}"}, "TLDR": {"value": "We introduce a new task, dataset, and framework for 4D Panoptic Scene Graph Generation."}, "paperhash": {"value": "yang|4d_panoptic_scene_graph_generation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission379/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission379/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325732198, "odate": 1698949692628, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "gYetLsNO8x", "number": 375, "cdate": 1682089348562, "tcdate": 1682089348562, "mdate": 1698949692546, "tmdate": 1698949692546, "signatures": ["NeurIPS.cc/2023/Conference/Submission375/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission375/Authors"], "forum": "gYetLsNO8x", "content": {"title": {"value": "Best Arm Identification with Fixed Budget: A Large Deviation Perspective"}, "authors": {"value": ["Po-An Wang", "Ruo-Chun Tzeng", "Alexandre Proutiere"]}, "authorids": {"value": ["~Po-An_Wang1", "~Ruo-Chun_Tzeng1", "~Alexandre_Proutiere1"]}, "keywords": {"value": ["Best arm identification", "Large deviation"]}, "abstract": {"value": "We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a fixed sampling budget. Characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in MABs. When arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. In this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated SR (Successive Rejects) algorithm \\cite{audibert2010best}, and (ii) to devise and analyze new algorithms. In particular, we present CR (Continuous Rejects), a truly adaptive algorithm that can reject arms in {\\it any} round based on the observed empirical gaps between the rewards of various arms. Applying our Large Deviation results, we prove that CR enjoys better performance guarantees than existing algorithms, including SR. Extensive numerical experiments confirm this observation."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/65a0531b6e86f8331691df557f12e954bf13ee90.pdf"}, "supplementary_material": {"value": "/attachment/db5b108302b496ec16fb5916a2ce5a5201eb1c5a.pdf"}, "_bibtex": {"value": "@inproceedings{\nwang2023best,\ntitle={Best Arm Identification with Fixed Budget: A Large Deviation Perspective},\nauthor={Po-An Wang and Ruo-Chun Tzeng and Alexandre Proutiere},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=gYetLsNO8x}\n}"}, "paperhash": {"value": "wang|best_arm_identification_with_fixed_budget_a_large_deviation_perspective"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission375/-/Revision", "NeurIPS.cc/2023/Conference/Submission375/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission375/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325732097, "odate": 1698949692526, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "KgqucdSwIe", "number": 366, "cdate": 1682085488663, "tcdate": 1682085488663, "mdate": 1698949692389, "tmdate": 1698949692389, "signatures": ["NeurIPS.cc/2023/Conference/Submission366/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission366/Authors"], "forum": "KgqucdSwIe", "content": {"title": {"value": "VoxDet: Voxel Learning for Novel Instance Detection"}, "authors": {"value": ["Bowen Li", "Jiashun Wang", "Yaoyu Hu", "Chen Wang", "Sebastian Scherer"]}, "authorids": {"value": ["~Bowen_Li7", "~Jiashun_Wang1", "~Yaoyu_Hu1", "~Chen_Wang2", "~Sebastian_Scherer1"]}, "keywords": {"value": ["Unseen object detection", "instance perception", "voxel representation"]}, "TLDR": {"value": "This work proposes VoxDet, a novel instance detector based on geometry-invariant voxel learning."}, "abstract": {"value": "Detecting unseen instances based on multi-view templates is a challenging problem due to its open-world nature. Traditional methodologies, which primarily rely on $2 \\mathrm{D}$ representations and matching techniques, are often inadequate in handling pose variations and occlusions. To solve this, we introduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes the strong 3D voxel representation and reliable voxel matching mechanism. VoxDet first ingeniously proposes template voxel aggregation (TVA) module, effectively transforming multi-view 2D images into 3D voxel features. By leveraging associated camera poses, these features are aggregated into a compact 3D template voxel. In novel instance detection, this voxel representation demonstrates heightened resilience to occlusion and pose variations. We also discover that a $3 \\mathrm{D}$ reconstruction objective helps to pre-train the 2D-3D mapping in TVA. Second, to quickly align with the template voxel, VoxDet incorporates a Query Voxel Matching (QVM) module. The 2D queries are first converted into their voxel representation with the learned 2D-3D mapping. We find that since the 3D voxel representations encode the geometry, we can first estimate the relative rotation and then compare the aligned voxels, leading to improved accuracy and efficiency. In addition to method, we also introduce the first instance detection benchmark, RoboTools, where 20 unique instances are video-recorded with camera extrinsic. RoboTools also provides 24 challenging cluttered scenarios with more than $9 \\mathrm{k}$ box annotations. Exhaustive experiments are conducted on the demanding LineMod-Occlusion, YCB-video, and RoboTools benchmarks, where VoxDet outperforms various $2 \\mathrm{D}$ baselines remarkably with faster speed. To the best of our knowledge, VoxDet is the first to incorporate implicit 3D knowledge for 2D novel instance detection tasks."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/99461e15e7a826c499270e277a457ad66964d10e.pdf"}, "supplementary_material": {"value": "/attachment/75fa65594031b88f23df056580581553ebb72955.zip"}, "_bibtex": {"value": "@inproceedings{\nli2023voxdet,\ntitle={VoxDet: Voxel Learning for Novel Instance Detection},\nauthor={Bowen Li and Jiashun Wang and Yaoyu Hu and Chen Wang and Sebastian Scherer},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=KgqucdSwIe}\n}"}, "paperhash": {"value": "li|voxdet_voxel_learning_for_novel_instance_detection"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission366/-/Revision", "NeurIPS.cc/2023/Conference/Submission366/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission366/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325731867, "odate": 1698949692371, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "fKwG6grp8o", "number": 363, "cdate": 1682084501014, "tcdate": 1682084501014, "mdate": 1698949692331, "tmdate": 1698949692331, "signatures": ["NeurIPS.cc/2023/Conference/Submission363/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission363/Authors"], "forum": "fKwG6grp8o", "content": {"title": {"value": "Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks"}, "authors": {"value": ["Blake Bordelon", "Cengiz Pehlevan"]}, "authorids": {"value": ["~Blake_Bordelon1", "~Cengiz_Pehlevan2"]}, "keywords": {"value": ["Deep Learning Theory", "Feature Learning", "Dynamics", "Ensembles"]}, "TLDR": {"value": "A theoretical analysis of finite width effects in strongly feature learning neural networks from dynamical mean field theory."}, "abstract": {"value": "We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Starting from a dynamical mean field theory  description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\\mathcal{O}(1/\\sqrt{\\text{width}})$ fluctuations of the DMFT order parameters over random initializations of the network weights. Our results, while perturbative in width, unlike prior analyses, are non-perturbative in the strength of feature learning. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with a variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final tangent kernel and final network predictions. We also show how initialization variance can slow down online learning in wide but finite networks. In deeper networks, kernel variance can dramatically accumulate through subsequent layers at large feature learning strengths, but feature learning continues to improve the signal-to-noise ratio of the feature kernels. In discrete time, we demonstrate that large learning rate phenomena such as edge of stability effects can be well captured by infinite width dynamics and that initialization variance can decrease dynamically. For CNNs trained on CIFAR-10, we empirically find significant corrections to both the bias and variance of network dynamics due to finite width."}, "pdf": {"value": "/pdf/8a796ac2309a3bd8235b66a60cbb895be860e55d.pdf"}, "supplementary_material": {"value": "/attachment/2a0b91f711c8bfe7056571f3f5dfc7087d58e34e.zip"}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "_bibtex": {"value": "@inproceedings{\nbordelon2023dynamics,\ntitle={Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks},\nauthor={Blake Bordelon and Cengiz Pehlevan},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=fKwG6grp8o}\n}"}, "paperhash": {"value": "bordelon|dynamics_of_finite_width_kernel_and_prediction_fluctuations_in_mean_field_neural_networks"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission363/-/Revision", "NeurIPS.cc/2023/Conference/Submission363/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission363/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325731755, "odate": 1698949692315, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "WaLI8slhLw", "number": 203, "cdate": 1681991055112, "tcdate": 1681991055112, "mdate": 1698949691258, "tmdate": 1698949691258, "signatures": ["NeurIPS.cc/2023/Conference/Submission203/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission203/Authors"], "forum": "WaLI8slhLw", "content": {"title": {"value": "DeWave: Discrete Encoding of EEG Waves for EEG to Text Translation"}, "authors": {"value": ["Yiqun Duan", "Charles Zhou", "Zhen Wang", "Yu-Kai Wang", "Chin-teng Lin"]}, "authorids": {"value": ["~Yiqun_Duan1", "~Charles_Zhou1", "~Zhen_Wang9", "~Yu-Kai_Wang1", "~Chin-teng_Lin1"]}, "keywords": {"value": ["EEG; Neural Encoding; Brain Computer Interface"]}, "abstract": {"value": "The translation of brain dynamics into natural language is pivotal for brain-computer interfaces (BCIs), a field that has seen substantial growth in recent years. With the swift advancement of large language models, such as ChatGPT, the need to bridge the gap between the brain and languages becomes increasingly pressing. Current methods, however, require eye-tracking fixations or event markers to segment brain dynamics into word-level features, which can restrict the practical application of these systems. These event markers may not be readily available or could be challenging to acquire during real-time inference, and the sequence of eye fixations may not align with the order of spoken words. To tackle these issues, we introduce a novel framework, DeWave, that integrates discrete encoding sequences into open-vocabulary EEG-to-text translation tasks. DeWave uses a quantized variational encoder to derive discrete codex encoding and align it with pre-trained language models. This discrete codex representation brings forth two advantages: 1) it alleviates the order mismatch between eye fixations and spoken words by introducing text-EEG contrastive alignment training, and 2) it minimizes the interference caused by individual differences in EEG waves through an invariant discrete codex. Our model surpasses the previous baseline (40.1 and 31.7) by 3.06% and 6.34\\%, respectively, achieving 41.35 BLEU-1 and 33.71 Rouge-F on the ZuCo Dataset. Furthermore, this work is the first to facilitate the translation of entire EEG signal periods without the need for word-level order markers (e.g., eye fixations), scoring 20.5 BLEU-1 and 29.5 Rouge-1 on the ZuCo Dataset, respectively."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/8a61dec9005aca4dd18c1de736990ec40bae0742.pdf"}, "supplementary_material": {"value": "/attachment/8e2ad08cc13fe75c0d7a62005c99f8de71cb2db7.pdf"}, "_bibtex": {"value": "@inproceedings{\nduan2023dewave,\ntitle={DeWave: Discrete Encoding of {EEG} Waves for {EEG} to Text Translation},\nauthor={Yiqun Duan and Charles Zhou and Zhen Wang and Yu-Kai Wang and Chin-teng Lin},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=WaLI8slhLw}\n}"}, "paperhash": {"value": "duan|dewave_discrete_encoding_of_eeg_waves_for_eeg_to_text_translation"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission203/-/Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission203/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325728333, "odate": 1698949691244, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "Z764QxwETf", "number": 120, "cdate": 1681959855783, "tcdate": 1681959855783, "mdate": 1698949690721, "tmdate": 1698949690721, "signatures": ["NeurIPS.cc/2023/Conference/Submission120/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission120/Authors"], "forum": "Z764QxwETf", "content": {"title": {"value": "Puzzlefusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving"}, "authors": {"value": ["Sepidehsadat Hosseini", "Mohammad Amin Shabani", "Saghar Irandoust", "Yasutaka Furukawa"]}, "authorids": {"value": ["~Sepidehsadat_Hosseini2", "~Mohammad_Amin_Shabani1", "~Saghar_Irandoust1", "~Yasutaka_Furukawa1"]}, "keywords": {"value": ["Diffusion", "Jigsaw", "puzzle solving"]}, "abstract": {"value": "This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.\nIn the latter task, for instance, the proposed system ``PuzzleFusion'' takes a set of room layouts as polygonal curves in the top-down view and aligns the room layout pieces by estimating their 2D translations and rotations, akin to solving the jigsaw puzzle of room layouts. A surprising discovery of the paper is that the simple use of a Diffusion Model effectively solves these challenging spatial puzzle tasks as a conditional generation process. \nTo enable learning of an end-to-end neural system, the paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi Jigsaw Dataset, a synthetic one where pieces are generated by voronoi diagram of 2D pointset; and 2) MagicPlan Dataset, a real one from a production pipeline by MagicPlan, where pieces are room layouts constructed by augmented reality App by real-estate consumers.\nThe qualitative and quantitative evaluations demonstrate that the proposed approach outperforms the competing methods by significant margins in all three spatial puzzle tasks. We have provided code and data in https://sepidsh.github.io/puzzlefusion."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "TLDR": {"value": "This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks."}, "pdf": {"value": "/pdf/2c871c960c5841b8911dbcd7124e940dbed64d2f.pdf"}, "supplementary_material": {"value": "/attachment/5a97d5b33d215e61257ad94997c07a09218c90c5.zip"}, "_bibtex": {"value": "@inproceedings{\nhosseini2023puzzlefusion,\ntitle={Puzzlefusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving},\nauthor={Sepidehsadat Hosseini and Mohammad Amin Shabani and Saghar Irandoust and Yasutaka Furukawa},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=Z764QxwETf}\n}"}, "paperhash": {"value": "hosseini|puzzlefusion_unleashing_the_power_of_diffusion_models_for_spatial_puzzle_solving"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission120/-/Revision", "NeurIPS.cc/2023/Conference/Submission120/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission120/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325726869, "odate": 1698949690705, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}, {"id": "MfiK69Ga6p", "number": 104, "cdate": 1681958782391, "tcdate": 1681958782391, "mdate": 1698949690547, "tmdate": 1698949690547, "signatures": ["NeurIPS.cc/2023/Conference/Submission104/Authors"], "readers": ["everyone"], "writers": ["NeurIPS.cc/2023/Conference", "NeurIPS.cc/2023/Conference/Submission104/Authors"], "forum": "MfiK69Ga6p", "content": {"title": {"value": "Protein Design with Guided Discrete Diffusion"}, "authors": {"value": ["Nate Gruver", "Samuel Don Stanton", "Nathan C. Frey", "Tim G. J. Rudner", "Isidro Hotzel", "Julien Lafrance-Vanasse", "Arvind Rajpal", "Kyunghyun Cho", "Andrew Gordon Wilson"]}, "authorids": {"value": ["~Nate_Gruver1", "~Samuel_Don_Stanton1", "~Nathan_C._Frey1", "~Tim_G._J._Rudner2", "~Isidro_Hotzel1", "~Julien_Lafrance-Vanasse1", "~Arvind_Rajpal1", "~Kyunghyun_Cho1", "~Andrew_Gordon_Wilson1"]}, "keywords": {"value": ["protein design", "diffusion model", "classifier guidance"]}, "abstract": {"value": "A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and  stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to several therapeutic targets under locality and developability constraints, attaining a 99\\% expression rate and 40\\% binding rate in exploratory in vitro experiments."}, "venue": {"value": "NeurIPS 2023 spotlight"}, "venueid": {"value": "NeurIPS.cc/2023/Conference"}, "pdf": {"value": "/pdf/55584373e26756ae36561e56362dbb11f05bec4a.pdf"}, "supplementary_material": {"value": "/attachment/457dcc4b441b662916ea1f43ca2f5372d408cbfc.pdf"}, "_bibtex": {"value": "@inproceedings{\ngruver2023protein,\ntitle={Protein Design with Guided Discrete Diffusion},\nauthor={Nate Gruver and Samuel Don Stanton and Nathan C. Frey and Tim G. J. Rudner and Isidro Hotzel and Julien Lafrance-Vanasse and Arvind Rajpal and Kyunghyun Cho and Andrew Gordon Wilson},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=MfiK69Ga6p}\n}"}, "paperhash": {"value": "gruver|protein_design_with_guided_discrete_diffusion"}}, "invitations": ["NeurIPS.cc/2023/Conference/-/Submission", "NeurIPS.cc/2023/Conference/-/Post_Submission", "NeurIPS.cc/2023/Conference/Submission104/-/Revision", "NeurIPS.cc/2023/Conference/Submission104/-/Supplementary_Material_Revision", "NeurIPS.cc/2023/Conference/-/Edit", "NeurIPS.cc/2023/Conference/Submission104/-/Camera_Ready_Revision"], "domain": "NeurIPS.cc/2023/Conference", "pdate": 1695325726528, "odate": 1698949690534, "version": 2, "details": {"replyCount": 0, "presentation": [{"name": "title", "order": 1}, {"name": "authors", "order": 2}, {"name": "authorids", "order": 3}, {"name": "keywords", "order": 4}, {"name": "TLDR", "order": 5, "fieldName": "TL;DR"}, {"name": "abstract", "order": 6, "input": "textarea", "markdown": true}, {"name": "pdf", "order": 7}, {"name": "supplementary_material", "order": 10}, {"name": "corresponding_author", "order": 11}, {"name": "financial_support", "order": 12}, {"name": "reviewer_nomination", "order": 13}, {"name": "primary_area", "order": 14, "input": "select"}, {"name": "claims", "order": 15}, {"name": "code_of_ethics", "order": 16}, {"name": "broader_impacts", "order": 17}, {"name": "limitations", "order": 18}, {"name": "theory", "order": 19}, {"name": "experiments", "order": 20}, {"name": "training_details", "order": 21}, {"name": "error_bars", "order": 22}, {"name": "compute", "order": 23}, {"name": "reproducibility", "order": 24}, {"name": "safeguards", "order": 25}, {"name": "licenses", "order": 26}, {"name": "assets", "order": 27}, {"name": "human_subjects", "order": 28}, {"name": "IRB_approvals", "order": 29}, {"name": "venue", "hidden": true}, {"name": "venueid", "hidden": true}, {"name": "_bibtex"}]}}], "count": 378}